Id,PostId,Score,Text,CreationDate,UserId,ContentLicense,UserDisplayName
1,3,7,"Could be a poster child for argumentative and subjective.  At the least, need to define 'valuable'.",2010-07-19T19:15:52.517,13,CC BY-SA 2.5,
2,5,0,"Yes, R is nice- but WHY is it 'valuable'.",2010-07-19T19:16:14.980,13,CC BY-SA 2.5,
3,9,1,"Again- why?  How would I convince my boss to use this over, say, Excel.",2010-07-19T19:18:54.617,13,CC BY-SA 2.5,
4,5,11,"It's mature, well supported, and a standard within certain scientific communities (popular in our AI department, for example)",2010-07-19T19:19:56.657,37,CC BY-SA 2.5,
6,14,10,"why ask the question here?  All are community-wiki, why not just fix the canonical answer?",2010-07-19T19:22:27.947,23,CC BY-SA 2.5,
7,18,1,also the US census data http://www.census.gov/main/www/access.html,2010-07-19T19:25:47.877,36,CC BY-SA 2.5,
9,16,1,Andrew Gelman has a nice R library that links Bugs to R.,2010-07-19T19:30:24.590,78,CC BY-SA 2.5,
10,23,8,I am not sure I understand the difficulty. If the functional form is known just take the derivative otherwise take differences. Am I missing something here?,2010-07-19T19:31:18.657,,CC BY-SA 2.5,user28
11,43,5,"There are many R GUI's available for Windows, so I'm not following your point.",2010-07-19T19:34:20.383,5,CC BY-SA 2.5,
12,38,0,"That's just an example - it might have a median that is much smaller, on the order of 200 (it depends on how I partition the data). That would preclude using a normal distribution, right?",2010-07-19T19:37:33.823,54,CC BY-SA 2.5,
13,20,2,What levels of kurtosis and skewdness are acceptable to meet the assumption of normality?,2010-07-19T19:38:01.450,24,CC BY-SA 2.5,
14,46,6,this is an incredibly unclear response. Please try to write in English.,2010-07-19T19:38:30.340,74,CC BY-SA 2.5,
15,3,3,"Maybe the focus shouldn't be on ""valuable"" but rather ""pros"" and ""cons"" of each project?",2010-07-19T19:44:47.753,24,CC BY-SA 2.5,
18,36,19,http://xkcd.com/552/,2010-07-19T19:48:32.580,68,CC BY-SA 2.5,
20,54,4,"I am not sure if characterizing one or the other as the 'wrong' formula is the way to understand the issue. It is just that the second one is 'better' in in the sense that it is an unbiased estimator of the true standard deviation. So, if you care about unbiased estimates then the second one is 'better'/'correct'.",2010-07-19T19:51:06.567,,CC BY-SA 2.5,user28
21,77,1,I like the first example you give. That will certainly get the students talking ;),2010-07-19T19:56:18.490,8,CC BY-SA 2.5,
22,56,17,I like the analogy. I would find it very useful if there were a defined question (based on a dataset) in which an answer was derived using frequentist reasoning and an answer was derived using Bayesian - preferably with R script to handle both reasonings. Am I asking too much?,2010-07-19T19:56:21.640,104,CC BY-SA 2.5,
24,73,1,"Very subjective question: this question cannot be answered, and is not suitable for a QA site.",2010-07-19T19:58:20.090,107,CC BY-SA 2.5,
27,77,1,There's an interesting discussion by Steve Steinberg on his blog here: http://blog.steinberg.org/?p=11 about some of the implications of 1 and where it might lead in terms of Weak AI.,2010-07-19T20:01:15.373,55,CC BY-SA 2.5,
29,79,1,"I wasn't suggesting it was, I was just curious as to why such a difference might have arisen, what sort of level of error following the wrong advice might give and whether there was a decent explanation of the difference I could give to my students.",2010-07-19T20:03:30.203,55,CC BY-SA 2.5,
31,54,0,"I was characterising the formula as ""wrong"" purely in the sense that in an exam if you use the formula which isn't proscribed by the syllabus you'll end up with the ""wrong"" answer. Plus if the values are not a sample of population per se then surely the first formula gives the more accurate value.",2010-07-19T20:05:22.803,55,CC BY-SA 2.5,
32,73,3,Should probably be community wiki; useful question here but doesn't have definitive answer.,2010-07-19T20:05:57.903,5,CC BY-SA 2.5,
33,73,2,"@Shane: good point. moved.
@ Egon: subjective indeed. but if the answers come from knowledgeable people i don't mind dose of subjectivity.
i've started learning R quite recently and have couple of dozens installed to explore, however i notice that there are tools that I use much more often irrespectively of the task at hand.",2010-07-19T20:06:56.373,22,CC BY-SA 2.5,
34,56,17,"The simplest thing that I can think of that tossing a coin n times and estimating the probability of a heads (denote by p). Suppose, we observe k heads. Then the probability of getting k heads is:

P (k heads in n trials) = (n, k) p^k (1-p)^(n-k)

Frequentist inference would maximize the above to arrive at an estimate of p = k / n.

Bayesian would say: Hey, I know that p ~ Beta(1,1) (which is equivalent to assuming that p is uniform on [0,1]). So, the updated inference would be:

p ~ Beta(1+k,1+n-k) and thus the bayesian estimate of p would be 

p = 1+k / (2+n)

I do not know R, sorry.",2010-07-19T20:11:13.040,,CC BY-SA 2.5,user28
35,43,0,"I wasn't aware of rapidminer. That looks nice, thanks!",2010-07-19T20:11:28.967,33,CC BY-SA 2.5,
36,3,1,"Or maybe even ""How X will help you get Y done faster/cheaper and kill the germs that cause bad breath.""",2010-07-19T20:15:51.763,13,CC BY-SA 2.5,
38,73,0,It would be interesting if StackExchange could support some method of linking community wiki posts across sites.  Because I will bet this question has been asked on Stackoverflow and I also think that Statistical Analysis may attract some people that wouldn't usually visit SO.,2010-07-19T20:19:28.543,13,CC BY-SA 2.5,
39,75,2,This is really a Stackoverflow question as it has to do with learning the R programming language.  With the current wording the question is only associated with statistics by virtue of R's focus on statistical analysis.,2010-07-19T20:23:01.700,13,CC BY-SA 2.5,
41,16,4,"I'd rephrase that ""the most popular statistical tool in bioinformatics""... Bioinformaticians doing microarray analysis use it extensively, yes. But bioinformatics is not limited to that ;)",2010-07-19T20:25:52.130,120,CC BY-SA 2.5,
42,65,4,"Colin, an unbiased estimator of the standard deviation does not have a closed form representation in the general case. What does exist is the unbiased estimator of the <i>variance</i> (s<sup>2</sup> in this case).
Noteworthy that both are consistent estimators of the population variance - and so by the continuous mapping theorem, are the two estimators of the standard deviations.
A related point is that s<sub>n</sub><sup>2</sup> has a lower MSE than s<sup>2</sup>. The additional advantage from imposing unbiasedness is arguable.",2010-07-19T20:30:23.307,47,CC BY-SA 2.5,
44,103,7,"i suggest everyone put their favourite image from the blog, so it's not just a collection of links...",2010-07-19T20:37:20.480,74,CC BY-SA 2.5,
45,73,0,"@Sharpie: there have been several interesting SO posts like http://stackoverflow.com/questions/1295955/what-is-the-most-useful-r-trick or http://stackoverflow.com/questions/1535021/whats-the-biggest-r-gotcha-youve-run-across however they are not focused on packages. and i agree, linkage of community wiki could be really useful.",2010-07-19T20:37:27.563,22,CC BY-SA 2.5,
46,44,7,This is an incredibly general question.,2010-07-19T20:38:14.037,46,CC BY-SA 2.5,
51,65,0,@Tirthankar - very sloppy of me. I've altered the answer slightly. Thanks.,2010-07-19T20:45:49.057,8,CC BY-SA 2.5,
52,38,1,"The normal approximation to the Poisson distribution is pretty robust, the difference between the CDFs is bounded by something like 0.75/sqrt(lambda), if I recall correctly.  I wouldn't be too worried about using lambda=200, but if you're more risk-averse then definitely go with the negative binomial.",2010-07-19T20:46:12.283,61,CC BY-SA 2.5,
54,80,0,"By ""January 1st"", I presume you mean the cut-off is an entire year and not 6 months period that applies to your children.",2010-07-19T20:48:04.963,58,CC BY-SA 2.5,
56,101,4,"Take your time! I won't be thinking about selecting a ""Best Answer"" for a week or so.",2010-07-19T20:54:37.757,13,CC BY-SA 2.5,
57,75,1,"I wouldn't vote to close it just yet- there could be a good question in there.  Perhaps something like ""Where can I find useful tutorials that focus on putting statistical concepts into practice using a tool such as R?"" or ""Where can I find useful tutorials that teach statistics by example using tools such as R?""",2010-07-19T20:58:26.750,13,CC BY-SA 2.5,
59,116,0,It doesn't have an RSS feed though :(,2010-07-19T21:11:30.023,8,CC BY-SA 2.5,
60,110,0,"Thanks for the great answer and for book advice. Also, do you know about relation of window function to generalized functions? It seems (from wikipedia article) they are suitable as domains for functionals.",2010-07-19T21:18:07.720,117,CC BY-SA 2.5,
62,116,0,http://cscs.umich.edu/~crshalizi/weblog/index.rss,2010-07-19T21:20:53.977,61,CC BY-SA 2.5,
64,118,39,"In a way, the measurement you proposed is widely used in case of error (model quality) analysis -- then it is called MAE, ""mean absolute error"".",2010-07-19T21:30:23.000,,CC BY-SA 2.5,user88
65,130,11,"If you would taste R, it is highly probable that you will resign from MATLAB (as in my case).",2010-07-19T21:33:33.003,,CC BY-SA 2.5,user88
67,130,0,"IMO, this should be community wiki (language ""versus"" type questions are pretty subjective).",2010-07-19T21:34:28.043,5,CC BY-SA 2.5,
68,130,0,This is definitely a question concerning programming languages and should be asked on Stack Overflow.,2010-07-19T21:35:25.750,13,CC BY-SA 2.5,
69,130,0,"I agree with Sharpie.  @Vivi: you should change the question title to be ""advantages and disadvantages for data munging"" or something along that line so that it's more on-topic.",2010-07-19T21:37:16.187,5,CC BY-SA 2.5,
70,134,1,I think this is the first candidate to be moved to Stack Overflow.,2010-07-19T21:38:27.890,,CC BY-SA 2.5,user88
71,120,25,Nice analogy of euclidean space!,2010-07-19T21:38:48.373,83,CC BY-SA 2.5,
74,116,1,"@Colin: The author of the blog would need to put `<link rel=""alternate"" type=""application/rss+xml"" title=""RSS"" href=""http://cscs.umich.edu/~crshalizi/weblog/index.rss"" />` in his the `<head>` section of the HTML document for Firefox to pick it up automatically :)",2010-07-19T21:42:04.757,66,CC BY-SA 2.5,
77,130,5,"@Sharpie, @Shane IMO to this extent it is a question about tools, so it is acceptable.",2010-07-19T21:54:08.290,,CC BY-SA 2.5,user88
80,130,0,"@mbq, we definitely need a set of community guidelines on these sorts of questions, help us decide on meta: http://meta.stats.stackexchange.com/questions/1/how-to-answer-r-questions",2010-07-19T21:59:52.943,13,CC BY-SA 2.5,
81,130,0,"@Sharpie It is not only in contex of R -- to this end I've made a separate meta discussion for that, http://meta.stats.stackexchange.com/questions/35/how-much-programming-here",2010-07-19T22:18:16.563,,CC BY-SA 2.5,user88
82,20,5,"Most statistical methods assume normality, not of the data, but rather of an assumed random variable, e.g. the error term in a linear regression. Checking involves looking at the residuals, not the original data!",2010-07-19T22:24:14.713,,CC BY-SA 2.5,Statprof
85,75,1,http://meta.stats.stackexchange.com/questions/35/how-much-programming-here,2010-07-19T22:39:45.467,,CC BY-SA 2.5,user88
86,110,0,"The full technical explanation is complicated.  The shorter, less technical explanation is that, since they are typically smooth, and either of compact support or they decay exponentially fast, many (but definitely not all) windowing functions make good 'test functions' to examine operator behavior with.  A good starting point might be the chapter on distributions in http://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf",2010-07-19T22:42:33.997,61,CC BY-SA 2.5,
87,130,0,@Sharpie : check this out: http://meta.stats.stackexchange.com/questions/1/how-to-answer-r-questions,2010-07-19T22:52:59.610,90,CC BY-SA 2.5,
88,157,1,+1 that problem twisted my brain when I first read and thought about it- and the solution is pretty simple but teaches a lot about probability.,2010-07-19T23:01:30.547,13,CC BY-SA 2.5,
89,86,2,"It's probably best not to use the term ""normal variable"" here when you do not mean a normally distributed random variable.",2010-07-19T23:28:39.953,159,CC BY-SA 2.5,
91,137,0,"This is a great answer! I'll look into your book suggestions, and the description of your process is also great. I especially like the suggestions for feature vectorizations.",2010-07-19T23:34:20.443,131,CC BY-SA 2.5,
92,137,0,"(If anyone would like to elaborate even further on the vectorization part, that'd be great.)",2010-07-19T23:35:23.343,131,CC BY-SA 2.5,
94,86,0,"Agreed.  Although I personally would look at someone funny for a few seconds if they said ""normal variable"" and didn't throw the word ""random"" or ""distributed"" in there somewhere to cue me that that is what they were discussing.  But I am also an engineer and not a statistician so I don't use that much domain-specific notation.",2010-07-19T23:56:45.997,13,CC BY-SA 2.5,
95,145,3,"Actually, I think the previous question is more a subset of this one. EAMann was asking for a datasets with some particular characteristics (although I'm not sure anybody is paying attention to those criteria); this question is wide-open.  For example, I feel comfortable voting up many of these answers, because they are, in fact, datasets, but not any of the answers on the previous question, because I haven't opened them up to see if they suit EAMann's requests.",2010-07-20T00:10:26.297,71,CC BY-SA 2.5,
96,101,1,Now that I've had a chance to come back and read the whole answer- a big +1 for the student height example.  Very clear and well laid out.,2010-07-20T00:12:40.647,13,CC BY-SA 2.5,
97,43,5,"""Windows-based"" may mean that it works on the Windows operating system (which R does) rather than meaning that it is a heavily GUI-oriented tool. Note the capital letter!",2010-07-20T01:01:12.440,173,CC BY-SA 2.5,
98,5,10,It's extensible and there's no statistical technique that can't be done in it.,2010-07-20T01:22:43.947,1356,CC BY-SA 2.5,
99,134,0,"Possibly, but it'd need a lot more explanation on SO.",2010-07-20T01:29:42.683,174,CC BY-SA 2.5,
100,86,7,"Random variables may be classified as *discreet* if they don't draw attention to themselves. If they're merely countable we say *discrete* :-P Also, you mean prescribe rather than proscribe, but I think *describe* might be more appropriate. Nice answer, anyway -- hopefully +1 will help mitigate the nitpicking!",2010-07-20T01:43:31.557,174,CC BY-SA 2.5,
101,46,2,"maybe so. is a person asking this question a person who walked in off the street, or a person who has at least opened a statistics book. 

Telling someone the standard deviation is just the square root of the variance is completely begging the question.",2010-07-20T01:59:53.420,62,CC BY-SA 2.5,
104,188,8,"That's a good explanation, but not for a nontechnical layperson. I suspect the OP wanted to know how to explain it to, say, the MBA who hired you to do some statistical analysis! How would you describe MCMC to someone who, at best, sorta understands the concept of a standard deviation (variance, though, may be too abstract)?",2010-07-20T02:18:02.333,6,CC BY-SA 2.5,
106,188,12,"@Harlan: It's a hard line to straddle; if someone doesn't at least know what a random variable is, why we might want to estimate probabilities, and have some hazy idea of a density function, then I don't think it *is* possible to meaningfully explain the how or why of MCMC to them, only the ""what"", which in this case would boil down to ""it's a way of numerically solving an otherwise impossible problem by simulation, like flipping a coin a lot to estimate the probability that it lands on heads"".",2010-07-20T02:39:24.543,61,CC BY-SA 2.5,
108,25,0,"do you mean a GUI graphical tool that runs on Windows, or a command line based one that runs on Windows (or either)",2010-07-20T04:09:28.610,74,CC BY-SA 2.5,
109,200,0,"coming up with a persuasive theoretical framework that explains the patterns is very hard, especially when there are thousands of variables involved. Real world is a complicated business.",2010-07-20T04:33:45.423,175,CC BY-SA 2.5,
110,101,3,"Nice work ... but we need to add 

(C) our model (embodied in the formula/statistical routine) is wrong.",2010-07-20T05:07:08.683,187,CC BY-SA 2.5,
112,54,14,"Srikant, I don't think that the second one is an unbiased estimator.  The square of it *is* an unbiased estimator of the true variance.  However, Jensen's Inequality establishes that the expectation of a curvilinear function of a random variable is not the same as the function of the expectation of the random variable.  Hence the second formula can't be an unbiased estimator of the true standard deviation.",2010-07-20T05:28:20.490,187,CC BY-SA 2.5,
113,200,2,@Ngu Nobody promised it would be easy. Expecting meaningful patterns just to magically emerge from a sea of data is what gives data miners a bad name.,2010-07-20T05:29:24.030,174,CC BY-SA 2.5,
115,167,0,Wouldn't that be FA then?,2010-07-20T05:52:50.417,144,CC BY-SA 2.5,
116,222,0,el chef has a condensed answer over here -> http://stats.stackexchange.com/questions/146/pca-scores-in-multiple-regression. HTH,2010-07-20T05:53:59.137,144,CC BY-SA 2.5,
117,224,0,"What do you mean, standalone application?",2010-07-20T06:05:54.420,5,CC BY-SA 2.5,
119,167,3,No. FA is not regression. I am referring to a response variable regressed against the principal components computed from a large number of explanatory variables. The principal components themselves are closely related to the factors of FA.,2010-07-20T06:14:32.780,159,CC BY-SA 2.5,
120,223,1,What's the significance of the fact that your friend is an MD?,2010-07-20T06:25:48.217,5,CC BY-SA 2.5,
122,224,0,By standalone application I mean an executable program.,2010-07-20T06:32:54.647,128,CC BY-SA 2.5,
123,223,1,"I think the significance of the MD is the person in question is a busy clinician, and not a researcher. Therefore we are being guided towards your less weighty tomes as source recommendations. Well, that's from the MD's I come across anyway.",2010-07-20T06:42:40.103,199,CC BY-SA 2.5,
124,25,0,I mean anything (GUI or command line based) that can run on Windows operating systems.,2010-07-20T06:57:11.337,69,CC BY-SA 2.5,
126,167,0,"I'm sorry, I should have been more precise in my comment. Your writing that the explanatory variables can be reduced to a small number of PC rang me ""factor analysis"" bell.",2010-07-20T07:32:47.977,144,CC BY-SA 2.5,
127,223,0,"Exactly! The fact that he is an MD poses a few restriction on (a) the volume of the introductory material (b) on what to assume about the ""starting point"" and (c) on the time willing to spend to reintroduce himself with basic stuff.",2010-07-20T07:48:49.047,79,CC BY-SA 2.5,
128,118,10,"In accepting an answer it seems important to me that we pay attention to whether the answer is circular.  The normal distribution is based on these measurements of variance from squared error terms, but that isn't in and of itself a justification for using (X-M)^2 over |X-M|.",2010-07-20T07:59:54.683,196,CC BY-SA 2.5,
129,10,6,Technically Likert scales are the sum of Likert-type items and as such end up being a reasonable approximation (at least according to many psychometricians in Psychology) of an interval data point.,2010-07-20T08:03:30.507,196,CC BY-SA 2.5,
130,240,1,"Univariate boxplots are useful for spotting univariate outliers. But they can completely miss multivariate outliers.

The regression idea is ok if I had a Y and a bunch of X variables. But as I said in the question, there is no Y so regression is inappropriate.",2010-07-20T08:12:10.470,159,CC BY-SA 2.5,
132,241,0,"Yes, I could compute the Mahalanobis distance of each observation from the mean of the data. But the observations with the greatest distance from the mean are not necessarily multivariate outliers. Think of a bivariate scatterplot with high correlation. An outlier can be outside the main cloud of points but not that far from the mean.",2010-07-20T08:14:21.200,159,CC BY-SA 2.5,
136,229,0,"Is there a way to express in plain language what the difference is between a given percentile and the maximum of N values?  From a lay perspective, it is hard to see why a datapoint that comes from a given (Y) percentile wouldn't be expected to be (on average) the same as the top scorer from a group of 100/Y.  

For example, if I found that your answers were ranked in the 90th percentile, I'd expect that your answer would usually be the top answer among any randomly selected group of 10 answers.",2010-07-20T08:45:52.963,196,CC BY-SA 2.5,
137,242,0,"could you edit the title to something like ""Using time series analysis to analyze/predict violent behavior""?",2010-07-20T08:56:46.340,87,CC BY-SA 2.5,
138,134,2,"Most programmers know ""median"".  (sort(array))[length/2] is a big enough hint for those who forgot.  Also at its most basic for each new point you only need to do a bisection/insert on one half of the array...",2010-07-20T09:04:20.020,87,CC BY-SA 2.5,
139,250,1,R is also lazy evaluated.,2010-07-20T09:04:48.753,,CC BY-SA 2.5,user88
140,189,2,"Thats the same as taking the derivative, but just more inaccurate so why would you do it?",2010-07-20T09:39:03.803,214,CC BY-SA 2.5,
143,261,5,"R produces some of the best quality graphics around. As an editor of an international research journal, I would love all our authors to use R.",2010-07-20T10:02:05.183,159,CC BY-SA 2.5,
145,263,0,Could you put a code snippet for completness? It would be very useful for people in the future finding this page.,2010-07-20T10:19:14.293,217,CC BY-SA 2.5,
146,262,0,How about saving to file?,2010-07-20T10:19:47.867,217,CC BY-SA 2.5,
149,257,3,"How do you define 'publication-quality'? Please elaborate on what aspects you like to see covered... e.g. color use, line widths, etc. Should answers focus on font size, instead?",2010-07-20T10:51:44.160,107,CC BY-SA 2.5,
150,263,0,"@≈Åukasz Hmm, some suggestion how to upload an svg figure?",2010-07-20T10:52:57.933,190,CC BY-SA 2.5,
151,261,1,".. see my comment on the question... how do you define 'publication-quality', or 'best quality'... from a editor perspective?",2010-07-20T10:53:25.593,107,CC BY-SA 2.5,
152,261,19,"I like to see vector graphics (no jpegs), graphical design following the principles of Tufte & Cleveland, readable fonts, uncluttered legends, no shaded backgrounds, sensible axis limits and tick intervals, labelled axes, no overlap of text and plotting characters or lines, etc. Most authors use the default settings of their software, so good software has good defaults. This is where Excel fails miserably and R does pretty well. But it is possible to produce lousy graphs in R and good graphs in Excel. It's just easier to produce high quality graphics in R.",2010-07-20T11:07:42.903,159,CC BY-SA 2.5,
154,267,0,"You should use Colin's answer, still your idea of making Monte Carlo simulation is also correct.",2010-07-20T11:14:06.997,,CC BY-SA 2.5,user88
157,274,0,"Good answer. I think you should go further into what you mean by ""do your inference based on that"". That's kind of the second part of my question.",2010-07-20T11:36:16.100,62,CC BY-SA 2.5,
158,274,0,"mmm... I didn't really understand what you meant by what common variables and statistics... Oh, do you mean like you use z distribution if you have the population variance and the t-distribution if you only have the sample variance and the sample size is small? Something along those lines?",2010-07-20T12:03:37.080,90,CC BY-SA 2.5,
159,278,1,"Your random reordering reminded me of this AI koan: In the days when Sussman was a novice Minsky once came to him as he sat hacking at the PDP-6. ""What are you doing?"", asked Minsky. ""I am training a randomly wired neural net to play Tic-Tac-Toe."" ""Why is the net wired randomly?"", asked Minsky. ""I do not want it to have any preconceptions of how to play."" Minsky shut his eyes. ""Why do you close your eyes?"", Sussman asked his teacher. ""So the room will be empty."" At that moment, Sussman was enlightened.",2010-07-20T12:40:15.697,56,CC BY-SA 2.5,
160,263,4,You could have mentioned in your answer that matplotlib allows rendering of all typography in the plot with LaTeX so it perfectly integrates visually.,2010-07-20T12:49:37.267,56,CC BY-SA 2.5,
162,213,3,"If a scatterplot matrix won't catch it, you could try a 3D scatterplot.  That won't work out to 4D, of course, but then you could create a 4th dimension as time and make a movie.  :)",2010-07-20T13:06:49.603,5,CC BY-SA 2.5,
163,298,1,Are you asking about how to reduce the effect of outliers or when to use the log of some variable?,2010-07-20T13:14:06.983,56,CC BY-SA 2.5,
164,298,36,"I think that the OP is saying ""I've heard of people using the log on input variables: why do they do that?""",2010-07-20T13:24:23.870,5,CC BY-SA 2.5,
165,274,0,"What I was getting at was that mean and standard deviation are parameters associated with the population, but they're estimated by the sample mean ((1/N)*\sum(x_i)) and the sample standard deviation ((1/(N-1))*\sum(x_i - x^bar)^2).",2010-07-20T13:42:11.253,62,CC BY-SA 2.5,
168,249,3,"The m_i's do not have to be equal

Wikipedia has a simplified description of the model.",2010-07-20T14:17:12.293,8,CC BY-SA 2.5,
172,119,4,I agree. Standard deviation is the *right* way to measure dispersion if you assume normal distribution. And a lot of distributions and real data are an approximately normal.,2010-07-20T14:40:02.050,217,CC BY-SA 2.5,
174,9,4,"If moving from Excel is the issue, you could try:

* http://www.coventry.ac.uk/ec/~nhunt/pottel.pdf 

* http://www.forecastingprinciples.com/files/McCullough.pdf 

* http://www.lomont.org/Math/Papers/2007/Excel2007/Excel2007Bug.pdf 

* http://www.csdassn.org/software_reports/gnumeric.pdf",2010-07-20T14:44:45.503,229,CC BY-SA 2.5,
176,301,1,"But still, using log changes the model -- for linear regression it is y~a*x+b, fo linear regression on log it is y~y0*exp(x/x0).",2010-07-20T14:50:05.523,,CC BY-SA 2.5,user88
177,309,4,Especially where students are concerned.,2010-07-20T14:51:13.023,71,CC BY-SA 2.5,
179,232,0,"+1 for Mondrian - very useful toy, especially for large data",2010-07-20T14:54:06.230,22,CC BY-SA 2.5,
180,310,0,"You're not missing something if all you're trying to do is estimate the parameter from a set of observations. That was definitely the main idea of the OP's question. However, she was also asking generally (if not rigorously) ""how to estimate poisson models"". Perhaps she wants to know the value of the pdf at a specific point. In that case, the normal approx. is probably going to be better than scaling the parameter, and the observations by 100, or whatever, if the observations are large enough to make calculating the factorial impractical.",2010-07-20T14:54:19.397,62,CC BY-SA 2.5,
183,10,2,"@drknexus - So, multiple items serve as a measurement triangulation for construct scales?  If yes, what are the criteria for determining that a researcher has enough relevant data points (i.e., items) to use the scale as an interval measurement?",2010-07-20T15:06:18.870,24,CC BY-SA 2.5,
185,313,0,"First of all, this is just a big example and doesn't really explain explain the concept of p-value and test-statistic. Second, you're just claiming that if you get fewer than 5 or more than 15 white marbles, you reject the null hypothesis. What's your distribution that you're calculating those probabilities from? This can be approximated with a normal dist. centered at 10, with a standard deviation of 3. Your rejection criteria is not nearly strict enough.",2010-07-20T15:21:27.113,62,CC BY-SA 2.5,
188,304,2,"Answers will be reordered based on votes, so please try not to refer to other answers.",2010-07-20T15:54:34.873,220,CC BY-SA 2.5,
189,301,1,"I agree - taking log's changes your model. But if you have to transform your data, that implies that your model wasn't suitable in the first place.",2010-07-20T16:00:09.427,8,CC BY-SA 2.5,
192,307,19,"I seen examples where a model has ten data points and nine parameters. On pointing out that the model has too many parameters, I was told that the R^2 was 0.999 so the model must be correct!",2010-07-20T16:03:44.010,8,CC BY-SA 2.5,
193,321,0,What other variants?  It might be helpful to tighten up this question a little (more specificity).,2010-07-20T16:03:59.503,5,CC BY-SA 2.5,
195,138,6,You should add your background. Programmers who came to R have different issues than people without a programming background.,2010-07-20T16:06:54.697,3807,CC BY-SA 2.5,
196,323,0,These were both listed in the original question...,2010-07-20T16:12:38.997,5,CC BY-SA 2.5,
197,114,3,I added the blogs in the question as answers to allow proper voting to find the most popular blogs.,2010-07-20T16:13:35.627,3807,CC BY-SA 2.5,
198,285,0,"Thanks Jeromy. I explored several possible transformations of data using Stata's gladder function for the ladder of powers.

Teoreically based solution will be one way to go, but before that I wanted to check for the data driven solution.",2010-07-20T16:19:56.930,22,CC BY-SA 2.5,
199,60,0,Thanks Reed. k-means is definitely a way to go and I explored several solutions. However I am trying to go for a solution without subjective decision of the number of clusters.   Hierarchical approach on the other hand produced huge classification and again it was hard for me to specify where to stop.,2010-07-20T16:22:43.323,22,CC BY-SA 2.5,
200,323,11,"By that reasoning another question which asks for the best blogs but lists other blogs in the starting question wouldn't be a duplicate of the question.
I think it makes sense to vote on all blogs to find out which are most popular including those already listed in the OP.",2010-07-20T16:22:54.993,3807,CC BY-SA 2.5,
201,268,0,Thanks Egon. SOM approach might be very interesting indeed. Will have a look at it.,2010-07-20T16:23:45.883,22,CC BY-SA 2.5,
202,254,0,"Colin, I tried to figure out how to expand the Wikipedia formulae to non-equal m_i's, but didn't have any success. Can you please help me with this? (sorry)",2010-07-20T16:38:50.220,213,CC BY-SA 2.5,
206,43,0,"fine, you win :)",2010-07-20T17:06:12.040,74,CC BY-SA 2.5,
209,321,0,Right; it is just one variant among others.,2010-07-20T17:11:25.697,,CC BY-SA 2.5,user88
211,333,1,"These are good references, but I disagree with your assessment of Ed Thrope as the founder of this field.  Statistical analysis of financial data and statistical arbitrage are not the same thing: one would perform statistical analysis for most financial analysis (e.g. modern portfolio theory).",2010-07-20T17:20:49.130,5,CC BY-SA 2.5,
213,333,2,"I agree, Markowitz definitely invented portfolio theory",2010-07-20T17:55:23.310,74,CC BY-SA 2.5,
214,86,0,@walkytalky Thanks for the corrections- I have made some fixes.,2010-07-20T18:00:44.033,13,CC BY-SA 2.5,
215,134,0,@walkytalky I don't think it would require any more explanation than any other algorithm question on SO.  Probably less as the median is a relatively basic concept.,2010-07-20T18:15:39.900,13,CC BY-SA 2.5,
216,127,6,"Agree strongly. Both great books. Start with Bayesian Computation With R, then get Gelman et al.",2010-07-20T19:12:45.460,247,CC BY-SA 2.5,
217,321,0,"OK, I'll ask for comparison to adaboost since that is perhaps the best known.",2010-07-20T19:24:06.430,220,CC BY-SA 2.5,
218,306,1,"Agreed - just to throw out some additional ideas on modeling: logistic to predict which patients will have 1+ violent outbursts, Poisson(esque) regression to predict which patients will have many outbursts,  multilevel to examine variations from room-to-room and/or ward-to-ward...",2010-07-20T20:13:46.233,71,CC BY-SA 2.5,
220,310,1,"@Srikant, you are right, to estimate the parameters the factorial is not an issue, but in general you will want the value of the likelihood for a given model, and you would have to use the factorial for that. Also, for hypothesis testing (e.g. likelihood ratio test) you will need the value of the likelihood.",2010-07-20T21:31:03.190,90,CC BY-SA 2.5,
221,310,0,"@Baltimark: yes, I want to know in general, whether it is valid to change the unit of measurement of Poisson. I was asked this question and I didn't know what to say.",2010-07-20T21:33:48.650,90,CC BY-SA 2.5,
222,302,1,"the two answers look too different to me. One says 20 to 30, the other says 20 to 30 times slopes. So if you have 5 slopes, one rule tells you 20 to 30, the other 100 to 150 observations. That doesn't seem right to me....",2010-07-20T21:37:31.240,90,CC BY-SA 2.5,
223,313,0,"I would agree that this is just an example, and I it is true I just picked the numbers 5 and 15 out of the air for illustrative purposes. When I have time I will post a second answer, which I hope will be more complete.",2010-07-20T22:00:38.863,226,CC BY-SA 2.5,
224,337,0,"I know, however that doesn't help me to decide whether using sample entropy or shannon entropy or some other kind of entropy is appropriate for the data that I'm working with.",2010-07-20T22:31:17.717,3807,CC BY-SA 2.5,
226,337,2,"What I wrote in my post is just that for a certain type of data/process/system there is only one *true* entropy definition. Sample Entropy is *not* an entropy measure, it is just some statistic with a confusing name. Make a question where you define the data for which you want to calculate the entropy, and will get the formula.",2010-07-20T23:17:28.040,,CC BY-SA 2.5,user88
227,302,1,They are pretty different guidelines.  I suspect the disconnect is whether you think that the test of the overall model matters (the lower N guideline) or the test of the individual slopes that matter (the higher N guideline).,2010-07-20T23:48:55.603,196,CC BY-SA 2.5,
228,358,1,The answer in that other thread don't really explain why 2 is a better value than other values that are very near to 2 but are no natural numbers.,2010-07-21T00:04:53.277,3807,CC BY-SA 2.5,
229,10,2,"I'm not sure; that might be a worthy question for the community in general.  I'd guess that it is probably in part a value judgement on the part of the researcher & area.  Some areas are completely willing to treat a single Likert item as interval even though it clearly is ordinal.  A reasonable answer might be to use a different analysis method, e.g. a permutation or bootstrapped test.  Another answer might be to conduct a simple test of normality, so long as the aggregate doesn't significantly depart from normality you are probably okay.",2010-07-21T00:07:20.837,196,CC BY-SA 2.5,
230,360,1,"Your point about month N's count not necessarily being correlated with N-1 is well-taken.  With a slow-growing disease like TB, that's something I'd have to look at carefully, but I'm pretty sure I could identify about how much lag there is between the time we report a source case and the time we report any secondary cases.",2010-07-21T00:11:05.730,71,CC BY-SA 2.5,
231,360,1,"However, it's your point about analyzing the distribution of monthly counts that's at the heart of my question.  There is a definite decline in TB, both nationally in the US and in my district.  For example, when I compare 2009 to the previous years, there are decidedly fewer cases.  2010 is on track to have fewer still.  What I'm trying to identify (which I did a poor job of explaining in the question) is whether or not these declines are part of an ongoing downward trend, or just a downward wobble.  Thanks - you've gotten me to think much more carefully about the problem.",2010-07-21T00:15:26.720,71,CC BY-SA 2.5,
232,358,0,I think it does; still I'll try to extend the answer.,2010-07-21T00:21:35.973,,CC BY-SA 2.5,user88
234,135,6,"For R ks.test in the default ""stats"" package can conduct the KS test without installing additional packages.",2010-07-21T00:23:25.580,196,CC BY-SA 2.5,
235,10,1,... but in general it seems like one could evoke the central limit theorem and suggest that 20 to 30 items should be sufficient to use the scale as an interval measurement.,2010-07-21T00:26:48.193,196,CC BY-SA 2.5,
236,310,0,"@Vivi: I am not sure why you would want to compute the likelihood with k_i! included as in most applications (e.g., likelihood ratio test, bayesian estimation) the constant will not matter. In any case, I do not think you can re-scale as you suggested. If I feel otherwise I will update my answer.",2010-07-21T00:28:39.063,,CC BY-SA 2.5,user28
237,310,0,"@Srikant, I see your point, but some softwares (Eviews, for example) include this by default, and large numbers are an issue you like it or not. I guess I was really after an explanation of why you can or can't do it rather than a way around it, but the discussion has been interesting and instructive nonetheless  :)",2010-07-21T00:42:20.127,90,CC BY-SA 2.5,
239,217,1,Protovis looks awesome but do you know what browser support it has? particularly IE?,2010-07-21T01:39:03.210,191,CC BY-SA 2.5,
240,134,0,@Sharpie Perhaps not more than other SO algo questions. But certainly more than what it actually says here!,2010-07-21T02:58:14.737,174,CC BY-SA 2.5,
241,227,1,"Sorry, but what are loadings (c in your formula) and how do you determine them?",2010-07-21T03:17:32.773,191,CC BY-SA 2.5,
242,234,0,Would there be any benefit or could you plot them on a 3-d scatter plot?,2010-07-21T03:18:06.567,191,CC BY-SA 2.5,
243,226,0,"How does a higher Y value ""explain"" a bigger chunk of the variance? Is it how the PCA is computed? If so I think I've got another question to post ;)",2010-07-21T03:31:11.207,191,CC BY-SA 2.5,
244,287,1,See http://en.wikipedia.org/wiki/Method_of_moments_(statistics) and  http://en.wikipedia.org/wiki/Generalized_method_of_moments,2010-07-21T03:52:40.053,,CC BY-SA 2.5,user28
245,30,1,Similar question on SO: http://stackoverflow.com/questions/56411/how-to-test-randomness-case-in-point-shuffling,2010-07-21T05:09:44.513,68,CC BY-SA 2.5,
246,215,0,"I agree completely with what you're saying, but the reason for looking at tests here is to satisfy others. The situation is modelling possible extreme operational losses bassed on historical loss experience and the regulator needs to be convinced that the choice of distribution is supported by the data. What the regulator thinks is reasonable and what the business thinks is reasonable for the results can differ quite a bit! Using a (reasonably) standard statistical test may provide a somewhat independent approach to the justifying a particular choice.",2010-07-21T05:40:47.770,173,CC BY-SA 2.5,
247,372,2,"I think this is a too broad question. Because almost all statistics can be used in data mining, I don't see any reason for this question to exist.",2010-07-21T06:11:42.967,190,CC BY-SA 2.5,
249,375,2,Or this one http://stats.stackexchange.com/questions/30/,2010-07-21T06:16:52.393,56,CC BY-SA 2.5,
250,368,0,"I don't have any requirements how correct my ASR should be. For example, when I change my model a bit my error goes down from 30% to 29.8%. For that change I want to now if it's significant",2010-07-21T06:17:05.280,190,CC BY-SA 2.5,
252,215,0,Then using a 'distance between distribution test' (like chi square or Kolmogorov-Smirnov or ... is a better idea because it is easely understood by the end user.,2010-07-21T06:57:31.783,223,CC BY-SA 2.5,
253,372,1,"All statistics are usable, but if your goal is to study the *most* important (or often used) parts of statistics when applied to Data Mining - a subset would be useful.",2010-07-21T07:46:57.840,252,CC BY-SA 2.5,
256,113,0,What is the difference between model selection http://www.modelselection.org/ (hot topic in statistic during the past 20 years) and method selection.,2010-07-21T08:30:20.000,223,CC BY-SA 2.5,
257,385,2,"I agree, but I was indeed worried about the k parameter -- if the unbalance will create some class-wise differences in the observation densities in the feature space, the same k will tend to a smaller sphere in feature space for an observation from denser class. Won't it influence the k parameter optimization then?",2010-07-21T08:45:38.653,,CC BY-SA 2.5,user88
258,372,0,I agree with @Peter; way too vague.,2010-07-21T09:40:14.487,5,CC BY-SA 2.5,
261,368,0,"In that case you would use test 2. See the table for ""Two-proportion z-test, pooled for d0 = 0"" at the wiki: http://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Common_test_statistics",2010-07-21T10:44:25.737,,CC BY-SA 2.5,user28
262,344,2,Can you give a description a problem where RNB gave you good results?,2010-07-21T10:57:06.420,217,CC BY-SA 2.5,
263,344,0,No ;-) This was only to revive the pool.,2010-07-21T11:05:37.750,,CC BY-SA 2.5,user88
264,395,1,"Look at http://stats.stackexchange.com/questions/173/time-series-for-count-data-with-counts-20/ . It's not a duplicate, but the problem is similar at a first sight.",2010-07-21T11:20:20.783,,CC BY-SA 2.5,user88
265,386,0,"How would this deal with cases where you don't know how many outliers you have, i.e. when the N-1 points still have a bias since they include outliers?",2010-07-21T11:58:10.653,56,CC BY-SA 2.5,
266,386,2,"if n is sufficiently large and the number of outlier is small then this bias is negligible. If there are a large number of outliers then, maibe it is not outliers and anyway, as I mentionned you can use leave k out strategy ... (in this case, you have to find out a strategy to avoid tracking all configurations which may be NP hard ... ) and if you don't know k, you can try many values for k and keep the most relevent.",2010-07-21T12:07:43.927,223,CC BY-SA 2.5,
267,154,5,Cool! Please post a link here to your thesis once it's complete and/or published!,2010-07-21T12:20:54.163,6,CC BY-SA 2.5,
268,293,2,"Yes, rather than starting with the data, start with the question and the data generating process.",2010-07-21T12:35:24.620,46,CC BY-SA 2.5,
269,192,3,This question doesn't make a lot of sense - where is your statistical/scientific question?,2010-07-21T12:35:50.530,46,CC BY-SA 2.5,
270,262,1,"Or a little more succinctly with melt and qplot: `m <- melt(d, id = ""x""); qplot(variable, value, data = m, colour = variable)`",2010-07-21T12:39:58.103,46,CC BY-SA 2.5,
271,400,19,I think you mean sent by fax at some point in the **past** ;),2010-07-21T12:42:57.863,46,CC BY-SA 2.5,
272,165,18,Here is my favorite paper about the topic: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.7133&rep=rep1&type=pdf,2010-07-21T13:28:05.280,,CC BY-SA 2.5,user88
273,217,2,"That's unfortunately one of Protovis's weak points (but it's an issue with any SVG library because IE doesn't support that).  Fortunately, Jamie Love has come up with a solution using SVGWeb.  See here: http://groups.google.com/group/protovis/browse_thread/thread/1a80f98a16736658?pli=1.",2010-07-21T13:37:53.340,5,CC BY-SA 2.5,
274,410,0,I've updated the question with the information on kind of time series/mean I'm talking about.,2010-07-21T13:41:09.180,219,CC BY-SA 2.5,
275,399,5,"Tufte's Visual Display of Quantitative Information (http://www.amazon.com/o/ASIN/0961392142/ref=nosim/gettgenedone-20) is better than Beautiful Evidence IMO. All four of his books are good though, and if you have an opportunity to attend one of his courses, do it.",2010-07-21T13:57:31.187,36,CC BY-SA 2.5,
276,415,1,Thanks!  I was out of town last week and missed that MO post.,2010-07-21T14:01:32.033,89,CC BY-SA 2.5,
277,133,0,I thought Chi-Sq was primarily for categorical data (contingency tables) vs. continuous?,2010-07-21T14:17:35.707,23,CC BY-SA 2.5,
278,13,5,"it's similar to using planet gravitational models to urban traffic. I find it absurd, but it works quiet accurately actually",2010-07-21T14:25:01.087,59,CC BY-SA 2.5,
279,405,0,We have done this in two different ways and it has proved very useful with the large movements this week lcearly being outside the histogram of typical results.,2010-07-21T14:47:33.937,210,CC BY-SA 2.5,
280,402,0,"Heh - this was actually one of the plots I made that didn't make it into the post.  The problem I had is deciding how to calculate the bounds - my initial attempt was with Poisson bounds, with lambda set to the mean of my data, but variance is too high to be a proper Poisson (high enough to matter in practice?  I don't know).",2010-07-21T15:01:32.140,71,CC BY-SA 2.5,
281,402,0,"A further problem is that the center of the distribution can change over time - for example, it wouldn't make sense to set those bounds using data from the early 1900s, when Colorado was a haven for TB patients.  So what's an appropriate way to keep the lines up-to-date with long-term changes in the process, while still being able to identify deviations?",2010-07-21T15:02:21.543,71,CC BY-SA 2.5,
282,217,1,IE 9 will also support SVG- so as long as you don't need to work with IE 6...,2010-07-21T15:08:29.597,13,CC BY-SA 2.5,
283,349,0,"The problem with the binning approach is that we do not have a good upper bound for the data, and so the midpoint for the largest bin would have to be huge. So, we'd need a huge number of bins (not enough memory for that), or have pretty wide bins (which would then lead to a fairly inaccurate answer.)

And the data is not very sparse.",2010-07-21T15:13:35.893,247,CC BY-SA 2.5,
284,404,0,"I'll have to read it more thoroughly later on, but yes, this package is definitely addressing the kinds of problems I'm facing here.  Thanks!  And also, thanks for the kind words about the plots ;p",2010-07-21T15:16:07.733,71,CC BY-SA 2.5,
285,352,3,"The data set is potentially too big to read in half of it...it is in a networking context where the device doing the processing can see tens of thousands of items per second, and probably has enough memory to store only a few hundred.

Also the data is definitely not Gaussian. In fact it does not fit well to any of the common distributions.",2010-07-21T15:17:56.660,247,CC BY-SA 2.5,
287,346,4,"Perhaps, asking on Stackoverflow may get better answers.",2010-07-21T15:33:20.303,,CC BY-SA 2.5,user28
288,113,2,"While model selection typically involves the scoring of models within a family of distributions, based on their fit and penalizing the number of parameters used (a la AIC and BIC), whereas method selection is more general.  Method selection involves being faced with a problem (e.g. test, classify, predict) for which we have some background knowledge (variables are known to be (e.g. independence, data type), and for which auxiliary assumptions are made (e.g. normality, homoscedasticity), and we must select a method.",2010-07-21T15:50:24.250,39,CC BY-SA 2.5,
289,113,1,"Now there are mathematical prescriptions along the lines of measurement type, convergence results, optimality, and time/space complexity, but no framework for their systematic application, that I am aware of, thus the question.",2010-07-21T15:53:08.243,39,CC BY-SA 2.5,
290,260,0,"Aptamer active motif selection, forest ground humidity forecasting, digit OCR, multispectral satellite image analysis, musical information retrieval, chemometry...",2010-07-21T15:58:58.210,,CC BY-SA 2.5,user88
295,428,0,"This is interesting, and where some statistical advice could come in! Assume in total I've got (say) 500,000 i.i.d. points and I look at groups of (say) 1,000 of them, and calculate the median of each group. Now I've got 500 medians. Is there theory that could allow me to calculate a confidence interval for the overall median based on these 500 medians?",2010-07-21T17:10:16.133,247,CC BY-SA 2.5,
298,223,2,"Is he looking to perform statistical analyses, interpret the output or critique published papers that use statistical methods?",2010-07-21T17:23:52.493,215,CC BY-SA 2.5,
300,437,0,I guess I should take the first suggestion as a vote of confidence.,2010-07-21T17:52:02.240,89,CC BY-SA 2.5,
301,410,0,@Silent: We would still need to guess what your problem really is. What is N? Number of sunflares in hundreds or the voltage you measure on some battery? It would suggest you plot your monthly means in a histogram. Fit this with a Gaussian an see if it fits and if you can justify a Gaussian fit from your model.,2010-07-21T19:24:05.017,56,CC BY-SA 2.5,
303,457,1,"Are you referring to a particular paper? I imagine I could find an answer to my question if I researched, studied, read a lot, but so could 95% of the questions other people ask here... Also, in some cases, particularly with macroeconomics data (which is my area), there is no more data to be collected. Data is scarce (the number of observations, I mean), and you just have to live with it. There is no ""get more data"" solution. I was hoping someone here would know the topic, but it doesn't seem like. Maybe once the website is opened to the general public?",2010-07-21T20:07:46.660,90,CC BY-SA 2.5,
304,461,1,Indeed some people are taking Tufte as gospel and not being particularly flexible...,2010-07-21T20:31:16.727,259,CC BY-SA 2.5,
305,464,0,"No, nothing to do with Qnotifier, and that's not mine, it's a commercial product. The data I'm working with is very similar to that though.

Completely agree with the ""color the part over the threshold"" but the plotting tools in use currently here don't allow that level of control. :-(",2010-07-21T20:42:35.483,259,CC BY-SA 2.5,
306,450,1,Can you give examples of alternatives? I'd like to look into that.,2010-07-21T20:56:10.893,77,CC BY-SA 2.5,
307,450,6,"The mostly known and the simplest is the Median-Median regression, well known from smart calculators (Sigh!). Consult also Wikipedia  http://en.wikipedia.org/wiki/Robust_regression and maybe CRAN's Robust task view http://cran.r-project.org/web/views/Robust.html",2010-07-21T21:20:20.287,,CC BY-SA 2.5,user88
308,464,0,Too bad... Still you can make the same trick only with the color of the threshold line.,2010-07-21T21:23:35.953,,CC BY-SA 2.5,user88
310,133,1,Hmmm I actually like the KS test answer better than mine !,2010-07-22T00:07:38.607,139,CC BY-SA 2.5,
311,185,0,"I actually scanned that at work yesterday. It's an interesting read - I wish I had more time to absorb the material in it, but I had to get what I needed and move on.",2010-07-22T00:39:46.840,110,CC BY-SA 2.5,
312,457,0,I suspect the answer to your question will be domain/model specific and hence I am not sure I can recommend a specific paper.,2010-07-22T01:19:43.857,,CC BY-SA 2.5,user28
313,247,3,"More specifically, the asker may be interested in [JFreeChart](http://www.jfree.org/jfreechart/) which powers a lot of Incanter graphics.",2010-07-22T03:23:01.707,13,CC BY-SA 2.5,
314,262,0,"Actually, an even easier way is to use R+deducer with ggplot2 (there is a new release of this which is about to come out in the next few months.  A beta is currently available)",2010-07-22T03:57:44.747,253,CC BY-SA 2.5,
316,423,0,"@sharpie: are jokes out?  We obviously don't want the entire site to be humor, but everyone benefits from a little educational humor in small doses.",2010-07-22T05:15:40.057,5,CC BY-SA 2.5,
317,479,0,"Totally agree. By making it a background it is clearly not data, but my making it coloured it shows a clear change in 'state'.",2010-07-22T09:04:20.237,210,CC BY-SA 2.5,
318,461,0,Leaving the reader to figure things out has been a source of countless problems. You have to communicate your message properly and if your message is the data over this line is an issue you must shown the line which is concerning.,2010-07-22T09:47:28.597,210,CC BY-SA 2.5,
319,151,116,"""Squaring always gives a positive value, so the sum will not be zero."" and so does absolute values.",2010-07-22T09:54:23.443,223,CC BY-SA 2.5,
320,483,0,"Sorry, bu I don't really understand your answer. What do you mean by ""In detail, each tree is build on a sample of objects drawn with replacement from the original set""

 Can you give more precision on where I find the details ""here""?",2010-07-22T10:04:51.520,223,CC BY-SA 2.5,
321,262,4,"Nice example, but the plot is hardly publication quality. Or at least none of the journals I publish in would accept it.",2010-07-22T11:02:42.330,214,CC BY-SA 2.5,
322,411,1,"I like the question, there may be already most of the possible answer in the question... do you have an idea of the type of answer/developpement you want?",2010-07-22T11:18:44.207,223,CC BY-SA 2.5,
323,485,2,Should be community wiki.,2010-07-22T11:21:23.360,,CC BY-SA 2.5,user88
324,483,1,This is how bagging works; check out http://en.wikipedia.org/wiki/Bootstrap_aggregating . Here is a link  (hardly visible in that theme I admit) to the detailed RF reference.,2010-07-22T11:24:12.683,,CC BY-SA 2.5,user88
326,490,0,"Is it a question or a pool? If the latter, it should be community wiki. If the first, give more details about what you want to achieve? For instance, is it all-relevant or rather  minimal-optimal selection? How much is many? How   hard is the classification problem?",2010-07-22T11:38:20.697,,CC BY-SA 2.5,user88
327,492,0,What about slow (standard) Fourier transform.,2010-07-22T11:45:24.460,,CC BY-SA 2.5,user88
328,490,0,pool... many means 1000 features or more and less than 100 observations.,2010-07-22T11:58:08.117,223,CC BY-SA 2.5,
329,498,1,"Also, I know this might get flagged off-topic, or not statistical, but maybe not. I wouldn't mind seeing any SAS/R/SPPS/Stata question be fair game here. Statisticians should be the most experienced in these packages.",2010-07-22T11:59:44.197,62,CC BY-SA 2.5,
330,498,0,"imho offtopic. Even though it is an ""Analysis"" program, the question has definitely nothing to do with statistical analysis. Suggestion: contact the manufacturer of SAS, they can fix the problem.",2010-07-22T12:05:58.567,190,CC BY-SA 2.5,
331,498,1,"See also http://meta.stats.stackexchange.com/questions/1/how-to-answer-r-questions . Option 3 seems to be favored here, which means that questions that have not anything to do with statistics are closed/move. I think this is the same case.",2010-07-22T12:12:06.527,190,CC BY-SA 2.5,
332,503,0,"Yes, certainly when I need to I output what I want into excel, or csv, or whatever, but at times, I just want a quick  C & P. It behave very oddly. . .not even giving me the option of ""copy"" from the ""edit"" drop-down.",2010-07-22T12:33:19.247,62,CC BY-SA 2.5,
334,460,0,"The procedure I already gave can be applied in large dimension, as I said, using a gaussian assumption.  If the dimension is really large with respect to the sample size (i.e. p>>n) then you can  make some sparcity assumption (assume that the parameters of your gaussian distribution lie in a low dimensional space for example)  and use a thresholding estimation procedure for the estimation of the parameters...",2010-07-22T13:16:30.577,223,CC BY-SA 2.5,
335,151,42,"@robin girard: That is correct, hence why I preceded that point with ""The benefits of squaring include"". I wasn't implying that anything about absolute values in that statement. I take your point though, I'll consider removing/rephrasing it if others feel it is unclear.",2010-07-22T13:19:20.007,81,CC BY-SA 2.5,
336,498,2,"I think questions about how to implement a statistical analysis in SAS/R/whatever are ok. But, purely interface questions are off-topic.",2010-07-22T13:38:33.143,,CC BY-SA 2.5,user28
337,423,0,"@Sharpie, feel free to close or reopen according to your feelings! I agree with Shane, a bit is ok, but not too much. For example, this question already included a funny cartoon. The jokes question not really a funny joke....",2010-07-22T13:58:03.300,190,CC BY-SA 2.5,
338,423,43,These cartoons are useful too; they can be included in a lecture on a particular topic where you are trying to explain a concept (e.g. correlation/causation above).  A little humor can help to keep an audience engaged.,2010-07-22T14:22:11.213,5,CC BY-SA 2.5,
339,411,1,"Not very specifically.  I'm quite ignorant of statistics and one of my reasons for asking is to learn which criteria statisticians would use to pick between different metrics.  Since I did already describe one important practical advantage of 1 (you can actually compute it) I'm especially interested in theoretical motivations.  Say, is the information provided by estimates of Kolmogorov distance frequently of direct use in applications?",2010-07-22T14:50:34.407,89,CC BY-SA 2.5,
340,328,0,"Given the current state of national economies due to the rescue of the various financial institutions, one may question the value of accepted knowledge in this field, save for greater fool theory.",2010-07-22T15:06:12.763,229,CC BY-SA 2.5,
341,36,5,"That pirates / global warming chart is clearly cooked up by conspiracy theorists - anyone can see they have deliberately plotted even spacing for unequal time periods to avoid showing the recent sharp increase in temperature as pirates are almost entirely wiped out.
We all know that as temperatures rise it makes the rum evaporate and pirates cannot survive those conditions.
;-)",2010-07-22T16:08:37.597,270,CC BY-SA 2.5,
342,461,0,"... right.  I wasn't suggesting he leave it to the reader *if it was a part of his message*.  I was saying that if the threshold weren't a part of the plot, he could either leave it out entirely or incorporate a subtle visual cue to make the threshold easier to find if the reader wanted to look for it.",2010-07-22T16:09:04.540,71,CC BY-SA 2.5,
344,517,8,"First, two lines from wiki: ""In computer science, semi-supervised learning is a class of machine learning techniques that make use of both labeled and unlabeled data for training - typically a small amount of labeled data with a large amount of unlabeled data. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data)."" Does that help?",2010-07-22T16:25:36.087,,CC BY-SA 2.5,user28
345,11,0,Can you be a little more specific?,2010-07-22T16:26:29.423,,CC BY-SA 2.5,user88
346,517,0,"What do you have in mind with ""Algorithmic approaches""? I gave some examples of applications in my answer, is that what you are looking for?",2010-07-22T16:49:10.537,190,CC BY-SA 2.5,
347,520,0,"Why the  downvote? Is it not a good answer, or do you think it's not a good tool?",2010-07-22T17:06:47.843,190,CC BY-SA 2.5,
348,411,0,"I forgot to end my previous comment with the more or less obvious: and if so, how?",2010-07-22T17:11:20.193,89,CC BY-SA 2.5,
349,479,2,"If you don't want too much colour, just show a band of a neutral eg light grey - but do it in the *foreground* with some high level of transparency. This means that any part of the data line which shows above this threshold will automatically ""pop"" as it has greater contrast to the background than to your 'normal' band.",2010-07-22T17:11:24.467,270,CC BY-SA 2.5,
353,113,0,"Can you give an example of method selection with more details (a link to a page or a paper could be fine), this could help me to figure out more precisely. Thanks in advance",2010-07-22T18:14:49.250,223,CC BY-SA 2.5,
354,452,0,"I am not sure of what you meant by ""Robust Standard Errors are reported as a matter of course"" standard errors of what? You said testing for ""it"" what is the test you are talking about?",2010-07-22T18:21:09.143,223,CC BY-SA 2.5,
355,498,1,"Vote to keep. Stat software questions have to go somewhere. Stackoverflow is not the right place. Either we need to answer them here, or start statssoftware.stackexchange.com",2010-07-22T18:22:12.137,74,CC BY-SA 2.5,
356,520,0,"I second the python recommendation here, especially if you have trouble fitting the data set into r. If you do go the python route,  have a look at ""Programming Collective Intelligence"" by Toby Segaran",2010-07-22T18:29:15.477,247,CC BY-SA 2.5,
358,519,0,"Interesting example, because it looks, at first glance, like a probable cause-and-effect relationship, unlike many of the silliest examples.",2010-07-22T19:10:45.160,77,CC BY-SA 2.5,
359,498,0,"Chief makes a good point. If not here, where? Is this purely ""statistical analysis"" just because of the name? Or is it a place that people working in the field can come for answers. Sometimes I just need a piece of SAS syntax that I've forgotten. I can dig through my archives, but I wouldn't mind coming here to get an answer/an alternate/a better way/ or just leave mental residue for future questioners.",2010-07-22T19:51:37.467,62,CC BY-SA 2.5,
360,26,0,"The same concept may have different explanations at different levels... A smart sixth grader's understanding of the concept might be very different from that of a phd student, who has thought about it a lot more. It would be nice to see the basics revisited in light of more advanced concepts. That would help me understand how everything connects",2010-07-22T20:23:56.797,35,CC BY-SA 2.5,
361,494,0,"Even if the constant is included, the AIC (AICc) can be negative.",2010-07-22T23:15:41.563,159,CC BY-SA 2.5,
364,532,1,"Thanks for this answer (should be a comment ? ) 

As I already mentionned, as a comment to Rich answer's High dimension are not a problem (even 1000 could work) if you make parametric structural assumption.",2010-07-23T06:43:46.077,223,CC BY-SA 2.5,
365,532,0,"@rob ""I'm not sure what would be a good threshold "" this would be the purpose of the multiple testing procedure I mentionned .... but I fully agree that things have to be filled in and I really like the outlier detection in the outlier detection !  who wants to writte a paper :) ?",2010-07-23T06:47:22.747,223,CC BY-SA 2.5,
367,494,1,That's what I've written.,2010-07-23T08:17:51.990,,CC BY-SA 2.5,user88
370,501,2,But the question says there are more variables than observations. So it is not possible to begin with the full set.,2010-07-23T09:50:30.923,159,CC BY-SA 2.5,
371,534,20,Correlation and a strong underlying reason for a link suggest causation until proven otherwise is probably the best you can get.,2010-07-23T09:54:58.050,229,CC BY-SA 2.5,
372,452,1,"Good point....I'm talking about the Standard Errors of regression coefficients in OLS regression and the problem of heteroscedasticity. The traditional approach would be to test for the presence of heteroscedasticity using, for example, White's test or Breusch Pagan test. If heteroscedasticity is found then one would report Robust Standard Errors, usually White Standard Errors.",2010-07-23T10:09:35.607,215,CC BY-SA 2.5,
374,541,1,"""[T]hen I am making very strong assumptions about the relative difference between consecutive values of the ordinal variable.""

I think this is the key point, really.  i.e. how strongly can you argue that the difference between groups 1 and 2 is comparable to that between 2 and 3?",2010-07-23T10:43:29.803,266,CC BY-SA 2.5,
376,520,0,"What downvote? I'm not familiar with Python yet, but it seems promising, I'll take a look.",2010-07-23T11:32:09.430,166,CC BY-SA 2.5,
377,520,0,"@Victor, the first vote was a downvote (so it's now 2 upvotes/1 downvote). Maybe someone didn't like this tool. Happy I could help :)",2010-07-23T11:34:05.013,190,CC BY-SA 2.5,
378,26,9,"I don't think the purpose of this site is to answer 6th graders questions. And my kid, when faced with such a question, would google for the answer.

If there is a specific part of the definition you don't understand, ask away. But such an unfocused question on such a basic topic indicates (to me anyway) that the poster didn't even try to find an answer. What is going to be next ""What is a number and how are they used?""",2010-07-23T12:05:21.897,247,CC BY-SA 2.5,
382,519,1,"What I like is that you can provoke lots of discussion about whether the ""effect"" was to actually impact fertility (in a medical sense of ability to conceive) or was it social (""I don't want to bring a child into this bad world""). Then drop the bombshell about the Pill if no-one else has brought it up. And then point out that even this can only be one possible factor and discuss some of the others.",2010-07-23T13:32:48.203,270,CC BY-SA 2.5,
383,501,0,What's the problem?,2010-07-23T13:41:29.023,,CC BY-SA 2.5,user88
385,541,1,"I think you should make some assumption about how the continuous variable should be distributed and then try to fit this ""psudohistogram"" of each categorical variable frequency (I mean find bin widths which will transform it into a fitted histogram). Still, I'm not an expert in this field, its a fast&dirty idea.",2010-07-23T13:56:36.227,,CC BY-SA 2.5,user88
387,555,3,"If I am not mistaken, 

linear regression is the estimation of coefficients that define a good linear map from X to Y.

 ANOVA is a test to know if there is significant differences in X when Y take two different values.  

Can you explain us why you think they are the same?",2010-07-23T15:29:16.043,223,CC BY-SA 2.5,
388,541,0,"Recasting binary categories as {0,1} makes sense, but turning that into a continuous [0,1] interval seems like a bit of a leap. On the broader front, I'm totally with your reluctance to weight ordinals equally unless there are powerful arguments from the model.",2010-07-23T15:29:27.463,174,CC BY-SA 2.5,
390,455,19,Statistical voyeurism? And there we were wondering what to call the site...,2010-07-23T15:48:25.713,174,CC BY-SA 2.5,
391,557,1,"Thanks for the Gelman reference. I will read his paper. But, can't we analyze multilevel models using classical maximum likelihood? I agree that OLS is inefficient/inappropriate for multi-level models.",2010-07-23T15:50:47.240,,CC BY-SA 2.5,user28
393,556,13,"Data can be discrete without being restricted to integers. Or numbers, for that matter. It's always possible to *represent* discrete data with integers, but that doesn't mean the data can only take such values.",2010-07-23T16:06:37.887,174,CC BY-SA 2.5,
394,557,3,"@Srikant - there any many ways to deal with multilevel data and Gelman is ""the king"" of this field. His point is that ANOVA is a simple/clear method of capturing the key features of complex and hierarchical data structures or study designs and ANOVA is a simple/clear way of presenting the key results. In this sense it's role is complementary or exploratory.",2010-07-23T16:30:00.160,215,CC BY-SA 2.5,
396,566,3,"Indeed, blending is one of the possible ensemble techniques. In particular, there are two when you combine the same sort of classifier, boosting (like Adaboost) and bagging (like Random Forest), and blending, where you combine different classifiers (what was Shane's question about).",2010-07-23T17:10:02.027,,CC BY-SA 2.5,user88
397,470,19,"Why pay $70 for the book?  To support the authors and to have a physical copy.  That's why I did it, anyway!",2010-07-23T17:13:42.517,5,CC BY-SA 2.5,
398,411,0,"I just reread my long comment above and realized that the last question I raised is as much a practical consideration as theoretical.  In any case, that's one of the kinds of issues I'd be interested to learn about.",2010-07-23T17:23:48.337,89,CC BY-SA 2.5,
399,26,1,you are right.. this question is way too basic.. (but there are others like http://stats.stackexchange.com/questions/118 which are things I has confused by at some point and am glad to see discussions on on a website such as this),2010-07-23T17:32:09.993,35,CC BY-SA 2.5,
400,566,3,"For blending, this paper from the netflix competition is worth reading: http://www.the-ensemble.com/content/feature-weighted-linear-stacking.",2010-07-23T17:42:07.760,5,CC BY-SA 2.5,
401,563,4,Don't you think that the Wikipedia article about it is enough?,2010-07-23T17:59:06.980,,CC BY-SA 2.5,user88
404,563,1,Questions such as this require a wiki / blog post type of response. I do think questions should not require such long answers.,2010-07-23T19:50:07.990,,CC BY-SA 2.5,user28
406,563,0,I'm not sure the right thing to do is to simply ignore this question and refer the asker to the wiki - especially during beta where we are trying to build up the content of the site.  Perhaps the question asker should submit each of these questions individually so that they can be better addressed.,2010-07-23T20:52:00.570,196,CC BY-SA 2.5,
407,529,0,"Sorry, I was meaning an analytical measurement method.  I've re-worded the question.",2010-07-23T20:58:13.660,114,CC BY-SA 2.5,
408,529,0,"In that case, I think the two-sample test of equality for means/proportions is what you may want to do.",2010-07-23T21:04:24.660,,CC BY-SA 2.5,user28
412,579,0,Does that mean that for certain sample sizes BIC may be less stringent than AIC?,2010-07-23T21:36:49.540,196,CC BY-SA 2.5,
413,121,5,"said ""it's continuously differentiable (nice when you want to minimize it)"" do you mean that the absolute value is difficult to optimize ?",2010-07-23T21:40:12.833,223,CC BY-SA 2.5,
414,118,2,"Do you think the term standard means this is THE standard today ? Isn't it like asking why principal component are ""principal"" and not secondary ?",2010-07-23T21:44:37.093,223,CC BY-SA 2.5,
415,311,2,My opinion is that this book is THE book about extrem value theory,2010-07-23T21:56:20.037,223,CC BY-SA 2.5,
416,532,0,"The principal component is a good powerfull idea. It will work in a lot of case. However, it is to avoid when the distribution has a trend or is multimodal... in those cases the direction of largest variation are not related to outliers.",2010-07-23T22:00:48.053,223,CC BY-SA 2.5,
417,579,1,"Stringent is not a best word here, rather more tolerant for parameters; still, yup, for the common definitions (with natural log) it happens for 7 and less objects.",2010-07-23T22:13:56.467,,CC BY-SA 2.5,user88
419,121,36,"@robin: while the absolute value function is continuous everywhere, its first derivative is not (at x=0). This makes analytical optimization more difficult.",2010-07-23T23:59:23.210,7,CC BY-SA 2.5,
420,470,2,I agree with Shane... and a hardcopy makes reading it much easier.,2010-07-24T00:07:57.327,7,CC BY-SA 2.5,
421,581,3,What do you mean by 'viterbi training' exactly?,2010-07-24T00:40:50.910,240,CC BY-SA 2.5,
422,579,1,AIC is asymptotically equivalent to cross-validation.,2010-07-24T01:47:58.633,159,CC BY-SA 2.5,
423,121,1,"Yeah, finding quantiles in general (which includes optimizing absolute values) tends to churn up linear programming type problems, which -- while they're certainly tractable numerically -- can get fiddly.  They typically don't have an analytical closed-form solution, and are a bit slower and a bit more difficult to implement than least-square-type solutions.",2010-07-24T02:55:02.763,61,CC BY-SA 2.5,
424,574,0,"There is a lot of training involved. Training a reasonable Finnish acoustic model can take 60-100 hours of computing time. Besides that, most corpora work with standardized training, development and evaluation sets.",2010-07-24T04:50:42.187,190,CC BY-SA 2.5,
425,588,7,If I'm right you mix up Viterbi training and Viterbi decoding.,2010-07-24T05:00:00.143,190,CC BY-SA 2.5,
427,121,2,"I do not agree with this. First, theoretically, the problem may be of different nature (because of the discontinuity) but not necessarily harder (for example the median is easely shown to be arginf_m E[|Y-m|]).  Second, practically, using a L1 norm (absolute value) rather than a L2 norm makes it piecewise linear and hence at least not more difficult. Quantile regression and its multiple variante is an example of that.",2010-07-24T06:01:42.113,223,CC BY-SA 2.5,
428,118,1,My understanding of this question is that it could be shorter just be something like: what is the difference between the MAE and the RMSE ? otherwise it is difficult to deal with.,2010-07-24T06:08:14.627,223,CC BY-SA 2.5,
429,527,0,"Can you give more details in your question? I don't understand what is ""the concentration of a particular molecule in a matrix"".",2010-07-24T06:41:14.763,223,CC BY-SA 2.5,
430,566,2,"IT is fun that meteorologist also use the word ""ensemble"" but not for combination: they use it for an ensemble of prediction (like scenario) obtained by perturbation of the initial conditions of the numerical model.",2010-07-24T06:46:49.650,223,CC BY-SA 2.5,
431,529,2,"Wouldn't a test of means/proportions only give you a point estimate of whether the two methods gave the same average response for a given set of responses?  Couldn't that approach yield a result of ""equal"" even if the two methods were actually negatively correlated with one another?",2010-07-24T07:44:03.637,196,CC BY-SA 2.5,
432,579,0,@Rob Can you give a reference? I doubt that it is general.,2010-07-24T08:03:12.467,,CC BY-SA 2.5,user88
433,579,0,"@Rob For what I could found, this is true only for linear models.",2010-07-24T08:13:15.780,,CC BY-SA 2.5,user88
434,587,0,"AIC is equivalent to K-fold cross-validation, BIC is equivalent to leve-one-out cross-validation. Still, both theorems hold _only_ in case of linear regression.",2010-07-24T08:23:58.813,,CC BY-SA 2.5,user88
436,574,0,"E, 100 cores and one may live with that. Seriously, this looks dubious from a ML point of view, still I can understand the reasons why you do it in such a way. So then stick to the Srikant solution.",2010-07-24T08:44:37.530,,CC BY-SA 2.5,user88
437,585,0,"I meant significantly different results. I also think there is none, at least real-world example. Still, I think I'll wait some time more.",2010-07-24T09:04:44.470,,CC BY-SA 2.5,user88
438,590,0,This question is a repost of http://stats.stackexchange.com/questions/536/when-a-serious-statistician-calls-a-geometric-distribution-a-geometric-density-t (see comments there). @Hibernating is the original question-asker,2010-07-24T09:09:35.053,190,CC BY-SA 2.5,
439,121,12,"Yes, but finding the actual number you want, rather than just a descriptor of it, is easier under squared error loss.  Consider the 1 dimension case; you can express the minimizer of the squared error by the mean: O(n) operations and closed form.

You can express the value of the absolute error minimizer by the median, but there's not a closed-form solution that tells you what the median value is; it requires a sort to find, which is something like O(n log n).

Least squares solutions tend to be a simple plug-and-chug type operation, absolute value solutions usually require more work to find.",2010-07-24T09:10:00.387,61,CC BY-SA 2.5,
444,587,8,"mbq, it's AIC/LOO (not LKO or K-fold) and I don't think the proof in Stone 1977 relied on linear models.  I don't know the details of the BIC result.",2010-07-24T11:01:35.353,251,CC BY-SA 2.5,
445,93,5,"I think that if you ask a question that can be understood by people that don't know what the hazard function is and if you elaborate a bit more on what you do (how do you estimate the parameter of you'r gausian process, how do you use the gaussian process at the end) you will increase the chances to get an answer and this will be an added value for stats.stackexchange :)",2010-07-24T11:01:43.027,223,CC BY-SA 2.5,
446,529,0,That is a good point.,2010-07-24T12:16:03.207,,CC BY-SA 2.5,user28
447,590,0,I am guessing it is a typo or an oversight.,2010-07-24T12:27:31.700,,CC BY-SA 2.5,user28
448,587,17,ars is correct. It's AIC=LOO and BIC=K-fold where K is a complicated function of the sample size.,2010-07-24T12:42:31.373,159,CC BY-SA 2.5,
449,512,0,We had a look at it. Worked okay but in this case the noise still seemed to be a bit too strong and if we changed the parameters to even out the distributions enough it appeared that the trend was damped down too much. Maybe in this case there just is no solution to the data and it is just a bit too noisy.,2010-07-24T17:10:43.037,210,CC BY-SA 2.5,
450,588,1,"You're right.  I wasn't aware that there was a procedure that used only the Viterbi algorithm to compute the transition probabilities as well.  It looks -- on further reading -- like there's some overlap of nomenclature between discrete time/discrete state HMM analysis, and discrete time/continuous state analysis using Gaussian mixture distributions.  My answer speaks to the DTDS HMM setup, and not the mixture model setup.",2010-07-24T19:08:38.050,61,CC BY-SA 2.5,
451,587,0,"Congratulations, you've got me; I was in hurry writing that and so I made this error, obviously it's how Rob wrote it. Neverthelss it is from Shao 1995, where was an assumption that the model is linear. I'll analyse Stone, still I think you, ars, may be right since LOO in my field has equally bad reputation as various *ICs.",2010-07-24T20:10:12.740,,CC BY-SA 2.5,user88
453,603,0,"No, no, no, it is about machine learning *not* model selection.",2010-07-24T23:09:02.483,,CC BY-SA 2.5,user88
454,605,0,"To get the behavior you wanted, try using a simple CART.",2010-07-24T23:29:48.473,,CC BY-SA 2.5,user88
455,604,0,I am not sure why you feel that in regression you will get R2 = 1 if you try to predict the predicted variable. Can you clarify?,2010-07-25T01:37:03.440,,CC BY-SA 2.5,user28
457,603,1,"Interesting distinction. I thought model selection was central to machine learning, in almost all meanings of the term.",2010-07-25T02:40:53.223,30,CC BY-SA 2.5,
458,13,5,"I am interested in the last statement: ""the very first commercially available (and working) Bankruptcy model implemented by the credit bureaus was created through a plain old linear regression model targeting a 0-1 outcome"". Which model was it? I believe that the first model was RiskCalc by Moody's, and even the first version was a logistic regression model. The developers of that model were not CS people with a background in ML, but rather in econometrics.",2010-07-25T02:58:55.117,30,CC BY-SA 2.5,
459,587,0,"The description on Wikipedia (http://en.wikipedia.org/wiki/Cross-validation_(statistics)#K-fold_cross-validation) makes it seem like K-fold cross-validation is sort of like a repeated simulation to estimate the stability of the parameters. I can see why AIC would be expected to be stable with LOO (since LOO can wasily be conducted exhaustively), but I don't understand why the BIC would be stable with K-fold unless K is also exhaustive. Does the complex formula underlying the value for K make it exhaustive?  Or is something else happening?",2010-07-25T03:18:18.817,196,CC BY-SA 2.5,
460,593,0,"Thanks, I think the Friedman test is interesting, but I can't quite figure out how it is doing that adjustment for Type I error in the post-hoc.  The comments say it is a ""Wilcoxon-Nemenyi-McDonald-Thompson test"" but I've never heard of that before could you explain it?",2010-07-25T04:29:21.710,196,CC BY-SA 2.5,
461,501,2,You can't fit a model that has more variables than observations. There are not enough degrees of freedom for parameter estimation.,2010-07-25T04:42:10.433,159,CC BY-SA 2.5,
462,411,0,I know you did not meant to be exhaustive but you could add Anderson darling statistic (see http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test). This made me remind of a paper fromo Jager and Wellner (see http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1194461721) which extands/generalises  Anderson darling statistic (and include in particular higher criticism of Tukey)...,2010-07-25T06:38:39.197,223,CC BY-SA 2.5,
463,604,1,"You could improve clarity in your question. Note that A (good) classification rule may make classification errors in the learning set, this is sometime a clue to avoide overfitting.",2010-07-25T07:37:44.797,223,CC BY-SA 2.5,
464,596,1,Can you elaborate on what you really want to do (what is the model you want to fit) ? If you don't give a real question there won't be a real answer.,2010-07-25T07:40:01.560,223,CC BY-SA 2.5,
465,594,0,I am not sure but I think it is possible to interpret it as a stocastic gradient descent optimization procedure. I'll think about that...,2010-07-25T07:41:33.880,223,CC BY-SA 2.5,
466,534,8,Isn't it Karl Popper that said man can't establish causality: scientific theories are abstract in nature. They can be falsifiable and the fact that we encouter difficulties in falsifying something make us think about causality...,2010-07-25T07:48:30.790,223,CC BY-SA 2.5,
468,501,0,"This is machine learning, so you can.",2010-07-25T08:08:08.653,,CC BY-SA 2.5,user88
469,603,0,"All those things work for trivial (mostly linear) models when you have few parameters and you just want to fit them to data to say something about it, like you have y and x and you want to check whether y=x^2 or y=x. Here I talk about estimating error of models like SVMs or RFs which can have thousands of parameters and are still not overfitting due to complex heuristics.",2010-07-25T08:22:10.460,,CC BY-SA 2.5,user88
470,604,0,I think it is a good question; if I haven't known what I know I would thought the same.,2010-07-25T08:36:42.360,,CC BY-SA 2.5,user88
473,512,1,Exponentially weighted moving averages are a special case of a kernel smoother (assuming you used a 2-sided MA rather than 1-sided). Better estimates that are generalizations of this are loess or splines -- see my answer.,2010-07-25T09:15:25.727,159,CC BY-SA 2.5,
475,47,0,"I don't see why you want to cluster your users without any subjective input (I refer to a comment you made @reed), you said you want ""the most appropriate number of clusters"", but unfortunatly you don't have a clear objective, if you want to cluster your population to show something particular, you should tell us what you want to show ? If you want statistic (the data) to tell you what you want to show this is another problem :)",2010-07-25T11:24:42.817,223,CC BY-SA 2.5,
476,194,0,"Maybe you are asking a bit to much to statistic: you said ""patterns that you extract out from the data are indeed true patterns, not statistical fluke"" and then you ask if there is a statistical procedure to answer the question... can I send that to xkcd :) ? more seriously, I think you should try to see if the pattern are meaningfull by trying to understand if they mean something",2010-07-25T11:29:52.403,223,CC BY-SA 2.5,
477,242,1,"I really like this type of question, I think this type of precise real worl problem will increase the interest of the site. It would be even better if you had the possibility to add a link to the data, or to tell us (as a complement to the post) what you finally did, what was the conclusions .... however I understand that this can be confidential ...",2010-07-25T11:38:04.900,223,CC BY-SA 2.5,
478,242,0,I whish I could vote up again to make you pass over the question about the definition of a random variable ;),2010-07-25T11:41:11.637,223,CC BY-SA 2.5,
479,608,0,"+1 The code would be simple to write, still I'm very interested in seeing a clear, illustrative dataset.",2010-07-25T12:31:16.200,,CC BY-SA 2.5,user88
482,614,0,"On the other hand, can a book be open source? It rather applies to code, so probably the better word is ""open book"".",2010-07-25T15:08:59.453,,CC BY-SA 2.5,user88
484,185,0,"agreed, that is an excellent book. It pretty much explains how Google works :)",2010-07-25T18:41:18.913,74,CC BY-SA 2.5,
485,615,3,"I'm still struggling to understand equations (biologist ahoy), which is why I turned to the community here, hoping it will help me explain the difference in layman's terms.",2010-07-25T19:18:23.953,144,CC BY-SA 2.5,
487,611,0,I am afraid I do not follow much of what you wrote as I am not a trained mathematician. Are you saying that all 'discrete distributions' are in some sense densities under a very general definition?,2010-07-25T19:39:03.403,,CC BY-SA 2.5,user28
488,611,3,"Without much mathematic you could say that a continuous variable has a density with respect to the Lebesgue measure, and a discrete random variable has a density with respect to the counting measure.",2010-07-25T19:56:22.177,223,CC BY-SA 2.5,
492,26,3,+1 to Peter... I think that we should directly close questions that find a direct answer on wikipedia http://en.wikipedia.org/wiki/Standard_deviation. I voted to close.,2010-07-25T20:07:35.910,223,CC BY-SA 2.5,
494,2,3,"did you try google/ wikipedia first ? 
http://en.wikipedia.org/wiki/Normal_distribution",2010-07-25T20:13:20.310,223,CC BY-SA 2.5,
495,206,2,"Did you try Google first? For me, it gives [this](https://web.archive.org/web/20130510121554/http://infinity.cos.edu/faculty/woodbury/stats/tutorial/Data_Disc_Cont.htm).",2010-07-25T20:15:33.723,223,CC BY-SA 2.5,
496,109,3,Can you give a definition or a link to the definition of the different test. Can you state the hypothesis you want to test (otherwise it is difficult to discuss the power ...).,2010-07-25T20:20:36.190,223,CC BY-SA 2.5,
497,244,0,"I tend to use some more than others- the tool I use most often for visualization is R and associated packages, but I left it off of this list because there is no easy way to compile R scripts to stand-alone ""executables"" that the OP wanted.  I can't really claim a single favorite- I would have to say it depends on 1) The task at hand and 2) The tools I am using",2010-07-25T20:26:16.973,13,CC BY-SA 2.5,
498,603,0,"These results are valid for regression of general linear models with arbitrary number of independent variables. The variables can be arbitrary learners. The crucial assumption is that as the number of observations goes to infinity the number of learners describing the true model stays finite. All of this works for regression, so for a classification task like yours I am not sure it helps.",2010-07-25T20:57:47.927,30,CC BY-SA 2.5,
499,615,0,"I think the ideology is that FA assumes that the process is driven by some 'hidden factors', while the data we have consists of some combinations of them. Because of that, the problem of FA is to reconstruct the hidden factors somehow. And there goes PCA -- a method which iteratively builds a new variables (PCs) by mixing the old ones such to greedy absorb the variance of the data. One may say the PCs are equal to the FA's factors, and here they will be indistinguishable. But one may also make some changes to the PCA to make it a base of some other 'FA sort', and so the problem begins.",2010-07-25T22:56:50.603,,CC BY-SA 2.5,user88
500,615,0,"So basically, you should think of what you want to do (not which buzzword you want to use). I know it is hard, especially while having biologists around (to some extend use-buzzword works well in biology, so they just assume that this is common to other disciplines); still this is the way science should be done. Than use Google (or this site) to assess the good algorithm for it. Finally, use the docks to find a function/button that does it and type/click it.",2010-07-25T23:19:04.467,,CC BY-SA 2.5,user88
502,596,0,"Ok, I admit that now I am also in deep confusion. Probably you could translate some of the document-retrieval jargon.",2010-07-25T23:45:52.910,,CC BY-SA 2.5,user88
503,566,0,"@robin It's not fun, its physics.",2010-07-26T00:03:50.127,,CC BY-SA 2.5,user88
504,596,0,"Doc retrieval? None of that. However, I can elaborate.",2010-07-26T00:25:00.770,240,CC BY-SA 2.5,
505,608,0,"I'm not sure what all would need to be in a clear and illustrative dataset, but I've made an attempt to include a sample dataset.",2010-07-26T05:50:33.367,196,CC BY-SA 2.5,
506,566,1,@mbq in fact they call themselves forecaster and they use statistic quite a lot ...,2010-07-26T06:46:28.900,223,CC BY-SA 2.5,
507,457,1,"Sorry for the late reply. I like your suggestion of simulation. That is not really easy, though. The truth is, what I see in practice is that researchers just do the test that is computationally easier or that give them the result they want.",2010-07-26T07:41:27.907,90,CC BY-SA 2.5,
508,566,1,"@robin I know, this is just why it's called ""ensemble"" not a set or something like this.",2010-07-26T07:46:15.477,,CC BY-SA 2.5,user88
509,608,0,"So look: what you provided is an example of an useless set, because the BIC and AIC give the same results: 340 v. 342 for AIC and 349 v. 353 for BIC -- so good.model wins in both cases. The whole idea with that convergence is that certain cross-validation will select the same model as its corresponding IC.",2010-07-26T07:53:50.170,,CC BY-SA 2.5,user88
510,570,0,"The term ""SEM"" is vague. It could also mean ""Search Engine Marketing"", for instance, for someone looking for statistical analysis techniques for studying ad click data or evaluating advertising effectiveness. Consider making the title more verbose.",2010-07-26T09:10:42.060,87,CC BY-SA 2.5,
511,608,1,I've made a simple scanning and for instance for seed 76 the ICs disagree.,2010-07-26T09:45:12.620,,CC BY-SA 2.5,user88
513,600,2,"Two quick notes on your notes. 1. The C-vM distance is precisely the L^2 cousin of the Kolmogorov (L^infinity) and (univariate) K-R (L^1) distances, and hence interpolates between them.  2. One advantage I didn't mention of the K-R and B-L distances is that they generalize more naturally to higher dimensional spaces.",2010-07-26T13:31:30.177,89,CC BY-SA 2.5,
514,609,1,"Yes, what I called the Kantorovitch-Rubinstein distance is also called the L^1 Wasserstein distance or W1.  It goes by many other names too.",2010-07-26T13:37:04.810,89,CC BY-SA 2.5,
515,411,0,All the answers so far are very nice and add some nice perspective from different directions.  I won't accept any because I see no reasonable criterion for singling out just one.,2010-07-26T13:39:31.297,89,CC BY-SA 2.5,
527,581,2,"In my problem I have an array of real valued data which I am modeling as a HMM (speficially a mixture of multiple density functions each with unknown parameters).   For now I assume that I know the state transition probabilites.   What I mean by Viterbi Trainig is the following algorithm.

1) Arbitrarily assign a state to each data point ( initialization)
2) Perform MLE of the density function parameters.
3) Re-estimate state for each point ( can be done with Viterbi Alg).
4) Goto step 2 and repeat unless stopping criteria is met.",2010-07-26T16:05:22.033,99,CC BY-SA 2.5,
528,632,0,"Isn't that a function of estimator is still an estimator? I still don't know \sigma, only X_i.",2010-07-26T16:45:07.923,,CC BY-SA 2.5,user88
529,632,0,"ok, then you will possibly estimate the square root of the variance of the estimation of the square root of the variance... right :) should be something like $\hat{\sigma}/n$ ?",2010-07-26T17:09:48.947,223,CC BY-SA 2.5,
530,567,1,"+1 For this resource. As the name says, excellent for a practical approach to engineering problems.",2010-07-26T17:42:34.383,77,CC BY-SA 2.5,
531,608,0,"Thanks mbq - I didn't understand what you meant/needed in terms of an illustrative dataset.  Also, in general, in terms of empirical demonstration I'd imagine that a single example from a single seed won't really do the trick.  I was imagining something like a metric from the employment of a cross-validation method being shown to be correlated with the calculated corresponding information criterion.  In that way the ICs don't necessarily need to give different answers in order to demonstrate the relation between the IC and the cross validation method.",2010-07-26T18:04:31.987,196,CC BY-SA 2.5,
532,636,1,"I didn't downvote, but I must say that some explanation or reasoning why to use random forest could make this answer very much more interesting. ;)",2010-07-26T18:14:28.533,190,CC BY-SA 2.5,
533,636,0,"I didn't downvote either, but why random forest over SVM (for instance)?",2010-07-26T18:16:06.860,5,CC BY-SA 2.5,
534,635,0,"Thanks a lot for your answer ! I have read this book from first to last page, but I think it was edition 1... I didn't know it was available online.",2010-07-26T18:24:35.673,223,CC BY-SA 2.5,
535,636,0,"I didn't downvote either, it didn't let me ;-) . @Peter extended. @Shane and why binomial regression? This is just my favorite algorithm and it may work well in this case.",2010-07-26T18:25:43.130,,CC BY-SA 2.5,user88
536,636,0,"+1 @mbq because to some extent, that's the ""standard"" and/or ""obvious"" answer when it comes to regression. :) (hope that's vague enough...) When it comes to a machine-learning approach, it doesn't seem to me that there is an obvious answer; but your experience is more than enough.",2010-07-26T18:26:52.067,5,CC BY-SA 2.5,
537,625,1,"Rob, your book is great!",2010-07-26T18:37:01.453,74,CC BY-SA 2.5,
538,26,13,"I think this question is ok. Actually, it was the most upvoted example on topic question on Area 51. Basics are ok here!",2010-07-26T18:52:44.213,190,CC BY-SA 2.5,
540,609,3,Just to clarify for anyone unfamiliar with Wasserstein distances who reads this and gappy's answer: the L^2 Wasserstein distance (W2) is *not* the same as the Cramer-von Mises distance.,2010-07-26T18:55:17.140,89,CC BY-SA 2.5,
541,130,0,This should be community wiki since it is subjective.,2010-07-26T18:55:47.077,5,CC BY-SA 2.5,
544,645,0,"This seems very vague to me.  What kind of data, and what kind of analysis?  Also, this should be community wiki if it is subjective.",2010-07-26T19:36:21.660,5,CC BY-SA 2.5,
545,650,0,"Peter, you beat me to the punch! I completely agree with storing data as text, though depending on the size (hundreds of millions of obs) it may be necessary to move into a map-reducible database (e.g., Pig, Cassandra, or one of the NoSQL options).",2010-07-26T19:37:03.767,302,CC BY-SA 2.5,
547,652,7,Related: http://stats.stackexchange.com/questions/421/what-book-would-you-recommend-for-non-statistician,2010-07-26T19:40:56.657,5,CC BY-SA 2.5,
550,652,1,You tagged this as bayesian and machine-learning.  What kind of data analysis are you interested in?,2010-07-26T19:46:47.423,5,CC BY-SA 2.5,
551,650,0,Oh ok interesting! So just take data for each variable and lay it out in row-column format and get to number crunching eh? Are there any tools I should be looking at or should I just be programming something?,2010-07-26T19:47:31.743,9426,CC BY-SA 2.5,
552,652,0,"Shane: Honestly, I don't really know yet. What kind of data analysis is there? Should I be posing this as yet another question for the site?",2010-07-26T19:49:24.203,9426,CC BY-SA 2.5,
553,647,0,"I can do Monte Carlo, I just wanted to do in a more 'sciency' way; still you're right that the distribution is not normal, so this sd will be useless for testing.",2010-07-26T19:50:17.197,,CC BY-SA 2.5,user88
554,650,0,R is a very extensive (and free) toolkit/programming language/library for statistics. My favorite for most things is however Python with SciPy / NumPy,2010-07-26T19:50:23.187,190,CC BY-SA 2.5,
555,648,0,I thought it shows the binomial distribution; I don't think that its asymptotics have a direct link with CLT.,2010-07-26T19:52:46.723,,CC BY-SA 2.5,user88
556,655,1,May be a bit too academic for myself being such a beginner...,2010-07-26T19:53:40.310,9426,CC BY-SA 2.5,
558,652,0,"There are many kinds.  :)  Those are just two specific areas, so it seems odd for tags on such a general question.",2010-07-26T19:54:21.243,5,CC BY-SA 2.5,
559,652,0,"@Justin If you don't know it yet, this question is too broad and vague. A nice thing to learn what a field contains, is to look on the tags of this sites and see what questions match with it. Also wikipedia and the books you bought can give you and idea of what you want (although the books maybe not name the field wherein they operate, more on the practical overall part)",2010-07-26T19:54:54.987,190,CC BY-SA 2.5,
560,648,2,bean machine by the author of the package animation... http://yihui.name/en/wp-content/uploads/2010/07/animation-useR2010-Xie.pdf,2010-07-26T19:55:09.510,223,CC BY-SA 2.5,
565,648,1,@mbq take a look at http://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation,2010-07-26T20:01:14.113,223,CC BY-SA 2.5,
566,658,0,what is the relation between the Cauchy distribution and the CLT or the failling of the CLT ?,2010-07-26T20:02:15.373,223,CC BY-SA 2.5,
568,656,9,That's great.  The standard way of dealing with outliers.,2010-07-26T20:06:32.370,5,CC BY-SA 2.5,
569,660,0,"What do you mean by *types* of ""textual"" content?",2010-07-26T20:08:17.403,68,CC BY-SA 2.5,
570,648,0,"@robin I have wrote about it, what's the problem?",2010-07-26T20:09:41.097,,CC BY-SA 2.5,user88
571,652,0,"@Peter Smit: Thanks I'll give that a shot.
@Shane: Thanks as well... I was trying to give some idea of context with my tags.

Beware the noobs!",2010-07-26T20:10:14.077,9426,CC BY-SA 2.5,
572,658,1,@robin Consult http://en.wikipedia.org/wiki/Cauchy_distribution#Properties,2010-07-26T20:12:30.357,,CC BY-SA 2.5,user88
573,652,0,@robin There are only 7 users able to vote for closing... so it can take a small while,2010-07-26T20:17:54.540,190,CC BY-SA 2.5,
574,600,0,"Regarding 1., that's correct. Regarding 2. In principle all of the above distances could carry over to R^n, however I don't know of popular non-parametric tests based on *any* distance. It would be interesting to know if there are any.",2010-07-26T20:18:19.937,30,CC BY-SA 2.5,
575,641,0,"I think a general wiki for where to get data is great, with a section for survey data.",2010-07-26T20:21:15.657,302,CC BY-SA 2.5,
579,660,0,Could you show some sample data?,2010-07-26T20:27:15.987,,CC BY-SA 2.5,user28
581,672,4,Related: http://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english,2010-07-26T20:34:14.470,5,CC BY-SA 2.5,
589,262,4,"""Hardly publication quality""???? I realise that it isn't perfect - the phrase ""...should you get you started.."" covers that bit. But with a little additional work, i.e. axis labels, I would say it's fine. BTW, what journals do you publish in?",2010-07-26T21:20:59.987,8,CC BY-SA 2.5,
590,652,0,@robin @peter: It isn't clear to me that it's an exact duplicate either.  Are you both referring to the one I linked above or another one?,2010-07-26T21:23:14.653,5,CC BY-SA 2.5,
592,667,3,So probability is pure mathematics and statistics is applied mathematics?,2010-07-26T21:24:35.293,327,CC BY-SA 2.5,
593,673,1,So statistics is synonymous with data analysis?,2010-07-26T21:25:04.843,327,CC BY-SA 2.5,
594,652,0,"@shane I agree that it is not exact duplicate, but then it is too broad or not enough and the title is quite similar. In addition, my view is that someone asking for a book on stat.stack could at least say for what purpose and for what level.",2010-07-26T21:29:12.007,223,CC BY-SA 2.5,
595,652,0,@robin: I agree 100% with that.,2010-07-26T21:33:06.110,5,CC BY-SA 2.5,
597,675,3,"So statistics observes what happens in the physical world, theorizes about the underlying process, and then having found the process, uses it in the sense of probability to predict what will happen next?",2010-07-26T21:34:19.947,327,CC BY-SA 2.5,
598,651,0,"I work in medical/epidemiological research, and I see this book on colleagues' shelves all the time.  Still haven't read it myself, but I can attest to its popularity.",2010-07-26T21:35:13.770,71,CC BY-SA 2.5,
599,673,4,I don't see any distinction.,2010-07-26T21:39:17.563,25,CC BY-SA 2.5,
600,667,4,Statistics may be applied and may be not; still the concept of data is always present.,2010-07-26T21:42:34.723,,CC BY-SA 2.5,user88
601,685,0,I feel that your question is not precise enough to get a reasonable answer.,2010-07-26T21:44:31.073,,CC BY-SA 2.5,user28
602,685,0,It could be rephrased as: In what ways are statistics misleadingly reported or cited?,2010-07-26T21:51:18.217,327,CC BY-SA 2.5,
603,479,0,"Found a way to do this (high transparency overlay on the ""danger"" part of the graph) with our toolset, thanks!",2010-07-26T21:53:17.343,259,CC BY-SA 2.5,
604,685,1,"Even if it is not off-topic, it should be community wiki.",2010-07-26T22:08:47.550,,CC BY-SA 2.5,user88
605,687,0,"About the glass, I think that just the boundary between phases lies in the half of its height.",2010-07-26T22:10:48.190,,CC BY-SA 2.5,user88
607,685,0,Your re-stated question is much better. I would either suggest asking another question along those lines or better still edit the current one along the lines of your comment.,2010-07-26T22:20:56.700,,CC BY-SA 2.5,user28
610,13,2,"I bet they used discriminant analysis before logistic regression, as DA was invented well before LR",2010-07-26T22:56:40.980,74,CC BY-SA 2.5,
612,194,0,"@robin, that would be the hard part. Sometimes when thousands of variables influencing something, it's hard to make sense of the patterns that emerged.",2010-07-27T00:09:31.367,175,CC BY-SA 2.5,
613,675,1,"I'm not a statistician, but from my understanding I'd say, yes, that *part* of what statistics does.",2010-07-27T00:10:53.173,89,CC BY-SA 2.5,
615,349,0,Since you are only interested in the median why couldn't you make the bins wider at higher values of your variable?,2010-07-27T00:23:15.770,196,CC BY-SA 2.5,
617,702,1,"With a millon points and an 8 parameter model, a goodness of fit test like chi-squared tells me that there is essentially no chance that the model is correct. (Which is not surprising, as there are endless factors influencing reality that are not in the model)

RMSE gives me a sense as to how good the model fits the data, but does not give me a sense of whether there is a better model",2010-07-27T01:07:20.330,72,CC BY-SA 2.5,
618,702,0,"Well in order to find out if there is a better model, you could either experiment with different formulations or you could use various plots (e.g.,  exit times vs time) to see if the data is consistent with your model assumptions. You could also plot predicted exit times for a small sample selected at random vis-a-vis actual times to for model improvement ideas.",2010-07-27T01:17:13.693,,CC BY-SA 2.5,user28
620,712,6,"What is your data, and what do you want to do with the anonymized data?",2010-07-27T04:42:39.970,190,CC BY-SA 2.5,
621,495,2,I have watched all of those videos. It's a very good introduction to probability and counting.,2010-07-27T06:12:47.193,339,CC BY-SA 2.5,
623,732,14,"I don't mind the down vote, but I maintain that this is a deep statistical point, not to be taken lightly.  ;-)",2010-07-27T07:18:30.913,251,CC BY-SA 2.5,
625,242,0,"I will come back to tell you what the results were, but it will be a while as I am working my way through this alongside lots of other tasks. Wasn't sure what you meant about ""pass over the question about random variable""? Is there a question you recommend I look at?",2010-07-27T07:46:32.190,199,CC BY-SA 2.5,
626,744,16,"I like this one, could be put as an advise when people  write questions on this site ?",2010-07-27T08:48:41.993,223,CC BY-SA 2.5,
627,705,0,"Srikant, thanks for the response!  I'm not sure what you mean by contour plots of covariances (obs v est) -- could you elaborate?  Thanks.",2010-07-27T08:52:31.750,251,CC BY-SA 2.5,
629,675,24,Induction vs Deduction?,2010-07-27T09:14:39.757,434,CC BY-SA 2.5,
630,686,0,I hadn't seen these ones before. They look good.,2010-07-27T09:40:01.247,183,CC BY-SA 2.5,
632,734,1,"i think only one of the classes in the Iris data set is linearly separable. (From the OP's Question, i think he's after data sets w/ *only* linearly separable classes).",2010-07-27T09:52:30.323,438,CC BY-SA 2.5,
634,734,0,"@doug, good point, still I don't think that one can get any non-synthetic fully linearly separable problem. Nevertheless I'll wait for the OP's reaction.",2010-07-27T10:13:30.293,,CC BY-SA 2.5,user88
636,705,0,"See this: http://en.wikipedia.org/wiki/Level_set. Let Sigma be a a 2 dimensional covariance matrix and Y ~ N(0, Sigma). An iso-contour line would plot the set of points Y for which f(Y|sigma) = c where c is a constant. Note that Y is a 2-dimensional vector. You would choose various values of c and hence obtain different iso-contour lines which would give you a sens of the spread of the distribution.",2010-07-27T10:54:50.017,,CC BY-SA 2.5,user28
640,748,53,Still it looks promising.,2010-07-27T11:20:02.893,,CC BY-SA 2.5,user88
642,741,0,"Thanks, this is just what I was looking for.

This is essentially a million subjects each with an entry and exit time.

Yes we are conditioning to account for the censoring.",2010-07-27T12:03:40.630,72,CC BY-SA 2.5,
644,337,0,I'm not interested into *truth* but in getting a function that works. I'm a bioinformatician and taught not to seek dogmatic *truth* but to seek statistics that work. I don't think that there work done with the kind of data that I want to work with that specifics what entropy works best. That's kind of the point why I want to work with the data.,2010-07-27T12:17:57.387,3807,CC BY-SA 2.5,
645,337,2,"Right, but this is not a discussion about dogmatic truths but about words. You have asked about entropy, so I answered about entropy. Because now I see that you indeed need an answer about time series descriptors, write a question about time series descriptors, only then you'll get an useful answer.",2010-07-27T12:32:45.983,,CC BY-SA 2.5,user88
647,768,0,So we can only find patterns that we were looking for in the first place?,2010-07-27T12:49:35.373,327,CC BY-SA 2.5,
648,769,4,Statistics is not a subset of data analysis -- it is a theory that is used in data analysis.,2010-07-27T12:56:46.077,,CC BY-SA 2.5,user88
649,618,1,"Nice, but I can't see anyone trying to draw a conclusion of causality there. Or are mexican lemon-truck drivers notoriously dangerous once they get over the border?",2010-07-27T12:57:59.863,270,CC BY-SA 2.5,
650,674,1,What if the random errors become greater than the observable factors over time?,2010-07-27T12:59:24.027,327,CC BY-SA 2.5,
652,579,0,"@mbq. I was thinking of Shao 1995 which is, indeed, only for linear models. I don't know if the result has been extended to other models.",2010-07-27T13:30:26.377,159,CC BY-SA 2.5,
653,769,6,Whatever you do in Excel does not count.  Just kidding...,2010-07-27T13:36:13.320,334,CC BY-SA 2.5,
654,769,2,@Dirk: Wow...your hatred towards Excel knows no bounds.  :),2010-07-27T13:37:37.183,5,CC BY-SA 2.5,
655,769,0,This should probably be community wiki since it is subjective/argumentative.,2010-07-27T13:40:46.570,5,CC BY-SA 2.5,
658,772,0,"Wow, Shane, thanks for the very fast response! I'll look into those references!",2010-07-27T14:02:33.473,445,CC BY-SA 2.5,
660,744,9,Absolutely...asking the right question is one of the most important skills.,2010-07-27T14:17:30.600,5,CC BY-SA 2.5,
661,790,1,maybe a reference to the paper could help ?,2010-07-27T14:52:37.723,223,CC BY-SA 2.5,
662,790,1,Slightly related question: http://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-var/,2010-07-27T14:53:37.487,5,CC BY-SA 2.5,
664,349,0,drknexus - because we don't know what the largest bin should be.,2010-07-27T14:58:50.750,247,CC BY-SA 2.5,
665,47,0,"@robin: thanks for comment. i haven't seen much research in this area and the input from data provider was minimal [so far they were not interested/capable of investigating it further]. after initial exploration of data it was quite clear for me that there are couple of distinctive patterns [examples being 'heavy downloaders' in just a few locations or 'frequent hoppers' with lots of small sessions in large number of locations].
my goal at this stage was to try to use data itself to tell me how can i divide it best and minimize subjective input.",2010-07-27T14:59:00.983,22,CC BY-SA 2.5,
666,790,0,"You may find this paper of interest, which discusses the log mean and how it refines the 'arithmetic mean - geometric mean' inequality: http://www.ias.ac.in/resonance/June2008/p583-594.pdf",2010-07-27T15:03:53.810,81,CC BY-SA 2.5,
667,704,1,"but then how to choose eta, and what doe sthis then mean statistically? i.e. how to form confidence intervals for the median from this result?",2010-07-27T15:08:20.650,247,CC BY-SA 2.5,
668,428,4,"So, according to a long lost colleague, the best apropoach seems to be Chiranjeeb Buragohain and Subhash Suri. Quantiles on Streams.
http://www.cs.ucsb.edu/~suri/psdir/ency.pdf

I also like Ian's approach, as these medians of smaller data sets will converge to a normal distribution, and so I can form conf intervals for the medians.",2010-07-27T15:10:34.293,247,CC BY-SA 2.5,
669,1,12,"Although I've accepted an answer, I would recommend that interested people should look at all the answers.",2010-07-27T15:22:18.650,8,CC BY-SA 2.5,
670,801,1,"sorry, i should have mentioned that i need to do this in an automated way. the option of ""doing it multiple times until i find the one that best suits my purpose"" won't work for me. has to be done computationally...",2010-07-27T15:34:03.697,476,CC BY-SA 2.5,
671,794,0,oh the bayesian is soooo Good...,2010-07-27T15:39:02.963,223,CC BY-SA 2.5,
672,799,2,"... and in that vein also the lme4 package (which I find easier to use than lme or nlme) and related packages from Baayen's above referenced book, languageR.",2010-07-27T15:40:33.200,196,CC BY-SA 2.5,
674,804,9,always these bayesian guys...,2010-07-27T15:45:32.327,442,CC BY-SA 2.5,
675,720,3,"I found all the answers to this question helpful, but I think that this one is the most practical.",2010-07-27T15:46:34.823,266,CC BY-SA 2.5,
677,779,0,"As I always understood it, the central limit theorem does not postulate something about averaging a large number of iid random variables. Rather, it states that when sampling means, the distribution of the means becomes normal (independent of the distribution underlying what is sampled from). So I question whether the antecedent for your question holds.",2010-07-27T15:57:43.397,442,CC BY-SA 2.5,
678,779,0,"But, if the sampling mean becomes normal irrespective of the distribution of the underlying distribution then is that not the same as saying 'averaging a large number of iid random variables' get us a normal distribution. To me they seem to be equivalent statements.",2010-07-27T16:11:46.867,,CC BY-SA 2.5,user28
679,660,0,"@Srikant Vadali - sample data could be press releases, news stories, etc .. the textual data would be free-form, likely obtained from rss feeds or similar. Market data for a given company is what I'm looking to analyze/correlate. So maybe Blogger Bill writes a story about an upcoming VMware feature release, and VMW jumps 10%. (Oversimplified, I know)",2010-07-27T16:11:57.037,292,CC BY-SA 2.5,
680,677,0,that looks like a really promising start :),2010-07-27T16:12:31.903,292,CC BY-SA 2.5,
681,750,20,"I hate this quote. It makes professions using statistics look like you could cheat. But, when someone profoundly uses statistics one knows that actually you cannot cheat. Because when provided with enough information about the statistical procedures used, one can draw a conclusion on the soundness of the procedures/results. If not enough information on the statistical (and other) procedures are provided, you should immediately question the results.",2010-07-27T16:15:40.670,442,CC BY-SA 2.5,
682,811,0,is it ? then we are all intelligent persons here:),2010-07-27T16:22:37.590,223,CC BY-SA 2.5,
683,799,0,"thanks for the comment, I totally agree with you. lme4 is simply the best around.",2010-07-27T16:22:39.947,447,CC BY-SA 2.5,
684,779,0,"Not in my eyes (but i would like to be convinced otherwise).
In the one case (the one i think of being meant by CLT) you draw samples from one distribution. Their means are normally distributed.
What i understand from the question and the quote ""average a large number of iid random variables"" is sth differnt: individual instantiations from different iid random variables determine (or make up) a trait. Hence, no averaging (i.e., computing a mean) from a single distribution and, hence, no application of the CLT.
I think mbq's answers points to the same issue.",2010-07-27T16:27:57.773,442,CC BY-SA 2.5,
686,779,1,Well the distribution need not be identical if some conditions hold. See: http://en.wikipedia.org/wiki/Central_limit_theorem#Lack_of_identical_distribution,2010-07-27T16:42:43.140,,CC BY-SA 2.5,user28
687,787,0,Hillaire Belloc?  Nice work on digging that up.,2010-07-27T16:43:12.517,5,CC BY-SA 2.5,
689,307,5,"As can be read in my and dave's post, saturated models do not per definition lead to perfect fit. but if you use the n-1  polynominal as the model they will. see Sue Doe Nihm's seminal paper on this topic http://psych.fullerton.edu/mbirnbaum/papers/Nihm_18_1976.pdf",2010-07-27T16:52:10.383,442,CC BY-SA 2.5,
690,783,0,I would love to take this one as my accepted answer ! too good to be true !,2010-07-27T17:10:56.593,223,CC BY-SA 2.5,
696,828,0,"Well, mutexes needed. As I commented on your answer, I only saw the first (raw) version and figured well, I may expand on mc and Rmpi.  And then you did and I look like a copycat.  Such is life.",2010-07-27T17:15:55.587,334,CC BY-SA 2.5,
697,828,0,"On the other hand, my answer is derived from reading your paper/presentation in the past.  So I guess I'm copying you as well.",2010-07-27T17:16:37.660,5,CC BY-SA 2.5,
700,825,0,I just don't see how the fact that importing data and running SVM has any relevance to the question.  That's why I think it's more of an SO question.  But I could see Xrefs as being a good long-term solution since it is R...,2010-07-27T17:20:58.323,5,CC BY-SA 2.5,
701,822,0,"""market basket analysis"" seems like it's what I'm looking for, thanks for input.",2010-07-27T17:22:33.487,488,CC BY-SA 2.5,
703,830,5,"I disagree somewhat. Revolution does a great sales job in getting mindshare (as evidenced by your post) but as of right now there is very little in the product you would not already get with the normal R (at least on Linux). Intel MKL, sure, but you can get Goto Blas.  On Windows, they offer doSMP which helps as multicore cannot be built there.",2010-07-27T17:42:14.653,334,CC BY-SA 2.5,
704,658,0,"The CLT requires that the MGF's exist in a neighborhood of 0. The Cauchy distribution does not have that property. CLT Win. 

Cauchy doesn't even satisfy the weaker requirements of a stronger version of CLT where all that is required is that mean and variance exist. The Cauchy distribution shows that the mean is required to exist for the CLT to hold. It doesn't make the CLT fail.",2010-07-27T17:46:38.767,62,CC BY-SA 2.5,
705,744,7,"I remember once where a private industry company commissioned a mathematician to solve a garbage collection routing problem. Long story short, the mathematician complained that the company was only interested in finding a ""close enough"" solution rather than an optimal solution. I think, ultimately he was fired, and an operations researcher was brought in instead.",2010-07-27T17:59:21.297,59,CC BY-SA 2.5,
706,658,0,"@Baltimark You have misunderstood my post -- its obvious that Cachy is not covered by CLT because of CLT assumptions, otherwise it would be impossible to prove CLT. I have gave this example because people believe that CLT works for all distributions; probably ""fail"" is not a perfect word, but still I don't think it is a reason for downvote. Ok, I have even changed it to not applicable.",2010-07-27T18:05:10.507,,CC BY-SA 2.5,user88
707,658,0,I prefer your edit. The Cauchy distribution is definitely very cool.,2010-07-27T18:11:05.113,62,CC BY-SA 2.5,
708,834,4,I would suggest reviewing the wiki link: http://en.wikipedia.org/wiki/Akaike_information_criterion and then edit the question so that you can highlight the aspect of AIC that you do not understand.,2010-07-27T18:14:03.267,,CC BY-SA 2.5,user28
709,658,0,"Yup; the another nice ""paradox"" comes from basic physics -- if you have a particle that has flown between two detectors with an uniform speed and you've measured the distance and time-of-flight with a normally distributed error, the speed obtained from v=s/t is Cauchy distributed, so one may say that it is undefined ;-)",2010-07-27T18:19:06.127,,CC BY-SA 2.5,user88
710,834,2,Read also this question: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other,2010-07-27T18:22:29.223,,CC BY-SA 2.5,user88
711,827,0,"I mainly use multicore, still I like snowfall more than snow and Rmpi for its fault tolerance and clean interface.",2010-07-27T18:25:39.873,,CC BY-SA 2.5,user88
712,754,0,"I guess I should remove it... poor Karl Pearson, one of the inventor of hypothesis testing not understood by the 21st century ... I would vote up if I could, but it's me that put it here :)",2010-07-27T18:31:32.247,223,CC BY-SA 2.5,
713,478,6,The Statistical sleuth is used a the textbook on that great introductory course (there are 64 lectures in total)  http://video.google.com/videoplay?docid=-3474013489970580510&hl=en&emb=1#,2010-07-27T18:36:31.503,339,CC BY-SA 2.5,
714,842,1,+1 Great point about how this doesn't deal with splitting up the cross-validation.,2010-07-27T18:45:28.817,5,CC BY-SA 2.5,
717,847,3,Have you tried asking Google itself?,2010-07-27T19:24:25.663,,CC BY-SA 2.5,user88
718,846,0,Maybe Wikipedia will be sufficient here: http://en.wikipedia.org/wiki/F-test#Regression_problems,2010-07-27T19:34:49.773,,CC BY-SA 2.5,user88
719,806,3,What do you precisely mean by normalization?,2010-07-27T20:13:18.697,,CC BY-SA 2.5,user88
720,744,2,@dassouki I think the quote is more about the question .... something like science is not about finding good answer but about finding good questions !,2010-07-27T20:21:49.737,223,CC BY-SA 2.5,
721,680,0,"This was awesome. Thank you so much for this response. You've given me a great jumping off point. Any books you recommend since you seem to ""get"" where I'm at.",2010-07-27T20:24:17.453,9426,CC BY-SA 2.5,
723,680,0,"you're very welcome. books:  

Statistics in Plain English to start. 
Multivariate Data Analysis by Hair after that .   
  
These are good web resources:  
  
http://www.itl.nist.gov/div898/handbook/  , 
http://www.statsoft.com/textbook/",2010-07-27T20:54:37.240,74,CC BY-SA 2.5,
725,349,0,"Do you have **any** intuition as to what the range will be?  If you're fairly sure that over half of the answers will be below number N, then you can make your last bin as large as you want.  Maybe your last bin is all numbers greater than 1 trillion - would that be high enough?  With the amount of memory in modern systems you can store a LOT of bins and achieve fairly high resolution. In terms of data structures, we're not talking anything fancy and memory intensive here.",2010-07-27T22:17:59.170,54,CC BY-SA 2.5,
726,850,0,"It's a good idea, but

    ""mtcars$dummy <- 1; lrm(am ~ dummy, data=mtcars);"" 

gives back:

    singular information matrix in lrm.fit (rank= 1 ) Offending variable(s):
    dummy 
    Error in lrm(am ~ dummy, data = mtcars) : 
    Unable to fit model using ‚Äúlrm.fit‚Äù",2010-07-27T23:03:52.330,501,CC BY-SA 2.5,
727,858,0,This will due for now.  Is the fact that this doesn't work in the Design package a bug?,2010-07-27T23:06:16.693,501,CC BY-SA 2.5,
728,734,0,"yes - only one of the classes in the iris data set is linearly separable - so, I've now started experimenting with algorithmically generated datasets.",2010-07-27T23:22:51.767,130,CC BY-SA 2.5,
732,841,3,I'm not sure why the information that the observations are paired is important to the hypothesis being tested; could you explain?,2010-07-28T00:11:01.913,196,CC BY-SA 2.5,
733,834,2,Consider asking the general question about the AIC separately from the stata question.,2010-07-28T00:16:29.750,196,CC BY-SA 2.5,
735,674,0,In that case you re-work your model as it is no longer consistent with reality.,2010-07-28T01:06:13.940,,CC BY-SA 2.5,user28
736,815,0,"the maximum entropy principle is also another reason why the Gaussian distribution is used.
For example, what are good reasons for using Gaussian errors in the linear model, except tractability ?",2010-07-28T02:10:27.877,368,CC BY-SA 2.5,
739,705,0,"@Srikant, thanks for the suggestion.  I spent some time trying it out and it seems like a good start at getting a quick visual comparison, especially when the fit is bad.",2010-07-28T04:35:31.360,251,CC BY-SA 2.5,
740,870,0,your reference about q-value should be http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1074290335,2010-07-28T05:54:26.307,223,CC BY-SA 2.5,
741,866,1,"""Say I want to estimate a large number of parameters"" this could be made more precise: What is the framework ? I guess it is linear regression?",2010-07-28T05:59:31.147,223,CC BY-SA 2.5,
742,870,1,"The Benjamini-Hochberg procedure is not for calculating the FDR, it is for controlling the FDR (keeping it under a predefined threshold)",2010-07-28T06:11:26.677,223,CC BY-SA 2.5,
743,870,0,"Your question, as it stands, is difficult to understand. What do you mean by ""referred to"" ?",2010-07-28T06:15:01.757,223,CC BY-SA 2.5,
744,858,1,"Rather a non-implemented feature, still you can send a bug report.",2010-07-28T09:23:10.217,,CC BY-SA 2.5,user88
747,835,0,aligatou gozaimasu,2010-07-28T09:35:29.553,223,CC BY-SA 2.5,
750,789,2,"This is Creative Commons but does not allow derivates... :( That way, I cannot use the bits in my own course material, just the bits I need them to know...",2010-07-28T11:00:41.763,107,CC BY-SA 2.5,
751,880,0,"But still, what do you want to measure with CV and in what purpose? To get a cutoff of attribute number?",2010-07-28T11:15:17.963,,CC BY-SA 2.5,user88
752,880,0,"@mbq: thanks for the advice. I have edited the question accordingly, hope it is more clear now !",2010-07-28T11:26:47.510,223,CC BY-SA 2.5,
753,887,0,"What do you mean by: ""we would like to test whether it is an vis-a-vis the general population""? and ""single sample S""?",2010-07-28T12:06:44.597,,CC BY-SA 2.5,user28
755,886,3,I don't see the interest of these questions about trying to bridge a fictive gap. what is the aim of all that ? in addition there are a lot of others idea that are fundamental in statistic... and loss function is at least 100 years old. can you reduce statistic like that ? maybe your question is about fondamental concept in datamining/statistic/machine learning however you call it ... Then the question already exists and is too wide http://stats.stackexchange.com/questions/372/what-are-the-key-statistical-concepts-that-relate-to-data-mining/381#381.,2010-07-28T12:19:52.493,223,CC BY-SA 2.5,
756,890,0,Maybe you can precise your problem?,2010-07-28T12:44:10.573,,CC BY-SA 2.5,user88
759,886,0,"Well, I do not know much about machine learning or its connections to statistics. In any case, look at this question: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning which suggests that at the very least that the approaches to answer the same questions are different. Is it that 'unnatural' to wonder if there is some sort of link between them? Yes, I agree that there lot of ideas in statistics. That is why I have fundamental in quotes and restricted the scope to estimating parameters of interest.",2010-07-28T13:13:32.700,,CC BY-SA 2.5,user28
760,886,0,"@Srikant link between what ? note that I really like to search link between to well defined objects, I find it really natural.",2010-07-28T13:20:05.620,223,CC BY-SA 2.5,
762,894,28,"It is not quite correct to say ""you cannot change z"". In fact, you have to change z to make the sum equal 10. But you have no choice (no freedom) about what it changes to. You can change any two values, but not the third.",2010-07-28T14:29:49.740,25,CC BY-SA 2.5,
764,899,0,Can you assume that the other two groups are from different Normal distributions?,2010-07-28T14:37:00.770,8,CC BY-SA 2.5,
765,908,0,"said ""You miss one important issue -- there is almost never such thing as T[i]""

 I wanted the answer to focus on the problem of selecting the number of variables. Construction (which I agree are not perfect) of T[i]  are discussed here http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification

Sometime, it is also usefull to discuss problem separatly.",2010-07-28T15:21:55.357,223,CC BY-SA 2.5,
766,902,0,This is an interesting paper.  Could you add to your answer with some more references and include (for instance) the paper's title and author?  Are there any particular academic research labs working in this area that would be relevant?,2010-07-28T15:37:02.833,5,CC BY-SA 2.5,
767,908,1,"@robin But here you can't tear those apart. The most of algorithms mentions in that question were created to address this issue -- forward selection is to remove correlated features, backward elimination is to stabilize the importance measure, mcmc is to include correlated features...",2010-07-28T15:37:10.917,,CC BY-SA 2.5,user88
768,908,0,"@robin the idea of making some exact importance measure was a base for so-called filter algorithms which are now mainly abandoned since they were just too weak. They have the advantage that they are computationally cheap, still this is not worth it.",2010-07-28T15:42:44.737,,CC BY-SA 2.5,user88
769,910,0,I'm not sure that this is entirely correct.  In what sense do machine learning methods work without parameter estimation (within a parametric or distribution-free set of models)?,2010-07-28T15:48:18.650,39,CC BY-SA 2.5,
770,910,1,"You are estimating/calculating something (the exact term may be different). For example, consider a neural network. Are you not calculating the weights for the net when you are trying to predict something? In addition, when you say that you train to match output to reality, you seem to be implicitly talking about some sort of loss function.",2010-07-28T15:59:39.143,,CC BY-SA 2.5,user28
771,779,1,@Henrik Is there any meaningful difference between a single sample from each of N independent and *identically distributed* RVs and N independent measurements of a single RV?,2010-07-28T16:03:13.433,174,CC BY-SA 2.5,
772,899,0,"@cgillespie: it is the same group, just with two modes, I guess, and therefore I probably cannot assume this.",2010-07-28T16:03:55.587,219,CC BY-SA 2.5,
773,910,0,"@John, @Srikant Learners have parameters, but those are not the parameters in a statistical sense. Consider linear regression y=a*x (without free term for simp.). a is a parameter that statistical methods will fit, feed by the assumption that y=a*x. Machine learning will just try produce a*x when asked for x within the range of train (this makes sense, since it is not assuming y=a*x); it may fit hundreds of parameters to do this.",2010-07-28T16:07:24.413,,CC BY-SA 2.5,user88
774,909,0,"`optimize` requires two distributions to be side-by-side as I understand. In my case one is inside the other, i.e., the values from the second population are on both side of the limits.",2010-07-28T16:09:25.503,219,CC BY-SA 2.5,
775,11,1,"Interpolation involves 3 things: 1) a class of functions to interpolate, e.g. sound, pictures, terrain; 2) grids of known / unknown points, 4 cases regular <-> scattered; and 3) a model of noise added to 1). There are many many interpolation methods for various cases, most ad hoc, not ""in common use"";
even IDW has variants. Can you describe what you're interpolating ?",2010-07-28T16:11:35.330,557,CC BY-SA 2.5,
776,910,0,"@Srikant About the second issue; yes, it can be called a loss, but on unseen data, not the training one. When statistical models are used for prediction, it is assumed that the train is a perfect representation of reality and contains the full scope of the true process, which is not the case of machine learning.",2010-07-28T16:18:22.453,,CC BY-SA 2.5,user88
778,349,0,"Any intuition? yes. And your approach could work in general. However, in this case we can not have a lot of memory/computation. It is in a networking application where the device could see tens of thousands of items per second, and have VERY little processing left over for this purpose. Not the ideal/typical scenario, I know, but that is what makes it interesting!",2010-07-28T16:43:50.167,247,CC BY-SA 2.5,
782,924,0,"By the way, is this a duplicate of what you posted here? http://stats.stackexchange.com/questions/920/test-if-probabilities-are-statistically-different",2010-07-28T17:42:49.030,,CC BY-SA 2.5,user28
783,924,0,I didn't think it was a duplicate. One question deals with probabilities and this one deals with a discrete variable.,2010-07-28T17:43:42.630,559,CC BY-SA 2.5,
787,926,0,"#JoFrhwld, would you happen to know the Matlab function to calculate this? I don't have access to R :(",2010-07-28T18:01:10.910,559,CC BY-SA 2.5,
788,930,0,Indeed we have suggested something like this in this topic; I'll dig for a link.,2010-07-28T18:05:47.203,,CC BY-SA 2.5,user88
789,930,0,And I think your criterion for 'intervality' is valid only for a uniform distribution.,2010-07-28T18:10:53.123,,CC BY-SA 2.5,user88
790,926,0,"Sorry, I don't have access to Matlab! I'd just google around for it. It's surprising you wouldn't have access to R, considering it's free and platform independent.",2010-07-28T18:14:50.540,287,CC BY-SA 2.5,
791,931,0,"And probably not easily put on an iPod, if that is essential to you.",2010-07-28T18:18:25.183,561,CC BY-SA 2.5,
792,929,1,I don't think it is a valid question here; just check out wiki and references there: http://en.wikipedia.org/wiki/Probability_interpretations,2010-07-28T18:20:32.713,,CC BY-SA 2.5,user88
793,929,0,"That wikipedia article is filled with ""dubious-discuss"" and other tags, so the question would become how comprehensive is that wiki article, what interpretations are missing from it.",2010-07-28T18:26:20.673,560,CC BY-SA 2.5,
796,928,0,"By the way, P70 - P50 represents the percentage of people who are between the 70th percentile and 50th percentile and that percentage is 20. Clearly that is the same as P50 -P30. When assessing if differences are equal I do not think you should look at the underlying scores.",2010-07-28T18:44:11.760,,CC BY-SA 2.5,user28
797,904,0,"Fully agree ! the questions that are asked are different. Registration, landmarking, estimation of derivatives can arise from the functional view. This convince me ! so the big deal with functional data (as it stands in statistical literature) would not be that it is defined on a continuous set but more that it is indexed on an ordered set?",2010-07-28T18:44:58.467,223,CC BY-SA 2.5,
798,929,1,"Still references are quite informative. And nevertheless even if it is discussive and mentions something that is not present in the book it is some kind of a clue, isn't it?",2010-07-28T18:45:07.110,,CC BY-SA 2.5,user88
799,926,0,"@MJoFrhwld, I need to use Matlab so I can incorporate it into my simulation framework.",2010-07-28T18:53:16.423,559,CC BY-SA 2.5,
801,439,3,"I've accepted this answer on the somewhat capricious basis that I now remember Wasserman's book being recommended to me by someone else several years ago.  The same person also recommended ""The Cartoon Guide to Statistics"" by Gonick and Smith.",2010-07-28T19:13:50.953,89,CC BY-SA 2.5,
802,290,2,"I am tempted to say ""R for Stata users"", but I would get voted down for this :)",2010-07-28T19:19:36.570,253,CC BY-SA 2.5,
803,917,2,"I'd argue from a standpoint of trying to create good guess as to what video game a person likes coding 1s and 0s for like and didn't like isn't a good approach.  A scale of 1-5 or 1-7 is quite easy to elicit and will require fewer datapoints to generate a good model (because each data point provides more information).  With caveats about treating ordinal data as interval data of course applying, but probably not really that important in this context.",2010-07-28T19:23:52.880,196,CC BY-SA 2.5,
804,936,3,Econtalk is one of the most intelligent podcasts out there.,2010-07-28T19:36:06.157,319,CC BY-SA 2.5,
805,917,0,"Agreed. I just answered in terms of the question, which was about logistic regression. I've fit proportional odds logistic regressions for ordered data like this in R using `MASS:polr`.",2010-07-28T19:42:06.937,287,CC BY-SA 2.5,
806,918,0,"I was actually going to discuss this same thing. I had a job evaluating CEP/ESP tools, such as Aurora, STREAM, and Esper once...",2010-07-28T20:40:34.987,110,CC BY-SA 2.5,
808,939,0,Read carefully: http://en.wikipedia.org/wiki/Yates'_correction_for_continuity,2010-07-28T20:55:03.517,,CC BY-SA 2.5,user88
810,898,0,"Bayesian methods are theoretically well suited for real time analyses because the essential Bayesian method is to take a prior belief, and using a bit of data, compute a posterior belief -- which can become the prior for a new cycle.  While that part sounds good, there are practical difficulties and significant hurdles in specifying interesting models on real, changing data that are tractable and easily computable.",2010-07-28T21:35:46.503,87,CC BY-SA 2.5,
812,948,0,Will t.test() compare the means across all levels of a factor?,2010-07-28T22:59:28.497,569,CC BY-SA 2.5,
814,917,0,Thanks guys! TONS O' INFO. :D So then what I'm doing now isn't exactly logistic regression and is instead proportional odds logistic regression?,2010-07-28T23:06:39.797,9426,CC BY-SA 2.5,
815,682,0,I ordered Statistics in Plain English... I'll accept your answer when I get it on Thursday if I dig it as much as I think I will.,2010-07-28T23:08:37.843,9426,CC BY-SA 2.5,
818,928,0,"And the same stands for calculating correlation coefficients, I guess?",2010-07-29T00:26:04.627,1356,CC BY-SA 2.5,
819,928,0,"Yes, that would be correct. In fact correlation would be ratio as 0 means no correlation and such a conclusion is scale invariant. By the way, I suspect that percentiles would also be classified as ratio as the 0 point is scale invariant but it does not really matter. Most statistic applications require interval level measurements not necessarily ratio.",2010-07-29T00:53:56.147,,CC BY-SA 2.5,user28
820,939,0,@mbq I did read the Wikipedia article but I wanted to confirm and verify it,2010-07-29T00:55:08.780,559,CC BY-SA 2.5,
821,944,3,I agree. I think R programming question would be a better fit on StackOverlfow.com than here. http://stackoverflow.com/questions/tagged/r,2010-07-29T02:45:20.880,319,CC BY-SA 2.5,
822,808,0,Thanks. Had the opportunity to work with a great statistician for a while and was always amazed by how much information he could get out of the least amount of data by asking very pointed questions. This quotation so reminds me of hm,2010-07-29T03:30:12.343,482,CC BY-SA 2.5,
823,917,0,"What you _want_ to do is a proportional odds logistic regression. I'm not exactly sure what to call what you _have_ done,",2010-07-29T03:47:12.360,287,CC BY-SA 2.5,
824,952,0,How are you defining redundancy?,2010-07-29T04:15:57.833,,CC BY-SA 2.5,user28
825,957,0,"I am not sure I understand what you are saying. I am not sure there is any relationship between a new draw x and the current sample mean mean(S) as x ~ N(mu,sigma^2). Clearly, the draw of x can be anywhere in the support of the distribution. It is more likely to be around mu and less likely to be in the tails but it does not have anything to do with mean(S).",2010-07-29T04:20:42.963,,CC BY-SA 2.5,user28
828,603,0,It does not; GLM is not machine learning. True machine learning methods are wise enough to hold their level of complexity independent of growing number of objects (if it is sufficient of course); even for linear models this whole theory works quite bad since the convergence is poor.,2010-07-29T07:43:01.653,,CC BY-SA 2.5,user88
829,707,0,Have you even read this paper? Nevertheless it is works only for linear models (even the title shows it!) it is about asymptotic behavior for infinite number of objects. 100 is way not enough.,2010-07-29T07:49:16.083,,CC BY-SA 2.5,user88
830,707,1,And I wish you luck making 10-fold cross validation on set with 9 objects.,2010-07-29T07:50:50.653,,CC BY-SA 2.5,user88
831,644,2,"CV using full set for model selection, huh? It's a common error (still even Wikipedia mentions it), because it is a hidden overfit. You need to make a higher level CV or leave some test to do this right.",2010-07-29T08:00:22.393,,CC BY-SA 2.5,user88
832,934,3,"I'd love to see this loss minimizing in clustering, kNN or random ferns...",2010-07-29T08:09:31.527,,CC BY-SA 2.5,user88
833,922,0,Statistical crowd is clicking randomly in SPSS until desired p-value appears...,2010-07-29T08:17:05.427,,CC BY-SA 2.5,user88
835,955,0,Have you voted for close? It is strange that since without Shane we couldn't close any question...,2010-07-29T10:33:19.223,,CC BY-SA 2.5,user88
836,750,5,"That would be true if everyone were knowledgeable enough in statistics to drive the correct conclusions. Alas, that quote is very applicable to many of those amusing human beings called politicians...",2010-07-29T11:22:35.377,582,CC BY-SA 2.5,
837,955,0,"I do think this could be rephrased in such a way that it would be on-topic (eg about the kind of work that statistical consultants do), but this is more like a job request.",2010-07-29T11:30:08.797,5,CC BY-SA 2.5,
838,607,28,Great post! Note that Vapnick had a PhD in statistics. I'm not sure there are a lot of computer scientist that know the name Talagrand and I'm sure 0.01% of them can state by memory one result of talagrand :) can you ? I don't know the work of Valiant :),2010-07-29T11:30:57.080,223,CC BY-SA 2.5,
840,955,0,"@mbq Sorry, I don't understand what you mean. Yes I voted to close. All 500+ rep users can (which means 8 users).",2010-07-29T11:45:13.843,190,CC BY-SA 2.5,
841,955,0,"@Shane So you're here... I've got an impression that you should be holidating.
@Shane, @Peter Still, the problem exists, because not all of this eight is active in closing.",2010-07-29T11:53:06.400,,CC BY-SA 2.5,user88
843,955,0,"@mbq: I am vacationing, but occasionally my iPhone gets service.  And let's face it: I'm addicted to this.",2010-07-29T13:18:13.197,5,CC BY-SA 2.5,
844,929,0,See the meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed where this question is proposed to be closed.,2010-07-29T13:25:38.920,,CC BY-SA 2.5,user28
845,966,0,"Your last line should have been with the log(P)'s, right? And isn't this also a simple consequence of Markov processes and the related exponential distribution?",2010-07-29T13:47:51.097,56,CC BY-SA 2.5,
846,980,0,I forgot this: The points along the x-axis come with varying spacing.,2010-07-29T14:06:54.140,,CC BY-SA 2.5,Pete
847,980,0,I am not sure I understand. Don't you have a y-axis?,2010-07-29T14:07:59.053,,CC BY-SA 2.5,user28
848,966,0,In the notation I borrowed:  x_t := log(X_t) to the p_t are the logs of the P_t.,2010-07-29T14:08:04.870,334,CC BY-SA 2.5,
849,980,0,"Ah, sorry. I misstyped. I have now changed it above.",2010-07-29T14:11:05.440,,CC BY-SA 2.5,Pete
851,905,11,"Yes, and a slight clarification is that online learning algorithms, at least as studied in Machine Learning, mostly make the assumption that your ability to store examples is very limited compared to the size of the data set. In the most limiting case, you only get to see one example at a time, and then you have to forget it after you've used it to update your classifier.",2010-07-29T14:14:16.260,6,CC BY-SA 2.5,
852,944,1,"@mbq: @JohnD.Cook: This is not a programming question. It is also not a question about a common tool of programmers. It is a question about a common tool of statisticians. That is why I asked it here. I don't mind that it was closed, though, since I received and accepted the correct answer.",2010-07-29T14:16:46.300,,CC BY-SA 2.5,anonymous
853,981,0,"Ah, interesting, but I need it to be predictable, i.e. to have the same result each time I view the data.",2010-07-29T14:18:32.553,,CC BY-SA 2.5,Pete
854,980,0,"I also think you need to provide a bit more information. For example, I still cannot visualize the graph. What is your goal?",2010-07-29T14:19:04.330,,CC BY-SA 2.5,user28
855,981,0,"In that case, generate the *n* indexes of the points you choose, and store those indexes.",2010-07-29T14:21:07.587,8,CC BY-SA 2.5,
856,981,0,Or store the seed to the RNG before sampling.,2010-07-29T14:25:18.947,334,CC BY-SA 2.5,
857,981,0,Dirk's solution regarding the seed is probably the better option.,2010-07-29T14:31:10.027,8,CC BY-SA 2.5,
858,983,0,Is there any way to do this without knowing in advance what the relative varience will be or is it just a case of having to take a guess at the relative varience to do anything at all?,2010-07-29T14:38:55.030,210,CC BY-SA 2.5,
860,980,0,"Ok, sorry. I have added some more details above.",2010-07-29T15:11:17.823,,CC BY-SA 2.5,Pete
861,982,0,"Yep, visualization is what I want. I have added some more info in the question.",2010-07-29T15:11:55.310,,CC BY-SA 2.5,Pete
862,984,0,"Ok, sorry. I have added some more details above.",2010-07-29T15:12:12.270,,CC BY-SA 2.5,Pete
863,980,0,"Ok, just so I understand better- your x-axis is time with the 0 point being the time of the first sample. Your y-axis is beats per minute. Is that right? I still do not know your goal: Why do you want to reduce the number of data points? - Reduce clutter? or See patterns better? or the existing points are too close to each other for you to figure out what is happening?",2010-07-29T15:17:06.110,,CC BY-SA 2.5,user28
864,911,0,Thanks! But I hoped for a statistic that is faster and more stable to calculate...,2010-07-29T15:25:23.073,506,CC BY-SA 2.5,
866,963,4,"You should be very careful in interpreting p-values from these t-tests with this looping approach. If you have 20 columns, and there is no real difference between any of them, at least one comparison is likely to appear significantly different at p < 0.05. You should at least multiply the p-values by the number of tests, to produce the Bonferronoi correction. Or, you could multiply them by their inverse rank for the Holm correction.",2010-07-29T15:36:54.343,287,CC BY-SA 2.5,
867,982,0,seconding plotting raw data with a smoothing line.,2010-07-29T15:43:10.833,287,CC BY-SA 2.5,
869,685,0,See the meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed where this question is proposed to be closed.,2010-07-29T15:48:02.003,,CC BY-SA 2.5,user28
870,971,0,"Thank you, that's exactly what I needed. I'm not completely sure of what you mean when you talk about the overdispersion (sorry, I'm not a statistician, maybe it's something very basic)... You say that the residual deviance should be equal to the residual degrees of freedom... how would I check that?",2010-07-29T16:10:48.980,582,CC BY-SA 2.5,
871,944,1,"It is a question about using a computer program use; so I think you should try SuperUser. Dirk is there, so R questions will be answered in a blink of eye ;-)",2010-07-29T16:13:58.040,,CC BY-SA 2.5,user88
872,981,0,"Calculating averages per each second is ok, but what I do when there is no data for a specific second. I guess I could do some interpolation from the seconds before and after it, but it would be great with some specific (named) method for this, so I don't try to invent something already invented.",2010-07-29T16:24:46.173,,CC BY-SA 2.5,Pete
873,948,0,"I'm not sure what you mean. If you have some continuous variable column A (say people's heights) and some grouping factor column B (say Country of origin), you really want to do a `pairwise.t.test()`. You would do this like `pairwise.t.test(df$A , df$B)`. This function will automatically correct for multiple comparisons.",2010-07-29T16:28:03.317,287,CC BY-SA 2.5,
875,982,0,thirding plotting raw data with a smoothing line --- You might want to also plot the change in BPM over time as a separate visualization.,2010-07-29T17:52:51.363,601,CC BY-SA 2.5,
876,981,0,The interpolation you are talking about is the moving average or smoothing bit.,2010-07-29T18:21:06.423,8,CC BY-SA 2.5,
877,993,0,You should really should state would language you sample code is for.,2010-07-29T18:33:14.403,8,CC BY-SA 2.5,
878,730,5,I use this quote a lot to explain the difficulties in mathematicians transitioning to statistics,2010-07-29T18:48:03.407,549,CC BY-SA 2.5,
879,971,0,If you give `summary(model1)` you'll see something like `Residual deviance: -2.7768e-28  on 0  degrees of freedom`,2010-07-29T18:55:47.400,339,CC BY-SA 2.5,
882,138,3,Refer to SO. http://stackoverflow.com/questions/192369/books-for-learning-the-r-language/2270793,2010-07-29T19:17:06.187,1356,CC BY-SA 2.5,
884,981,0,"But moving average will not reduce the number of data points, will it? As far as I know it will _only_ smoothen. Or are there optional moving averages that does both?",2010-07-29T19:29:56.217,,CC BY-SA 2.5,Pete
885,763,0,this is great information.  Do you know of any papers that cover this?,2010-07-29T19:39:40.220,5,CC BY-SA 2.5,
886,981,0,I rewrote the answer. Hopefully it's clear and useful now.,2010-07-29T20:07:35.143,8,CC BY-SA 2.5,
887,993,0,"@csgillespie: sorry; it's Matlab code, as Elpezmuerto requested in his comment to the other answer.",2010-07-29T20:44:37.253,506,CC BY-SA 2.5,
888,899,1,Do you know that members of the second group aren't included in the first group or are you just willing to mistakenly label those members as belonging to the first group?,2010-07-29T20:54:53.360,3807,CC BY-SA 2.5,
890,926,1,There is a chi-square test function in Matlab Central File Exchange: http://www.mathworks.com/matlabcentral/fileexchange/4779,2010-07-29T21:19:19.083,128,CC BY-SA 2.5,
891,1004,1,"KS test always turns out to be non-normal with very large sets (I have dataset larger than 1 million data points). Also, I would like to have a quantifying measure that tells me about goodness of fit rather than just a test.

Am I asking for too much?

Thanks in advance,
A",2010-07-29T21:21:41.907,608,CC BY-SA 2.5,
892,1004,0,"I agree with the question answerer.  What you are looking for is exactly a KS test.  The heightened ability of a KS test to detect violations of normality in large datasets is not a reason to toss it aside.  But, you may want to set different thresholds in terms of the D statistic depending on your sample size or get into the guts of the equation and see if you can remove the increased likelihood of rejecting the null as a function of sample size.",2010-07-29T21:50:42.850,196,CC BY-SA 2.5,
893,1001,0,Spearman's Correlation Coefficient is used to compare relative rank orders.  It strikes me that when comparing normal distributions in this way you are particularly unlikely to detect differences in kurtosis.,2010-07-29T21:53:37.250,196,CC BY-SA 2.5,
894,981,0,"Thanks, very clear! Random sampling sounds like a great tool. But what can I do if the original data points are very unevenly spread out - I still want, for example, one final data point per 5 second interval, and that might not be present (maybe not even before the random sampling). That means I need to do some kind of interpolation.",2010-07-29T22:09:51.903,,CC BY-SA 2.5,Pete
895,1001,0,"Not adding that this code is a total junk; you must normalize histograms somehow, cutting the end is not a good idea (still I have no idea how to do it). And make code a code (indent with 4 spaces).",2010-07-29T22:13:58.017,,CC BY-SA 2.5,user88
896,1004,0,"And as drknexus wrote, it is easy to extract the statistic and use it for comparison. Even the p-values will do.",2010-07-29T22:15:59.590,,CC BY-SA 2.5,user88
898,1004,0,Is it actually valid to use p-values as a qualitative measure if they are not statistically significant (i.e. below .05)?  Wouldn't it be much better to have some kind of effect size?,2010-07-29T23:25:52.993,608,CC BY-SA 2.5,
899,1012,1,"What is ""not meaningful"" about the standard deviation of a uniform distribution? It is a measure of spread for the uniform, just as it is for almost every other distribution. It may not be the best measure of spread, but it is certainly meaningful.",2010-07-29T23:32:38.380,159,CC BY-SA 2.5,
900,965,0,"+1 from me--domain-independent answers are particularly useful here. It seems to me that in interdisciplinary domains, the ""why"" is often ignored because of the (understandable) emphasis on practical application.",2010-07-29T23:32:42.013,438,CC BY-SA 2.5,
901,1023,3,community wiki?,2010-07-29T23:55:06.170,,CC BY-SA 2.5,user28
902,1004,5,"Why below 0.05, not 0.04973? Statistical significance does not have any in-depth meaning, it is just an accepted probability of analysis failure.  The operation of transforming statistic into a p-value is monotonic, so there is no problem with comparison. Still obtaining a significance level of this comparison is problematic (I have no better idea than bootstrap).",2010-07-30T00:20:42.420,,CC BY-SA 2.5,user88
903,1004,0,"Hi drknexus, you said ""set different thresholds in terms of the D statistic depending on your sample size"". How can this be achieved with kstest. I did not find a way to manipulate thresholds. I assume that with ""getting into the guts of the equation"" you mean transformations before testing. My distribution looks multi-modal and I have already an idea what these multi-modal tendencies might be i.e. how I can single them out...",2010-07-30T00:42:25.733,608,CC BY-SA 2.5,
904,429,3,"Besides... Shapiro-Wilk's test is often used when estimating departures from normality in small samples. Great answer, John! Thanks.",2010-07-30T01:24:13.157,1356,CC BY-SA 2.5,
905,871,1,See the meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed where this question is proposed to be closed.,2010-07-30T02:00:21.963,,CC BY-SA 2.5,user28
907,957,0,"But you don't and can't know mu or sigma. What you know are the mean and SD of the sample, mean(S) and stdev(S).",2010-07-30T03:51:27.510,25,CC BY-SA 2.5,
908,1027,0,"Thanks for your thoughts. Maybe I wasn't clear though. I do have paired sets. They are the standardized quadratic scores of the test-retest results, ie some distributions were elicited twice. I will edit the question.",2010-07-30T06:42:57.067,108,CC BY-SA 2.5,
910,1012,0,"All I meant was that the standard deviation is not really a parameter that defines or describes the uniform distribution well.  In my mind, the standard deviation refers to the spread of a normal, or near normal distribution.  

Simply because a value is calculable does not mean that it is interesting or meaningful.  For example, I might be able to calculate what rate parameter from an exponential distribution best matches a normal distribution, but to me such a value would not be particularly meaningful because the distribution being described is not actually exponential.",2010-07-30T06:51:57.670,196,CC BY-SA 2.5,
912,916,0,"It also seems as if $y$ is always larger than some linear function of $x$, $y>C x$.",2010-07-30T10:11:17.393,56,CC BY-SA 2.5,
914,981,0,@Pete - see answer,2010-07-30T10:45:12.760,8,CC BY-SA 2.5,
915,916,0,"Possibly; still it is hard to tell without a zoom on this dense area (points are not transparent, so one cannot judge the density, and so this plot may be deceiving).",2010-07-30T10:48:57.713,,CC BY-SA 2.5,user88
917,445,0,"In referring to other questions, don't say ""above"" or ""below"" as the order can change depending on votes or whether the answer is accepted, for example.",2010-07-30T11:33:28.503,159,CC BY-SA 2.5,
918,1012,5,"I can define a uniform distribution with mean 0.5 and standard deviation 1/12. It is perfectly well defined, but it is not the most natural parameterization. There is nothing about standard deviations that implies normality.",2010-07-30T11:36:04.613,159,CC BY-SA 2.5,
922,21,0,What kind of demographic data are you trying to forecast? Every data could (and should) be treated differently.,2010-07-30T13:43:17.743,614,CC BY-SA 2.5,
923,1040,1,You can latex on this site. Please enclose the tex with $ $. See this meta thread: http://meta.stats.stackexchange.com/questions/218/tex-processing-for-stats,2010-07-30T14:29:26.027,,CC BY-SA 2.5,user28
924,1027,0,"Could you clarify how you are computing standardized quadratic scores? And, what exactly your are correlating?",2010-07-30T14:44:55.257,,CC BY-SA 2.5,user28
927,1027,0,@Srikant: I updated the question with some background information,2010-07-30T17:16:03.600,108,CC BY-SA 2.5,
928,1045,2,"Not an answer (because I don't know) but I just saw this new book on the Springer web site: ""A Comparison of the Bayesian and Frequentist Approaches to Estimation"" http://www.springer.com/statistics/statistical+theory+and+methods/book/978-1-4419-5940-9 that (one would assume) may have some answers.",2010-07-30T17:20:19.320,247,CC BY-SA 2.5,
929,1019,0,I don't understand... one set per person?,2010-07-30T17:40:15.650,,CC BY-SA 2.5,user88
930,1019,0,@robin & @mbq I would suggest keeping it one dataset per post. This so people can indicate with votes which of the suggested ones there also suggest/support,2010-07-30T17:59:24.413,190,CC BY-SA 2.5,
931,1049,0,"Welcome -- Nice to see you here, Pat!",2010-07-30T18:35:19.240,334,CC BY-SA 2.5,
934,1051,1,"I understand what you wrote but I am not sure that answers my question. The method you describe and the methods in the linked question in my ps avoid estimating the nuisance parameter on the grounds that the nuisance parameter is not of interest. Such an approach is fine but then without estimating the nuisance parameter we cannot construct a confidence interval for this parameter. Thus, the sentence in the wiki makes sense only if we estimate the nuisance parameter using standard maximum likelihood.",2010-07-30T22:45:06.690,,CC BY-SA 2.5,user28
935,1051,0,"So my question is: if we estimate the nuisance parameter using maximum likelihood how is its treatment any different than estimating any other parameter. By the way there seems to be a typo in your eqns as the second term f(z|-) should not depend on theta, right?",2010-07-30T22:47:15.360,,CC BY-SA 2.5,user28
937,1051,0,"The equation looks right to me -- I hope I'm not having a blind moment staring at the screen here.  In each case, we have one component that depends on theta and the other (nuisance) component may or may not.  The crucial point is that we must find a transformation that isolates theta to *some* extent to obtain either a marginal or conditional.  When we can't, we must work with profile and other estimated likelihoods.  We could argue that ML is losing information, but that's another question entirely.  Still, perhaps this sheds light on the differences?",2010-07-30T23:33:50.077,251,CC BY-SA 2.5,
939,934,0,"Well, for a loss function characterization of k-means nearest neighbor, see the relevant subsection (2.5) of this paper: http://www.hpl.hp.com/conferences/icml2003/papers/21.pdf",2010-07-30T23:41:56.227,39,CC BY-SA 2.5,
940,628,2,"Note that the last definition here is an *Integrated* (or Bayesian) Likelihood, not a Marginal Likelihood.",2010-07-31T00:09:58.993,251,CC BY-SA 2.5,
941,628,0,"Is this correct in the RHS for partial likelihood: ""L2(Œ∏|theta)""?",2010-07-31T00:13:14.947,461,CC BY-SA 2.5,
943,728,1,It has actually led to very interesting articles... :),2010-07-31T01:07:04.477,253,CC BY-SA 2.5,
944,1053,2,community wiki please. Your question does not have an 'objective best' answer.,2010-07-31T02:04:28.317,,CC BY-SA 2.5,user28
946,1055,3,"Correlation is not necessarily linear - Spearman's rho relies on the monotonic function, and yet, we refer to it as a ""correlation coefficient"", not ""mutual information coefficient"". And for a good reason: it provides an information about association between two variables. Mutual information, redundant information, mutual variance, correlation - these terms are so similar, and this question refers to _network reconstruction_, so I guess that we ended up in the wrong area with right terminology. This is quite specific question...",2010-07-31T02:26:27.953,1356,CC BY-SA 2.5,
947,1055,1,Good point. I've edited my answer to include monotonic relationships. I don't know anything about network reconstruction.,2010-07-31T03:04:58.373,159,CC BY-SA 2.5,
948,1051,0,"I am not sure how far we should take this discussion as SE is not a good outlet for discussions. I agree with what you said but I am not sure you are addressing the issue I raised. If you use a nuisance free likelihood how can you construct a confidence interval for the nuisance parameter? In any case, I should probably stop here and I will let you have the last comment.",2010-07-31T04:27:08.830,,CC BY-SA 2.5,user28
949,1015,0,Is your your probability score identical to what is called the Brier score? (See: http://en.wikipedia.org/wiki/Brier_score),2010-07-31T04:34:21.133,,CC BY-SA 2.5,user28
951,1051,1,"Sorry, I wrote we don't estimate the nuisance, let me try again with some examples that address it.  Partial likelihood just discards information about the nuisance.  Profile replaces it with the MLE at fixed theta, but this doesn't account for uncertainty in lambda.  Contrast with the Bayesian and specification of a prior.  I think I get what you're saying that it's just estimation like any other parameter.  But the treatment matters because it's how you account for the uncertainty due to the nuisance which affects the intervals, whether through the posterior or the likelihood.",2010-07-31T04:54:33.057,251,CC BY-SA 2.5,
953,1019,0,"@Peter, OK, I follow your idea, I have changed the question accordingly.",2010-07-31T06:14:49.843,223,CC BY-SA 2.5,
954,728,0,@Tal: Fully agree! I think the whole area on optimal separation in minimax testing is starting from this idea ... and it is still so confused for a lot of statistician. For those interested see the paper of donoho http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1085408492 (and the references in the paper ! since things are much older than donoho's paper),2010-07-31T06:19:15.793,223,CC BY-SA 2.5,
955,1015,0,@Srikant: Yes it is. It is known by both names.,2010-07-31T06:49:42.253,108,CC BY-SA 2.5,
957,1061,0,"Aww, you've beat me. +1",2010-07-31T10:44:50.723,,CC BY-SA 2.5,user88
958,1061,0,"Thanks! Is it the same as the df=6 that LogLik returns? Could I have used -2*logLik(fit)+2*(attr(logLik(fit),""df""))  ??",2010-07-31T10:58:19.690,339,CC BY-SA 2.5,
959,255,4,Check out the outlier function in the randomForest package http://lib.stat.cmu.edu/R/CRAN/web/packages/randomForest/randomForest.pdf,2010-07-31T11:01:22.993,339,CC BY-SA 2.5,
960,213,11,"5th D in color hue, 6th D in color intensity, 7th D in point size and we can go ;-)",2010-07-31T12:36:14.223,,CC BY-SA 2.5,user88
961,963,0,You are right. I was feeling that something was wrong. They constitute a family of confidence intervals. Perhaps its even better to adjust using Tukey Honest Significant Differences (TukeyHSD in R),2010-07-31T15:45:10.863,339,CC BY-SA 2.5,
962,1062,2,Orthogonal parameterization has a broad range of uses.  Please be more specific.  What are you applying it to?,2010-07-31T15:48:18.880,601,CC BY-SA 2.5,
963,1061,0,"@user603 Also, how did you ""draw"" that indexed sigma?",2010-07-31T15:48:24.497,339,CC BY-SA 2.5,
964,1065,0,"On 2) I believe the confusion is what p(X=.01) means when X is a continuous random variable.  Intuitively, the probability seem to be zero everywhere because there is no chance X is exactly .01. The questioner should review the definition of a density function in the continuous case, which is defined as the derivative of the cumulative density function.",2010-07-31T17:15:21.020,493,CC BY-SA 2.5,
965,827,0,@mbq +1 for snowfall- abstracts snow even further and makes parallel computing with R pretty simple.,2010-07-31T18:14:41.427,13,CC BY-SA 2.5,
966,823,0,It's unfortunate that consensus has so much control over what kind of science gets funded.,2010-07-31T18:26:15.927,13,CC BY-SA 2.5,
968,1066,0,"You mean that you are looking at the distribution of subsample **lengths** of subsegments covering each position, right? On the other hand (probably I'm wrong) I'm guessing it has something to do with the sequencing?",2010-07-31T18:40:22.233,,CC BY-SA 2.5,user88
969,1066,0,"No, I'm looking at the distribution of the **number of sub-segments** covering a specific position. For example, if we focus on position 1 in the large segment, in the first simulation we have 4 sub-segments covering it; in the second simulation we have 1 sub-segment covering it, etc.

I don't care what are the lengths of the sub-segments covering each position, just how many sub-segments there are.

You are not completely wrong, this has started as a part of an exercise I've been working on in a biology course, but has gone to another domain :)",2010-07-31T18:45:20.927,634,CC BY-SA 2.5,
971,1066,0,"Still, one more doubt -- what is a distribution of the lengths of subsamples? I think it is crucial to the answer. (I'm suspecting it is a binomial distribution)",2010-07-31T18:58:00.717,,CC BY-SA 2.5,user88
972,1066,0,"the lengths of the sub-segments? A list of lengths is given. I believe the lengths distribute quite normally, but I did not check it.",2010-07-31T19:01:45.513,634,CC BY-SA 2.5,
973,1066,0,sounds like a poisson distribution...,2010-07-31T19:08:22.567,601,CC BY-SA 2.5,
974,1068,0,"I think I got the idea, although I will have to think about it some more. One other thing I intentionally did not mention at the beginning, is the fact we might have predefined ""hot"" subranges along our long segment. This means that every subrange that is drawn and completely includes one of the ""hot"" subranges will not be counted (it will be totally elimnated, not drawn again).",2010-07-31T19:12:59.453,634,CC BY-SA 2.5,
975,1068,0,"For example, if we have a predefined ""hot"" subrange 20..25 and we are now drawing a subrange of length  40, and we happen to draw 11..50, than we don't count anything (we throw this subrange). However, if we happen to draw 21..60 we count as normal (since our subrange does not include the complete hot subrange, only part of it).",2010-07-31T19:13:25.787,634,CC BY-SA 2.5,
976,1068,0,"If you want the pmf, then it seems the only sensible thing to do is to actually construct it.  Some sort of counting approach probably works best, similar to what I suggested where you don't need to actually count everything.  It's not clear what your goal is though.  If you're interested in the pmf, then an exact pmf is obviously better than some ad hoc approximation.  If you're interested in some underlying parameter, the fact that you can easily simulate from your distribution opens other possibilities.",2010-07-31T19:25:14.010,493,CC BY-SA 2.5,
977,1068,0,"OK, so what I really want is the following: I do not know if where the hotspots are (how many are there or how long is each hotspit), but I do have a few hypotheses (each hypothesis is a set of hotspots; each hotspot is just a subrange). These are hidden variables.

I also have one ""true"" mapping - where all the lengths were drawn and mapped to the long range. This is my data.

What I aim to do is to check which of my hypotheses is most likely given the data (the ""true"" mapping).",2010-07-31T19:36:44.343,634,CC BY-SA 2.5,
978,1068,0,"(and sorry I can't vote-up yet, I'm still new here :))",2010-07-31T19:40:30.527,634,CC BY-SA 2.5,
979,1068,0,"I don't exactly understand your data.  It sounds like you'll end up wanting to do something like EM or set up a full Bayesian model. If the probability of exactly matching your data using simulation is reasonable (above one in a million or something), you could just do rejection sampling to get the posterior probabilities of your hypotheses.  Unless your application is simple though rejection sampling may not be feasible since the match probability is too small.  In that case, having the pmf will help.",2010-07-31T19:57:37.427,493,CC BY-SA 2.5,
980,893,9,"It would be nice to have an explanation for *why* degrees of freedom is important, rather than just what it is.  For instance, showing that the estimate of variance with 1/n is biased but using 1/(n-1) yields an unbiased estimator.",2010-07-31T20:12:29.810,493,CC BY-SA 2.5,
981,1068,0,"What I thought of doing is: for each scenario, get the pmf for each position, then calculate the likelihood of the data to be produced given the scenario (multiple probability of each position, assuming positions are independent, although they actually are not). Then we have a likelihood for the data given each of the scenarios and we can choose the most likely scenario (from the set of given scenarios).

There is no chance I will fully match my data using simulations. There is too much noise and the segments are quite long.",2010-07-31T20:20:15.360,634,CC BY-SA 2.5,
982,1070,0,"I like your explanation (+1)-- it is probably more to the point for Alekk.  I'll leave mine up in the hope that it casts a different, perhaps useful, light on the role of orthogonality.",2010-07-31T20:41:58.913,39,CC BY-SA 2.5,
983,1068,0,"I've done some thinking. If I understand your suggestion correctly, I need to keep for each position in my long range m bernouli variables (one for each length, saying what is the probabilty the i-th subrange will cover this position).
Since I usually work with n=200k and m=20k, this means 4*10^9 variables, which is way too much. This could perhaps be ""compressed"" since many subsequent positions will have the exact same set of variables, but I can not think immediately of an easy way to do that. On the other hand, perhaps I can do all the calculations on the fly and not save those variables...",2010-07-31T20:44:17.923,634,CC BY-SA 2.5,
985,1026,1,"Actually, to the best of my memory, Harrell strongly discourages the use of AIC.  I guess cross-validation would probably be the safest method around.",2010-08-01T01:54:35.720,253,CC BY-SA 2.5,
986,1072,1,"Is there some way to quantify what is "" increasingly prefered "" these days ?",2010-08-01T01:55:20.693,253,CC BY-SA 2.5,
987,1070,0,Can you comment on how your definition of marginal likelihood relates to the one given in the wiki link? (See: http://en.wikipedia.org/wiki/Marginal_likelihood),2010-08-01T01:57:42.390,,CC BY-SA 2.5,user28
988,1026,2,"AIC is asymptotically equivalent to CV. See answers to http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other. I checked Harrell before I wrote that answer, and I didn't see any discouragement of the AIC. He does warn about significance testing after variable selection, with the AIC or any other method.",2010-08-01T02:54:39.570,159,CC BY-SA 2.5,
989,282,20,"Effort is commendable -- BUT -- neither PC1 nor PC2 tells you who did best in all subjects.  To do so the PC subject coeffcients would all have to be positive.  PC1 has positive weights for Math and Music but negative for Science and English.  PC2 has positive weights for Math and English but negative for Science and Music. What the PCs tell you is where the largest variance in the dataset lies.  So by weighting the subjects by the coefficients in PC1, and using that to score the students, you get the biggest variance or spread in student behaviors. It can classify types but not performance.",2010-08-01T02:58:57.017,87,CC BY-SA 2.5,
991,1073,2,"Nice explanation.  Doesn't explain people's cognitive failings, but +1 anyway.",2010-08-01T03:17:33.713,87,CC BY-SA 2.5,
997,1051,0,"[moving this comment from the orthogonal question] You might find this paper by Berger et al. interesting: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1009211804 -- marginal likelihood is defined on p.5 before they try to sell the integrated form to ""likelihoodists"". Also check out the historical references on that page, Cox 1975 and Basu 1977. The former is the origin of partial likelihood and the latter is a critique of Fisher likelihood w.r.t. nuisance parameters.",2010-08-01T04:34:25.180,251,CC BY-SA 2.5,
998,1070,0,"All the cites on that page (Bos; MacKay/p.29) are to Bayesian concepts of likelihood (post ~ lik x prior). There's a connection, but it's not the marginal likelihood in the sense of Fisher and Likelihood theory. [see comment on Berger paper here: http://stats.stackexchange.com/questions/1045/is-there-a-radical-difference-in-how-bayesian-and-frequentist-approaches-treat-nu/1051#1051 ]",2010-08-01T04:37:58.330,251,CC BY-SA 2.5,
999,1026,0,"@Tal: Perhaps from one of his papers rather than the RMS book, I remember Harrell objecting to the use of AIC for simply choosing among a pool of *many* models.  I think his point was that you must add a variable at a time and compare two models methodically or use some similar strategy.  (To be clear, this is in line with Rob's answer.)",2010-08-01T05:14:48.807,251,CC BY-SA 2.5,
1001,282,1,"+1 good comment, cheers. You are of course correct, I should have have written that better and have now edited the offending line to make it clear I hope.",2010-08-01T09:29:48.767,81,CC BY-SA 2.5,
1003,825,3,"I have no problem with this sort of Q&A here.  R isn't such a mainstream language (like Python or Java) that a quant would naturally say, ""Oh this is a general programming question so I should go to StackOverflow or similar and ask this or look there for solutions"".  Actually it is more a question for an R mailing list or group site.  To serve those budding analysts who want to learn R we should be glad to have an answer here as well.",2010-08-01T10:32:26.773,87,CC BY-SA 2.5,
1006,226,2,"That's right - if the variance of PC is, say 3.5, then that PC ""explains"" variability of 3.5 variables from the initial set. Since PCs are additive, `PC1 > PC2 > ... > PCn`, and the sum of their variances is equal to the sum of the variances of the initial variable set, since PCA is computed upon covariance matrix, i.e. variables are standardised (SD = 1, VAR = 1).",2010-08-01T11:51:45.170,1356,CC BY-SA 2.5,
1007,282,0,"You could standardise the vars, hence calculate the sum, in order to see who's the best, or if you prefer, in R: `apply(dtf, 1, function(x) sum(scale(x)))`",2010-08-01T12:08:34.560,1356,CC BY-SA 2.5,
1008,1072,0,I think that it is recognized to be scientifically more correct in many field in the sense that the shrinkage approach is used more in recent applied stat papers than the *.IC approach. That shows a certain -at least tacit- theoretical consensus.,2010-08-01T12:12:41.710,603,CC BY-SA 2.5,
1012,981,0,Great method for dealing with uneaven data. I will try that. A huge thanks for all your help!,2010-08-01T14:50:13.037,,CC BY-SA 2.5,Pete
1014,632,0,"What Srikant found (and what seems confirmed at PhysicsForums) there should be $\sqrt{2}$, so rather $\hat{\sigma}\frac{\sqrt{2}}{2n}$.",2010-08-01T15:34:01.873,,CC BY-SA 2.5,user88
1015,632,1,"Aww, those comments locks; $\frac{\hat{\sigma}}{\sqrt{2n}}$. At least this one gives the result in agreement with bootstrap.",2010-08-01T15:58:24.160,,CC BY-SA 2.5,user88
1016,1069,0,"[Ack, deleted a previous comment when I thought it would edit, back to try again.]  This is interesting though I don't understand information geometry +1.  I'm intrigued by this question at quora, which is unfortunately unanswered: http://www.quora.com/What-does-it-mean-to-say-In-information-geometry-the-E-step-and-the-M-step-are-interpreted-as-projections-under-dual-affine-connections",2010-08-01T17:59:05.207,251,CC BY-SA 2.5,
1017,1069,0,"I had wondered where that went!  If I get some time, I think I'll try to answer that question over at quora, though I should admit that I an information geometry novice.  Most of what I know of it is incidental to an an effort to pin down how the features of mathematical spaces (topological, algebraic, and otherwise) enable the application of learning methods.  This, in turn, is part of a quite difficult pet project of mine.",2010-08-01T18:51:44.180,39,CC BY-SA 2.5,
1018,1069,0,If you would like to correspond about any of these subjects feel free to email me at johnnylogic at gmail.,2010-08-01T19:09:46.810,39,CC BY-SA 2.5,
1019,113,0,"The aforementioned paper addresses method selection, generally. As for specific examples and more details they may be found scattered among specific meta-methodological disciplines (measurement theory, algorithmic learning theory, statistical learning theory, complexity theory), but I have not found a systematic treatment, thus the question. If you wish to discuss these issues generally, you may email me at johnnylogic at gmail.",2010-08-01T19:11:47.790,39,CC BY-SA 2.5,
1021,1095,0,Reg Latex: See this meta thread: http://meta.stats.stackexchange.com/questions/218/tex-processing-for-stats,2010-08-01T22:58:47.517,,CC BY-SA 2.5,user28
1022,1094,0,"I am already modeling the probability of each cell as a Dirichlet, $p \sim Dirichlet(\alpha)$.  Normalizing by $n$ wouldn't quite work since a cell with large $p_i$ would have small likelihood whenever $n$ is fairly large, even when we indeed observed $x_i=1$ (unnormalized).  Does that make any sense?",2010-08-01T23:08:31.900,647,CC BY-SA 2.5,
1023,1094,0,I am afraid you lost me by the mention of a 'cell' when the question does not mention this word at all. Could you provide some more context so that I can understand better?,2010-08-01T23:13:24.467,,CC BY-SA 2.5,user28
1024,1094,0,Sorry: By cell I mean each $x_i$.,2010-08-01T23:14:44.807,647,CC BY-SA 2.5,
1025,1093,0,"By the way, will the binomial not work for you? You have k trials and n successes. right?",2010-08-01T23:16:12.010,,CC BY-SA 2.5,user28
1026,1094,0,"In other words, I want to sample $n$ integers between 1 and $k$ without replacement using the probability vector $p$.",2010-08-01T23:17:14.770,647,CC BY-SA 2.5,
1027,1094,0,This getting confusing to me. Your $X_i$ is either 0 or 1 but now you want to sample $n$ integers between 1 and $k$?,2010-08-01T23:23:32.467,,CC BY-SA 2.5,user28
1028,1094,0,Sorry: I'm saying that it's an equivalent formulation to sample the indices of $X$ without replacement.  Then $x_i=1$ for those integers that are sampled and $x_i=0$ for those integers that are not.,2010-08-01T23:39:34.080,647,CC BY-SA 2.5,
1029,1094,0,"In that case, you have basically the following scenario: You flip a coin $k$ times and your constraint is that you need $n$ successes as that will ensure that $X_i$ sum to $n$. This is a binomial distribution. If this satisfies your requirements I will change my answer. If it does not can you explain why this will not work?",2010-08-02T00:04:19.553,,CC BY-SA 2.5,user28
1030,219,0,"Would this be good for implementing on GPUs?  Link to details, references?",2010-08-02T00:27:47.807,646,CC BY-SA 2.5,
1031,146,1,"Why did you put `r` tag and what do you mean by ""why this is so""? PC's are not correlated, i.e. they're orthogonal, additive, you cannot predict one PC with the another. Are you looking for a formula?",2010-08-02T00:38:33.680,1356,CC BY-SA 2.5,
1032,167,0,"In the set with n variables, n PCs can be extracted, but you can decide how many you'd like to keep, e.g. Guttman-Keiser criterion says: keep all PCs that have eigenvalue (variance) larger than 1. So there...",2010-08-02T00:43:29.833,1356,CC BY-SA 2.5,
1033,1094,0,"Slightly different: I have $k$ different coins, each with a different probability (ie. $p_i$), and I have $n$ successes.  Sorry for the confusion!",2010-08-02T01:14:41.317,647,CC BY-SA 2.5,
1035,1093,0,"Slightly different: I have $k$ different coins, each with a different probability (ie. $p_i$), and I have $n$ successes. Sorry for the confusion!",2010-08-02T01:43:26.147,647,CC BY-SA 2.5,
1036,1094,0,"For the second proposal, what is the form of this posterior?  Another Dirichlet?  It makes sense that the Dirichlet would be conjugate here, just like if we were using a multinomial.  But we're not using a multinomial.  So what is the pdf of the distribution you propose?  (It's a multivariate bernoulli that has been conditioned on observing n successes, right?)",2010-08-02T02:07:56.790,647,CC BY-SA 2.5,
1037,1094,0,"Unfortunately, it is not a dirichlet. The bernoulli can be written as: $f(X_i;p_i) = p_i^{X_i} (1-p_i)^{1-X_i}$ which is not conjugate to the dirichlet. You will have to use Metropolis-Hastings to estimate the parameters.",2010-08-02T02:14:54.870,,CC BY-SA 2.5,user28
1039,910,3,"[citation needed]. In other words, intriguing answer, although it doesn't jive (at least) with a lot of ML literature.",2010-08-02T05:03:16.727,30,CC BY-SA 2.5,
1041,146,0,I was wondering about the principles behind the logic (in my quest to understand PCA). I used R tag because R people might read this and maybe show R examples. :),2010-08-02T06:13:21.407,144,CC BY-SA 2.5,
1042,910,1,"Classical one is Breiman's ""Statistical Modeling: The Two Cultures"".",2010-08-02T06:42:56.153,,CC BY-SA 2.5,user88
1043,1025,0,Interesting posting. I will look into that too. You are probably right.,2010-08-02T07:53:58.937,,CC BY-SA 2.5,Pete
1045,146,0,"Oh, why didn't you say so? Have you seen http://www.statmethods.net/advstats/factor.html",2010-08-02T11:01:14.140,1356,CC BY-SA 2.5,
1046,1113,0,"Thanks for your answer. That solves the problem of bounding.

For my data it goes to 1 very quickly for my data so I guess the next thing I need to do is to scale this information to concentrate on the interesting range which I could do based on the history of it without fear of leaving the bound, just hitting the limit.",2010-08-02T15:19:49.210,652,CC BY-SA 2.5,
1047,1114,0,"erf is not a very handy function, provided you don't want to rather use it for its derivative.",2010-08-02T15:26:48.747,,CC BY-SA 2.5,user88
1048,1026,1,"Doing a quick search, I found Harrell writing the following ""Beware of doing model selection on the basis of P-values, R-square, partial R-square, AIC, BIC, regression coefficients, or Mallows' Cp.""  He wrote that on 12/14/08, on a mailing list titled [R] Obtaining p-values for coefficients from LRM function (package Design) - plaintext.  I guess I misunderstood his meaning.",2010-08-02T16:20:19.767,253,CC BY-SA 2.5,
1049,1115,3,Could you please make your question a little more understandable?,2010-08-02T16:38:41.897,,CC BY-SA 2.5,user88
1050,1118,0,"Thank you- mbq and stoplan - I computed mean in time1 and time2, and percent change. There are several obs for each idno, but not an equal  no. of obs for each idno in each time period.  There are 2 or 3 vars that cause change in dep. var, but the means are diff from time1 to time2. I am not clear on repeated measures - in SAS I used proc mixed w/repeated option but I am not clear on results!",2010-08-02T17:32:45.887,474,CC BY-SA 2.5,
1051,1118,0,"There is a specific change in 1 indep. var, X1 in time2. What I want to do is show statistically what is causing change in dep var from time1 to time2.  How much is due to change in X1? Am I still unclear?",2010-08-02T17:37:02.900,474,CC BY-SA 2.5,
1052,1118,0,"@Tailltu I think, ideally, editing the question itself to lend clarity is preferable. That way your question would show up in searches by other people who may have an issue similar to yours.",2010-08-02T17:58:52.207,,CC BY-SA 2.5,user28
1054,763,0,"(didn't get a notifier from SA of your comment) Well, i wasn't referring to any papers when i wrote that, rather just informally summing pieces of my experience relevant to your Question. I'll look through my files and see what i have that is relevant though.",2010-08-02T18:32:24.133,438,CC BY-SA 2.5,
1058,1115,0,"Or for example mention what stat package you are using ? (R, SAS, SPSS, etx ?)",2010-08-02T20:30:33.360,253,CC BY-SA 2.5,
1059,1133,1,"I once asked the same question on the R mailing list, and didn't get a response.  I'd suggest you to change your title since your question is regarding ""post hoc analysis of chi square - to detect the cause of the significance"" (a shorter titles then the one I proposed would be better :) )",2010-08-02T20:33:00.797,253,CC BY-SA 2.5,
1060,1144,0,"Thank you for your response, but what if the signal exhibits a high seasonality (i.e. a lot of network measurements are characterized by a daily and weekly pattern at the same time, for example night vs day or weekend vs working days)? An approach based on standard deviation will not work in that case.",2010-08-02T20:57:49.240,667,CC BY-SA 2.5,
1061,1144,0,"For example, if I get a new sample every 10 minutes, and I'm doing an outlier detection of the network bandwidth usage of a company, basically at 6pm this measure will fall down (this is an expected an totaly normal pattern), and a standard deviation computed over a sliding window will fail (because it will trigger an alert for sure). At the same time, if the measure falls down at 4pm (deviating from the usual baseline), this is a real outlier.",2010-08-02T20:58:18.850,667,CC BY-SA 2.5,
1062,1132,2,"I reckon loads of computer games are modelling problems dressed in disguise. SimCity for example - the goal of the game is to build as good a model as possible of the hidden game mechanics, then use that model to build a functioning city! (This is all probably a gross over-justification for wasting my youth playing SimCity)",2010-08-02T21:03:44.017,668,CC BY-SA 2.5,
1065,1147,2,"Yeah, this is exactly what I am doing: until now I manually split the signal into periods, so that for each of them I can define a confidence interval within which the signal is supposed to be stationary, and therefore I can use standard methods such as standard deviation, ...
The real problem is that I can not decide the expected pattern for all the signals I have to analyze, and that's why I'm looking for something more intelligent.",2010-08-02T21:37:03.073,667,CC BY-SA 2.5,
1066,1142,3,"Just for clarity, here's the original question on SO: http://stackoverflow.com/questions/3390458/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series",2010-08-02T21:42:06.913,71,CC BY-SA 2.5,
1067,1142,4,I think we should encourage posters to post links as part of the question if they have posted the same question at another SE site.,2010-08-02T21:47:31.343,,CC BY-SA 2.5,user28
1068,886,7,"As, arguably, a machine learner, I'm here to tell you we maximise the heck out of likelihoods. All the time. Loads of machine learning papers start with ""hey look at my likelihood, look how it factorises, watch me maximise"". I'd suggest that it's dangerous to claim a fundamental basis of either discipline in terms of inference techniques. It's more about which conference you go to!",2010-08-02T21:47:52.763,668,CC BY-SA 2.5,
1070,1148,1,"Again, this works pretty well if the signal is supposed to have a seasonality like that, but if I use a completely different time series (i.e. the average TCP round trip time over time), this method will not work (since it would be better to handle that one with a simple global mean and standard deviation using a sliding window containing historical data).",2010-08-02T22:02:16.023,667,CC BY-SA 2.5,
1071,1148,2,Unless you are willing to implement a general time series model (which brings in its cons in terms of latency etc) I am pessimistic that you will find a general implementation which at the same time is simple enough to work for all sorts of time series.,2010-08-02T22:06:07.470,,CC BY-SA 2.5,user28
1072,1148,1,"Another comment: I know a good answer might be ""so you might estimate the periodicity of the signal, and decide the algorithm to use according to it"", but I didn't find a real good solution to this other problem (I played a bit with spectral analysis using DFT and time analysis using the autocorrelation function, but my time series contain a lot of noise and such methods give some crazy results mosts of the time)",2010-08-02T22:06:34.413,667,CC BY-SA 2.5,
1073,1148,1,"A comment to your last comment: that's why I'm looking for a more generic approach, but I need a kind of ""black box"" because I can't make any assumption about the analyzed signal, and therefore I can't create the ""best parameter set for the learning algorithm"".",2010-08-02T22:09:22.837,667,CC BY-SA 2.5,
1074,1147,1,"Here is a one idea: Step 1: Implement and estimate a generic time series model on a one time basis based on historical data. This can be done offline. Step 2: Use the resulting model to detect outliers. Step 3: At some frequency (perhaps every month?), re-calibrate the time series model (this can be done offline) so that your step 2 detection of outliers does not go too much out of step with current traffic patterns. Would that work for your context?",2010-08-02T22:24:42.510,,CC BY-SA 2.5,user28
1075,108,0,I love that blog!,2010-08-02T22:31:03.767,582,CC BY-SA 2.5,
1076,1147,0,"Yes, this might work. I was thinking about a similar approach (recomputing the baseline every week, which can be CPU intensive if you have hundreds of univariate time series to analyze).
BTW the real difficult question is ""what is the best blackbox-style algorithm for modeling a completely generic signal, considering noise, trend estimation and seasonality?"".
AFAIK, every approach in literature requires a really hard ""parameter tuning"" phase, and the only one automatic method I found is an ARIMA model by Hyndman (http://robjhyndman.com/software/forecast/). Am I missing something?",2010-08-02T22:38:45.803,667,CC BY-SA 2.5,
1077,1147,1,"Please keep in mind I'm not too lazy for investigating these parameters, the point is that these values need to be set according to the expected pattern of the signal, and in my scenario I can't make any assumption.",2010-08-02T22:40:16.397,667,CC BY-SA 2.5,
1078,1147,1,ARIMA models are classic time series models that can be used to fit time series data. I would encourage you to explore the application of ARIMA models. You could wait for Rob to be online and perhaps he will chime in with some ideas.,2010-08-02T22:44:41.197,,CC BY-SA 2.5,user28
1079,1026,0,"@Tal. Thanks for finding that. While it sounds like a general statement, I *think* he probably meant ""Beware of looking at p-values after doing model selection on the basis of ...."" I don't think it makes sense otherwise. Certainly the discussion in that thread was all about p-values",2010-08-02T23:12:30.950,159,CC BY-SA 2.5,
1080,1128,2,"+1 Ha. Speaking of overlap, this may be the only answer with none over Epstein's reasons.",2010-08-02T23:49:07.253,251,CC BY-SA 2.5,
1082,1098,0,"Interesting, thanks.  I wonder though if it's not the Fisher that's more appropriate here -- since in the urn formulation of the problem, N is observed after the experiment, and you condition on the sum.  Quite likely, I'm one of the ""confused"" the wikipedia article refers to. :-)",2010-08-02T23:52:10.697,251,CC BY-SA 2.5,
1083,795,0,"Do you mean normalize to (0,1)? Please explain clearly what you are asking.",2010-08-03T01:08:43.380,159,CC BY-SA 2.5,
1084,1126,0,I think I like the overall classification given by the answers here more than Epstein's.,2010-08-03T01:44:08.383,251,CC BY-SA 2.5,
1086,1149,4,Really great question.  The best way to understand something is from multiple direction of explanation.,2010-08-03T02:29:31.550,253,CC BY-SA 2.5,
1087,1026,3,"@Tal, @Rob: In that thread, he does say ""Be sure to use the hierarchy principle"".  Perhaps of interest, this discussion from medstats (scroll down for Harrell's response): http://groups.google.com/group/medstats/browse_thread/thread/86c44163b849572",2010-08-03T02:38:20.563,251,CC BY-SA 2.5,
1088,1153,1,"+1 from me, excellent. So > 1.5 X inter-quartile range is the consensus definition of an outlier for time-dependent series? That would be nice to have a scale-independent reference.",2010-08-03T03:06:39.653,438,CC BY-SA 2.5,
1089,1153,1,"The outlier test is on the residuals, so hopefully the time-dependence is small. I don't know about a consensus, but boxplots are often used for outlier detection and seem to work reasonably well. There are better methods if someone wanted to make the function a little fancier.",2010-08-03T03:45:06.910,159,CC BY-SA 2.5,
1090,538,0,"I have some trouble understanding why arrow directions in a corresponding Bayesian network have any relation to causation. For instance, A->B and B->A represent different directions for causality, but Bayesian networks for those two structures are equivalent",2010-08-03T06:41:21.290,511,CC BY-SA 2.5,
1091,1160,0,Probably better suited to stackoverflow since it has no particular data analysis relevance.,2010-08-03T08:28:02.897,5,CC BY-SA 2.5,
1092,795,0,This question is proposed to be closed. See this meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed,2010-08-03T08:58:14.633,,CC BY-SA 2.5,user28
1093,1160,0,This question is proposed to be closed. See this meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed,2010-08-03T08:58:48.143,,CC BY-SA 2.5,user28
1102,1171,0,"Ah, yes, I knew that overlapping CI's didn't necessitate zero difference, but hadn't connected that to this example, where clearly the above procedure would always compute difference ""CI"" that includes zero whenever the individual CIs overlap. Thanks!",2010-08-03T12:39:56.020,364,CC BY-SA 2.5,
1103,1061,1,@gd047. LaTeX code: `$\sigma_{\epsilon}$`,2010-08-03T13:23:24.293,159,CC BY-SA 2.5,
1105,1170,4,"""sometimes"" is an understatement... the authors logic isn't often this direct but the stimulus / reward scenario is such that people will do this as a matter of conditioning",2010-08-03T13:52:52.740,601,CC BY-SA 2.5,
1106,1175,0,The data isn't necessary rates. It could be anything.,2010-08-03T14:07:10.257,8,CC BY-SA 2.5,
1107,1171,2,A useful fact given in these references is that non-overlapping 84% confidence intervals *do* approximate a 95% level test.,2010-08-03T14:08:29.083,279,CC BY-SA 2.5,
1108,1176,1,Is it available as a video? It sounds great.,2010-08-03T14:12:15.673,442,CC BY-SA 2.5,
1109,1176,1,"I think the word is ""will be eventually"" -- keynotes got recorded.",2010-08-03T14:18:07.673,334,CC BY-SA 2.5,
1110,1173,7,Helping your field come to a consensus on just the se v. sd question would be a huge advance.  They mean completely different things.,2010-08-03T14:41:26.260,601,CC BY-SA 2.5,
1111,1170,2,"I don't researchers are being dishonest so much as acting out of ignorance.  They don't understand what statistics mean or what assumptions they require, but as you said they clearly understand the stimulus/reward: p > 0.05 => no publication.",2010-08-03T14:44:25.520,319,CC BY-SA 2.5,
1112,1175,0,"Subscription link, unfortunately.",2010-08-03T14:57:26.783,71,CC BY-SA 2.5,
1113,1176,1,"this is easy in ggplot I think, i.e. http://had.co.nz/ggplot2/geom_jitter.html",2010-08-03T14:58:29.120,668,CC BY-SA 2.5,
1115,1175,0,... but here's the Wikipedia link on funnel plots: http://en.wikipedia.org/wiki/Funnel_plot,2010-08-03T14:59:02.240,71,CC BY-SA 2.5,
1116,1180,7,"And so if the mean is the same as the variance, could you conclude that the data was Poisson? Hardly!",2010-08-03T14:59:46.230,247,CC BY-SA 2.5,
1118,1182,1,"I should add that my ""plot only the data and uncertainty"" recommendation should be qualified: when presenting data to an audience that has experience/expertise with the variable being plotted, plot only the data and uncertainty. When presenting data to a naieve audience and when zero is a meaningful data point, I'd first show the data extending to zero so that the audience can get oriented to the scale, then zoom in to show just the data and uncertainty.",2010-08-03T15:10:49.760,364,CC BY-SA 2.5,
1119,1176,1,`jitter` is also in plain R.,2010-08-03T15:14:10.347,,CC BY-SA 2.5,user88
1120,1176,0,"Well, I didn't mention a need to jitter. Frank's example had relatively few points.  I simply mentions alpha blending because it may make mixing the dots and the bars easier.",2010-08-03T15:18:15.560,334,CC BY-SA 2.5,
1121,1173,0,I agree - se is usually chosen because it gives a smaller region!,2010-08-03T15:20:39.920,8,CC BY-SA 2.5,
1122,1182,0,"since you've went to trouble of writing R code, could you include a jpeg image of the final plot. I find just uploading the image to http://img84.imageshack.us/ and linking to it is fairly easy. Oh thanks for the answer :)",2010-08-03T15:26:43.643,8,CC BY-SA 2.5,
1123,1179,0,I've added a small section on why I dislike these plots.,2010-08-03T15:34:58.710,8,CC BY-SA 2.5,
1124,1121,0,This can all be implemented in Max/MSP/Jitter with the [peak] and [trough] objects for the first example and with [jit.3m] for the second example.,2010-08-03T15:40:01.393,162,CC BY-SA 2.5,
1126,1182,0,@csgillespie: done.,2010-08-03T15:46:09.180,364,CC BY-SA 2.5,
1127,1153,2,"Really thank you for your help, I really appreciate. I'm quite busy at work now, but I'm going to test an approach like yours as soon as possible, and I will come back with my final considerations about this issue. One only thought: in your function, from what I see, I have to manually specify the frequency of the time series (when constructing it), and the seasonality component is considered only when the frequency is greater than 1. Is there a robust way to deal with this automatically?",2010-08-03T15:59:18.127,667,CC BY-SA 2.5,
1128,1180,3,True. Necessary but not sufficient.,2010-08-03T16:44:18.040,319,CC BY-SA 2.5,
1129,1173,0,Maybe some more informative title?,2010-08-03T18:36:41.790,,CC BY-SA 2.5,user88
1130,1182,0,"I've found that it's easier to read a plot like this with `geom_ribbon()` indicating the error. If you don't like producing apparent estimates for regions between 1 and 2, at least reduce the width of the error bar.",2010-08-03T19:20:49.397,287,CC BY-SA 2.5,
1131,752,2,"I don't get what is so deep in that one, is it only playing with words ?",2010-08-03T19:29:45.997,223,CC BY-SA 2.5,
1133,595,0,"Agreed; wavelets are excellent for picking out non-stationary behavior in high amounts of noise.  You do have to be careful with the DWT, though.  It's not rotation-invariant (although there are modifications of the DWT that are, see e.g. Percival and Walden 2000), so you can lose sharp transients depending on the starting point for your data.  Also, most implementations of the DWT do implicit circularization of the data, so you'd still need to control for that.",2010-08-03T19:57:01.273,61,CC BY-SA 2.5,
1134,1193,0,Edited to disambiguate 'above.',2010-08-03T20:10:46.107,455,CC BY-SA 2.5,
1136,1194,10,"Rather than saying ""this should be closed. Someone should defend it"" how about starting with explaining why you want it closed. Too vague? Then ask for clarification. This seems a reasonable question to me. The asker presents a paper and asks about the difference is between predictive and explanatory statistics. The only change I would make to the question is to clarify exactly the question thus making it easier to vote.",2010-08-03T20:39:24.053,29,CC BY-SA 2.5,
1137,1194,2,I have already offered a reason on the meta thread. I feel that 'meta discussions' about the question would clutter up this particular page.,2010-08-03T20:41:09.837,,CC BY-SA 2.5,user28
1138,1160,1,I wish we could move a question to Stack Overflow the way you can move a question from Super User...,2010-08-03T20:42:08.510,29,CC BY-SA 2.5,
1139,1194,3,@Srikant @JD I'll beef up the question. Thanks for the feedback. I do think that this is a topic that merits discussion.,2010-08-03T20:44:41.797,11,CC BY-SA 2.5,
1140,1194,1,"Your question would good for the community if, instead of telling us your life,  you could define what is (according to you) predictive model and explanatory model. I think nice debates start with clear definitions...",2010-08-03T20:49:23.517,223,CC BY-SA 2.5,
1141,1194,1,Could you pose a question here?  Is the question whether the paper is right?,2010-08-03T20:52:48.987,5,CC BY-SA 2.5,
1142,595,0,"If my memory is good, package wavethresh contains translation invariant denoising (my reference was Coifman 1995) (Note that you talked about rotation, arn't we talking about temporal signals?).",2010-08-03T20:53:23.853,223,CC BY-SA 2.5,
1143,1194,0,"@srikant you seem to not understand the nature of comments under the questions. They are, by definition, meta. They are not answers. They are not questions. They are meta. Having a convention where comments become a pointer to meta conversations in some other location is wasteful and silly.",2010-08-03T20:59:14.013,29,CC BY-SA 2.5,
1145,1194,1,"@JD Perhaps. But, in order to get some control on the process of closing questions there is a meta thread devoted to this issue. If I do not make a mention of the fact that this question is proposed to be closed then the community will not get a chance to have a say whether it should stay open or not. One other issue is consider a person who stumbles on this question in the distant future. All this discussion about whether and why we should keep the question open is a bit irrelevant. I feel that comments should be used to clarify the question not to debate its merits.",2010-08-03T21:19:05.843,,CC BY-SA 2.5,user28
1146,1194,0,"@srikant. That's articulately stated and clear. This discussion probably belongs in the Meta area, however, as it is not specific to the question above. :) yeah, ok, that was a bit of a jackass comment... I could not resist it!   You make a good point. I think we can agree that @wahalulu needed to clarify his question. I think he's moving in the right direction.",2010-08-03T21:46:16.827,29,CC BY-SA 2.5,
1148,1031,1,"Since I am not a mathematician nor a statistician, I would like to restate what you were saying to make sure I did not mis-understand. 

So, you are saying that taking ds^2 (twice the KL) would have a similar meaning as R^2 (in a regression model) for a general distribution. And that this could actually be used to quantify  distances geometrically? Does ds^2 have a name so I can do more reading about this. Is there a paper that directly describes this metric and shows applications and examples?",2010-08-03T22:22:05.357,608,CC BY-SA 2.5,
1149,608,1,"Wow, this is something that will be even harder do obtain I'm afraid; my general point in the whole discussion is that the convergence of those theorems is too weak so the difference may emerge from random fluctuations. (And that it is not working for machine learning, but I hope this is obvious.)",2010-08-03T23:19:47.353,,CC BY-SA 2.5,user88
1150,1153,2,"Yes, I have assumed the frequency is known and specified. There are methods to estimate the frequency automatically, but that would complicate the function considerably. If you need to estimate the frequency, try asking a separate question about it -- and I'll probably provide an answer! But it needs more space than I have available in a comment.",2010-08-03T23:40:47.553,159,CC BY-SA 2.5,
1151,608,0,"That is interesting.  If the convergence is so weak I wonder why people even brought it up in the other question about AIC/BIC, or how they figured out that the cross validation and IC methods were actually convergent; I'm not really involved in machine learning, so the last bit that you hope is obvious isn't obvious to me, could you expand on that point?  Besides, the entire thing may be moot, as I say below - a correlational approach to the problem doesn't look like it will work (at least not while sample sizes are constant).",2010-08-04T00:05:11.603,196,CC BY-SA 2.5,
1153,109,0,"I've been watching this question for a while waiting for clarification from the question asker, but none has been forthcoming.  I've nominated this question to be closed: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed",2010-08-04T00:38:26.360,196,CC BY-SA 2.5,
1154,1185,5,"But this raises an interesting pedagogical problem, at least in Psychology, because as far as I know most introductory statistics books being used in my field do not really discuss robust measures except as an aside.",2010-08-04T00:43:19.297,196,CC BY-SA 2.5,
1155,1205,0,What is N? What is the nature of the weighting scheme? Are the weights known?,2010-08-04T01:25:01.977,,CC BY-SA 2.5,user28
1156,1182,0,"@JoFrwld: I like ribbons too, though I tend to reserve them for cases where the x-axis variable it truly numeric; my version of the ""don't draw lines unless the x-axis variable is numeric"" rule that I profess violating in my answer above :Op",2010-08-04T01:40:54.393,364,CC BY-SA 2.5,
1157,1206,3,Why/how would an important explanatory variable reduce predictive accuracy?,2010-08-04T02:34:10.030,,CC BY-SA 2.5,user28
1158,1210,0,Interesting.  Did you ever come across an R implementation of this ?,2010-08-04T03:16:52.187,253,CC BY-SA 2.5,
1159,1206,3,"@Srikant. This can happen when the explanatory variable has a weak but significant relationship with the response variable. Then the coefficient can be statistically significant but hard to estimate. Consequently, the MSE of predictions can increase when the variable is included compared to when it is omitted. (The bias is reduced with its inclusion but the variance is increased.)",2010-08-04T05:15:22.890,159,CC BY-SA 2.5,
1161,1178,0,"+1 thank you. often I get some ""weired"" results, for example, a normal distribution gets a higher p-value then a poisson one, where lambda is relatively small (so by looks only the normal and poisson are not similiar at all)",2010-08-04T06:51:10.857,634,CC BY-SA 2.5,
1163,562,0,"i added what i think is a pretty good source (unfortunately a Textbook) w/ annotation, in light of your comment/question below my answer. I edited my original answer, so it appears at the end.",2010-08-04T07:07:48.703,438,CC BY-SA 2.5,
1164,1216,0,"ok, let's size I need to calculate for size 45. Every delivery is about once every 3 months.
I'm using excel here. =POISSON(4;98;FALSE). Ressult: 0... Could you give me some more clues?",2010-08-04T07:16:39.543,698,CC BY-SA 2.5,
1165,1216,0,"If the deliveries are every 3 months, and the data in the question are annual, then the average sales between deliveries is 95/4. So you want y such that 1-POISSON(y,95/4,TRUE) is smaller than 0.05. Choosing y=32 will give the out-of-stock probability of 4.1%.",2010-08-04T08:12:41.370,159,CC BY-SA 2.5,
1166,752,0,"I like to think of it as the statisticians equivalent of ""guns don't kill people, people kill people"" not very deep, but important to realise from time to time",2010-08-04T09:17:53.497,127,CC BY-SA 2.5,
1167,1222,2,"Good point, still the main differences are somewhere else;  first, statistics is about fitting a model to the data one has, ML is about fitting a model to data one will have; second, statistics ASSUME that a process one observes is fully driven by some embarassingly trivial ""hidden"" model that they want to excavate, while ML TRIES to make some complex enough to be problem-independent model behave like reality.",2010-08-04T09:28:38.720,,CC BY-SA 2.5,user88
1168,934,0,"@John Still, this is mixing aims with reasons. To great extent you can explain each algorithm in terms of minimizing something and call this something ""loss"". kNN wasn't invented in such a way: Guys, I have thought of loss like this, let's optimize it and see what will happen!; rather Guys, let's say that decision is more less continuous over the feature space, then if we would have a good similarity measure... and so on.",2010-08-04T09:38:57.930,,CC BY-SA 2.5,user88
1169,1176,0,@Dirk I was commenting Mike's comment about ggplot. As a general comment I also used to like these seas of dots for the sake that nothing will better represent the distribution -- until we've made figures that were crushing Acrobat Reader ;-) .,2010-08-04T09:54:30.090,,CC BY-SA 2.5,user88
1170,1222,0,@mbq. That's a rather harsh caricature of statistics. I've worked in five university statistics departments and I don't think I've met anybody who would think of statistics like that.,2010-08-04T09:59:17.247,159,CC BY-SA 2.5,
1172,1223,1,I don't think this is a duplicate because of the nature of the data. The problem discussed on the other question concerned regularly observed time series with occasional outliers (at least that's how I interpreted it). The nature of tick-by-tick data would lead to different solutions due to the exchange opening effect.,2010-08-04T10:09:10.590,159,CC BY-SA 2.5,
1173,1223,0,possible duplicate of [Simple algorithm for online outlier detection of a generic time series](http://stats.stackexchange.com/questions/1142/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series) This question is proposed to be closed as a duplicate. Could you please let us know at the meta thread if and how your context is different from the question I linked?,2010-08-04T10:10:59.450,,CC BY-SA 2.5,user28
1174,1223,0,"@Rob But the exchange opening effect only determines when you have to run the algorithm. The fundamental issue remains the same. Even in network data you have the 'office opening effect' where traffic peaks as soon as an office opens. At the very least, the OP should link to that question, scan the answers there and explain why the solutions there do not work so that a suitable answer can be posted for this question.",2010-08-04T10:12:49.820,,CC BY-SA 2.5,user28
1176,1224,1,I think we need a bit more information. What are you comparing? What sort of data do you have? Is it a single outcome you a measuring per centre?,2010-08-04T10:41:18.757,8,CC BY-SA 2.5,
1177,1223,1,"I agree with @Rob. This kind of data can pose unique challanges, so this is not a duplicate.",2010-08-04T10:46:15.037,5,CC BY-SA 2.5,
1178,1222,2,"@Rob Caricature? I think this is what makes statistics beautiful! You assume all those gaussians and linearities and it just works -- and there is a reason for it which is called Taylor expansion. World is hell of a complex, but in linear approx. (which is often ninety-something% of complexity) embarrassingly trivial. ML (and nonparametric statistics) comes in in these few per cent of situations where some more subtle approach is needed. This is just no free lunch -- if you want theorems, you need assumptions; if you don't want assumptions, you need approximate methods.",2010-08-04T10:54:35.570,,CC BY-SA 2.5,user88
1179,1223,0,This question might eventually get served better here due to its domain-specificity: http://area51.stackexchange.com/proposals/117/quantitative-finance,2010-08-04T11:09:19.307,5,CC BY-SA 2.5,
1180,1222,0,@mbq. Fair enough. I must have misinterpreted your comment.,2010-08-04T11:21:07.587,159,CC BY-SA 2.5,
1181,1205,0,see also this related question: http://stats.stackexchange.com/questions/856/fishers-exact-test-with-weights,2010-08-04T11:22:20.907,442,CC BY-SA 2.5,
1185,1223,1,"I think it belongs here. The question is about analyzing irregularly spaced, very noisy time series.

Have you had a look at ""An Introduction to High-Frequency Finance"" by Dacorogna, Olsen and a bunch of others? Or the papers by the same authors?",2010-08-04T12:30:40.983,247,CC BY-SA 2.5,
1187,1183,2,I'm not sure boxplots or vioplots would be suitable with such a small sample size (n = 6),2010-08-04T12:39:43.527,8,CC BY-SA 2.5,
1188,1099,1,I think asking another question separately is much better than updating your previous question.,2010-08-04T12:42:04.597,,CC BY-SA 2.5,user28
1189,1223,0,I saw the other answer and followed Rob's reasoning. I amended my question to address differences I see.,2010-08-04T13:18:51.890,127,CC BY-SA 2.5,
1190,1170,12,"You must also present something that those ""in power"" (decision makers, supervisors, reviewers) understand. Therefore it has to be in the common language which evolves quite slowly, as those people tend to be older and more resistant to change, largely as it may invalidate their careers hitherto!",2010-08-04T13:29:34.113,229,CC BY-SA 2.5,
1191,1164,14,The Black Swann by Nassim Nicholas Taleb explains why simple models have been used in the financial world and the dangers this has led to. A particular fault is equating very-low probabilities with zero and blindly applying the normal distribution in risk management!,2010-08-04T13:38:31.913,229,CC BY-SA 2.5,
1192,1223,0,@jilles I do not see any edits to your question. Did you save your edits? It may help if you post a link to that question as well and indicate the changes to your question by something like 'Edit'.,2010-08-04T13:39:08.930,,CC BY-SA 2.5,user28
1193,1223,0,I have the Olsen book and it doesn't address the exchange open/close question.,2010-08-04T13:39:26.560,5,CC BY-SA 2.5,
1194,1223,0,"@Srikant: done, @PeterR: do you know of any specific paper of those authors that addresses this question?",2010-08-04T13:46:53.597,127,CC BY-SA 2.5,
1195,1223,0,I wish there is a way to undo my close vote! I think it is clear now that it is not a duplicate.,2010-08-04T13:49:44.400,,CC BY-SA 2.5,user28
1196,1234,0,"I've tried something like this, but this method is not very good at dealing with abrupt changes in the volatility. This leads to underfiltering in quiet periods and overfiltering during more busy times.",2010-08-04T13:53:00.447,127,CC BY-SA 2.5,
1197,1183,0,"Right, I admit I haven't read the question carefully enough, so it was rather a general idea; nevertheless I think that 6 points is minimal but enough for a boxplot. I have made some experiments and they were meaningful. On the other hand, obviously boxplot does not indicate the number of observations (which is an important bit of information here), so I would rather use a combination of it and points.",2010-08-04T13:55:26.417,,CC BY-SA 2.5,user88
1199,1235,0,"By using only the returns you become very vulnerable to ladders (i.e. a sequence of prices that climbs or drops away from the norm, where each individual return is acceptable, but as a group they represent an outlier). Ideally you'd use both the return and the absolute level.",2010-08-04T14:01:02.803,127,CC BY-SA 2.5,
1200,1224,0,Agree; what is the null hypothesis?,2010-08-04T14:01:59.993,,CC BY-SA 2.5,user88
1201,1206,0,"First paragraph is a very, very good point. Still sometimes is even worse; here PMID: 18052912 is a great example that sometimes a better model can be made on the noise part of the set than on a true one -- it is obvious that one can do a good model on random data, but this is a bit shocking.",2010-08-04T14:20:30.940,,CC BY-SA 2.5,user88
1202,825,2,Vote to keep open; very relevant to statisticians because the ways in which our problems can or can not be broken down into parallel streams is of relevance to the question being asked.,2010-08-04T14:40:38.203,196,CC BY-SA 2.5,
1203,830,2,"But, of course, doSMP is exactly what the OP would be looking for if they were working in a Windows environment.",2010-08-04T14:44:16.370,196,CC BY-SA 2.5,
1206,1214,0,"Thank you. Again, I will try this approach as soon as possible and will write here the final results.",2010-08-04T14:49:55.500,667,CC BY-SA 2.5,
1208,1224,0,Comparing a biomarker using hazard ratio as a measure of effect (will be adjusing for other covariates also),2010-08-04T15:18:46.687,,CC BY-SA 2.5,user712
1209,1241,0,What is a ROC curve? Could you please provide a link?,2010-08-04T15:19:32.367,,CC BY-SA 2.5,user28
1210,1243,0,Your last point about regression would only be true if X is random. If X is fixed then Y would also be a normal. no?,2010-08-04T15:28:24.657,,CC BY-SA 2.5,user28
1211,1210,0,"No, not directly.  However, R will give you everything you need to do this--such as: the observed counts, the expected values, and the residuals for each cell.

x <- matrix(c(12, 5, 7, 7), ncol = 2)
chisq.test(x)$expected
chisq.test(x)$observed
chisq.test(x)$residuals",2010-08-04T15:55:25.703,485,CC BY-SA 2.5,
1212,1245,0,"+1 yes, nothing is perfect. Tickdata.com (whose paper is mentioned) also includes outliers and they also strip out too much good data (when compared with another source). Olsen's data is close to being terrible, and I generally just indicative. There's a reason that banks pay big operations teams to work on this.",2010-08-04T16:10:44.320,5,CC BY-SA 2.5,
1213,1241,0,http://en.wikipedia.org/wiki/Receiver_operating_characteristic,2010-08-04T16:54:06.347,,CC BY-SA 2.5,user88
1214,1210,0,"I'll give you the tick, since this should be useful for my research life. However, this approach is applicable to an i x j matrix. However, my question involves an i x j x k matrix,",2010-08-04T17:02:58.283,287,CC BY-SA 2.5,
1215,1183,0,With 6 points - scatter plot is probably best (maybe with adding a red dot for the mean),2010-08-04T17:20:09.427,253,CC BY-SA 2.5,
1216,1031,1,"I think you are far from understanding the point, and I am not sure you should try to go further now. If you are motivated, you can read the paper from Bradley Efron I mentionned or that paper from Amari http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176345779.",2010-08-04T17:25:16.690,223,CC BY-SA 2.5,
1217,1183,2,"I generally use boxplots with superimposed points, I find it very ""visual"". Violin plots, instead, are a bit hard to understand in my opinion.",2010-08-04T17:30:20.317,582,CC BY-SA 2.5,
1219,1254,2,"Something similar to that but includes more information rather than just the name of the test. We have some of these charts in urban and transportation modelling. They'll show a large table where they specify tests per type of problem. They also list caveats, expected time/duration, input and outputs, etc",2010-08-04T17:35:31.277,59,CC BY-SA 2.5,
1220,1255,0,"To compute similarity, I need to count the number of times two items were rated together by the same person.  That count becomes my numerator.  The formula I'm using to calculate similarity between items A & B is 

    AB / A + B - AB

Where AB is the number of times items A and B were rated by the same person, and A and B are the number of times each item was rated overall.

In order to perform that count, I need to generate all of the itemA,itemB pairs in my mapper, and then sum the counts in my reducer.

The items are rated in a binary fashion-if a (userid,itemid) exists, then 1.",2010-08-04T17:43:38.923,738,CC BY-SA 2.5,
1221,1249,2,This question is for Mathoverflow: http://mathoverflow.net/. Seems to be a good question anyway. I remember I saw a generalization of the results in Baraud's paper done by Rozenholc an other collaborators but I can't find the paper anymore...,2010-08-04T17:44:19.610,223,CC BY-SA 2.5,
1222,1210,2,"Chi-square partitioning is extensible to multi-way contingency tables.  Here's the article that Agresti cites in his book, in fact...

H. O. Lancaster (1951) ""Complex Contingency Tables Treated by the Partition of œá2"" Journal of the Royal Statistical Society. Series B (Methodological), Vol. 13, No. 2",2010-08-04T18:13:20.323,485,CC BY-SA 2.5,
1223,1214,2,"Your idea is quite good, but in my case, it fails to detect the periodicity of a really simple (and not so noisy) time series like http://dl.dropbox.com/u/540394/chart.png. With my ""empirical"" approach (based on the autocorrelation), the simple algorithm I wrote returns an exact period of 1008 (having a sample every 10 minute, this means 1008/24/6 = 7, so a weekly periodicity). My main problems are:
1) It's too slow to converge (it requires a lot of historical data) and I need a reactive, online approach;
2) It's inefficient as hell from a memory usage point of view;
3) It's not robust at all;",2010-08-04T18:14:17.607,667,CC BY-SA 2.5,
1224,1257,0,PS: This was posted on mathoverflow.net sometime back. See: http://mathoverflow.net/questions/18302/likelihood-function-for-sequential-random-variables,2010-08-04T18:29:49.823,,CC BY-SA 2.5,user28
1226,1102,0,"Didn't see this for a while. Users don't get notifications about new answers to questions they've contributed answers to themselves. If you leave a followup as a comment instead, you might get an additional response sooner.",2010-08-04T19:18:37.620,287,CC BY-SA 2.5,
1227,1102,0,"Actually, `P(y | grassland)` would be `exp(-2.8) / (1 + exp(-2.8))` A point in the forest would be `exp(-2.8 - 3) / (1 + exp(-2.8  - 3))`

However, you could estimate those probabilities without doing a logistic regression. What would be more interesting would be to look at `exp(-3)`, or `1/exp(-3)`. This says that an animal is 20x more likely on grassland than forest, and if the p-value is <0.05, then that difference is significant.",2010-08-04T19:23:49.840,287,CC BY-SA 2.5,
1228,1249,0,Do you want a lower or a upper bound? The first sentence says upper but the last sentence mentions lower.,2010-08-04T19:24:01.340,,CC BY-SA 2.5,user28
1229,1256,0,I assume that 'without multicore' means without parallelism at all?,2010-08-04T19:37:59.470,,CC BY-SA 2.5,user88
1230,1249,0,@Srikant: you should read the mentionned paper by Barraud which is great to understand the whole thing... heuristically lower bounds are larger (testing is more difficult) when distributions (from the null and alternative) are close. Hence the upper bound asked by mkolar is related to the chi square distance between two distributions (the least favorable in the alternative and the null) if this distance is small the minimum error will be large...,2010-08-04T19:40:09.047,223,CC BY-SA 2.5,
1231,1263,1,"And actually the sapply, could be coded into a multi core framework...",2010-08-04T19:49:36.047,253,CC BY-SA 2.5,
1232,1160,0,I moved it manually to stackoverflow. JD - I agree with you.,2010-08-04T20:14:47.510,253,CC BY-SA 2.5,
1233,1263,0,Agree; still OP wrote that this is not an option.,2010-08-04T20:33:25.307,,CC BY-SA 2.5,user88
1235,1268,0,"Good question! I think you can improve the title, there could be something like ""missing data"" somewhere or ""times series of different length"".",2010-08-04T21:19:03.420,223,CC BY-SA 2.5,
1236,1269,0,+1 replication/resampling are definitely better than zero-padding.. still I'll wait and see if there are any other ideas out there :),2010-08-04T21:43:04.953,170,CC BY-SA 2.5,
1237,1268,1,"I wouldn't call it ""missing data"", perhaps ""SVD dimensionality reduction for time series of different length""?",2010-08-04T21:45:44.267,170,CC BY-SA 2.5,
1238,1261,0,"Do i, j, k have to be integers? In general you have an unidentifiable model (that is the solution is not unique) because if you let p2=p^i, then p^j=p2^j2, where j2=j/i.",2010-08-04T21:45:48.200,279,CC BY-SA 2.5,
1239,1263,0,"#Actually sapply is about 3 times slower than my baseline method
library(rbenchmark);
baseline <- function()
{;
 ncol <- 100;
 nrow <- 100;
 x <- matrix(rnorm(ncol*nrow),nrow,ncol);
 y <- matrix(rnorm(ncol*nrow),nrow,ncol);
 return(diag(cor(t(x),t(y))));
};
sapply.method <- function()
{;
 ncol <- 100;
 nrow <- 100;
 x <- matrix(rnorm(ncol*nrow),nrow,ncol);
 y <- matrix(rnorm(ncol*nrow),nrow,ncol);
 return(sapply(1:100,function(i) cor(x[i,],y[i,])));
};
benchmark(baseline=baseline(),sapply.method(),replications=10);",2010-08-04T21:49:38.477,196,CC BY-SA 2.5,
1240,1263,0,(your mileage may vary on by what factor it is slower),2010-08-04T22:10:15.153,196,CC BY-SA 2.5,
1241,1265,0,"Simulation supports your answer, and even with a transpose operation added to handle cases with more items than subjects, sapply beats diag(cor()), but only when there are notable asymmetries between the N of items and the N of subjects.",2010-08-04T22:21:18.920,196,CC BY-SA 2.5,
1242,1243,0,"Yes, this is true, but for general regression problems (as opposed to anova or designed problems), X really isn't fixed but are observations from the underlying process. However, for the Poisson case, the point still holds, since mixtures of Poissons aren't necessarily Poisson.",2010-08-04T22:23:54.907,732,CC BY-SA 2.5,
1243,1274,4,Correlated with what? Each obs to the one before it?,2010-08-04T22:26:35.010,287,CC BY-SA 2.5,
1244,1263,1,"For 100x100 matrix it is obvious; I believe that the problem requires a lot larger matrices and there my method will be faster. Still, I have written ""your code is equivalent to:"" instead of ""use:"" having that in mind -- this is how possible C/Fortran chunk should work.",2010-08-04T22:29:35.553,,CC BY-SA 2.5,user88
1245,1256,0,That is correct.,2010-08-04T22:32:32.473,196,CC BY-SA 2.5,
1246,1263,0,"Also, from no-multicore requirement one can conclude that this is indeed memory-bounded, so reducing memory complexity from $N^2$ to $N$ is also something worth playing.",2010-08-04T22:35:12.953,,CC BY-SA 2.5,user88
1248,1261,0,I think to demonstrate non-identifiability you have to show that the likelihood is invariant to a transformation of parameters. I am not sure if what you did is sufficient to demonstrate lack of  identification.,2010-08-04T22:50:19.640,,CC BY-SA 2.5,user28
1250,1170,13,"Good point. ""I understand p-values.  Just give me a p-value.""  Ironically, they probably do *not* understand p-values, but that's another matter.",2010-08-05T00:22:37.490,319,CC BY-SA 2.5,
1251,1277,0,Will the usual dw test be appropriate?,2010-08-05T00:33:10.403,273,CC BY-SA 2.5,
1252,752,0,"So it's an inane platitude used for quibbling over semantics? ""Cigarettes don't cause cancer; people cause cancer."" ""Landmines don't maim people; people maim people.""",2010-08-05T00:36:02.957,753,CC BY-SA 2.5,
1253,823,6,"That's because consensus is a practical means of establishing the validity of a scientific position. Sound science cannot be conducted by a single person. It requires peer review. Having achieved a popular consensus implies that the science in question has passed peer review. Whereas a theory or position held by a lone individual and opposed by the rest of the scientific community is likely to have failed the process of peer review.

If you're not an expert in field X, then statistically you're better off following popular consensus within field X.",2010-08-05T00:50:10.697,753,CC BY-SA 2.5,
1254,1277,1,"The dw test is designed for checking autocorrelation in the residuals from a regression. There is no regression here, so it is not appropriate.",2010-08-05T00:57:48.733,159,CC BY-SA 2.5,
1255,823,5,"To elaborate: if you're a politician who has no background in field X, then it is far better that you simply consult the consensus of experts in that field of research than to misinterpret the data first-hand.

What is problematic is when lay persons disregard overwhelming scientific consensus for data misrepresented to them by a fringe minority--especially when these lay persons are in charge of policy decisions. It's much easier to mislead a handful of politicians than thousands of expert researchers...",2010-08-05T01:02:53.433,753,CC BY-SA 2.5,
1256,1273,0,"this seems interesting, but sadly I can't find even a portion of the book from google books, will have a look on it, thanks!",2010-08-05T01:14:49.277,588,CC BY-SA 2.5,
1260,1271,1,Thank you for the lead Brett!  I wonder if someone got to implement it by now in R (I'd guess not).,2010-08-05T01:43:35.037,253,CC BY-SA 2.5,
1261,1261,0,"@Srikant Vadali I just did. Replace p^i with p, j with j/i and k with k/i and you get the exact same probabilities as an output with a different set of parameters (unless i=1 - with that constraint the model is identifiable).",2010-08-05T03:08:28.630,279,CC BY-SA 2.5,
1262,1278,0,Can you have a control group (ward)?,2010-08-05T03:10:53.713,279,CC BY-SA 2.5,
1263,1271,1,"Right. R has lots of routines for assisting with bootstrap and other randomization methods, but I don't know that you'll find anything specific to this problem.",2010-08-05T03:15:46.050,485,CC BY-SA 2.5,
1264,1274,0,I'd hope not - the correlation at lag one is very close to 0.,2010-08-05T03:37:28.007,196,CC BY-SA 2.5,
1266,1278,0,"no, the ward itself will be its control",2010-08-05T04:34:52.127,588,CC BY-SA 2.5,
1267,1277,0,I agree. Books mislead again. http://sas-and-r.blogspot.com/2010/07/example-81-digits-of-pi.html,2010-08-05T04:47:04.203,273,CC BY-SA 2.5,
1268,1274,0,"Or in Prof Wecker's words: ""The result was a = .01, which implies little or no serial dependence in the data.""",2010-08-05T04:55:59.030,273,CC BY-SA 2.5,
1269,1268,1,I like the title you propose !,2010-08-05T05:27:53.393,223,CC BY-SA 2.5,
1270,1286,0,Has anyone noticed that the number of questions that should be in StackOverflow R rocketed up here recently?,2010-08-05T05:32:45.083,601,CC BY-SA 2.5,
1271,1256,0,"the question in addition to the answers make me think this should be stackexchange... there should be a discussion on meta about questions with ""how to speed up"" ....",2010-08-05T05:33:21.887,223,CC BY-SA 2.5,
1273,1286,1,Why not making random sampling?,2010-08-05T07:21:32.873,,CC BY-SA 2.5,user88
1274,752,4,well... there is a fine line between inane platitude and profound wisdom. I like the quote for it's poetic quality. Any insight is of secondary importance to me.,2010-08-05T07:25:08.297,127,CC BY-SA 2.5,
1275,1245,0,I like your idea about using known arbitrage relations. have you tried this at all in your previous job?,2010-08-05T07:27:53.860,127,CC BY-SA 2.5,
1276,1256,0,The discussion Robin called for is now on meta: http://meta.stats.stackexchange.com/questions/248/how-to-speed-up-questions,2010-08-05T07:51:58.757,196,CC BY-SA 2.5,
1277,1263,0,"The no-multicore was more a practical restriction, my colleague does not have a multicore machine, so parallelism wasn't going to help him.  I'm sure there is an entire other question to be asked regarding how to know a priori whether a parallel approach (with associated overhead) will be faster than a single threaded approach.",2010-08-05T07:58:09.417,196,CC BY-SA 2.5,
1278,1286,4,"@John: If you feel that way discuss the issue at http://meta.stats.stackexchange.com/questions/248/how-to-speed-up-questions, no need to be snarky.",2010-08-05T07:59:42.723,196,CC BY-SA 2.5,
1279,1286,0,"@mbq:  Random sampling will quickly provide a reasonable approximate, especially with well behaved data.  However, I did specify that my goal was an exact test.",2010-08-05T08:04:02.603,196,CC BY-SA 2.5,
1280,1286,0,@drknexus That's why it was a comment not an answer.,2010-08-05T09:19:40.690,,CC BY-SA 2.5,user88
1281,1289,4,I think you mean bar charts rather than histograms,2010-08-05T10:09:06.270,159,CC BY-SA 2.5,
1283,1261,0,"@Aniko Perhaps, I am missing something here. But, when I substituted the two sets of parameters into the likelihood function that I suggested they are not identical (assuming that I did not make any errors which is always possible).",2010-08-05T10:26:27.343,,CC BY-SA 2.5,user28
1284,1289,0,@Rob: Isn't histogram a special type of bar chart that represents a frequency distribution? I am trying to visualize category frequencies for many bookstores.,2010-08-05T10:28:07.700,760,CC BY-SA 2.5,
1285,1290,0,But will this cope with very large combinations?,2010-08-05T10:40:03.770,8,CC BY-SA 2.5,
1286,1289,1,"@nimcap No, because histogram is over a continuous variable, and book category is a categorical variable.",2010-08-05T10:46:38.593,,CC BY-SA 2.5,user88
1287,1291,0,"Thank you for your valuable answer. Situation is hard to describe even in my native language :) Let me  try. I am not interested if bookstores are favoring particular categories but I want to see if they favor categories. Actually this is what I am expecting. Let's say I have 3 bookstores (B1, B2, B3) and 4 categories (C1, C2, C3, C4). These are their sales data: B1(1, 1, 20, 20) B2(90, 1, 1, 1), B3(1, 1, 1, 30). Looking at this data I can tell they favor some categories to others. But if data was like B1(20, 30, 20, 20) B2(90, 100, 100, 100), B3(30, 30, 40, 40) I cant say that.",2010-08-05T10:53:24.123,760,CC BY-SA 2.5,
1288,1289,0,"@mbq Let's say a book store has 3 books, and their categories are: B1:[c1, c2, c3] B2:[c1, c3] B3:[c1, c4]. When we aggregate the category counts we get [c1 x 3, c2 x 1, c3 x 2, c4 x 1]. Isn't this enough to generate a histogram?",2010-08-05T10:56:34.337,760,CC BY-SA 2.5,
1289,1290,0,"@csgillespie Well, I believe so -- it works *in situ*,  so only one combination is stored in memory at a time, and the results of simulation can be also aggregated to eliminate the need of storing them. This will of course work terribly long, but exhaustive searches usually do. For speed it could be written in C, but then along with the simulation part, which probably is way slower than a generator step.",2010-08-05T11:11:21.973,,CC BY-SA 2.5,user88
1290,1289,2,"@nimcap No, it is enough to generate a bar chart. Histogram can be done for instance for a price of a book.",2010-08-05T11:13:42.140,,CC BY-SA 2.5,user88
1291,1289,0,"@mbq I really don't get it, wherever I look it says histogram is about frequencies, can you point to any sources mentioning histogram is for continuous variables.",2010-08-05T11:39:43.660,760,CC BY-SA 2.5,
1292,1287,0,"I assume you have the book, what's in chapter 17 (it's referenced in the document you provided)",2010-08-05T11:41:58.197,59,CC BY-SA 2.5,
1293,1245,0,"No, we never fully formalized that.  But I think we used some simple ones (ie ETF vs underlying index etc).  It's been a few years though.",2010-08-05T11:43:48.130,334,CC BY-SA 2.5,
1294,1294,1,"Parallel plots depend heavily on the ""right"" ordering of variables, so for too many categories this will become tedious. And the correct source seems to be A.Inselberg, 1981.",2010-08-05T11:44:55.993,56,CC BY-SA 2.5,
1295,1294,3,They're called parallel coordinate plots: http://en.wikipedia.org/wiki/Parallel_coordinates,2010-08-05T12:12:39.053,495,CC BY-SA 2.5,
1296,1298,0,Well I already know p. I also know the amount of events detected: k. So the total events is somewhere around k/p. I would like to find out an interval around k/p so I can be say 95% sure that the total number of events is inside it. Does that make more sense?,2010-08-05T12:24:21.903,762,CC BY-SA 2.5,
1297,1295,0,"i'm wonder if this is a practical limitation though--for a variable that only weakly influences classification, my intuition is that Tree won't likely split on that variable (i.e., it's not going to be a node) which in turn means it's invisible as far as Decision Tree classification goes.",2010-08-05T12:52:48.133,438,CC BY-SA 2.5,
1298,1291,0,"In my example, shops O-Y favour romance books. This is why these shops are in a distinct group in the PC plot.",2010-08-05T13:00:10.447,8,CC BY-SA 2.5,
1299,1295,0,"I am talking of weak interactions, not weak effects on classification. An interaction is a relationship between two of the predictor variables.",2010-08-05T13:03:44.480,159,CC BY-SA 2.5,
1300,1295,2,"This may be inefficient, but tree structure can handle it.",2010-08-05T13:12:07.147,,CC BY-SA 2.5,user88
1301,1295,0,"That's why I said inefficient rather than biased or incorrect. If you have loads of data, it doesn't matter much. But if you fit a tree to a few hundred observations than the assumed interactions can greatly reduce the predictive accuracy.",2010-08-05T13:15:49.477,159,CC BY-SA 2.5,
1302,1273,0,"Actually, you don't need the book for this (you definitely should read it, but not for your current problem). All you have to do is plot the time intervals between infections and build your control chart based on this variable. Try it, you'll see that it's quite simple.",2010-08-05T13:18:39.300,666,CC BY-SA 2.5,
1303,1289,0,"@nimcap It is about frequencies that some continuous variable fits in a particular interval (bin). After making the cut of space into bins, it boils down to a bar chart, still with richer interpretation.",2010-08-05T13:21:43.727,,CC BY-SA 2.5,user88
1304,1294,0,"@Simon thanks; @honk I agree, this is one reason why I don't use them.",2010-08-05T13:24:00.787,,CC BY-SA 2.5,user88
1310,1309,0,I am not sure if the multiple types solution will work here.,2010-08-05T14:32:32.040,,CC BY-SA 2.5,user28
1311,1293,0,Converted to community wiki.,2010-08-05T14:33:26.487,,CC BY-SA 2.5,user88
1312,1308,1,I presume that it's bioinformatics problem,2010-08-05T14:41:27.627,8,CC BY-SA 2.5,
1314,1309,0,I think that generalisation still only works for 2 or more people sharing a birthday - just that you can have different sub-classes of people.,2010-08-05T14:45:02.190,765,CC BY-SA 2.5,
1315,1308,3,"It is actually a bioinformatics problem, but since it boils down to the same concept as the birthday paradox I thought I'd save the irrelevant specifics!",2010-08-05T14:50:57.560,765,CC BY-SA 2.5,
1316,1308,4,"Normally I would agree with you, but in this case the specifics might matter since there could already be a bioconductor package that does what you ask.",2010-08-05T14:53:30.957,8,CC BY-SA 2.5,
1317,1291,2,"I voted this up as a good general answer but as a practical answer, dealing with that many data points is going to be brutal.",2010-08-05T15:16:13.710,601,CC BY-SA 2.5,
1318,1308,0,"If you really want to know, it's a pattern finding problem where I'm trying to accurately estimate the probability of a given level of enrichment of a subsequence within a set of larger sequences.  I therefore have a set of subsequences with associated counts and I know how many subsequences I observed and how many theoretically observable sequences are available.  If I saw a particular sequence 10 times out of 10,000 observations I need to know how likely that was to have occurred by chance.",2010-08-05T15:21:10.573,765,CC BY-SA 2.5,
1319,1290,2,"That looks almost identical to how R's combn function is already doing things.  I wrote up a version of combn that does take combinations off the stack one at a time, and as mbq says because it is only storing one combination in memory at a time it can handle very large combinations.  The problem with doing it in R is that doing a step-by-step approach in a function typically involves reading the state variables into the function, manipulating them, then storing them back out to global - which seems to just slow everything /way/ down.",2010-08-05T15:34:43.297,196,CC BY-SA 2.5,
1320,1311,0,"Will this solution suffer from the curse of dimensionality? If instead of n=365, n=10^6 is this solution still feasible?",2010-08-05T15:38:04.510,8,CC BY-SA 2.5,
1322,1311,0,"Some approximations may have to be used to deal with high dimensions. Perhaps, use Stirling's approximation for factorials in the binomial coefficient. To deal with the product terms you could take logs and compute the sums instead of the products and then take the anti-log of the sum.",2010-08-05T15:45:53.913,,CC BY-SA 2.5,user28
1324,1312,0,"There may be notational ambiguities in what I wrote but $X_3$ is a random variable as it is a function of two random variables $Y_1$ and $Y_2$. In fact, you can compute $P(X_3=X_{31}) = P(Y_1 \  Y_2 > 0)$.",2010-08-05T16:11:28.367,,CC BY-SA 2.5,user28
1325,1311,0,There are also several other forms of approximations possible using for example the Taylor series expansion for the exponential function. See the wiki page for these approximations: http://en.wikipedia.org/wiki/Birthday_problem#Approximations,2010-08-05T16:15:05.790,,CC BY-SA 2.5,user28
1326,1214,0,"Thank you. Unfortunately, this still doesn't work as I would expect. For the same time series of the previous comment it returns 166, which is only partially right (from my point of view, the evident weekly period is more interesting). And using a very noisy time series, like this one http://dl.dropbox.com/u/540394/chart2.png (a TCP receiver window analysis), the function returns 10, while I would expect 1 (I can't see any obvious periodicity). BTW I know that it will be really difficult to find what I'm looking for, since I'm dealing with too different signals.",2010-08-05T16:17:41.543,667,CC BY-SA 2.5,
1327,1312,0,"$X_3$ is random, but a deterministic function of other random quantities in your model. It therefore should not occur explicitly in the likelihood. In the context of graphical modelling, you would refer to $X_3$ as a deterministic node.",2010-08-05T16:30:06.397,643,CC BY-SA 2.5,
1328,1291,1,"+1 This is certainly not what OP wants, still it is certainly what she/he should want.",2010-08-05T16:39:07.180,,CC BY-SA 2.5,user88
1329,1297,2,"From a ML point of view trees can be tested in a same way as any other classifier (CV for instance). Still it rather shows that heavy overfit happened ;-) Also RF escapes multicollinearity not because it is ensemble, but because its trees are suboptimal.",2010-08-05T16:48:36.623,,CC BY-SA 2.5,user88
1330,1295,2,"Agree; I just wanted to highlight it. Still I think that reduction of predictive accuracy can be removed by using proper training; in phylogenetics the similar problem (greedyness) is reduced by Monte Carlo scanning of the possible tree space to find maximum likelihood ones -- I don't know is there a similar approach in stats, probably no-one was bothered by this problem to such extent.",2010-08-05T16:55:49.580,,CC BY-SA 2.5,user88
1333,1153,0,"I finally had the time to play a bit with this method, and this seems  a good compromise (even if I still have to evaluate how complex is the stl() routine from a memory/cpu point of view), which works pretty well if we known the frequency of the time series. BTW, I noticed that the results of the detection heavily depend on the kind of signal I'm analyzing, and in order to obtain the optimum results I have to manually tune the probabilities for the quantiles or the multiplier for the IQR.
Since you're an expert of this topic, is there a ""smart"" and ""de facto"" solution to deal with this issue?",2010-08-05T18:33:56.440,667,CC BY-SA 2.5,
1337,1316,0,"i was actually going to efforts to *avoid* mentioning the Gamma distribution. i saw it on Wikipedia, i cannot actually find the formula for the distribution, or the formulas to estimate the parameters in that formula. And then i got really nervous when i saw *""There is no closed-form solution for k.""*  And i tried it anyway with some formulas - but when you get a packet that comes back in 0ms, the ln(0) blows up.",2010-08-05T19:40:33.160,775,CC BY-SA 2.5,
1338,1316,0,"Because while i have good understanding of the normal distribution, from my university days, i am over my head when we get to things like *""Kullback‚ÄìLeibler divergence""*.",2010-08-05T19:42:41.593,775,CC BY-SA 2.5,
1339,1315,1,"Just looking at it, it looks like a skewed normal distribution. Are you sure the outliers are necessary for your analysis?",2010-08-05T19:43:35.447,776,CC BY-SA 2.5,
1340,1316,0,"If you have a packet that comes back in 0ms it is not 'really' zero, right? Perhaps, you can set it to a small value and in any case you need not worry about ln(0) as the parameter $k$ does not refer to the data you have. I will update my answer with some details about the estimation process.",2010-08-05T19:45:12.510,,CC BY-SA 2.5,user28
1341,1315,1,My analysis will consist solely of drawing a pretty graph over-top the bars :)   But it would be cheating to pretend there was no top tail...,2010-08-05T19:53:13.040,775,CC BY-SA 2.5,
1342,1316,0,"Yes, technically it should be referred to as `<1ms`. And this plot doesn't include zero, because it's going over a higher latency link  (modem). But i can run the program just as well over a faster link (i.e. ping another machine on the LAN), and routinely get `<1ms` and `1ms`, with much less occurrences of `2ms`. Unfortunately Windows only provides resolution of `1ms`. i could manually time it using a high-performance counter, getting ¬µs; but i was still hoping to be able to put them into buckets (to save memory). Perhaps i should add 1ms to everything... `1ms ==> (0..1]`",2010-08-05T19:55:28.323,775,CC BY-SA 2.5,
1343,1318,0,Really i want to draw the mathematical curve that follows the distribution. Granted it might not be a known distribution; but i can't imagine that this hasn't been investigated before.,2010-08-05T19:57:17.073,775,CC BY-SA 2.5,
1345,138,1,http://stackoverflow.com/questions/3375808/learning-r-where-does-one-start on SO,2010-08-05T21:05:06.750,776,CC BY-SA 2.5,
1346,1321,2,"You mean like any one of the numerous ggplot2 example charts such as
[this](http://learnr.wordpress.com/2009/05/18/ggplot2-three-variable-time-series-panel-chart/) or did you have something more specific in mind?",2010-08-05T21:46:47.150,334,CC BY-SA 2.5,
1347,1321,1,"For what it is worth, I like plotting with `xts` and `zoo` objects where different series can be merged easily.",2010-08-05T21:52:29.420,334,CC BY-SA 2.5,
1352,1326,0,"I doubt that anyone is going to top this one. Thanks Matt, great answer!",2010-08-05T23:48:22.267,1356,CC BY-SA 2.5,
1354,1322,5,"Here's an example of your suggestion from the NYTimes

http://www.nytimes.com/imagepages/2010/05/02/business/02metrics.html",2010-08-06T00:57:28.210,287,CC BY-SA 2.5,
1355,1297,2,"For a probabilistic framework of decision trees, see DTREE (url: http://www.datamining.monash.edu.au/software/dtree/index.shtml) which is based on the paper ""Wallace C.S. & Patrick J.D., `Coding Decision Trees', Machine Learning, 11, 1993, pp7-22"".",2010-08-06T01:17:00.280,530,CC BY-SA 2.5,
1357,1322,0,"That's an interesting visualization. JoFrhwld, thanks for the example.",2010-08-06T02:14:34.127,776,CC BY-SA 2.5,
1359,1337,10,I made this community wiki as there is no correct answer.,2010-08-06T02:44:34.817,159,CC BY-SA 2.5,
1360,1214,0,"166 is not a bad estimate of 168. If you know the data are observed hourly with a weekly pattern, then why estimate the frequency at all?",2010-08-06T03:01:01.493,159,CC BY-SA 2.5,
1361,1340,2,"http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1043351251

Freely available document written by Peter Huber on John Tukey's contributions to robust statistics. Reasonably easy read, light on the formulae.",2010-08-06T03:08:39.293,781,CC BY-SA 2.5,
1364,1214,0,"Because I have to analyze a lot of time series (suppose 100 network metrics), and only some of them have a weekly periodicity. In any case, I guess in my implementation I will use an algorithm similar to your function, and I'll manually distinguish the weekly periodicity. Really thanks for your support, I really appreciate (and keep up the good work with the forecast library :-))",2010-08-06T03:55:59.060,667,CC BY-SA 2.5,
1365,1339,0,"Thank you for your precious informations, I'll look at that book for sure.",2010-08-06T03:57:38.187,667,CC BY-SA 2.5,
1367,1304,0,"Thanks, that looks great. I think this is the answer I was looking for.",2010-08-06T06:18:16.917,762,CC BY-SA 2.5,
1368,1327,0,"+1 i thought binomial as well, when i first saw the histogram. (Not sure why this got downvoted).",2010-08-06T06:35:35.057,438,CC BY-SA 2.5,
1370,1349,1,A simple help through the rseek.org or r-help mailing list will get you on your way with dealing with most (all?) of the methods in R (which is the program package I would suggest to anyone). Good link.,2010-08-06T07:47:25.943,144,CC BY-SA 2.5,
1373,1356,1,I think originaly taken from Yihui XIE's Statistics Jokes Slides (http://www.yihui.name/en/attachment.php?f=attachment/jokes_yihui.pdf),2010-08-06T09:57:54.707,114,CC BY-SA 2.5,
1375,1355,2,"It'd be fun to see your code once you write it :)  Sorry, I don't remember coming across such a thing...",2010-08-06T11:04:58.043,253,CC BY-SA 2.5,
1376,1318,4,Look up 'density estimation'.,2010-08-06T11:21:01.607,247,CC BY-SA 2.5,
1377,1355,0,I did something similar a while back to analyze the most-frequently-landed-on spaces in Monopoly (wrote a small simulator)... not sure if there's another way of doing it.,2010-08-06T11:47:54.013,292,CC BY-SA 2.5,
1378,1355,0,Seems off-topic. I am not sure I see the link to statistical analysis in the question. This seems appropriate for http://stackoverflow.com.,2010-08-06T12:21:39.367,,CC BY-SA 2.5,user28
1379,1355,0,"Strikant - since it can be solved with simulation, I think keeping the question will be o.k.",2010-08-06T12:45:52.503,253,CC BY-SA 2.5,
1380,1365,0,"Thank you, this is really helpful. (Shame on me for not recognizing a Fourier series even when hurtling towards it...)",2010-08-06T12:55:07.460,650,CC BY-SA 2.5,
1381,1355,0,I agree with it Tal. It's a statistical computing problem.,2010-08-06T12:58:32.320,8,CC BY-SA 2.5,
1383,1275,0,extrapolation would include smoothness in the filled part that does not exists in the existing part. You have to add randomness... hence resampling (and resmapling on the extrapolation seems to be a good idea),2010-08-06T14:28:13.300,223,CC BY-SA 2.5,
1384,1369,4,I don't really understand what you are asking. Could you try and reword your question?,2010-08-06T14:28:50.943,8,CC BY-SA 2.5,
1385,1275,0,Extrapolating the model would require sampling the error term which would induce the desired randomness.,2010-08-06T14:53:04.030,,CC BY-SA 2.5,user28
1387,1350,2,"A 'correct' answer is dependent on the model and the code/commands you are using. So, a brief description of both would be useful.",2010-08-06T15:08:53.033,,CC BY-SA 2.5,user28
1388,1355,0,Definitely in topic,2010-08-06T15:10:40.767,582,CC BY-SA 2.5,
1389,1359,3,"One of the problems that I have found with several of the DOE books that I am familiar with is that they heavily emphasize analysis.  Is there are good book that really does the design elements well--e.g. blocking, replication, randomization, choosing factor levels, using repeated measurements, split-plots, etc.  What about the (even more often neglected) design of non-experimental analytic studies, like case-control designs or surveys for analytical purposes (a very different prospect than simply estimating a population parameter in terms of how you might structuring that research).",2010-08-06T15:21:35.050,485,CC BY-SA 2.5,
1390,1359,0,"I would look for books in epidemiology and economics for what you are describing.  But sadly, I don't have a recommendation to give.",2010-08-06T15:46:26.883,253,CC BY-SA 2.5,
1395,1234,0,"I do not understand this 
""This leads to underfiltering in quiet periods and overfiltering during more busy times""
care to explain ?",2010-08-06T16:25:05.877,603,CC BY-SA 2.5,
1396,1374,0,minus five minus seven? so minus twelve? that doesn't make sense,2010-08-06T17:24:55.827,74,CC BY-SA 2.5,
1397,1365,0,Does this mean that the moments of a circular distribution should be compared to the characteristic function of a linear distribution rather than to its moments?,2010-08-06T17:53:04.410,650,CC BY-SA 2.5,
1398,1374,1,Five to seven -- I'll try to convert it to en-dash.,2010-08-06T17:53:36.657,,CC BY-SA 2.5,user88
1399,1338,13,That is certainly true!,2010-08-06T18:30:34.200,253,CC BY-SA 2.5,
1400,1374,0,"That made me laugh, my god!",2010-08-06T18:31:09.193,253,CC BY-SA 2.5,
1401,1338,3,"My mom in law said to me that this is based on the line ""being in love means you never need to say you are sorry"".  From a book called ""love story"" by arik sigall (I think).",2010-08-06T18:33:19.983,253,CC BY-SA 2.5,
1402,1372,0,Thanks that got me on the right way and i was able to solve it!,2010-08-06T18:46:05.887,791,CC BY-SA 2.5,
1403,1365,0,"@Rasmus: I guess that depends on exactly what you want to do with the information, but in general I'd say yes.",2010-08-06T19:13:44.303,89,CC BY-SA 2.5,
1404,1368,55,"This joke falls flat for me because I can't imagine a statistic professor saying this - a student who failed the course, sure.",2010-08-06T19:25:07.807,196,CC BY-SA 2.5,
1405,1316,0,simply fitting gammas with R: http://docs.google.com/viewer?a=v&q=cache:bl4TbieigsEJ:cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf+fitting+gamma+distribution+R&hl=en&gl=us&pid=bl&srcid=ADGEESjkOC5S7YMbhWRHBDF9cjFaKv4F33c19EPChCnV9rgtdRyO28LCncXNgq9NYJHYe1DeqHPoW3Pl5uM9m0uLeofqruGfCfH6vRhZ6ioinju5ukyWdUYm6D4CVRYOF5xVymC7dTHs&sig=AHIEtbSdTddbrI7ADPWiM_N7gLaLeara0w,2010-08-06T20:15:00.690,291,CC BY-SA 2.5,
1406,1376,0,thanks for editing to have proper math. I couldn't find info on how to render math in 'markdown' language. looks just like raw \LaTeX does it?,2010-08-06T21:33:34.720,795,CC BY-SA 2.5,
1407,1376,0,"Yes, it is Latex. See this meta thread: http://meta.stats.stackexchange.com/questions/218/tex-processing-for-stats",2010-08-06T21:46:02.110,,CC BY-SA 2.5,user28
1408,1275,0,IMO both suggestions boil down to predicting future values from existing ones (AR/ARMA models perhaps?). I guess I'm still hoping for a solution that doesn't involve sampling values (thus the possibility of introducing error).. Besides estimating such models is in itself a form of dimensionality reduction :),2010-08-06T22:19:00.363,170,CC BY-SA 2.5,
1409,1335,1,"Here is a post with the code and some illustrations:

http://www.markmfredrickson.com/thoughts/2010-08-06-combinadics-in-r.html",2010-08-06T23:50:07.863,729,CC BY-SA 2.5,
1410,786,8,According to Wikiquote it is misattributed to Joseph Stalin; the origin is Kurt Tucholsky: http://en.wikiquote.org/wiki/Joseph_Stalin#Misattributed,2010-08-07T00:36:13.317,509,CC BY-SA 2.5,
1411,485,0,Just noticed this related question: http://stackoverflow.com/questions/570029/learning-applied-statistics-with-a-focus-on-r,2010-08-07T08:36:28.870,183,CC BY-SA 2.5,
1412,1371,68,"I remember seeing George Burns on TV being interviewed on his 100th birthday. He was puffing on a cigar. The interviewer made some comment about the incongruity of longevity and smoking.


George Burns: ""Twenty years ago my doctor told me that these cigars were going to kill me""
 
Interviewer: ""What does he say now?""
 
George Burns: ""I don't know. He's dead""",2010-08-07T09:17:31.607,521,CC BY-SA 2.5,
1413,1383,0,"you should include a better description of what data you want to compress... otherwise we will have 10 different answers trying to summarize different part of the huge theory of compression.  

In addition, the two point you give to be more specific are not detailed enough to be understood.",2010-08-07T09:32:00.023,223,CC BY-SA 2.5,
1414,1378,0,+1 for interesting real case ! can you help us to understand what is blood test ? (how do you calculate it),2010-08-07T09:35:06.900,223,CC BY-SA 2.5,
1416,1391,2,"+1 nice and clear answer. Jeromy, can I ask a question about point 3? I do understand the reasoning behind transforming the data, but something always bothered me about doing that. What is the validity of reporting the results of the t-test on the transformed data to the untransformed data (where you're not ""allowed"" to do a t-test)? In other words, if two groups are different when data is, for instance, log transformed, on what bases can you say the raw data is different too? Bare in mind, I'm not a statistician, so maybe I just said something absolutely stupid :)",2010-08-07T09:50:59.823,582,CC BY-SA 2.5,
1417,618,2,"Obviously an unforseen side-effect of the profusion of lemon laws in the US. For example see:
http://en.wikipedia.org/wiki/Lemon_law",2010-08-07T10:03:20.123,521,CC BY-SA 2.5,
1418,823,31,"-1 This is an intellectually dishonest quote from a bad novelist, playing up a popular romantic myth of scientists as Rand-esque revolutionary loners in order to pander to anti-science crankery. Moreover, it has nothing whatsoever to do with statistics. What is it even doing in this list?",2010-08-07T14:34:40.253,174,CC BY-SA 2.5,
1419,1391,2,"@nico I'm not sure about how to report or think about the results, but if all you want to show is that for some X and Y, mu_X != mu_Y, it should be true that for all X_i < X_j, log(X_i) < log(X_j) and for all all X_i > X_j, log(X_i) > log(X_j). That's why for non-parametric tests which operate with ranks, transformations of the data don't affect the result. I think from this, you can assume that if some test shows that mu_log(X) != mu_log(Y), then  mu_X != mu_Y.",2010-08-07T15:57:56.737,287,CC BY-SA 2.5,
1420,1391,0,"thanks for the answer(s). indeed, the t-test appears to maintain nominal type I rate under mildly skewed/kurtotic input. however, I was hoping for something with more power. re: 2, I have implemented Wilcox' `trimpb` and `trimcibt`, but they are a bit too slow to do my power tests, at least for my taste. re: 3, I had thought of this method, but I am interested in the mean of the un-transformed data (i.e., I am not comparing 2 R.V.s with a t-test, in which case, a monotonic transform would be fine for a rank-based comparison, as noted by @JoFrhwld.)",2010-08-07T16:11:08.237,795,CC BY-SA 2.5,
1421,1390,0,"the data is most definitely not normal. the excess kurtosis is on the order of 10-20, the skew is on the order of -0.2 to 0.2. I am doing a 1-sample t-test, so I'm not sure I follow you regarding 'unequal variances', or the U-test.",2010-08-07T16:14:18.203,795,CC BY-SA 2.5,
1422,1378,0,"Sure - briefly, it's a test for latent tuberculosis infection.  Blood is drawn from the patient into three tubes: one that has no antigens, one that has selected TB antigens, and one that has mitogen.  We then compare amount of immunologic response in the tube with no antigens to the one with TB - if they have a strong boost of response in the TB tube, they may have latent TB.  (The mitogen tube serves as a check to make sure that the person is capable of producing an immunologic response at all - most people have very strong reactions to it)",2010-08-07T17:25:37.110,71,CC BY-SA 2.5,
1423,1396,0,It won't help if just the numerical algorithm is diverging for certain parameters.,2010-08-07T18:43:22.720,,CC BY-SA 2.5,user88
1425,1405,1,what's the raw information you have?... i.e. how did you come to two DORs?,2010-08-08T01:37:27.917,601,CC BY-SA 2.5,
1426,1391,2,"@nico If the population distribution of residuals is the same in two groups, then I imagine any time there is a difference in raw population group means there would also be differences in group means of an order-preserving transformation. That said, p-values and confidence intervals will tend to change slightly based on whether you are using raw data or transformed data. In general I prefer to use transformations when they seem like a meaningful metric for understanding the variable (e.g., Richter scale, decibels, logs of counts, etc.).",2010-08-08T03:49:01.213,183,CC BY-SA 2.5,
1427,1337,6,It probably makes sense to leave cartoons in this question: http://stats.stackexchange.com/questions/423/what-is-your-favorite-data-analysis-cartoon,2010-08-08T11:49:27.500,183,CC BY-SA 2.5,
1428,1408,0,"The first point you mention is false in general I suggest that you read the first sentence of the chapter http://books.google.fr/books?id=Nxnh48rS9jQC&pg=PA167&lpg=PA167&dq=Orthonormal+bases+of+compactly+supported+wavelets,&source=bl&ots=tbUc1teQFB&sig=gmzBJyET8S5SXd-oPmzyh8BWDKg&hl=fr&ei=DPNeTOeBC8nT4gbsuazaBw&sa=X&oi=book_result&ct=result&resnum=4&ved=0CDAQ6AEwAw#v=onepage&q=Orthonormal%20bases%20of%20compactly%20supported%20wavelets%2C&f=false in the book of Daubechie. In addition, If you had read my answer I already mentionned the nice property of the DWT in the 2nd part of my answer...",2010-08-08T18:13:13.153,223,CC BY-SA 2.5,
1429,1405,0,"If you have the raw information available that you used to calculate the odds ratios a statistical test will be possible.  Alternatively there might be some reasonable simulation approaches if you know how many observations went into each odds ratio.

P.S. For those of us who only have a passing familiarity with DORs would you please provide the formula used for calculating one?",2010-08-08T18:17:59.023,196,CC BY-SA 2.5,
1430,1414,0,"I'm not sure I understand your answer.  I think you are saying I do not need to specify a link function because the intercept of my model will adjust based on the data and effectively account for the fact that at high difficulties people will not be able to perform the task.  Is that correct?  If so, how does this prevent a flattening of the slope estimate due to the same values at difficulties 4 and 5?  I may need a less technical explanation (if possible).",2010-08-08T18:40:26.170,196,CC BY-SA 2.5,
1431,1408,0,"To the first point, you're right.  I should have said ""Most commonly used/implemented discrete wavelet basis functions""; I'll edit to reflect that.

To the second point, you gave a good answer for how the some CWTs (most often a DOG wavelet or the related Ricker wavelet; something like e.g. the Gabor wavelet would not provide the behavior your describe) can detect anomalies of the singularity kind.  I was trying to give an analogous description of how DWT can be used for detecting other kinds of anomalies.",2010-08-08T19:21:26.600,61,CC BY-SA 2.5,
1432,242,0,"sorry if I wasn't clear, I mean't that I prefer (personal subjective opinion) questions like yours than the question that asks ""what is a random variable""... but I guess my pleasure is not that of everyone :)",2010-08-08T19:27:36.960,223,CC BY-SA 2.5,
1433,1414,0,"*I'm not sure I understand your answer...Is that correct?*

yes

The limitation with the logit is one of symmetry; going back to your example, you force the slope between 4 and 5 to be the same as the slope between 1 and 2.

If symmetry is a problem, you might want to try a complementary log-log link (not implemented in LMER).",2010-08-08T20:09:07.203,603,CC BY-SA 2.5,
1434,1364,0,That's perfect ! I was hoping for a pointer but that's a really good answer.  Thanks.,2010-08-08T20:11:57.883,114,CC BY-SA 2.5,
1435,1414,0,"I think your example is somewhat boggus in that the variable ""difficulty of the discrimination"" is really a ordinal scale and should be introduced in your regression as such. The problem you would have in this case would be due to you wrongly introducing ""difficulty of the discrimination"" as a continuous predictor when  it is not, not to the logit link per see.",2010-08-08T20:17:42.503,603,CC BY-SA 2.5,
1436,1414,0,"@kwak: I introduced it as an ordinal because it is easier to describe that way.  Difficulty of discrimination in many of these experiments is a quantifiable ratio scale variable, e.g. contrast or noise introduced to the signal.",2010-08-08T21:29:05.707,196,CC BY-SA 2.5,
1437,1414,0,@kwak: Does your suggestion to use a complementary log-log link still apply given that the IV is an interval scale variable?,2010-08-08T21:36:55.993,196,CC BY-SA 2.5,
1438,1414,0,"@drknexus#1:> understood, but the mishandled ordinal scale was what was causing the expected behavior of the logit seem inappropriate, so i pointed it out.
@drknexus#2:> what you mean by ""IV""?",2010-08-08T21:46:14.610,603,CC BY-SA 2.5,
1439,1421,0,"The example is a case where a single predictor linear equation, though rationally sound given the dataset, will provide a suboptimal solution to the problem.  If in the example case you fit a quadratic as well you'd likely get an improved model fit would you not?  The improved model fit wouldn't be a consequence of some underlying real quadratic effect of the IV, just that you'd hit a measurement floor.",2010-08-08T22:06:43.480,196,CC BY-SA 2.5,
1440,1421,0,"I'll take a look at Dixon - thanks for the reference - it looks right up my alley.  The alternative link I was imagining was an explicitly 2AFC link, e.g. mafc.logit(2) in the psyphy package of R.",2010-08-08T22:14:26.050,196,CC BY-SA 2.5,
1441,1414,0,"I'm not sure what you mean by ""mishandled ordinal scale"".  Is the problem that the data itself is not spread out in a continuous fashion?  I'd like to simulate this with continuous data, but I can't think of a way of generating values that don't beg the question.  Any ideas?

By IV I meant ""independent variable""/""predictor"", sorry for using shorthand.",2010-08-08T23:19:29.193,196,CC BY-SA 2.5,
1443,608,0,mbq; Is the question in its current form unanswerable or just difficult?,2010-08-08T23:40:54.377,196,CC BY-SA 2.5,
1444,1424,0,"By 'such event' do you mean exactly 3 aces, or at least 3 aces?",2010-08-09T00:20:36.233,455,CC BY-SA 2.5,
1445,219,2,"Thomas, Howes, Luk - 2009 - A comparison of CPUs, GPUs, FPGAs, and massively parallel processor arrays for random number generation. http://doi.acm.org/10.1145/1508128.1508139.

Discussion + benchmarks of a set of PNGs executing on CPU, GPU, FPGA and Massively Parallel Processor Arrays.",2010-08-09T01:12:09.227,154,CC BY-SA 2.5,
1446,1424,0,"That's right, exactly 3 aces, because you roll exactly 3 dices.",2010-08-09T01:39:46.790,1356,CC BY-SA 2.5,
1450,1419,0,"Yes, I now remember looking at one or two of these videos a few years back. They are quite mathematical.",2010-08-09T06:33:34.333,183,CC BY-SA 2.5,
1451,1429,0,"I am not trying to compute a reduced-rank approximation of X, rather a transformed X. You see my goal is not to filter noisy sequences, but to find a representation with a reduced dimensionality (to be used for classification/clustering of time series)... Could you elaborate a bit on the EM-PCA approach?",2010-08-09T06:38:02.670,170,CC BY-SA 2.5,
1454,1421,0,"Doesn't the AIC adequately assess model fit for these purposes? I only provided the curves to demonstrate/visualize what is going wrong.  The problem isn't simply because the floor has been hit.  To convince yourself of this, try difficulty 4 at 60% and 5 at 55%.  The logistic curve still is dropping down into ""impossible"" territory.  Yes, it might be nice to not assess difficulty levels where the task is ""impossible"" but you don't always know where those levels are going to be in advance of preliminary data collection.",2010-08-09T07:09:45.270,196,CC BY-SA 2.5,
1455,1421,0,"My take on it is this: the problem we are observing is analogous to why using percentages as linear predictors is fine until you start reaching the extremes of the percentages - the logistic function and a linear function are reasonably matched until you get upwards of 80%. Likewise, in the low end here the basic logistic function is a reasonable match until the probability of correct answers starts dipping below 60% or so. Then you find yourself in part of the curve where the logistic function is predicting a continued drop off in accuracy whereas the 2AFC function predicts a leveling.",2010-08-09T07:15:08.003,196,CC BY-SA 2.5,
1457,1408,0,The second point you mention is also likely to be false: wavelet support (if it is compact) is giving information about the temporal localization of the wavelet not the frequency localization.,2010-08-09T07:30:30.167,223,CC BY-SA 2.5,
1458,1431,2,"Perhaps I am a little confused, or we are using different definitions?  Am I correct in thinking that a process is 2nd order stationary if the joint marginal cdfs
$F_{X(t),X(t+œÑ)}$ are equal for all $\tau$?

Similarly for a process to be 1st-order stationary the marginal cdfs $F_{X(t)}$ need to be the same for every $t$. So all the moments of the $X(t)$ must be equal. 

2nd-order stationary implies 1st-order stationary, correct?",2010-08-09T07:44:33.563,352,CC BY-SA 2.5,
1459,1431,1,"Extending this, a process is $N$th order stationary if for every $t_1, t_2, \dots, t_N$ the marginal cdfs $F_{X(t_1 + \tau), X(t_2 + \tau)\dots,X(t_N+\tau)}$ are the same for all $\tau$.

Strictly stationary is Nth order stationary for all N.",2010-08-09T07:45:28.883,352,CC BY-SA 2.5,
1460,1431,0,"see http://www.statistik.tuwien.ac.at/public/dutt/vorles/geost_03/node49.html for order 2 stationnarity. Anyway, I have tryed to clarify my answer, hope it is better now...",2010-08-09T08:05:01.520,223,CC BY-SA 2.5,
1463,1431,1,I believe that article describes stationary in the weak sense and this is not what I meant. I should clarify this in the question. See http://en.wikipedia.org/wiki/Stationary_process for a description of the various types of stationarity.,2010-08-09T08:24:15.200,352,CC BY-SA 2.5,
1464,1431,0,"@robby OK... I didn't know this ""second order stationnarity"" I think you should not say second order stationnarity in the question and give the definintion instead. For more clarity you should ask another question. do you have a paper that refers to this second order stationnarity ?",2010-08-09T08:32:42.733,223,CC BY-SA 2.5,
1466,501,0,"Can you define how you ""check the error increase"" please?",2010-08-09T09:03:43.360,521,CC BY-SA 2.5,
1467,497,0,"How do you ""train"" your classifier? Presumably this is done on the training set. If it is a Support vector Machine (SVM) there are several parameters to try during training. Is each tested against the validation (test) set? Or are you using k-fold cross validation? How many times are you using the validation (test) set to check your performance - presumably this is accuracy. Sorry to be pedantic, but this is a poorly defined answer and risks over-fitting.",2010-08-09T09:08:36.263,521,CC BY-SA 2.5,
1468,1431,0,"I'll put a second question together that better describes what I mean.  In the mean time, your answer is correct for the question I stated!",2010-08-09T09:11:45.773,352,CC BY-SA 2.5,
1469,1291,1,"+1 Nice example of a ""down-to-Earth"" application of PCA.",2010-08-09T10:49:45.883,582,CC BY-SA 2.5,
1470,1438,0,"not long at all, I am comparing different methods, although I was not sure whether correlating with main components does make sense in terms of time series data. On the other hand the major trends are indeed orthogonal hence PCA should give me solid results",2010-08-09T10:56:26.593,,CC BY-SA 2.5,CLOCK
1471,1203,0,note that this will be your accepted answer for the bounty (in case you do not choose an answer the bounty automatically select the answer with the most points),2010-08-09T11:21:10.687,223,CC BY-SA 2.5,
1473,1421,0,"As to your first comment, I see that you have a good answer about how to look at the residuals from another posted question but you still don't have why.  AIC, log-likelihood, etc. all tell you how good you fit is but they don't tell you the nature of the fit.. it's like comparing SD and histograms for looking at variability.  They both tell you about variability.",2010-08-09T12:22:15.760,601,CC BY-SA 2.5,
1474,1421,0,"As to your second problem, no.  Of course the chance end of the scale can be variable but pretty much anyone who's extensively studied SAT curves can attest (Ratcliff, Lappin, Pachella) that there isn't much useful variability in the chance end of the scale. It just ditches catastrophically.  All of the above names work with models other than just logistic to come to this.  It's also a simple measurement issue.  At the low end of the scale you have a good measure of error.  At the high end you don't.  So, you can trust your low accuracy measures better.",2010-08-09T12:22:37.540,601,CC BY-SA 2.5,
1475,1421,0,And your comment about imagining that accuracy started going up... typically in one curve you want to model a single psychological process.  I doubt accuracy increasing after difficulty exceeded some point does that.  It would be tapping a different process... or just be some chance variation.,2010-08-09T12:24:55.630,601,CC BY-SA 2.5,
1476,1421,0,"BTW, to really understand modelling accuracy over a change in difficulty you might want to look at the SAT literature since it does exactly that.",2010-08-09T12:25:27.300,601,CC BY-SA 2.5,
1477,1406,0,I think it is really popular.... but not it statistic (we don't want to loose our job because physisics are improving their experiment :) ). Anyway I think Rutherford belongs to this class of spiritual scientist... +1,2010-08-09T12:32:10.467,223,CC BY-SA 2.5,
1478,1393,0,"+1 for the web site VideoLecture, I did not know, did you mention it in the question about video lectures ?",2010-08-09T12:34:34.143,223,CC BY-SA 2.5,
1479,1393,0,I've only being reading about this stuff recently. I really like Candes and Tao's recent paper on the topic http://arxiv.org/abs/0903.1476,2010-08-09T12:55:23.790,352,CC BY-SA 2.5,
1480,1386,0,Is there a reason that prohibit the use of the Welch t-test? Have a look at my answer to this question (http://stats.stackexchange.com/questions/305/when-conducting-a-t-test-why-would-one-prefer-to-assume-or-test-for-equal-varia) where i refer to a paper advocating the use of Welch in case of non-normality and heteroscedasticity.,2010-08-09T13:09:08.537,442,CC BY-SA 2.5,
1481,338,5,"Also the paper ""Information Theoretic Inequalities"" by Dembo Cover and Thomas reveals a lot of deep aspects",2010-08-09T13:36:23.177,223,CC BY-SA 2.5,
1482,6,24,"Thanks @robin; made CW.  Although I don't entirely see this as ""argumentative""; there are two fields which have informed each other (this is a fact), and the question is how much they have evolved together over the last decade.",2010-08-09T14:17:51.890,5,CC BY-SA 2.5,
1484,1441,1,I see round 2 is more difficult :) Mathoverflow may give you a faster answer than stat.overflow...,2010-08-09T14:41:14.627,223,CC BY-SA 2.5,
1485,1447,0,your question is not about R vs R-square (you understand that $0.8^2=0.64$) it is about interpretation of $r^2$. Please reformulate the title.,2010-08-09T15:03:53.263,223,CC BY-SA 2.5,
1486,1408,0,"Discrete wavelets -- or at least the vast majority of ones that are implemented and commonly used -- are typically designed to have useful frequency-based properties under the compact support constraint.  The vanishing moment condition of Daubechies, for instance, is more or less equivalent to flatness in the pass-band.  The frequency localization properties of wavelets are usually what lead the coefficients to be sparse representations and allow estimation of the noise variance under the ""signal + additive zero-mean noise"" assumption.",2010-08-09T15:05:47.657,61,CC BY-SA 2.5,
1489,1410,0,Thanks Jeromy.  Just read the Wikipedia entry for MDS -- seems like it could lead somewhere.,2010-08-09T16:24:53.383,251,CC BY-SA 2.5,
1493,1386,1,"well, the problem is that I want a 1-sample test, not a 2-sample test! I am testing the null $E[X] = \mu$, and not $E[X_1] = E[X_2]$. I will look up the Kubinger et. al., paper (Ich kann schlecht Deutsche).",2010-08-09T17:02:43.710,795,CC BY-SA 2.5,
1495,1421,0,I understand now why looking at the residuals directly might be useful.  In this case it would indicate where the model was beginning to fail.  I don't think I said that accuracy should go up at increased levels of difficulty (although that is what the quadradic model would imply).  I just said that accuracy in 2AFC will hit a floor after which it will not continue to go down as a function of increased difficulty.  This is a result of a fairly simple psychological process interacting with the 2AFC design.,2010-08-09T17:16:52.797,196,CC BY-SA 2.5,
1496,1421,0,"I'll grant that in the current model, it may make sense simply to drop the high difficulty levels that are giving the logistic model problems. Though if, as you say, you can trust your low accuracy measures better, then aren't you are advocating I throw away the very observations that should be most trusted?  I'll shelve the question for now and may come back to it in a couple months after I've had time to look further at the literature you've cited.",2010-08-09T17:25:19.193,196,CC BY-SA 2.5,
1497,1428,0,"Hi Rob,

Thanks for the tip on BFAST - it looks very promising for my purpose! As I briefly mentioned, in my case the rolling aggregation doesn't actually remove the seasonality (there are slacks and surges in effort on a more-or-less predictable schedule, coupled with turnover in the measure denominator), and I have noticed that even with a lag(12) in my breakpoints() model specification, strucchange is a little more sensitive to these seasonal spikes than I'd prefer.",2010-08-09T17:46:42.490,394,CC BY-SA 2.5,
1498,1428,0,"Follow up q: Given this, is it fair to say that the pre-aggregation of the data may somewhat dilute (but not completely eliminate) the sensitivity of the BFAST approach to detect breaks in the seasonal component?",2010-08-09T17:47:52.847,394,CC BY-SA 2.5,
1499,1457,0,Have no idea why latex looks so bad. Can someone please help and fix this?,2010-08-09T18:05:55.353,,CC BY-SA 2.5,user28
1500,847,0,"[The question has been asked here](http://www.google.com/support/forum/p/websiteoptimizer/thread?tid=449ac5085f82912e&hl=en), where Google employees patrol, and there has been no answer. Also I asked someone that was able to talk to the engineers to ask this question for me, and he said that they would not say; so I wonder..",2010-08-09T18:08:39.250,500,CC BY-SA 2.5,
1501,1456,0,I had the feeling it would be something of this sort - many thanks!,2010-08-09T18:13:10.487,253,CC BY-SA 2.5,
1502,497,0,"@Thylacoleo This is a very crude basic and greedy method. Often you keep your validation set the same over runs, but whatever you like is ok.",2010-08-09T18:20:12.313,190,CC BY-SA 2.5,
1503,1463,0,"Thanks, it's been very helpful.  I'll vote it up when I have enough rep.",2010-08-09T19:23:41.040,148,CC BY-SA 2.5,
1504,1458,9,"not really an answer to your question, but have you encountered False Discovery Rates (FDR)? ""Beyond Bonferroni"" by Narum: http://www.springerlink.com/content/c5047h0084528056/",2010-08-09T20:21:27.547,291,CC BY-SA 2.5,
1505,1203,1,well - awarding the bounty to myself seems silly - but nobody else has submitted an answer.,2010-08-09T21:01:31.007,196,CC BY-SA 2.5,
1506,1468,0,I think you have a typo in your hypotheses -- they both seem to be the same...,2010-08-09T21:50:44.347,174,CC BY-SA 2.5,
1507,1428,0,Pre-aggregation will remove or greatly reduce the seasonality. But I don't think that would reduce the ability of BFAST to detect breaks in the seasonal component. It is rare to have breaks in seasonality (at least in all the data I've looked at). Slow changes in seasonality are more common.,2010-08-09T22:12:37.153,159,CC BY-SA 2.5,
1509,1474,1,"See http://meta.stats.stackexchange.com/questions/252/should-we-allow-more-computing-questions the current consensus is that this is an appropriate question, if you disagree chime in at meta, don't torment the new question asker.",2010-08-09T22:53:26.200,196,CC BY-SA 2.5,
1510,1474,1,"ok, but you'll probably have a bigger audience to get responses for a pure R question (at least for now) -- hence, the ""probably"". ""torment"" is a strong word eh? :)",2010-08-09T23:51:42.210,291,CC BY-SA 2.5,
1511,1474,1,"Sure, torment is a bit strong; I'm sorry about that.  I was/am just frustrated with repeat offender users (of which you are not one) who persist in redirecting question askers to StackOverflow without having made any comment on meta (where the consensus seems pretty clear).",2010-08-10T00:15:39.493,196,CC BY-SA 2.5,
1512,1397,0,"yes, that is the paper I am looking for, but I cannot find a version of it online. The R-code might be useful, but I am working in Matlab, and don't have time to do the translation (the R code looks nontrivial). I would even be willing to settle for the bibliography of this paper at the moment.",2010-08-10T00:18:02.327,795,CC BY-SA 2.5,
1513,1390,0,"I am accepting the 'use a parametric test' advice. it doesn't exactly solve my question, but my question was probably too open-ended.",2010-08-10T00:35:20.870,795,CC BY-SA 2.5,
1514,1466,0,"Good point.  If it's not a normal distribution, how do I go about finding  a different ""fit"" ?   (What terminology should I research?)",2010-08-10T00:41:22.523,6967,CC BY-SA 2.5,
1516,1461,0,Thank you so much. It seems that the log transformation is widely practiced in economics. It is not part of economic theory but is widely accepted. I appreciate your very useful response.,2010-08-10T00:56:28.027,834,CC BY-SA 2.5,
1519,1435,0,"I've corrected an error in my original answer. I first wrote p=logit(X*beta). In fact the predicted probability is the inverse logit of the linear combination, p=inv-logit(X*beta). In R this is calculated as p<-plogit(X*beta), which is p=exp(X*beta)/(1+exp(X*beta)).",2010-08-10T01:07:25.020,521,CC BY-SA 2.5,
1521,1385,2,"There's also ""reduced-model"" approach where you train a classifier for every pattern of missing values encountered during testing. IE, to make prediction for x where i'th attribute is missing, remove i'th attribute from all instances of training data and train on that. http://jmlr.csail.mit.edu/papers/v8/saar-tsechansky07a.html",2010-08-10T01:27:11.713,511,CC BY-SA 2.5,
1522,1452,0,@ars. I think the `boxcox` function in MASS only estimates $\lambda_1$ and assumes $\lambda_2=0$.,2010-08-10T01:27:33.897,159,CC BY-SA 2.5,
1523,1448,1,"I do appreciate your attempt at helping, but unfortunately, this just made things 10x worse.  Are you really introducing trigonometry to explain r^2?  You're way too smart to be a good teacher!",2010-08-10T01:49:49.367,6967,CC BY-SA 2.5,
1524,1450,0,"> The ratio between the variation explained and the original variation is your R^2

Let's see if I got this.  



If the original variation from mean totals 100, and the regression variation totals 20, then the ratio = 20/100 = .2   You're saying R^2 = .2 b/c 20% of the mean variation (red) is accounted for by the explained variation (green)  



(In the case of r=1) If the original variation totals 50, and the regression variation totals 0, then the ratio = 0/50 = 0 = 0% of the variation from the mean (red) is accounted for by the explained variation (green)  I'd expect R^2 to be 1, not 0.",2010-08-10T01:51:53.620,6967,CC BY-SA 2.5,
1525,1452,1,"@Rob: Oh, sorry.  Diggle's geoR is the way to go -- but specify `lambda2=TRUE` in the arguments to `boxcox.fit`.  (Also updated the answer.)",2010-08-10T02:01:31.553,251,CC BY-SA 2.5,
1530,1450,1,"R^2 = 1-(SSR/SST) or (SST-SSR)/SST. So, in your examples, R^2=.80 and 1.00. The difference between the regression line and each point is that left UNexplained by the fit. The rest is the proportion explained. Otherwise, that's exactly right.",2010-08-10T04:17:49.067,485,CC BY-SA 2.5,
1531,1450,0,"I edited that last paragraph to try to make it a bit clearer. Conceptually (and computationally) all you need is there. It might be clearer to actually add the formula and refer to the SST SSE and SSR, but then I was trying to get at it conceptually",2010-08-10T04:18:19.300,485,CC BY-SA 2.5,
1533,498,0,SAS questions (and questions related to other statistical software) are now allowed. See http://meta.stats.stackexchange.com/questions/252/should-we-allow-more-computing-questions for discussion.,2010-08-10T05:36:45.163,159,CC BY-SA 2.5,
1534,1469,1,You need to compute the Fisher matrix ? what is your model ? I am sure you can make your question more precise.,2010-08-10T05:50:21.163,223,CC BY-SA 2.5,
1535,1466,0,@stats: See my edit.,2010-08-10T06:53:52.817,56,CC BY-SA 2.5,
1537,1489,0,Would you please give a link or two to explanation of data standardization?,2010-08-10T07:05:58.687,213,CC BY-SA 2.5,
1538,1448,0,"I thought that you wanted to know why correlation^2 = R^2. In any case, different ways of understanding the same concept helps or at least that is my perspective.",2010-08-10T08:47:06.010,,CC BY-SA 2.5,user28
1539,1493,3,your gaussian variables are iid ?,2010-08-10T08:56:47.737,223,CC BY-SA 2.5,
1540,1478,0,Could you give a couple of examples of what your code does?,2010-08-10T09:42:11.907,8,CC BY-SA 2.5,
1541,1497,0,Does this book provide the code to reproduce the graphics that it shows? (I think it's S that is used),2010-08-10T11:08:22.910,339,CC BY-SA 2.5,
1542,1483,0,Raw data is from a 2 x 2 diagnostic contingency table. DOR equation = (TP*TN)/(FP*FN),2010-08-10T11:10:51.487,,CC BY-SA 2.5,Jay
1543,1386,0,Thanks for clarifying. In this case the Kubinger paper will not be very helpful to you. I am Sorry.,2010-08-10T11:18:25.560,442,CC BY-SA 2.5,
1544,1472,0,Are you working on the automated procedure?  :),2010-08-10T13:31:06.803,5,CC BY-SA 2.5,
1545,1501,3,possible duplicate of [Locating freely available data samples](http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples),2010-08-10T13:55:30.473,,CC BY-SA 2.5,user28
1546,1450,0,"ie: R^2 is the proportion of the total variation from mean (SST) that is the difference b/w the expected regression value and mean value (SSE).  In my example of hours vs. score, the regression value would be the expected test score based on correlation with hours studied.  Any additional variation from that is attributed to SSR.  For a given point, hours studied variable/regression explained x% of the total variation from the mean (SST).  With a high r-value, ""explained"" is big percentage of SST compared to SSR.  With a low r-value, ""explained"" is a lower percentage of SST compared to SSR.",2010-08-10T13:56:30.207,6967,CC BY-SA 2.5,
1547,1467,0,I don't know R syntax.  I almost need pseudo-code to get the gist.   I will google some of the concepts you used.  Thanks!,2010-08-10T13:58:05.820,6967,CC BY-SA 2.5,
1548,1501,0,Maybe that going into a more fused and detailed catalogue such as Free data set for very high dimensional classification: http://stats.stackexchange.com/questions/973/free-data-set-for-very-high-dimensional-classification could be an idea but otherwise I also feel this is a duplicate.,2010-08-10T14:17:26.180,223,CC BY-SA 2.5,
1549,1498,0,"Thank you, that's exactly what I was trying to avoid (having a lot of samples as baseline), because I would like a really reactive approach (e.g. online detection, maybe ""dirty"", after 1-2 weeks of baseline)",2010-08-10T14:49:29.207,667,CC BY-SA 2.5,
1550,1470,0,This is also definitely worth citing: http://prefrontal.org/files/posters/Bennett-Salmon-2009.jpg,2010-08-10T14:59:40.647,582,CC BY-SA 2.5,
1551,1482,0,"Thanks for the suggestion, but the ability to choose my own measure of distance is critical, so this won't work to me. Someone else may find it useful, though.",2010-08-10T15:06:57.837,,CC BY-SA 2.5,anonymous
1552,1484,0,"Thanks, but is there any benefit to using clusterfly rather than calling ggobi directly? The website only mentions clustering methods, which are interesting, but not my primary goal just yet. gcexplorer has less informative website, but looks like it is for visualizing data after it has already been split to clusters. I will give them a try once I get to that point, but not what I need right now.",2010-08-10T15:12:13.177,,CC BY-SA 2.5,anonymous
1553,1491,0,"Thanks for the suggestion, @Shane. ggobi looks promising, I am installing it right now and will give it a try :)",2010-08-10T15:12:59.887,,CC BY-SA 2.5,anonymous
1554,1502,0,"I thought of this but there doesn't seem to be a reasonable cut point, and domain experts can't justify one either.",2010-08-10T15:15:48.673,,CC BY-SA 2.5,anonymous
1555,1500,0,"Looks like a useful program, but their webpage does not do a good job of convincing me it will solve this exact problem. It looks like it may be too broad, too many features I don't care about, making it hard to do the simple things. I will give it another look if the other choices don't work out.",2010-08-10T15:19:55.883,,CC BY-SA 2.5,anonymous
1556,1499,0,"Here's some criticism of that paper in 4 from the guy who runs eager eyes:
http://junkcharts.typepad.com/junk_charts/2010/05/8-red-flags-about-the-useful-chartjunk-paper.html

http://junkcharts.typepad.com/junk_charts/2010/05/8-red-flags-about-the-useful-chartjunk-paper.html",2010-08-10T15:21:27.100,287,CC BY-SA 2.5,
1557,1488,0,"Orange's features page says 'under construction' and they don't list screenshots like what I am doing. weka has no features list at all. They may be able to do what I want, but if they don't promote the feature, how can I tell. I am more convinced by the other choices.",2010-08-10T15:24:40.237,,CC BY-SA 2.5,anonymous
1558,1501,1,I voted to close as duplicate.,2010-08-10T15:36:09.417,5,CC BY-SA 2.5,
1559,1501,0,@Shane if I could I would do the same,2010-08-10T16:01:36.483,223,CC BY-SA 2.5,
1560,1501,0,"I already voted to close. Perhaps, one of the of the moderators could close this question.",2010-08-10T16:16:12.043,,CC BY-SA 2.5,user28
1561,1507,0,"Why would you calculate the probability of exactly 39 in case 1, but 39 or higher in case 2?",2010-08-10T16:20:35.893,196,CC BY-SA 2.5,
1562,1507,0,"@drknexus Case 2 is a continuous approximation of case 1. Thus, it does not make sense to talk about P(Z=10.3) in case 2.",2010-08-10T16:24:29.650,,CC BY-SA 2.5,user28
1563,1478,0,"briefly `pv = AD_test_uni(xs)` takes a sample vector `xs`, which are restricted to the range $[0,1]$ and returns a p-value, `pv` under the null that `xs` are drawn from the Uniform distribution. I can then use this elsewhere to test other tests which spit out a p-value.",2010-08-10T16:28:49.007,795,CC BY-SA 2.5,
1564,1479,0,"There is indeed an infinite regress problem here. However, it is certainly not the case that all external libraries are error-free (I have found such errors in the past). Because all of my tests ultimately rest on this A-D test, I thought it might be worthy of extra scrutiny.",2010-08-10T16:44:51.017,795,CC BY-SA 2.5,
1565,1502,0,"I would think this could be fairly arbitrary for your stated purpose - honestly, you might not even need to actually cut into binary, just recode a tie value label on a scale of 1 to some manageable number, then progressively hide/show the ties at various levels (optionally also hiding/eliminating any pendants & orphans along the way).

Not directly responding to your request as written, but why not take a more typical approach and use a hybrid clustering method that doesn't use initial centroids to identify preliminary clusters, then feed the centroids from that result into your new analysis?",2010-08-10T16:45:55.197,394,CC BY-SA 2.5,
1566,1479,0,"Perhaps, you can use multiple external libraries whose code base does depend on each other. It is unlikely that independent libraries have the same errors which should eliminate some of your concerns.",2010-08-10T16:49:52.710,,CC BY-SA 2.5,user28
1567,1491,1,"Works fine on other platforms, but gtk does not play nice with OSX.",2010-08-10T17:02:53.457,,CC BY-SA 2.5,anonymous
1568,1502,0,"I am guessing you mean to try for many different cutoffs until I see some nice results? I wish to avoid that for standard multiple comparisons reasons. re: your second suggestion I guess I just trust myself better than those algorithms. I use the computer to process large amounts of data too tedious to do by hand, not to replace my thinking.",2010-08-10T17:06:47.270,,CC BY-SA 2.5,anonymous
1569,1493,0,What kind of answer are you looking for?  A name?  A simple formula for the density?,2010-08-10T17:23:56.213,89,CC BY-SA 2.5,
1570,1499,0,very interesting. thx for pointing this one out!,2010-08-10T17:34:56.570,22,CC BY-SA 2.5,
1571,1501,0,"It's just my two cents, but I think if we limit stats.se to 1 datasets question it makes it harder to share that knowledge.  I'm sorry if I didn't word my question/explanation better to differentiate this question, but I foresee there being lots of 'what open dataset exists to do x?' where x is what stops them from all being dupes.",2010-08-10T17:55:45.410,114,CC BY-SA 2.5,
1572,1503,1,"I have seen that question, and your answer was good (and I do rely on a lot of the datasets that ship with R) but not many of them include defined (or should I say well characterized) answers to particular statistical questions. (although I believe that if the dataset has been used in an example to a function in the helpfile, then the package will include the results of running that example, so you could use that)",2010-08-10T17:58:26.543,114,CC BY-SA 2.5,
1573,1507,0,@srikant-vadali By the common definition of p-value it should include more extreme cases in addition to the one observed. So the first calculation should be P(39 or 40). Still very small though.,2010-08-10T18:05:53.237,279,CC BY-SA 2.5,
1574,1507,0,"@Aniko I think we need to calculate p-values if the question involves inference about a parameter. But, the OP's qn is about the probability of observing a particular event. So, I think case 1 calculation is fine. If we want to compute the probability that we would see at least 39 such events then you would compute P(39 or 40) but perhaps I misinterpreted the OP's goals here.",2010-08-10T18:19:19.450,,CC BY-SA 2.5,user28
1575,1501,0,I think it makes a lot of sense for this question to remain open if you could edit the question to indicate how your request is different from the one that exists and why the answers to the existing question do not satisfy your requirements. See Robin's comment as an example of what I am suggesting.,2010-08-10T18:24:15.303,,CC BY-SA 2.5,user28
1576,1469,0,"I could give more details if that's of interest...as an overview, I'm trying to compare efficiency/bias of n-iteration Contrastive Divergence estimator for fitting Boltzmann Machines with an estimator that uses n iterations of Belief Propagation",2010-08-10T18:31:36.163,511,CC BY-SA 2.5,
1577,1312,0,"I thought a bit more about what you said and it makes sense. I suppose one would write $f(Y_3|Y_1,Y_2)$ as $f(Y_3|X_{31},-) I(Y_1 Y_2>0)$",2010-08-10T18:55:01.157,,CC BY-SA 2.5,user28
1578,1511,0,"I do not understand your answer. Suppose, you do not know P(H) and you have to estimate it. You have to write Prob(4H, 1T 39 times out of 40) in terms of P(H) and estimate P(H). Your answer suggests that the probability of observing the event in question is always 1 and hence independent of P(H). I am not sure if that makes sense. Did I miss something?",2010-08-10T19:13:43.613,,CC BY-SA 2.5,user28
1579,1507,0,"@srikant-vadali Then per drknexus the second calculation should also do the same, something like P(38.5<X<39.5).",2010-08-10T19:27:03.677,279,CC BY-SA 2.5,
1580,1511,2,"The student observed a result that appears odd.  But once it's observed it's no longer odd, it simply is random chance.  The probability of observed data occurring after you've already observed it is always 1.0.  I put the smiley in because I recognize that you want to know if it's plausible.  But that would involve not only working out the probability of it occurring once but the probability of it occurring given all the students who've ever attempted it and the number of attempts they made.  (also, consider that 4 heads and 1 tail in 39 of 40 attempts has the same probability)",2010-08-10T19:36:42.093,601,CC BY-SA 2.5,
1581,1507,0,"@Aniko Perhaps, that would be a better choice.",2010-08-10T19:40:51.413,,CC BY-SA 2.5,user28
1582,1511,0,How does what you say reconcile with the estimation problem I posed in my comment?,2010-08-10T19:42:03.110,,CC BY-SA 2.5,user28
1583,1511,2,"First, I was making a stats joke. But there is a serious point. If you believe the student then the probability is 1.0 because the data have already been observed.",2010-08-10T19:52:38.340,601,CC BY-SA 2.5,
1584,1507,0,"Is this a statistics assignment?  If so, then if you are testing the student's plausibility you need to calculate the number of students you've ever assigned this task to and multiply the result by that.",2010-08-10T19:54:25.717,601,CC BY-SA 2.5,
1585,1511,1,"But, that is exactly the point isn't it? The OP does not trust the student and wants to make an assessment of how likely the event is under reasonable assumptions (P(H)=0.5, independence etc).",2010-08-10T20:00:30.320,,CC BY-SA 2.5,user28
1586,1507,0,"Or do I?  I'd think I can just base the calc on theoretical probability of the fair coin, and ignore Experimental Probability of what students got?",2010-08-10T20:34:15.700,6967,CC BY-SA 2.5,
1587,1507,0,"Yes, in case 1, I could have done P(30 or 40), but I would have just added another 0%, so I skipped it.  I used the inequality in case 2 b/c I was interested in the odds of 39 or anything more extreme.",2010-08-10T20:35:37.507,6967,CC BY-SA 2.5,
1588,1511,0,"John, I got the joke.  LOL",2010-08-10T20:36:13.667,6967,CC BY-SA 2.5,
1590,538,7,They're not equivalent in the face of interventions.,2010-08-10T22:35:51.347,858,CC BY-SA 2.5,
1591,571,3,"Might as well use Pearl's word for ""directly setting [a variable's] value"": an intervention.",2010-08-10T22:36:53.263,858,CC BY-SA 2.5,
1593,1517,0,You could learn lot of things but for useful suggestions you should ideally let us know what kinds of questions you like to ask and what kind of answers do you want to those questions. Some exemplars would really help as otherwise your question is too broad.,2010-08-10T22:57:41.880,,CC BY-SA 2.5,user28
1594,1517,0,My thought was that the tools I'm seeking are generic.  Standard Deviation is a generic tool.  What's another one like it?  The basic goal is to remove noise from data.  A bit later I will provide more about why Inventory Turn Rate is difficult for us and why I think more stats knowledge would help.,2010-08-10T23:15:43.580,857,CC BY-SA 2.5,
1595,1507,0,"No, you can't ignore how many students do the task.  If you have a 1000 students doing the same assignment and one of them reports a 1:1000 event then that's not terribly unusual.  If you have 200 students a year for the last 10 years and one reports a 1:2000 event it's not unusual... even a higher on like 1:20000, which is now 1:10 is not unusual.  This is similar to the Bulgarian lottery problem   
(of course this is just a theoretical argument... the probability here is far too low for it to matter)",2010-08-10T23:47:55.430,601,CC BY-SA 2.5,
1596,1518,0,"Linear Regression is precisely what I'm looking for to deal with one problem (forecasting sales from current customer inquiries).  It looks difficult, but I will read more.  I can't tell about Comparing Means.  Thank you.",2010-08-10T23:52:10.180,857,CC BY-SA 2.5,
1597,1518,1,"""Forecasting sales from customer inquiries"" is the type of example that you should have mentioned in the question as that enables a better response. Another tool that helps in this context is correlation coefficient (See: http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient). This concept is closely related to linear regression.",2010-08-11T00:37:42.773,,CC BY-SA 2.5,user28
1598,1518,1,"Likewise, you should indicate whether you are interested in individual sales--will this customer buy--or aggregate sales--how much will we sell this quarter--as those will require different models.",2010-08-11T03:22:30.037,485,CC BY-SA 2.5,
1599,1501,0,"I like your question (presumably after being edited).
It sounds like your question is not especially about datasets. Rather its about the resources and infrastructure for comparing reference results to a new implementation.",2010-08-11T04:53:40.700,183,CC BY-SA 2.5,
1600,847,0,How did I not answer your question? Read the JMP book. It's free.,2010-08-11T06:33:44.667,74,CC BY-SA 2.5,
1601,1523,1,What's the duplicate? I can't see anything obvious.,2010-08-11T06:43:03.150,159,CC BY-SA 2.5,
1602,1523,1,Pretty similar to this one I thought: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning,2010-08-11T07:25:58.733,74,CC BY-SA 2.5,
1603,1521,1,duplicate? http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning,2010-08-11T07:26:33.567,74,CC BY-SA 2.5,
1604,1523,1,"ok. I was searching for data mining, not machine learning. Please vote to close if you think it is a duplicate.",2010-08-11T07:36:37.613,159,CC BY-SA 2.5,
1605,1525,0,I'm just wondering whether the edited version of this question should retain the aspect of the original question regarding how the distinction between probabilities and proportions could be described in lay terms.,2010-08-11T08:26:37.557,183,CC BY-SA 2.5,
1606,1527,12,"+1 for proportion is empirical, and it is often a good estimate of a probability which is theoretical !",2010-08-11T08:38:14.050,223,CC BY-SA 2.5,
1608,1160,0,Following on from http://meta.stats.stackexchange.com/questions/252/should-we-allow-more-computing-questions I've reopened this question.,2010-08-11T10:43:21.717,8,CC BY-SA 2.5,
1609,1523,0,"Hmm, so Data Mining == Machine Learning?",2010-08-11T10:47:42.087,251,CC BY-SA 2.5,
1610,1531,10,You need to change your question as many people won't (and shouldn't) run a random excel file on their computer. Pull out the formula from the file and ask a question about that.,2010-08-11T11:06:49.453,8,CC BY-SA 2.5,
1611,1502,1,"You're using hypothesis testing language but yet talking about a very exploratory, know-it-when-you-see-it approach @ the same time - so it's not clear what your goal really is for this part of your analysis. If you have hypotheses you're testing later (e.g. predicting cluster membership or using clust membership as predictor) then you can choose not to do things that will tempt bias there. But ""multiple comparison"" issues don't really figure into the exploratory problem you're describing. The viz cutoffs are just to help you see what's there - but your trust may still be misplaced.",2010-08-11T11:48:50.397,394,CC BY-SA 2.5,
1612,1517,0,"After learning the basics, it would be quite helpful if you use some tool i.e. Matlab, R, Excel to view your results in a more human understandable format.",2010-08-11T12:01:36.487,734,CC BY-SA 2.5,
1616,1521,3,If it's duplicate I can guess that data mining and machine learning are the same thing!,2010-08-11T14:16:22.953,339,CC BY-SA 2.5,
1617,1532,1,"This is a great paper and is consistent with my perspective on what data mining is and how it differs from statistics.  The catch is, it's from 1997!  Note an indictment of the paper or your recommendation, but the degree to which I have kept up with data mining.  It sounds like I need to grab a current book on data mining to catch up.",2010-08-11T14:19:58.720,485,CC BY-SA 2.5,
1619,1522,0,"I think points 2 and 3 are useful comments and consistent with what I see as the distinction between the two SA and DM.  I'm not so sure about your first point.  I've done statistical work where I was interested in improving understanding about causal relationships.  However, I've also done statistical work where the task was to take known relationships and develop models with the sole purpose of prediction but which shared non of the other features of ""data mining"".",2010-08-11T14:39:04.117,485,CC BY-SA 2.5,
1620,1526,0,"I can buy that.  Data mining is more exploratory application of statistical techniques.  Though, I don't think that distinction is enough.  When I'm doing EDA on my set of 100 observations from a designed experiment, I don't think anyone would call that data mining, would they?",2010-08-11T14:42:14.797,485,CC BY-SA 2.5,
1621,1523,1,"1) I'm not seeing the comp stat distinction. There's not much that statisticians do that doesn't require a computer. I suppose you mean computationally intensive procedures such as iterative solutions, etc? But then, these are also common in modern statistical work that is not data mining. 2) In my own (stats) work, I've been interested in model building for explanation and for prediction, depending on the problem-I wouldn't have considered that data mining. 3) I'm left with the conclusion that modern DM is a particular application of statistics, which I think is a fine conclusion.",2010-08-11T14:44:15.727,485,CC BY-SA 2.5,
1625,1539,0,Shouldn't we apply the CLT to all finite-dimensional marginal distributions?,2010-08-11T15:26:01.897,650,CC BY-SA 2.5,
1626,1470,0,I'm sure they had lots of fun asking a dead salmon about its emotions!!!,2010-08-11T15:48:04.470,582,CC BY-SA 2.5,
1627,1551,0,"About N.B.2 - I think you're exactly right regarding the connotation of data mining and I had not made the connection to machine learning.  My training always emphasized the problems of over-fitting, spuriousness, and capitalizing on chance and as such I have been skeptical on DM--and still am, perhaps until someone actually tells me WHAT they are doing and HOW.  Thanks.",2010-08-11T16:05:00.513,485,CC BY-SA 2.5,
1628,1539,0,you asked for an intuitive answer :) also I choose not to bother you with the tricky mathematical part which is to show that the convergence for all t implies the convergence (in law) of the supremum... do you want me to complete the answer ?,2010-08-11T16:11:46.967,223,CC BY-SA 2.5,
1629,1551,1,"My only quibble on the ML/DM distinction would be that I think DM is broader.  For example, OLAP and related tools include mining technologies.  But these come from the database side of computer science rather than machine learning.  The role of commerce in shaping the ""meaning"" of data mining is hard to ignore -- it brings in elements of management sciences, operations research, machine learning and statistics as required.  It also gives the impression of something flimsy, but that's usually a problem for purists not practitioners.",2010-08-11T16:47:59.927,251,CC BY-SA 2.5,
1630,1539,0,"Dear robin girard, I think your answer is fine as it stands. Thank you!",2010-08-11T17:16:09.460,650,CC BY-SA 2.5,
1631,1532,0,"Heh, I kept the date out on purpose because I thought it would be amusing to notice the time span.  :)  The books by Michael Berry and Gordon Linoff are pretty good and will appeal to statisticians (for the broader exposure rather than learning statistical techniques).  If you want a sense of the fuzzy, ""enterprise"" side of this field, skimming through one of the books on a vendor product, like SAS's Enterprise Miner or SPSS's Clementine, may help.  I wouldn't recommend buying them unless you're going to work with the product itself.",2010-08-11T17:50:21.430,251,CC BY-SA 2.5,
1633,1338,1,"@Tal Galili: http://en.wikipedia.org/wiki/Love_means_never_having_to_say_you%27re_sorry. (My favorite is the line from *What's Up, Doc?*)",2010-08-11T19:23:58.057,877,CC BY-SA 2.5,
1634,1551,0,"@ars: I agree.  I was trying to say that a little by saying ""machine learning techniques are used in data mining"" (i.e. data mining is a super-set).  Your point about the commercial applications is also spot on.  Although someone in a commercial application now-a-days might refer to their work as something else (e.g. ""data science"").",2010-08-11T19:36:59.117,5,CC BY-SA 2.5,
1635,1551,0,"Right, I should have said I was trying to flesh out the differences, rather than actually quibble with what you wrote.  Apologies for the misdirection.  Good point on changing times and terms like the adoption of ""data science"".  Doesn't one of Gelman's books start with something like ""statistics is the science of data""?  So ""they're"" stealing from statisticians.  Again. :)",2010-08-11T20:39:16.520,251,CC BY-SA 2.5,
1636,1555,2,"You need to explain the data structure more, otherwise it is not clear how would a chi-square test apply. What do the values represent? Are the rows meaningful?",2010-08-11T21:45:06.457,279,CC BY-SA 2.5,
1640,1031,1,"This seems to be a characterization of directional derivative of KL rather than of KL itself, and it doesn't seem possible to get KL divergence out of it because unlike the derivative, KL-divergence doesn't depend on the geometry of the manifold",2010-08-11T23:37:49.470,511,CC BY-SA 2.5,
1642,1572,0,"the priors are distributed normal N(b0, B0^-1). note above, that I use a precision (B0) of .00001, which is pretty diffuse.",2010-08-12T00:59:36.530,291,CC BY-SA 2.5,
1643,1572,1,"Actually, I think one other factor that matters is sample size. I am not sure if you can expect identical values with a sample size of just 15 odd points. With only 15 points there is not enough information in the likelihood to outweigh the prior. In other words, the likelihood is not 'concentrated enough' which suggests that when you integrate out the parameters you may get a different mcmc marginal likelihood. Does that make any sense? You can do a small simulation to check the above.",2010-08-12T01:13:30.637,,CC BY-SA 2.5,user28
1644,1573,0,"your paper will take time to digest. it looks like the -logLik changes fairly significantly if I changed the number of digits in the prior precision. MCMCpack doesn't let me specify other priors (aside from uniform), so I'm going to try another package. btw, what is a ""disjoint support?""",2010-08-12T01:55:20.850,291,CC BY-SA 2.5,
1645,1573,0,"On second though, my paper may not be so important here. But the point about the priors is important.  Look at the differences in the priors at the empirical mean of the data.  If that difference is large, that could explain your problem.",2010-08-12T02:10:24.543,319,CC BY-SA 2.5,
1646,1573,0,"As for the paper, disjoint support means there is no region to which both hypotheses assign positive probability.  i.e. the alternative really is an alternative.",2010-08-12T02:12:21.450,319,CC BY-SA 2.5,
1647,1573,0,"as I changed the precision of the prior from .01, .001, .0001, .00001, .000001, the -logLik were: -0.85, -2.00, -3.15, -4.31, -5.46 respectively. All the estimates (mean of mean, sd; sd of mean, sd) were the same at least to the 8th digit. (i used mean of 0.38 ~ the sample avg)",2010-08-12T02:23:19.783,291,CC BY-SA 2.5,
1648,1412,0,BTW Brian D. Ripley thinks it is a problem too: https://stat.ethz.ch/pipermail/r-help/2006-December/122353.html,2010-08-12T02:27:04.893,196,CC BY-SA 2.5,
1649,1573,0,"@John Why would the priors matter as sample size goes to infinity? Asymptotically, the likelihood function 'collapses' (technically, a dirac delta function) at the true parameter vector. Thus, it seems to me that the effect of the prior on marginal likelihood should weaken as sample size increases and should vanish asymptotically. Is the above intuition not correct?",2010-08-12T03:12:07.187,,CC BY-SA 2.5,user28
1650,1525,12,"If you eat Hamburgers _every_ Tuesday, the probability of you eating a hamburger in any given week is 1.",2010-08-12T03:28:16.310,776,CC BY-SA 2.5,
1651,1573,3,"it looks this problem has been widely known since 1939, called Jeffrey-Lindley paradox. Gelman's book (p185 & 250) also says uninformative priors w/ Bayes Factors are a no-no. Basically, they say that formulating point null hypotheses are stupid, and tests should be constructed from the posteriors. http://www.artsci.uc.edu/collegedepts/economics/research/docs/Wppdf/2009-01.pdf",2010-08-12T04:50:06.480,291,CC BY-SA 2.5,
1652,1581,0,"Thanks for the answer. I had these thoughts.
(a) Multicollinearity: I agree. Wihtout it, the coefficients should not change. (b) Is it interesting? I actually think that the sign flipping can have interesting theoretical interpretations in some instances; but perhaps not from a pure prediction perspective. (c) Residualisation: I'd be keen to hear what other people think of this approach.",2010-08-12T06:05:59.343,183,CC BY-SA 2.5,
1653,1581,0,"I'm not sure if multicollinearity could be interesting. Say you had some outcome `O`, and your predictors are `Income` and `Father's Income`. The fact that `Income` is correlated with `Father's Income` is intrinsically interesting, but that fact would be true no matter the value of `O`. That is, you could establish that `O`'s predictors are all collinear without ever collecting your outcome data, or even knowing what the outcome is! Those facts shouldn't get especially more interesting once you know that `O` is really `Education`.",2010-08-12T06:25:19.883,287,CC BY-SA 2.5,
1654,1581,0,"I'm suggesting that the suppressor effect can be theoretically interesting, of which presumably multicollinearity provides a starting point for an explanation.",2010-08-12T06:35:33.963,183,CC BY-SA 2.5,
1655,1559,1,"Hmmmm... maybe you should have a look at http://en.wikipedia.org/wiki/Percentage 1 and 100% ARE the same, as are 0.35 and 35% or 2.24 and 224%.",2010-08-12T06:42:33.890,582,CC BY-SA 2.5,
1657,1413,0,"It is unfortunate that nobody could chime in with a package where this is easy to do.  However, it turned out that adding the appropriate functions to lme4 wasn't as difficult as I had feared.  Now the only challenge that remains is to find the ""correct"" link function (see: http://stats.stackexchange.com/questions/1583/appropriate-link-function-for-2afc-data).  So I'll accept ars' answer, but someone may want to ask this again in the future since next time the answer may be different.",2010-08-12T07:09:59.960,196,CC BY-SA 2.5,
1658,1582,1,"Thanks for the suggestion to explore ridge or PCA regression.
Just a side point regarding your comment ""if your variables are positively correlated, then the coefficients will be negatively correlated leading to sign reversal."": positively correlated predictors do not typically lead to sign-reversal.",2010-08-12T07:11:15.230,183,CC BY-SA 2.5,
1659,1432,2,"There are elements to this question that remain unanswered, e.g. the nature of the ""pearson"", ""working"",""response"", and ""partial"" residuals, but for now I will accept Thylacoleo's answer.",2010-08-12T07:12:16.117,196,CC BY-SA 2.5,
1660,1335,0,"I'm accepting this answer because it solves (what I think) is the harder of the problems I was looking for a solution to - picking a particular combination out without calculating the preceding values.  Unfortunately, it is still very slow.  Perhaps as mentioned here and elsewhere a binary search would help speed things up.  Perhaps the best approach is to have one thread generating the combinations stepwise as in mbq's answer and another thread reading them off and testing them.",2010-08-12T07:15:40.647,196,CC BY-SA 2.5,
1661,1582,0,"Sorry, that's a botched one line explanation written in haste.  Fixed now, thanks.",2010-08-12T07:19:53.383,251,CC BY-SA 2.5,
1662,1586,1,Good point. All the examples of Simpson's Paradox apply to categorical variables. Is the concept of a supressor variable the numeric equivalent?,2010-08-12T07:55:46.557,183,CC BY-SA 2.5,
1663,967,0,I guess Val Harian is not a statistician if he is not kidding... what is a sexy job ? for me it is like the sitation with the sword of the century... fun but a bit trivial :),2010-08-12T07:58:10.670,223,CC BY-SA 2.5,
1664,1585,0,+1 for this paper ! my reference for the link between thresholding and penalized estimation was http://www.google.fr/url?sa=t&source=web&cd=1&ved=0CB0QFjAA&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.8.6694%26rep%3Drep1%26type%3Dpdf&ei=X6xjTILlHsiU4gashYThCg&usg=AFQjCNE2kNFinNcuvG4qkBBPQAi2omY-9A&sig2=D0NgkuN_KfgV8ZdzADKcxg,2010-08-12T08:11:28.517,223,CC BY-SA 2.5,
1665,435,8,"This one reminds me of the recent bailout in the States, where they just made up 700 billion number - they said they just wanted a really large number. :)",2010-08-12T08:53:40.443,144,CC BY-SA 2.5,
1668,1493,0,"yes , they are iid",2010-08-12T09:49:25.610,852,CC BY-SA 2.5,
1674,1569,1,"+1 I like this interpretation! could you clarify "" p below e ""? why do you take small e ? you say ""the probability of making the opposite mistake is"" it is an upper bound  or exact probability? If I remember, this type of approach is due to Chernoff, do you have the references (I find your first reference is not clarifying the point :) ) ?",2010-08-12T11:08:21.647,223,CC BY-SA 2.5,
1675,1383,0,"Thank you for updating! you said: ""I'm talking about universal compression obviously"" even if things are obvious within your community you maximise the chance of having original instructive answer by recalling things we few words or a link.",2010-08-12T11:31:11.297,223,CC BY-SA 2.5,
1676,1383,0,"My personal view is that we should not mix comments and question. Comment are made to give feedback or to ask questions about the question, most often it is a dialogue between two people. The question should be something clear, easy to read and understandable by as much people as possible... it's you explaining your problem to get an answer.",2010-08-12T11:36:18.137,223,CC BY-SA 2.5,
1678,1249,0,"@srikant an upper bound is needed here. The last sentence just states that an upper bound on the desired quantity will be used for proving the minimum error in the testing problem, as @robin explained.",2010-08-12T13:13:54.340,168,CC BY-SA 2.5,
1679,1573,2,"Srikant: The problem is that you have *two* posteriors becoming more concentrated and you're looking at their relative fit.  Either prior alone would become irrelevant in the limit, but the priors continue to impact the ratio of posterior model probabilities.",2010-08-12T13:56:30.090,319,CC BY-SA 2.5,
1680,1555,0,"@Aniko, the data structure is a frequency structure where each column represent a specific treatment and the rows represents result broken up into bins.",2010-08-12T14:31:34.373,559,CC BY-SA 2.5,
1681,1598,31,Inserting shameless plug for Rcpp :),2010-08-12T14:37:32.143,334,CC BY-SA 2.5,
1682,1546,0,"i am trying to measure whether or not i am getting more visitors after i turn on SEO, and if possible to quantify that ""lift"".  it's important to compare before and after on a per-account basis, and then say something about the comparison in aggregate.

it is possible that there is a ""ramp up"" period after turning on SEO, so i'm not sure that just looking at the days where SEO is on and the days where it is off is sufficient (maybe an average within a window before and after turning it on?)",2010-08-12T14:44:45.760,125,CC BY-SA 2.5,
1683,1589,0,"It is an interesting idea to fit the previous dataset here, it would tell me the extent to which the ""problem"" I'm worried about is solved by using these functions.  Also, I may fit existing psychometric data individually by subject using glm and produce the output curves on the probability scale.  Perhaps one will pass the eyeball test and the other won't.  I'll update the question when I know more.",2010-08-12T14:49:59.117,196,CC BY-SA 2.5,
1684,1589,0,"ASIDE: I'll grant that the model should match the process.  In the case of modeling successes and failures of the robot arm you propose, values past its maximum movement are non-nonsensical, consequently they are past the point of observed values; so if I did that I'd be committing another error, attempting to extrapolate beyond the range of observed values.",2010-08-12T14:57:40.567,196,CC BY-SA 2.5,
1685,1589,0,"ASIDE continued: However, in psychophysics we have a system that starts off not working at all, gradually starts being able to process the information provided to it, and then asymptotes near 100% when the information is clearly available.  Here I may have presented stimuli that are too hard to discriminate (beyond its range on the lower bound) or presented stimuli that are too easy to discriminate (beyond its range on the upper bound).  Here it is reasonable ask what the system will do at those extreme values.",2010-08-12T15:03:25.373,196,CC BY-SA 2.5,
1686,956,0,"We sort the test results in increasing order by their un-corrected original p-value, then, iterating over the list, calculate the FDR expected if we were to reject the null hypothesis for this and all tests prior in the list, using the B-H correction using an alpha equal to the observed, un-corrected p-value. We then take, as what we've been calling our ""q-value"", the maximum of the previously corrected value (FDR at iteration `i - 1`) or the current value (at `i`), to preserve monotonicity. Does this sound like the procedure you described in your second paragraph?",2010-08-12T15:46:43.107,520,CC BY-SA 2.5,
1687,1603,0,"Given that $z^2$ can be quite large, ignoring higher order terms in the Taylor expansion is pretty fishy.",2010-08-12T15:48:20.983,89,CC BY-SA 2.5,
1688,1603,0,hmm on second thoughts I may have my inequality reversed. The bound I computed is a lower bound and not a upper bound. Please vote it down to oblivion. :-),2010-08-12T15:53:35.007,,CC BY-SA 2.5,user28
1689,1605,1,"I believe that the sums need to be normalized (e.g., use the mean score) for the distribution to tend to normality.",2010-08-12T16:05:45.593,,CC BY-SA 2.5,user28
1690,870,0,"@robin Many thanks for your comments. I apologize for my confusion of terminology. I have updated the question to include a more complete description of our correction procedure, in the hopes that it provides clarification. I have also updated the q-value link; thanks for pointing me to that.",2010-08-12T16:09:26.790,520,CC BY-SA 2.5,
1691,1588,0,"I like his 2008 paper on FDR. I don't use R, though :(",2010-08-12T16:21:50.837,795,CC BY-SA 2.5,
1692,1560,0,"I guess I'm wondering *why* the James-Stein estimator isn't widely used. Is it subsumed by these other techniques, or are the conditions of the theorem not met in practice?",2010-08-12T16:23:45.047,795,CC BY-SA 2.5,
1696,1587,1,"I think you have the interpretation of the R code correct (to be sure, hadcru.mcmc.zero has no slope parameter, it is just a mean model). I don't understand why the prior should be constrained just to the positive values though (I also wouldn't know how to code or write it down mathematically either; I want a diffuse prior but ignore negative values?). Shouldn't the H1 model include all values of the slope? Maybe I need to test whether H0: \theta <= 0 and H1: \theta > 0, in that case, I wouldn't know how to code it either :)",2010-08-12T17:07:31.950,291,CC BY-SA 2.5,
1698,1605,1,"Yes, that is correct. In my example I assumed the classes would have the same number of students, which is not realistic. Thank you.",2010-08-12T17:22:06.107,666,CC BY-SA 2.5,
1699,1249,0,Link to the paper http://math.unice.fr/~baraud/publications/min-rev1.pdf,2010-08-12T17:30:09.167,223,CC BY-SA 2.5,
1702,1610,0,"Terminology is a bit vague. I changed error to typeI-errors and typeII-errors. Hope that is fine. Also, your question should be community wiki as there is no correct answer to your question.",2010-08-12T20:00:22.107,,CC BY-SA 2.5,user28
1703,1610,0,"@Srikant: in that case, we should make questions like this cw as well: http://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english.",2010-08-12T20:01:51.153,5,CC BY-SA 2.5,
1704,1610,2,"Older literature calls H2 the null hypothesis, H1 the alternate hypothesis, then it's natural to call type i error as the error of mistakenly accepting Hi hypothesis",2010-08-12T20:03:10.947,511,CC BY-SA 2.5,
1705,1610,0,@Shane: I will abstain from commenting on your point as the present top voted answer is mine (conflict of interest). I will go with what the community feels is appropriate.,2010-08-12T20:04:53.140,,CC BY-SA 2.5,user28
1706,1610,0,@Shane you're totally right I think the question you mentionned should be community too.,2010-08-12T20:06:40.377,223,CC BY-SA 2.5,
1707,1610,0,"The tags in this case should be type-i-errors and type-ii-errors as the dashes replace spaces. A terminology tag would help people to find questions that ask about definitions of words, like this one. Finally, I'm not making this community wiki as there is a correct answer that I will be accepting - whatever technique I feel best helps me to remember the difference.",2010-08-12T20:07:02.480,110,CC BY-SA 2.5,
1708,1569,1,"Why do I take small e...hmm...that's what Balasubramanian's paper did, but now, going back to Kullback, it seems his bound holds for any e, and he also gives bound for finite n, let me update the answer",2010-08-12T20:07:21.047,511,CC BY-SA 2.5,
1709,1249,0,Can you give us the hypothesis you want to test ?,2010-08-12T20:08:37.610,223,CC BY-SA 2.5,
1710,1610,0,ok I will edit as per your request. Reg CW- fine with me as long as that makes sense to everyone else.,2010-08-12T20:10:04.003,,CC BY-SA 2.5,user28
1711,1610,5,"Honestly, perhaps the community wikiness of this question should be discussed on meta. I personally feel that there is a singular right answer to this question - the answer that helps me. However, that singular right answer won't apply to everyone (some people might find an alternative answer to be better). Personally, I want to give reputation to the person or people who help me with my problem, but if the community wants this to be community wiki, I can make it happen (but not without a discussion on meta first).",2010-08-12T20:11:46.837,110,CC BY-SA 2.5,
1712,1610,0,I started a thread here: http://meta.stats.stackexchange.com/questions/290/what-is-community-wiki.  I think that I agree with @Thomas; we should reward good answers on questions like these.,2010-08-12T20:16:14.597,5,CC BY-SA 2.5,
1715,1612,0,"I've never even thought of it pictorially before. Normally, thinking in pictures doesn't work for me, but I'll read that article and maybe this is a special case where it will help me.",2010-08-12T20:27:30.903,110,CC BY-SA 2.5,
1716,1614,1,Isn't that agism?  :),2010-08-12T20:33:46.193,5,CC BY-SA 2.5,
1717,1608,0,"The binning is unavoidable. Each bin represents a geometric region. I also use the Kruskal-Wallis test for continuous variables for the entire region, which was worked out fine. 

The rows are not ordered.",2010-08-12T20:42:55.063,559,CC BY-SA 2.5,
1718,1614,16,"How about ""once bitten, twice shy""? No funnier, but commonplace enough to remember. And no ageism required!",2010-08-12T20:54:07.470,174,CC BY-SA 2.5,
1720,1616,2,I kind of like that. I'm thinking this might work for me.,2010-08-12T21:42:13.480,110,CC BY-SA 2.5,
1721,1616,3,"it's sort of like how in elementary school kids would ask ""are you not not cool?""",2010-08-12T22:10:11.923,900,CC BY-SA 2.5,
1722,1569,0,"ok, we don't need small e (now called b, Type II error) to be small for bound to hold, but b=0 is the value for which the simplified (exp(-n KL(p,q)) bound matches the more complicated bound above. Curiously enough, lower bound for Type I error given 0 Type II error is <1, I wonder if <1 Type II error rate is actually achievable",2010-08-12T22:54:33.343,511,CC BY-SA 2.5,
1723,1619,1,"But you still have to associate type I with an innocent man going to jail and type II with a guilty man walking free. So in the end, it really doesn't get me anywhere.",2010-08-12T23:07:53.913,110,CC BY-SA 2.5,
1724,1616,2,"yes, now you just need to remember *which* hypothesis (null or alternate) :P",2010-08-12T23:17:20.717,511,CC BY-SA 2.5,
1725,1616,1,It's easier to remember that both types of errors deal with the null hypothesis. I'm seriously considering accepting this answer - I think this one helps me the most.,2010-08-12T23:33:33.170,110,CC BY-SA 2.5,
1726,1609,0,"Good references, but I don't understand your last comment about a parametric distribution. Splines are usually used for modelling the conditional mean of Y|X and are not directly to do with the distribution of either Y or X. I know you can fit spline-based density estimates, but I doubt that is what Henry was asking about.",2010-08-12T23:45:04.853,159,CC BY-SA 2.5,
1727,1616,11,Actually it's the alternate hypothesis in both cases,2010-08-13T00:10:23.680,511,CC BY-SA 2.5,
1728,134,1,Re-opened following discussion at http://meta.stats.stackexchange.com/questions/276/should-we-unclose-computing-questions,2010-08-13T00:31:38.923,159,CC BY-SA 2.5,
1729,1603,0,"by pigeonhole principle, $\bar{P} >= 1/(n+1)$, thus I think your proposed upper bound based on the mode is going to be, asymptotically, the same as the trivial upper bound.",2010-08-13T01:52:33.443,795,CC BY-SA 2.5,
1731,1582,0,Great point about the importance of causal mechanisms.,2010-08-13T02:24:18.780,183,CC BY-SA 2.5,
1732,1446,9,Every answer to my question has provided useful information and I've up-voted them all. But I can only select one answer and Srikant's provides the best overview IMO.,2010-08-13T02:25:01.587,159,CC BY-SA 2.5,
1734,1609,0,"Yes, the latter, but due to meandering and unrelated thoughts, not meant for ""publication"".  I was thinking about density estimation because of some other question and, in light of this question, was working through explaining spline methods to *myself*.  Strangely, I remember thinking what I typed was cogent -- because of what was in my *head*.  Ridiculous.  :-/  Thanks for the catch; impressed that it occurred to you, though I shouldn't be based on your quality answers.",2010-08-13T02:44:42.490,251,CC BY-SA 2.5,
1735,1616,0,"Yaroslav - I thought the same.  But well, the researcher doesn't usually want the H0 (H NULL) to be true...",2010-08-13T03:33:10.347,253,CC BY-SA 2.5,
1736,1610,0,I just wanted to say I loved your question - and I love the answers even more!,2010-08-13T03:35:25.357,253,CC BY-SA 2.5,
1738,1579,7,The rule of thumb there is highly useful. Thanks for that.,2010-08-13T03:38:38.353,776,CC BY-SA 2.5,
1739,1619,6,"+1, I like.  @Thomas: Given an ""innocent until proven guilty"" system, you could think of type I as a primary error to avoid (imprisoning the innocent) and type II as a secondary error (guilty go free).",2010-08-13T03:42:03.563,251,CC BY-SA 2.5,
1740,1444,30,I've summarized some of the answers plus some other material at http://robjhyndman.com/researchtips/transformations/,2010-08-13T04:28:23.740,159,CC BY-SA 2.5,
1741,151,19,"Much of the field of robust statistics is an attempt to deal with the excessive sensitivity to outliers that that is a consequence of choosing the variance as a measure of data spread (technically scale or dispersion).
http://en.wikipedia.org/wiki/Robust_statistics",2010-08-13T05:15:29.947,521,CC BY-SA 2.5,
1742,1619,2,"That's actually quite deep ars, thanks! My way of remembering was admittedly more pedestrian: ""innocent"" starts with ""I"".",2010-08-13T05:32:10.217,830,CC BY-SA 2.5,
1743,1622,0,"When you say that the Sharpe ratio is the mean divided by the sample standard deviation... ""Up to a constant factor (sqrt(n), where n is the number of observations)""... what does this last part mean?  Is it really the mean divided by the standard error of the mean?",2010-08-13T05:44:29.807,196,CC BY-SA 2.5,
1749,1456,0,Some correction should be made to formula above. It is var(log(OR)) not var(OR).,2010-08-13T06:34:43.027,419,CC BY-SA 2.5,
1751,1587,3,"The Bayes factor is the ratio of the marginal likelihood assuming H1 to the marginal likelihood assuming H0.  In this case, the alternative hypothesis H1 is that the slope is positive, so your prior fo H1 must exclude negative values for the slope parameter as H1 says they are implausible a-priori.  One way of achieving this might be to use an exponential prior, which is strictly positive, for the slope and a Gaussian prior for the intercept.  You could then implement a ""negative slope"" null hypothesis by flipping the data upside-down and using the same model.

HTH",2010-08-13T07:23:13.433,887,CC BY-SA 2.5,
1752,1598,1,curious if you've tried PyMC and how the performance compares (relative to python/C) for your models.,2010-08-13T07:31:00.843,251,CC BY-SA 2.5,
1753,1607,0,"If we apply t-tools to Box-Cox transformed data we will get inferences about the difference in means of the transformed data. How can we interpret those on the original scale of measurement? (The mean of the transformed values is not the transformed mean). In other words (if I'm correct), taking the inverse transform of the estimate of the mean, on the transformed scale, does not give an estimate of the mean on the original scale.",2010-08-13T07:39:21.070,339,CC BY-SA 2.5,
1755,1621,1,"Fisher's information gives lower bound in parameter estimation. It is a natural metric because it roughly says something like ""in this direction the difficulty of my problem cannot decrease more than that"".  What you call generalization bounds are upper bounds ? do you want to know the performence of the method that use Fisher metric (the large body you mention is a good list)? sorry but I don't really get the question :) can you reformulate that point ?",2010-08-13T08:45:45.920,223,CC BY-SA 2.5,
1756,1637,20,One point: t-tests use the t distribution not the Z distribution,2010-08-13T10:12:21.990,183,CC BY-SA 2.5,
1758,1444,6,excellent way to transform and promote stat.stackoverflow !,2010-08-13T11:49:16.957,223,CC BY-SA 2.5,
1759,1598,2,"@ars: In the case above, each iteration (of the 10^8 iterations) involved solving 5 ODEs. This really had to be done in C. The rest of the code was fairly simple and so the C code was straightforward. My application was non-standard and so PyMC wasn't applicable - also it was ~2 years ago.",2010-08-13T12:33:10.720,8,CC BY-SA 2.5,
1760,452,0,Would you put a link to Angrist and Pischke.,2010-08-13T13:37:18.213,8,CC BY-SA 2.5,
1761,1608,0,"@Elpezmuerto If each row represents a region and they are not ordered, then how are you doing a Kruskall-Wallis test? That test only applies to ordered outcomes. Please, please, please add a compelete description of your problem by editing your original question.",2010-08-13T13:50:46.057,279,CC BY-SA 2.5,
1762,1651,1,"I hope I did not mangle the eqns while converting them to latex. You can use latex on this site. See: http://meta.stats.stackexchange.com/questions/218/tex-processing-for-stats Could you also please clean up the question a bit. For example, the first line is a bit odd, what do you mean by "" I know the value of $y_{ij}$, but it could be more than that value."" etc.",2010-08-13T14:32:35.100,,CC BY-SA 2.5,user28
1764,1639,4,"I can look up a citation, but this is easy enough to to test empirically.  F from an ANOVA with two groups is exactly equaly to t^2 and the p-values will be exactly the same.  The only reason it wouldn't be equivalent in the case of unequal variances is if you apply a correction.  Otherwise, they are the same.",2010-08-13T14:54:42.113,485,CC BY-SA 2.5,
1765,1621,0,Let's say that Fisher's Information Matrix gives our Riemannian metric tensor. It lets us find arclength of any curve by integrating. Then you define the distance between p and q as the smallest arclength over all curves connecting p and q. This is the distance measure I'm asking about. Same with volume.,2010-08-13T15:19:41.580,511,CC BY-SA 2.5,
1766,1653,0,Would be interested to know how to fit repeated measures ANOVA models using lm.,2010-08-13T15:51:49.717,702,CC BY-SA 2.5,
1767,1653,2,"The issues of coding categorical variables, the equivalence of regression and ANOVA models, and regression coding for repeated measures are described in this article. http://dionysus.psych.wisc.edu/Lit/Topics/Statistics/Contrasts/Wendorf2004a.pdf Here's the citation... Wendorf, C. A. (2004). Primer on multiple regression coding: Common forms and the additional case of repeated contrasts. Understanding Statistics 3, 47-57.",2010-08-13T16:11:17.003,485,CC BY-SA 2.5,
1768,1648,0,"Thanks Shane, great answer! Well, ""the others"" from my field often use SPSS, so they use Kolmogorov-Smirnov (if they check normality at all), though IMHO the Lilliefors' test is a better choice when the data is gathered from a sample (when parameters are unknown). I was taught that Shapiro-Wilk's is appropriate for small samples, and just wanted to get more info about ""small samples normality tests""...  BTW, I use nortest in R! =)",2010-08-13T16:11:51.620,1356,CC BY-SA 2.5,
1769,1646,0,"Not to start a sub-question, but I've never understood the utility of intra-class correlation; it strikes me that the scenario you describe can be adequately captured by talking about (1) the variance of the team means and (2) the mean within-team variance.",2010-08-13T16:15:00.807,364,CC BY-SA 2.5,
1770,1658,0,Do we vote for the first one or the second one :) ?,2010-08-13T16:35:07.670,223,CC BY-SA 2.5,
1771,1635,0,"yes, my question was poorly worded and ambiguous. to your last point: there are well known techniques for comparing Sharpe given paired observations. My Q is for independent observations, applicable for:
1. non-overlapping times, or
2. the case where summary statistics are provided by a third party (e.g. in a fund prospectus).",2010-08-13T16:46:03.257,795,CC BY-SA 2.5,
1772,1647,2,"I agree. I performed a quick test of the A-D test, Jarque-Bera, and Spiegelhalter's test (1983), under the null, with sample size 8, repeating 10,000 times. The A-D test maintains nominal rejection rate, and gives uniform pvals, while J-B test is terrible, Spiegelhalter is middling.",2010-08-13T17:18:29.993,795,CC BY-SA 2.5,
1773,1607,0,"@gd047, some tests assume normality of the distribution of the mean, not the data.  t-test tends to be pretty robust w.r.t to underlying data.  You're right though -- with post-transformation tests, results are reported after inverse-transforming, and interpretation can be very problematic.  It comes down to how ""un-normal"" your data is, can you get away without transforming or applying, say, a log transform which is easier to interpret.  Otherwise, it's contextual on the actual transformation and domain and I don't really have a good answer.  Might be worth asking to see what others say?",2010-08-13T17:42:32.747,251,CC BY-SA 2.5,
1777,1661,0,"What is the question you want to answer? Do you want to compare males to females, or wing length to tail length, or something else? And why are you restricted to a univariate test?",2010-08-13T17:50:24.760,279,CC BY-SA 2.5,
1778,1662,0,I'm testing individual component means,2010-08-13T18:04:05.077,,CC BY-SA 2.5,Matt
1779,1662,0,I'm restricted to unvariate t-tests.,2010-08-13T18:05:27.273,,CC BY-SA 2.5,Matt
1780,1573,0,"i'm going to make this the best answer, since it provided a good starting point of study. Thanks for the discussion guys!",2010-08-13T18:06:57.707,291,CC BY-SA 2.5,
1781,328,0,"@James: Alternatively, this might argue for more education in the field, not less...",2010-08-13T18:11:14.860,5,CC BY-SA 2.5,
1782,1662,0,"What do you mean by individual components? What are these 'components'? Ideally, you should edit the question to clarify so that others can also post appropriate answers. See also Aniko's comments to your question.",2010-08-13T18:13:08.507,,CC BY-SA 2.5,user28
1783,1651,0,"I don't know of an R procedure that would do it, however there is a Stata module CENSORNB that does exactly this. Perhaps looking at the code might give you ideas.",2010-08-13T19:23:40.617,279,CC BY-SA 2.5,
1784,1642,0,"Interesting idea and it makes sense. However, the exam that I'm studying for uses Type I and Type II errors.",2010-08-13T20:07:47.593,110,CC BY-SA 2.5,
1785,1664,0,"i need to compare tail length male vs tail length female and in a separate test compare wing length male vs wing length female.

Ho: X1-male = X1-female
H1: not equal

and

Ho: X2-male = X2-female
H1: not equal

I thinking 2 separate univariate t-tests?",2010-08-13T20:45:05.053,,CC BY-SA 2.5,Matt
1786,1664,0,That's what my answer describes.,2010-08-13T20:58:11.683,8,CC BY-SA 2.5,
1787,1569,1,"Actually a much easier to understand reference for this is Cover's ""Elements of Information Theory"", page 309, 12.8 ""Stein's Lemma""",2010-08-13T21:25:20.333,511,CC BY-SA 2.5,
1788,1664,0,"In R:

##X1 TEST

t.test(T6.11.dat[1],T5.12.dat[1])

RIGHT?",2010-08-13T21:28:04.970,,CC BY-SA 2.5,Matt
1789,1669,1,"Bookmarked. I wish they offered PDFs, but you can't go wrong with NIST's documents.",2010-08-14T00:47:50.010,110,CC BY-SA 2.5,
1791,1677,0,Good call. I use SO and I was unaware that question even existed. It's good to have a cross-linking for times when questions might appear on multiple exchanges.,2010-08-14T01:46:17.650,110,CC BY-SA 2.5,
1794,452,1,"Angrist, Joshua D. and Jorn-Steffen Pischke. 2009. Mostly Harmless Econometrics: An Empiricist's Companion. Princeton University Press: Princeton, NJ.",2010-08-14T02:40:04.060,401,CC BY-SA 2.5,
1796,1594,0,"Thanks, your post looks really nice, and its what I'd expect from
a statistician too. I'd appreciate the explanation of the choice
of logistic regression though :)
But unfortunately it doesn't help me at all. Like, your function
fitting is LMS, right? Even with logit, does it really minimize
the (abs) entropy? Also all the common models in data compression are discrete -
eg. a bit run model.
Anyway, I guess I'd really have to ask a few more specific questions
instead.",2010-08-14T09:15:01.753,799,CC BY-SA 2.5,
1797,1676,0,"Making community wiki, and this is a really close duplicate of the other. Is the searching acting up? I searched for ""r packages"" with and without quotes and didn't see that...I wonder if it had something to do with the downtime last night.",2010-08-14T09:41:38.147,110,CC BY-SA 2.5,
1798,1594,0,"It's usually fitted using maximum likelihood estimation.  You really need to say some more about what might have generated the binary data to progress any further.  Also is there a specific data set?  I see you linked to one, but was that just randomly chosen or do you have another specific one in mind?",2010-08-14T10:51:58.903,702,CC BY-SA 2.5,
1800,1594,0,"Its good then if its maximum likelihood, but it turns into bruteforce in real cases which is not very practical. As to ""specific data"", I can post some (eg from a text compressor), but the whole point is that we don't know a good model for it. So instead we have to ""mix"" predictions of a few not-so-good models.",2010-08-14T11:46:12.530,799,CC BY-SA 2.5,
1801,73,0,This one's _definitely_ for SO!,2010-08-14T11:58:19.323,1356,CC BY-SA 2.5,
1802,1594,0,posted an example,2010-08-14T12:11:31.277,799,CC BY-SA 2.5,
1803,73,2,"I disagree that it's for SO, especially considering the discussions on meta that supporting tools for statistical analysis (including software) is on-topic....",2010-08-14T12:25:54.850,110,CC BY-SA 2.5,
1804,1684,2,"No disrespect, but that strikes me as entirely subjective. I have used latex for since the late 80s and R since the 90s yet I see no good reason to move away from embedding eps files. Everybody's mileage will differ here, and Thomas should maybe look at Task Views for particular problem domains.",2010-08-14T13:04:26.380,334,CC BY-SA 2.5,
1805,63,3,"Judgement is a good word, since all we can ever observe is correlation. All that experiments and/or clever statistics can do is allow us to exclude some alternative explanations for what could have caused an effect.",2010-08-14T18:18:08.860,198,CC BY-SA 2.5,
1806,1600,4,"I have done a lot of number crunching, one website and few administrative scripts in R and they are working quite nice.",2010-08-14T19:40:50.960,,CC BY-SA 2.5,user88
1807,501,0,"@Thylacoleo By comparing the error of a full set and the set with some feature knock-outed, for instance by shuffling its values across objects. Read about RF importance to learn more about the concept.",2010-08-14T19:49:45.370,,CC BY-SA 2.5,user88
1808,1676,2,"While not being moderator, I would vote to close as a duplicate; we don't need more than one R package feed.",2010-08-14T19:56:33.717,,CC BY-SA 2.5,user88
1809,1664,0,@Matt: that's correct,2010-08-14T21:42:37.077,8,CC BY-SA 2.5,
1810,1691,0,On second thoughts my answer may not make much sense. I will leave it up in case it gives you some ideas.,2010-08-14T22:15:33.003,,CC BY-SA 2.5,user28
1812,538,0,"Those Bayesian networks are equivalent in a sense that given data sampled from one of them, you can't tell which one it was",2010-08-14T23:52:21.040,511,CC BY-SA 2.5,
1814,596,0,"EM is simply one way of maximizing likelihood when your log-likelihood function involves logs of sums. This can happen when you have latent variables either in unsupervised or unsupervised models. So for instance, if you fit a CRF and assume some of the outputs are unobserved, it would be suitable to use EM. But you could also maximize that (conditional) likelihood by gradient descent.",2010-08-15T06:48:34.390,511,CC BY-SA 2.5,
1815,1699,1,The link you gave points to your own machine - 127.0.0.1,2010-08-15T07:00:41.767,8,CC BY-SA 2.5,
1816,1699,0,@csgillespie Fixed.,2010-08-15T08:41:40.557,,CC BY-SA 2.5,user88
1820,1709,0,It is not clear what you want. Do you want a visualization of the 5 points you collected or a plot of the theoretical distribution that you feel generated the data? Please clarify the question so that an appropriate answer can be provided.,2010-08-15T15:08:06.470,,CC BY-SA 2.5,user28
1822,1709,0,"I'm not sure I understand. If you know they're distributed normally then... well they'll be distributed normally! If you don't know how they are distributed then plot your data and think of which distribution seems to be the more likely to fit them. Also, do you have any knowledge at all about the process you are analysing? Could you infer the distribution from that?",2010-08-15T15:24:36.453,582,CC BY-SA 2.5,
1823,1709,0,@Srikant: I tried to clarify the question. Better?,2010-08-15T15:27:11.980,198,CC BY-SA 2.5,
1824,1709,0,"@nico: I don't know how the data are distributed. I mentioned the normal distribution only to help you understand the problem. Since we are trying to find out what process generates the distribution, we unfortunately don't know anything a priori about the distribution. I would like to visualize the data this way so that we can better identify patterns in the distribution and how the distribution changes with experimental perturbations.",2010-08-15T15:31:45.553,198,CC BY-SA 2.5,
1826,1708,2,"Can you write some crash-introduction of what you want to do (including Eigenstrat step)? People are rather lazy, and are much more prone to answer questions that don't require too much googling just to be understood.",2010-08-15T16:12:09.910,,CC BY-SA 2.5,user88
1827,1706,0,That is not finding the tangency portfolio. It is just discretizing the returns space and plotting points of min variance on the graph.,2010-08-15T18:47:03.793,862,CC BY-SA 2.5,
1828,1708,0,"For example, are you working with SNP arrays?",2010-08-15T18:57:02.150,8,CC BY-SA 2.5,
1829,1716,0,"Thank you for the suggestions. These are what I'd normally do with this kind of data. However, as I hope to explain in my edit, I really want to try something different.",2010-08-15T20:07:55.577,198,CC BY-SA 2.5,
1830,1717,0,"Thank you very much. This looks very promising! What if the data is not normally distributed? How bad is it to still use $\kappa=3-L$ ? Also, I guess if I wanted to have points with equal weight, I'd just disregard the first one, right?",2010-08-15T20:09:39.627,198,CC BY-SA 2.5,
1832,1621,1,"So, just as an example, Rodriguez gets a significant improvement by using ""information volume"" as measure of model complexity, but surprisingly I can't see anyone else trying this out",2010-08-16T00:54:39.823,511,CC BY-SA 2.5,
1834,1720,0,Inference is conditional on the BC parameter $\lambda$ -- does this have an easy interpretation on the original scale?  I think the usual course is simply to report it that way and leave it at that (usually resting on some result about asymptotic equivalence which may not apply as generally).,2010-08-16T01:06:16.333,251,CC BY-SA 2.5,
1836,1702,0,+1 the Marsland book is quite good and filled a big gap in the existing selection of ML books.,2010-08-16T03:11:15.627,251,CC BY-SA 2.5,
1837,1714,0,I can only second that... from what I've seen can you select data with the mouse in one projection while looking how the selected subset looks like in another projection.,2010-08-16T06:22:04.397,961,CC BY-SA 2.5,
1838,1681,1,deviation from the mean of what? what are the assumption on your random variable or what type of bound do you need (exponential?). I think your question would be easier to answer if you could formalize it a bit more and if you add some details. take a look at http://en.wikipedia.org/wiki/Large_deviations_theory,2010-08-16T06:37:35.847,223,CC BY-SA 2.5,
1839,1717,0,"The choice of $\kappa$ has to do with the properties of the *transformed* points (if you want to non-lineraly transform your random variables). A proper choice of $\kappa$ can also scale the points towards or away from the mean. You can safely select $\kappa$ so that $0\lt L+\kappa \lt k $ where $k$ the kurtosis of your distribution. Set $\kappa=0$ to disregard the point on the mean, without even changing the formulas.",2010-08-16T06:43:50.793,339,CC BY-SA 2.5,
1840,1720,0,Thank you. Maybe because the sample (from a population that I think it should follow an approximately symmetric distribution) might just happened to be skewed by chance.,2010-08-16T07:10:32.687,339,CC BY-SA 2.5,
1841,1729,0,"Thanks for the tags Rob, I was unable to create the latex tag myself.",2010-08-16T08:42:16.583,913,CC BY-SA 2.5,
1842,1731,0,Thank you for the tips on writing a custom export and the useful functions.,2010-08-16T08:47:58.710,913,CC BY-SA 2.5,
1843,1730,0,"This works perfectly, an easy method of achieving exactly what I wanted.",2010-08-16T08:48:27.287,913,CC BY-SA 2.5,
1844,1719,0,Should be a community wiki,2010-08-16T09:32:58.870,8,CC BY-SA 2.5,
1846,63,0,Very good comment about the symmetric/asymmetric relations. One also might claim that global warming causes piracy to increase.,2010-08-16T11:02:28.133,961,CC BY-SA 2.5,
1847,1725,0,"I've tried that series before, and that guy although he's awesome; he umms and umms and umms and umms a lot. It is really distracting",2010-08-16T11:21:58.523,59,CC BY-SA 2.5,
1848,1735,3,"A few suggestions:
1. Replace acronyms with complete words,
2. Elaborate a little more on what you are trying to achieve statistically.
Are you simply trying to determine which regression coefficients are larger or smaller?",2010-08-16T11:45:12.003,183,CC BY-SA 2.5,
1849,1710,3,"It's also a good example of the ecological fallacy (i.e., making inferences about the individual-level from group-level data).",2010-08-16T11:50:18.967,183,CC BY-SA 2.5,
1850,1736,12,Use Rpy to call R from Python ;-),2010-08-16T12:35:36.380,,CC BY-SA 2.5,user88
1852,1709,0,Picking one of your actual experimental results randomly is a good way to illustrate a typical result and can be used for simulation as well (a'la bootstrap).,2010-08-16T13:44:05.787,279,CC BY-SA 2.5,
1854,1740,0,"Thank You! This worked, although I had to drop a comma in the call to data.frame.

    stats = ddply(
    .data = ords
    , .variables = .(Symbol,SysID,Hour) 
    , .fun = function(x){ 
        to_return = data.frame(
             s = sum(x$Profit)
            , m = mean(x$Profit)
        )
        return(to_return)
    }
    , .progress = 'text'
)",2010-08-16T16:06:37.417,,CC BY-SA 2.5,humble Student
1855,1739,1,fastest running correct answer,2010-08-16T16:37:33.770,601,CC BY-SA 2.5,
1856,1181,3,"This line: freq.ex<-(dpois(0:max(x.poi),lambda=lambda.est)*200) produces errors with some real world data, because the length of freq.ex won't match freq.obs on this line acc <- mean(abs(freq.os-trunc(freq.ex))).  I adapted this line to freq.ex<-(dpois(seq(0,max(x.poi))[seq(0:max(x.poi)) %in% x.poi],lambda=lambda.est)*200) but something still isn't quite right because goodfit produces warnings.",2010-08-16T17:15:34.603,196,CC BY-SA 2.5,
1857,1747,0,PS -- the real reason I included the code is to prompt responses from anyone in the programming realm.  This function is probably found (and more elegantly) in some 'VBA stats snippets' resource ... I should search on that.,2010-08-16T17:18:20.713,857,CC BY-SA 2.5,
1858,1527,0,"You change the viewpoint here. You could just as easily say, ""the proportion of heads on any one flips is .50"". 

I contend that probabilities and proportions are essentially the same.",2010-08-16T18:13:04.040,74,CC BY-SA 2.5,
1859,1751,0,"Using the information that what I was looking for was the negative binomial distribution I was able to find the correct function in R: qnbinom(c(.025,.975),1,1/104) - Thanks a bunch!",2010-08-16T18:27:32.137,196,CC BY-SA 2.5,
1860,1753,0,"What was on the x, what was on the y, and what was the point of the graph?",2010-08-16T18:46:40.523,702,CC BY-SA 2.5,
1861,1753,0,"@Andy: It doesn't matter. This is a software package which customers will use to their own needs with figures that can represent literally anything. In my test case I have a set of six points of data which are all positive and sit in the upper, right cartesian quadrant.",2010-08-16T18:55:11.880,968,CC BY-SA 2.5,
1862,1753,0,"Perhaps, the line is drawn such that it is consistent with your points 1, 2 and 3. In any case, your question is so vague that I am not sure a reasonable answer can be given.",2010-08-16T19:11:44.513,,CC BY-SA 2.5,user28
1863,1644,15,To give an example: Often one wants to know the P(model | data) ). Frequentist analysis gives you P(data | model) however (which then people often read as P (model |data). By assuming a prior probability P(model) you can get P(model | data) in Bayesian statsitics. But then you can debate what P(model) should be.,2010-08-16T19:20:24.543,961,CC BY-SA 2.5,
1864,1753,0,Added a rephrase of the question to hopefully clarify via an example.,2010-08-16T19:37:30.187,968,CC BY-SA 2.5,
1866,1753,3,"No, what was on the axes definitely does matter.  Based on what you've written, I can imagine two different plots: one that is illustrating change in the variability of Y as X increases (in which case a simple line might be valid), and one that's illustrating the actual relationship of X and Y, which might include the mean and and SD lines... or might not.  Why 1 SD?  Why not 2?  We can only give plotting advice with some kind of context.  If people will be plotting whatever they want, all you can do is make it as flexible as possible.",2010-08-16T19:50:46.800,71,CC BY-SA 2.5,
1867,1754,0,"Thanks. That's exactly what i'm wrestling with. I have a deprecated app from a defunct company that i'm supposed to resurrect while at the same time troubleshooting their original intent. For example, the only clue (and the basis of my question) is a check box on a graph titled ""show standard dev."" Now, i'm supposed to recreate the functionality. Thanks for your patience btw. Any further insight is greatly appreciated.",2010-08-16T20:04:55.097,968,CC BY-SA 2.5,
1869,1681,0,"Hm...I thought it was unambiguous given that I'm talking about Chernoff bound for absolute error...added a link in question (it seems to break in comment)...in that notation, I'm looking for epsilon as a function of m",2010-08-16T21:55:47.023,511,CC BY-SA 2.5,
1870,1754,0,"Wow.  I don't even you that task.  In that context, I think what I'd do is have use the mean line + 2 SD for continuous X, and then use bars like csgillespie's for categorical data.  Not sure how you'd be able to differentiate reliably between cont. and cat., but I'm sure many of attempted that before.",2010-08-16T22:29:48.727,71,CC BY-SA 2.5,
1872,1559,0,They are not the same if one represents a probability and the other a proportion.,2010-08-17T02:00:28.800,776,CC BY-SA 2.5,
1873,1176,2,"Just for the protocol, Frank's talk (in video) is now online: http://www.r-bloggers.com/RUG/2010/08/user-2010-conference-videos/",2010-08-17T02:43:58.303,253,CC BY-SA 2.5,
1874,1758,0,"Sorry, but I don't understand the point of derivations",2010-08-17T03:30:40.437,511,CC BY-SA 2.5,
1875,1674,1,"It seems from the question, the entropy/volume connection is established, similarly we know of the volume/surface area connection. So the point here is to connect the trace of the FI to entropy. Or am I missing the point of your question?",2010-08-17T04:26:15.600,251,CC BY-SA 2.5,
1877,1708,0,"Yes, I am working with SNP data from SNP arrays.
But the problem is much simpler. I am looking for a statistical test that will give an estimate of max. variation along 3 eigenvalues.",2010-08-17T05:16:29.570,952,CC BY-SA 2.5,
1878,1755,0,I don't know... Paul provided xy coordinates but not SD for all the points. We should see a graph with 5 points with perhaps some error bars (assuming we know the SD).,2010-08-17T05:27:37.150,144,CC BY-SA 2.5,
1879,1764,1,Those Excel guides look pretty spooky...,2010-08-17T05:41:55.500,,CC BY-SA 2.5,user88
1880,1737,1,Question is misleading as you ask about unique combinations of factors and then in details you ask about summary by unique combinations.,2010-08-17T05:59:00.613,419,CC BY-SA 2.5,
1881,1665,0,It rather does not change your knowledge about distribution of other variable. Definition of independence: P(Y|X)=P(Y). We know value of other variable only in rare cases of perfect prediction (correlation equals 1 or -1).,2010-08-17T06:10:20.527,419,CC BY-SA 2.5,
1882,1764,5,Can you imagine how a tutorial on R looks to a person who has never seen a line of code in his or her life? :),2010-08-17T06:50:39.917,144,CC BY-SA 2.5,
1883,262,0,For my part graphs that are black and white plots without shaded backgrounds that use different icons or line types are important.  Also an easy way to do confidence intervals (where appropriate) would be nice too.,2010-08-17T07:10:38.873,196,CC BY-SA 2.5,
1884,1676,4,I would close as a duplicate; also this question is within the scope of the site.,2010-08-17T07:13:50.033,196,CC BY-SA 2.5,
1885,1483,0,Where TP is Test Positive and TN is Test Negative and FP is False Positive and FN is False Negative?,2010-08-17T07:20:04.357,196,CC BY-SA 2.5,
1886,1762,0,"About the chronographs' +/- 4%: A friend and I setup our two chronographs in tandem so that a single bullet fired would be measured by both. In the shade, his would measure a consistent 2% lower than mine: 940.0 fps on mine would record about 921.2 fps on his. When we moved the chronos to brighter lighting conditions, his would consistently record about 1.5% faster than mine, but both of our chronos were also recording about 1% higher than in the shade.",2010-08-17T07:40:01.063,937,CC BY-SA 2.5,
1887,1483,0,How would the question asker obtain SE^2_1 and SE^2_2?,2010-08-17T07:52:09.567,196,CC BY-SA 2.5,
1888,1758,0,"Perhaps, I misinterpreted what you wanted. Did you not want to express $\epsilon$ as a function of $m$? The above does not exactly get you there but I thought it is a start towards that goal.",2010-08-17T08:19:50.873,,CC BY-SA 2.5,user28
1889,1764,3,"Ok, but I can also imagine all those people drawing manually dozens of bar breaks in Excel and believing that it is an only (and thus easiest and fastest) way to do this. Or people spending hours trying to unify the formatting in a large Word document.",2010-08-17T08:54:25.050,,CC BY-SA 2.5,user88
1891,1774,2,I really do not know the rate. To be honest all I know is that my program crashed with a division-by-zero and that I need to handle that case somehow.,2010-08-17T09:54:40.183,977,CC BY-SA 2.5,
1893,1773,1,"Presumably you're attempting to quantify performance of some diagnostic procedure; is there any reason you're not using a proper signal detection theory metric like d', A', or area under the ROC curve?",2010-08-17T11:19:07.750,364,CC BY-SA 2.5,
1894,1775,13,"Extending your answer: If TP=0 (as in both cases), recall is 1, since the method has discovered all of none true positives; precision is 0 if there is any FP and 1 otherwise.",2010-08-17T11:44:27.600,,CC BY-SA 2.5,user88
1895,1658,0,"first one, I'm not famous... yet ;o)

BTW, the second one is intended as a complement, just in case there was any doubt.",2010-08-17T11:50:40.957,887,CC BY-SA 2.5,
1897,1334,4,shame they often only look better... :-(,2010-08-17T12:05:19.850,887,CC BY-SA 2.5,
1898,1764,1,"""Father, forgive them, for they do not know what they are doing."" comes to mind. :)",2010-08-17T12:19:12.163,144,CC BY-SA 2.5,
1899,1776,0,"Cool, butterfly data. If you would be willing to discuss about your work, drop me an email. I'm registered at GMail.",2010-08-17T12:30:12.967,144,CC BY-SA 2.5,
1901,485,0,"for more stats 101 videos, see: http://stats.stackexchange.com/questions/1761/statistics-probability-videos-for-beginners",2010-08-17T13:54:19.727,183,CC BY-SA 2.5,
1902,1776,1,"@downvoters: Could you please leave a comment as to how the question can be improved? I am not sure if downvoting without leaving a comment is helping the OP.

@Elaine I think you should fix the qn to give the correct value for R^2 and any other edits that you feel would help others understand your question.",2010-08-17T14:07:54.247,,CC BY-SA 2.5,user28
1903,1781,2,Just so I understand correctly the censoring issue: When you dilute a sample the concentration of a compound falls so low that the test instrument can fail to detect its presence. Is that an accurate re-phrasing of the censoring problem?,2010-08-17T14:34:17.977,,CC BY-SA 2.5,user28
1904,1771,0,"Thanks for the help.  This is where show my novice.  I , simply have been working through the basic examples in the r packages and aggregated the data to a point where I created my own graph.  Up to this point, it's been an exercise to get started in network analysis and data visualization.  This is a huge step forward though, thanks.",2010-08-17T14:34:31.827,569,CC BY-SA 2.5,
1905,1754,0,"Er... don't envy, I meant.",2010-08-17T14:43:19.733,71,CC BY-SA 2.5,
1906,1758,0,"Yes, as function of $m$ and $P$. Since $P$ is given, it doesn't make sense to rewrite it",2010-08-17T16:41:00.963,511,CC BY-SA 2.5,
1907,1723,0,"Thanks for a great answer, and a great question afterwards. It's crucial to get an insight about the background of the problem. Well, so many times I've seen people doing t-test, Pearson's r or ANOVA without getting any idea about the shape of distribution (which is often heavy-skewed) - parametric techniques ""need"" satisfied normality assumption. In psychology (which is my field of interest), we often deal with small samples, therefore I need appropriate normality test.",2010-08-17T16:41:53.863,1356,CC BY-SA 2.5,
1908,1791,0,"thanks! however - which one would I use under which circumstance, and how would I deal with >2 methods?",2010-08-17T16:56:14.407,979,CC BY-SA 2.5,
1909,1758,0,"Oh, I understand.",2010-08-17T17:18:02.640,,CC BY-SA 2.5,user28
1910,1787,3,Any reason not to like the the standard errors produced by glm?,2010-08-17T17:21:16.363,279,CC BY-SA 2.5,
1911,1791,0,"Good question; test will only handle two methods, because it can only say if they are not equal. Still, you can convert scores for each sample to ranks and there take mean rank per method and select this with smallest mean. Then you can use test to justify if the best one is significantly better than the second.",2010-08-17T17:32:00.323,,CC BY-SA 2.5,user88
1912,1782,0,Didn't know pscal handled classical NB models (besides zero inflated and hurdle).  Thanks.,2010-08-17T17:45:03.203,251,CC BY-SA 2.5,
1913,1773,4,"@Mike, precision and recall are common evaluation metrics in, e.g., information retrieval where ROC, or in particular specificity is awkward to use because you already expect a high number of false positives.",2010-08-17T18:02:36.650,979,CC BY-SA 2.5,
1914,1791,1,You may also look at http://en.wikipedia.org/wiki/MaxDiff,2010-08-17T18:09:25.747,,CC BY-SA 2.5,user88
1915,878,73,"Good quote, but it's not true!  Absence of evidence is not *proof* of absence, but it certainly is *evidence*.  Why do we think magnetic monopoles (or unicorns, for that matter) don't exist?  Because we've looked and haven't found any.",2010-08-17T18:15:29.727,319,CC BY-SA 2.5,
1916,1781,1,"Yes, that is correct: dilution by a factor of D increases all detection limits by a factor of D as well.  (The matrix interference issue is more difficult to quantify and the general situation is extremely complex.  To simplify this, the conventional model is that a suite of tests on one sample yields a vector (x[1], ..., x[k]) where the x[i] are either real numbers or are intervals of reals, typically with left endpoint at -infinity; an interval identifies a set in which the true value is assumed to lie.)",2010-08-17T18:20:52.273,919,CC BY-SA 2.5,
1917,1632,20,"All answers were both helpful and useful, and would all deserve to be accepted. This one, however, does a very good job at answering the question: with Python, you have to put together lots of pieces to do what you want. These pointers will no doubt be very useful for anyone wanting to do statistics/modeling/etc. with Python. Thanks to everyone!",2010-08-17T18:42:30.127,890,CC BY-SA 2.5,
1918,1787,4,Standard error is a measure of the expected quality of your estimate while a standard deviation is a measure of variability.  The former decreases with n while the latter does not.,2010-08-17T18:53:33.487,601,CC BY-SA 2.5,
1919,1483,0,"@Jay: didn't see your comment sooner, but the above is applicable if the DOR are independent.  @drknexus: updated the answer for how to obtain SE.",2010-08-17T18:59:43.803,251,CC BY-SA 2.5,
1920,1764,3,Official band of stats.stackexchange.com: The Broken Axes.,2010-08-17T19:09:18.933,71,CC BY-SA 2.5,
1921,1762,0,Sorry if this is a dumb question but what do the lighting conditions have to do with chronograph accuracy?,2010-08-17T19:40:22.167,666,CC BY-SA 2.5,
1922,1762,0,"Old chronos worked by shooting metal screens at start and stop gates which would break a circuit to mark start and stop times. Modern chronos use a light/IR sensor to detect the bullet passing over the gates. These sensors are affected by different ambient light levels. At major matches, the staff will typically build a box with lights inside around a pair of tandem chronos to provide consistent lighting to eliminate this factor.",2010-08-17T20:21:07.453,937,CC BY-SA 2.5,
1923,1796,0,"But the variability of results among one method is dominated by the variability of tests, so I am afraid the spread may be negligibly meaningful.",2010-08-17T21:22:41.067,,CC BY-SA 2.5,user88
1924,1781,2,Why would the detection limits go up? Are they not a feature of the test instrument rather than that of the sample being tested?,2010-08-17T21:33:50.633,,CC BY-SA 2.5,user28
1925,1796,0,"But, that is the point. If a method's performance is not robust to diversity of test situations then I would tend to prefer it less than a method that is more robust assuming of course that on average they are comparable. Of course, this is all abstract as we do not know if the OP really cares about reliability/robust methods.",2010-08-17T21:39:00.617,,CC BY-SA 2.5,user28
1926,1804,0,"this procedure results in an estimate of $M^T M$ that may not be positive semidefinite, which would be bad.",2010-08-18T00:40:16.183,795,CC BY-SA 2.5,
1927,1723,6,"But normality is *never* satisfied. It's sometimes a reasonable description of the data, but they're not actually normal.While it's sensible to check for non-normality when you assume it, it's not particularly useful to test it (for the reasons I described above). I do a qq-plot, for example, but a hypothesis test answers the wrong question in this situation. t-tests and anova usually work reasonably well if the distributions aren't heavily skew. A better approach might be to use procedures that don't assume normality - perhaps resampling techniques.",2010-08-18T00:50:33.493,805,CC BY-SA 2.5,
1929,1723,0,"Or you can use non-parametric tests, at cost of having less power. And nothing is absolutely satisfied in statistics, it's not solely a normality issue. However, bootstrapping or jackknifing are not a solution when introducing someone to t-test and/or ANOVA assumptions. I doubt that resampling techniques solve normality issues at all. One should check normality both graphically (density plot, boxplot, QQplot, histogram) and ""numerically"" (normality tests, skewness, kurtosis, etc.). What do you suggest? This is completely off topic, but how would you check, say, ANOVA normality assumptions?",2010-08-18T01:55:18.857,1356,CC BY-SA 2.5,
1932,1815,3,see this similar existing question: http://stats.stackexchange.com/questions/1352/references-for-how-to-plan-a-study,2010-08-18T09:27:20.310,183,CC BY-SA 2.5,
1933,1796,0,"Hi, thanks for the input. You're right about mean/variance tradeoff one has to look at. However, in the given situation, mbq's right, that's not so much the issue. The point is that results almost never looks like your [0.80, 0.60] / [0.71, 0.69] example: The first one would mostly stay better, just on a different scale. Still, this is a helpful reference, since I might use this at another point (I also have different families of sample generators which might need to be compared), and also I'll update my question.",2010-08-18T10:15:24.213,979,CC BY-SA 2.5,
1934,1816,0,How would I go about using the information from all generations?,2010-08-18T10:37:23.367,986,CC BY-SA 2.5,
1935,1816,0,"The easiest way is to do multiple tests, i.e. test at every generation, then use a Bonferroni or fdr correction.",2010-08-18T10:43:58.960,8,CC BY-SA 2.5,
1936,1332,0,"The best antidote to ""...lies, damned lies, and statistics.""",2010-08-18T10:44:57.120,521,CC BY-SA 2.5,
1937,328,0,"@Shane: Yes, absolutely. I was being slightly facetious in my comment, but I do believe important insights are being withheld by the private-sector community as competetive advantages, which is ultimately to the detriment of an efficient global economy.",2010-08-18T11:18:07.490,229,CC BY-SA 2.5,
1938,1816,0,"When comparing at every generation, I would have to test at a significance level of 1/1000 * 0.05 ? Isn't that a bit harsh?",2010-08-18T11:18:28.283,986,CC BY-SA 2.5,
1939,1821,1,It is the variance which is an unbiased estimator. S is not an unbiased estimate of the population standard deviation.,2010-08-18T12:13:50.883,159,CC BY-SA 2.5,
1940,1816,0,"True, but you're also doing lots of testing - can't have everything ;) You could rank the p-values, use them as a guide to see where possible errors may occur.",2010-08-18T12:26:57.970,8,CC BY-SA 2.5,
1941,1821,0,"Thanks Rob, I stand corrected! This is a subtlety that I was previousy unaware of. Others may wish to see:

http://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation",2010-08-18T12:41:14.290,521,CC BY-SA 2.5,
1942,1824,0,"sounds reasonable, any more details?",2010-08-18T12:59:49.713,986,CC BY-SA 2.5,
1943,1823,0,Thank you for the answer Jeromy.  Did you happen to find something similar in R ?,2010-08-18T13:01:36.480,253,CC BY-SA 2.5,
1945,1815,1,"@Jeromy Fair enough, but all the answers there so far have managed to avoid recommending even a single book!",2010-08-18T13:14:37.717,174,CC BY-SA 2.5,
1946,1826,4,This question is to help with any confusion over the site name proposal: http://meta.stats.stackexchange.com/questions/21/what-should-our-site-be-called-what-should-our-domain-name-be/53#53.,2010-08-18T13:23:01.910,5,CC BY-SA 2.5,
1948,1830,2,"Technically, this is holdout validation but one can imagine extending the subway example to a cross-validation context. If it helps I will re-write the example and the rest of the text to be specific to cross-validation.",2010-08-18T13:42:31.487,,CC BY-SA 2.5,user28
1949,1831,0,"that book looks pretty nice! i just installed the 'arm' package, too. thanks shane!",2010-08-18T13:52:25.473,283,CC BY-SA 2.5,
1950,1815,2,I agree more discussion from a different angle would be great. I was just posting the other question as related supplementary reading.,2010-08-18T13:53:58.497,183,CC BY-SA 2.5,
1951,1831,0,@mropa: That is my favorite statistics text: enjoy it!,2010-08-18T13:54:29.773,5,CC BY-SA 2.5,
1952,1781,1,"As an example, suppose an instrument's detection limit is 1 microgram per Liter (ug/L).  A sample is diluted 10:1 (with great precision, so we don't worry about error here) and the instrument reads ""<1""; that is, nondetectable, for the diluted sample.  The laboratory infers that the concentration in the sample is less than 10*1 = 10 ug/L and reports it as such; that is, as ""<10"".",2010-08-18T14:06:17.390,919,CC BY-SA 2.5,
1953,1804,0,"That's a good point, but the result might not be so bad.  What one hopes is that the estimate of M*M is close enough to the true value that the perturbation of eigenvalues is reasonably small.  Thus, by projecting to the eigenspace corresponding to the largest eigenvalues, you achieve only a slight perturbation of the correct solution, still achieving the sought-after dimension reduction.  Perhaps the biggest problem may be algorithmic: since you can no longer assume semidefiniteness, you might need to use a more general-purpose algorithm to find the eigensystem.",2010-08-18T14:09:39.020,919,CC BY-SA 2.5,
1954,1828,2,It seems given discussions elsewhere on this site that k-fold cross validation is just one type of cross validation and describing it does not do the general job of describing what cross validation is.,2010-08-18T14:31:12.633,196,CC BY-SA 2.5,
1955,1836,0,That is counter-intuitive!,2010-08-18T14:41:53.250,,CC BY-SA 2.5,user28
1956,1836,4,"Yes it is!  That's why it's well worth studying: challenges to our intuition are exceptionally educational.  I first learned of this from a clear paper on Carlos Rodriguez' (SUNY Albany) Web page but I couldn't find it this morning: it appears the server is down.  Try Googling ""carlos rogriguez statistics"" later.  (His paper is supposed to be at http://omega.albany.edu/8008/confint.html , but this might be an old URL.)",2010-08-18T14:51:48.607,919,CC BY-SA 2.5,
1957,1797,0,"Since in essence the nesting in a lmer makes it a repeated measures design is there a way in which your question about the appropriate confidence interval around the effect size is related to the question in repeated-measures ANOVA about which measure of effect size to report?  Specifically, it is unclear whether the error term should include subject variance or not (etc)?",2010-08-18T14:55:54.510,196,CC BY-SA 2.5,
1958,1813,0,"Just as a clarification: isn't it the case that a genetic algorithm searches randomly for a solution, so that the initial segment of any run is unlikely to produce any worthwhile solution?  Also, what exactly do you mean by ""the minimum error in the population""?  If you mean the minimum difference between a known true value and any solution out of the 1000 values in a run, then isn't that a biased indication of the run's result?  After all, in practice you would accept the final solution in each run and reject everything that precedes it, right?",2010-08-18T14:57:26.877,919,CC BY-SA 2.5,
1959,1823,2,"@Tal Quick-R lists a few power analysis procedures in R: http://www.statmethods.net/stats/power.html ; or you can use R to run a simulation for custom power analysis. 
The lme4 package is good for multilevel modelling.",2010-08-18T15:01:28.740,183,CC BY-SA 2.5,
1960,1311,1,"Suppose y=2, n=4, and there are just two birthdays.  Your formula, adapted by replacing 365 by 2, seems to say the probability that exactly 2 people share a birthday is Comb(4,2)*(2/2)^2*(1-1/2)*(1-2/2) = 0.

(In fact, it's easy to see--by brute force enumeration if you like--that the probabilities that 2, 3, or 4 people share a ""birthday"" are 6/16, 8/16, and 2/16, respectively.)

Indeed, whenever n-y >= 365, your formula yields 0, whereas as n gets large and y is fixed the probability should increase to a non-zero maximum before n reaches 365*y and then decrease, but never down to 0.",2010-08-18T15:05:43.567,919,CC BY-SA 2.5,
1961,1797,0,Nevermind - I didn't think that all the way through.,2010-08-18T15:06:22.877,196,CC BY-SA 2.5,
1963,1813,0,"By error I basically mean 1/fitness, so I'm talking about the value of the best individual in a generation.

I've recorded the fitness value of the best individual for every generation.

So I have 1000*20*2 numbers, each corresponding to the ""fitness"" of the best individual in a particular generation of a particular run.",2010-08-18T15:12:47.640,986,CC BY-SA 2.5,
1964,1828,3,"@drknexus: That's fair, but I mention that it's k-fold and I wanted to provide a visualization of the process to help explain it.",2010-08-18T15:16:04.913,5,CC BY-SA 2.5,
1965,1839,0,"By algorithm, I mean roughly the software implementation used for fitting a line to model the mean of a distribution.",2010-08-18T15:18:39.907,988,CC BY-SA 2.5,
1967,1834,0,"Right, that's why I consider OLS to be an ""algorithm"" used in linear regression...",2010-08-18T15:22:58.910,988,CC BY-SA 2.5,
1968,1813,0,"I guess the initial question was ill-posed, I've added some clarifications ..",2010-08-18T15:23:11.033,986,CC BY-SA 2.5,
1969,1834,3,"Ordinary least squares is an estimator. There are a variety of  algorithms for computing the estimate: usually some sort of orthogonal matrix decomposition, such as QR, is used. See http://en.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares",2010-08-18T15:34:31.490,495,CC BY-SA 2.5,
1970,1842,0,"Uh, I tried setting up df = data.frame(t=days, values=c(data2,cum), type=rep(c(""Bytes"", ""Changes""), each=1001)), but it gives an Error in rbind.zoo(...) : indexes overlap",2010-08-18T15:36:42.490,990,CC BY-SA 2.5,
1971,1311,0,"Why you are replacing 365 by $n$? The probability that 2 people share a birthday is computed as: 1 - Prob(they have unique birthday). Prob(that they have unique birthday) = (364/365). The logic is as follows: Pick a person. This person can have any day of the 365 days as a birthday. The second person can then only have a birthday on one of the remaining 364 days. Thus, the prob that they have a unique birthday is 364/365. I am not sure how you are calculating 6/16.",2010-08-18T15:39:58.903,,CC BY-SA 2.5,user28
1972,1842,0,"That's because data2 and cum are zoo objects. Use as.vector(data2) to get the raw values. Also, I used 1001 because I had 1001 observations. You will need something different.",2010-08-18T15:42:29.480,8,CC BY-SA 2.5,
1973,1843,0,"That's interesting, but how do we tell the reader which scale corresponds to which line?",2010-08-18T15:52:35.470,990,CC BY-SA 2.5,
1974,1842,0,"Noob R user here: Error in data.frame(t = days, values = c(as.vector(data2), as.vector(cum)), : arguments imply differing number of rows: 1063, 1300, 2",2010-08-18T16:01:35.510,990,CC BY-SA 2.5,
1975,1842,0,"Type ""days"", ""data2"" and ""cum"" to look at your data. Then look at ""length(days)"", etc. You need to match up the time points with the values.",2010-08-18T16:05:46.333,8,CC BY-SA 2.5,
1976,1843,0,"Have a look at this graph: http://imgur.com/K8BCr.png
There, we present y-axis labels and ticks only where they apply to the data (i.e., for the left axis on the top of the graph, as the corresponding data, and for the right axis on the bottom of the graph, as the correspoding data).
Additionally we used different colors (as in the example above) and line types and explained it in the caption.
You could also use a line chart on the left and a bar chart on the right axis to make the distinction clearer.",2010-08-18T16:11:38.543,442,CC BY-SA 2.5,
1977,1816,1,"Instead of bonferroni correction, you could always use the more powerful bonferroni holm. See my anyswer here: http://stats.stackexchange.com/questions/575/post-hocs-for-within-subjects-tests/760#760",2010-08-18T16:19:04.000,442,CC BY-SA 2.5,
1980,1844,0,How do these estimates relate to the one given by Cox regression? That's gotta be the gold standard for estimating HR.,2010-08-18T16:41:19.750,279,CC BY-SA 2.5,
1983,1636,0,"A similar technique proceeds by computing the $\phi$ statistic of the contingency table. Since $\phi$ is actually the correlation coefficient of two 0-1 vectors (one based on prediction, the other on reality), one can use the Fisher R-Z transform (just $\atanh$) to get an approximately normal statistic. Comparing $\atanh{(\phi_1)}$ to $\atanh{(\phi_2)}$ becomes an exercise in comparing independent normals with known variances. A similar technique for tetrachoric coefficient probably exists as well. Regrettably, these techniques are fairly low power...",2010-08-18T17:05:15.583,795,CC BY-SA 2.5,
1985,1720,4,"A nice example of a need to make inferences about the means, no matter what, is afforded by some environmental risk assessments. To simplify greatly, imagine you are planning to develop land into a park. You test the soils for some compound of concern and, as is often the case, find its concentration is approximately lognormally distributed.  Nevertheless, people using the park--who might directly become exposed to these soils--will effectively ""sample"" the soils uniformly at random as they move around. Their exposure over time will be the arithmetic mean concentration, not its geometric mean.",2010-08-18T18:42:14.327,919,CC BY-SA 2.5,
1994,1825,0,"Hi Mike, thank you for your feedback. Temperature levels X1....X5 are specific degree values (i.e., heat stimuli = 42C, 44C, 46C, 48C, 50C; cold stimuli = 8C, 6C, 4C, 2C, 0C); however, they can also be conceptualized as stimuli ranging from ""likely to cause minimal discomfort (i.e., 42C or 8C)"" to ""likely to cause maximum discomfort permissible by my research ethics board (i.e., 50C, 0C)"". With this being the case do you think that a series of univariate repeated measures or a multivariate analysis is ok?<br/>
I too am am interested to hear suggestions from any mixed effects/multivariate gurus",2010-08-18T20:39:53.807,835,CC BY-SA 2.5,
1995,1856,0,"Just to be clear: your question is about the size of the data, not about the setting, correct?",2010-08-18T20:40:34.477,5,CC BY-SA 2.5,
1996,1856,0,"Exactly, I wonder if there are any references about the ""smallest"" n (wrt. to a high number of variables), or more precisely if any cross-validation techniques (or resampling strategy like in RFs) remain valid in such an extreme case.",2010-08-18T20:45:10.597,930,CC BY-SA 2.5,
1997,1843,0,The example you've given is very good... How did you managed to vertical offset each axis?,2010-08-18T20:55:13.987,990,CC BY-SA 2.5,
1998,1454,0,"You can find more ideas and code in ""Separating Two Populations in a Sample,"" http://stats.stackexchange.com/questions/899/separating-two-populations-from-the-sample.  (Is there perhaps some better way of cross-referencing threads than I've done here?)",2010-08-18T21:49:36.920,919,CC BY-SA 2.5,
2000,1326,0,Forgotten bonus: unbalanced data are still useful!,2010-08-18T23:17:53.043,71,CC BY-SA 2.5,
2001,1855,0,"That's my second scenario, the confidence interval is too large to have any inferential value within the design of the experiment since differences between conditions are based on effects with between S variability removed.  It seems it always has a compromise meaning and needs it's own special name because you can't use it like a regular CI.",2010-08-18T23:19:21.933,601,CC BY-SA 2.5,
2002,1855,0,Blouin & Riopelle (2005) called them narrow and broad inference confidence intervals but given that the general scientific populace outside stats has a hard enough time with regular ones...,2010-08-18T23:25:54.043,601,CC BY-SA 2.5,
2003,1836,4,Amazing. I didn't know that. Thanks for the reference.,2010-08-19T00:29:41.940,159,CC BY-SA 2.5,
2005,1823,0,"Thanks Jeromy.  I think I'll just ask here on how to do that.  I have wrote code in the past for power analysis, but they tended to get complex - I would be curious to see how betteR coder then me would solve this.",2010-08-19T01:32:30.810,253,CC BY-SA 2.5,
2006,1865,0,Apparently this has an easy solution.,2010-08-19T01:38:51.183,994,CC BY-SA 2.5,
2007,1847,0,I am guessing you mean this:  http://www.stat.columbia.edu/~gelman/arm/  I just read a good review about it here: http://www.r-bloggers.com/bookshelf-remodelling/,2010-08-19T02:02:15.573,253,CC BY-SA 2.5,
2008,1865,1,Want to post it ?,2010-08-19T02:28:26.027,253,CC BY-SA 2.5,
2009,1862,0,How about PCA/FA? You would shrink correlated variables into factors and work from there...,2010-08-19T05:46:16.097,144,CC BY-SA 2.5,
2010,1560,0,according to the paper I quote both James stein and soft/hard thresholding satisfy oracle inequalities. I guess James Stein is more difficult to manipulate og to understand intuitively than hard thresholding but the why question is a good question!,2010-08-19T06:32:30.733,223,CC BY-SA 2.5,
2011,1874,0,you mean a a good 3D image reconstruction algorithm? maybe you only need an algorithm to estimate 3-D connex region? something like level set segmentation?,2010-08-19T06:47:33.400,223,CC BY-SA 2.5,
2012,1850,1,In case it's useful for a potential answerer formulas are listed here: http://en.wikipedia.org/wiki/Effect_size,2010-08-19T06:58:32.677,183,CC BY-SA 2.5,
2013,1883,0,The only way you can keep this question open is by making it community wiki so please tick the case.,2010-08-19T07:03:02.727,223,CC BY-SA 2.5,
2014,1883,2,"However, I have the feeling that this is subjective, argumentative and will require extended discussion please read http://stats.stackexchange.com/faq
I vote to close but encourage you to ask a more specific question (since the idea of the question is good but way too wide).",2010-08-19T07:05:59.710,223,CC BY-SA 2.5,
2015,1883,0,one of the extended discussion that could start: are you sure that Prof Rob Hyndman was a reasearcher when parzen and Rozenblatt proposed exponential smoothing :) ?,2010-08-19T07:08:13.167,223,CC BY-SA 2.5,
2016,1879,0,"Very helpful answer!. ""Discovering Statistics Using SPSS"" is the one I was also using. Perhaps I'll also have to review SPSS in the year to come. :-(",2010-08-19T07:17:32.367,339,CC BY-SA 2.5,
2017,1883,1,"I think with the availability of more powerful computers, different kinds of methods suddenly become practical and important (would one use e.g. boosted decision trees without fast computers ?)",2010-08-19T07:32:31.487,961,CC BY-SA 2.5,
2018,1857,0,"Thanks! I have Hastie's book and that of C. Bishop (Pattern Recognition and Machine Learning). I know that such a small n would lead to spurious or unreliable (see Jeromy Anglim's comment) association. However, the RF algorithm as implemented by Breiman allows to cope with a limited number of features each time a tree is grown (in my case, 3 or 4) and although OOB error rate is rather high (but this should be expected), analyzing variable importance lead me to conclude that I would reach similar conclusion using bivariate tests (with permutation test).",2010-08-19T07:35:00.717,930,CC BY-SA 2.5,
2019,1883,0,"To add to Robin's comment, your question seems more appropriate for a blog or a discussion forum.",2010-08-19T07:36:07.750,,CC BY-SA 2.5,user28
2020,1858,1,"Thanks! I was attending the IVth EAM-SMABS conference last month, and one of the speaker presented an application of ML in a biomedical study; unfortunately, this was a somewhat ""standard"" study with N~300 subjects and p=10 predictors. He is about to submit a paper to *Statistics in Medicine*. What I am looking for is merely articles/references wrt. standard clinical study with, e.g. outpatients, where generalizability of the results is not so much an issue.",2010-08-19T07:39:58.490,930,CC BY-SA 2.5,
2021,1423,21,Prediction about the past can also be surprisingly tricky!,2010-08-19T07:58:29.943,174,CC BY-SA 2.5,
2022,1850,0,"A simulation in R with varying n1, n2, s1, s2, and population difference would make a nice exercise. Anyone?",2010-08-19T08:04:16.043,183,CC BY-SA 2.5,
2023,1843,2,"Really good example. The only issue with your graph, is that both Y variable names are overlapping. In this case you would want one on the left and the other on the right (possibly even in a vertical position). To upgrade your example from ""really good"" to ""perfect"", you might wanna use the mtext function from R to do the variable names",2010-08-19T09:18:15.950,447,CC BY-SA 2.5,
2024,1883,0,To subjective. I would vote close.,2010-08-19T09:25:47.103,8,CC BY-SA 2.5,
2025,1877,0,Yes in this case you should go for binomial test instead of using asymptotic chi squared test.,2010-08-19T09:28:02.877,994,CC BY-SA 2.5,
2026,813,0,"By God, he was right!",2010-08-19T09:32:41.740,994,CC BY-SA 2.5,
2027,729,70,God must bring data too.,2010-08-19T09:33:49.293,994,CC BY-SA 2.5,
2029,1843,0,@Hugo @Dave: See my update for an incorporation of both comments.,2010-08-19T10:00:12.460,442,CC BY-SA 2.5,
2030,750,4,"Whoever said this had no basic understanding of Statistics, or he was joking.",2010-08-19T10:21:44.893,994,CC BY-SA 2.5,
2031,835,0,"I would say data is the bullet, Statistics is the gun.",2010-08-19T10:23:50.663,994,CC BY-SA 2.5,
2032,1406,0,All physical experiments I have ever seen have a standard deviation attached to it in some form (most commonly a +/- range). So he must have been joking.,2010-08-19T10:25:27.590,994,CC BY-SA 2.5,
2033,1888,0,"The Clason and Dormody article looks good.
Your response distribution comments are interesting to contemplate. I agree that differences in distributions might be of interest. But if you were only interested in whether population group means were different, it would not necessarily matter what distributions gave rise to such equality.",2010-08-19T10:33:16.880,183,CC BY-SA 2.5,
2034,1889,0,like so ;-),2010-08-19T10:44:24.840,,CC BY-SA 2.5,user88
2035,1849,2,"I agree. Also, depending on the domain a weighted composite of categories might be a more appropriate index of an overall rating.",2010-08-19T10:55:16.013,183,CC BY-SA 2.5,
2036,1848,2,"Interesting question. Initially, I had some trouble understanding your pseudo-code, and then I read: http://www.thebroth.com/blog/118/bayesian-rating",2010-08-19T10:57:16.523,183,CC BY-SA 2.5,
2037,1888,0,"In this case, you are assuming that your Likert scale (in other words, the perceived difference between, e.g. much satisfied and ""just"" satisfied) behaves ideally and is perceived as having the same meaning in both population. Thus you are implicitly making the assumption that this is a numeric scale, but I agree that this is often considered as such in applied research, especially if participants come from the same country. My point was just to emphasize the categorical data analysis perspective, as usually found in the Factor Analysis tradition, like in my reply to Question #10.",2010-08-19T11:01:26.003,930,CC BY-SA 2.5,
2039,1892,0,What about SAS and spss?,2010-08-19T11:29:12.243,5,CC BY-SA 2.5,
2040,1883,0,There are answers from three persons that can vote for closing and only csgillepsie and I voted for closing. I guess this means people want it open. It is important to discuss that. I have openned a discussion about that on meta http://meta.stats.stackexchange.com/questions/336/what-is-the-limit-in-subjective-argumentative-questions,2010-08-19T11:40:28.403,223,CC BY-SA 2.5,
2041,1890,0,is turing the site into a discussion forum a revolution ;) ?,2010-08-19T11:41:47.620,223,CC BY-SA 2.5,
2042,1896,0,"I would go with variance and/or standard deviation (sd is just the square root of the variance) instead of the absolute deviation, because variance and sd punish extreme outliers stronger. This seems desirable in your case.",2010-08-19T12:13:10.383,442,CC BY-SA 2.5,
2043,1896,0,"When I go with standard deviation or variance, I come up with the problem of how to combine the results for the subbenchmarks into a whole. Absolute variance (assuming that's what I did) gives me a single number.",2010-08-19T12:17:38.690,1001,CC BY-SA 2.5,
2044,1896,0,"Note that I tried to combine my sub-variances using ""the variance of the total is the mean of the variances of the subgroups + variance of the means of the subgroups"", which I got from http://en.wikipedia.org/wiki/Variance.",2010-08-19T12:18:48.187,1001,CC BY-SA 2.5,
2045,1892,0,And don't forget Stata.,2010-08-19T12:38:20.613,521,CC BY-SA 2.5,
2046,1892,0,Probably deserving of its own question: Which statistical package has made the most revolutionary contribution to the science and practice of data analysis and statistics?,2010-08-19T12:44:34.500,183,CC BY-SA 2.5,
2047,1897,0,I'm told that the results of the benchmarks (the times in ms) are not normally distributed. Will the variances be normally distributed? How does that affect the choice of 1.96?,2010-08-19T12:49:45.737,1001,CC BY-SA 2.5,
2048,1892,0,That would be overly argumentative.  I think having an answer here that acknowledges all statistical software is on point.,2010-08-19T13:06:15.627,5,CC BY-SA 2.5,
2049,1883,0,"I would vote to close, but when moderators vote to close, it's closed immediately.  I think that Colin and I are showing restraint and letting the community decide.",2010-08-19T13:07:50.927,5,CC BY-SA 2.5,
2050,1883,1,"Answering the question is not really a clear indication that you would vote to close. People saw my comment, they saw your answer, ... 10 very fast heterogeneous answers in less than an hour ! looks like a chat room ;)",2010-08-19T13:25:07.753,223,CC BY-SA 2.5,
2051,1883,0,"That's fair, although so long as the question is open, I think that it should be answered correctly.  :)  If the close votes get to 4, then I will vote to close.",2010-08-19T13:28:31.897,5,CC BY-SA 2.5,
2052,1892,0,"@Shane. Fair enough. I used to use SPSS. Now I use R. R revolutionised the way that I think about and conduct data analysis. It made data analysis fun. I can't speak too much about Stata and SAS, so I'll leave it for others to justify why they might be revolutionary.",2010-08-19T13:32:03.713,183,CC BY-SA 2.5,
2053,1896,1,"A strong case can be made for robust techniques that do the opposite of ""punishing"" outliers, because outliers in benchmarks often occur unavoidably due to other processes and external perturbations causing a process to wait from time to time.  This could be the reason the benchmarks do not appear to be normally distributed, too.  However, as cgillespie's answer points out, you can go a long way with the standard hypothesis testing machinery applied to SDs.  As a compromise, a lightly Winsorized version of the SD might be a good choice.  (http://en.wikipedia.org/wiki/Winsorising )",2010-08-19T13:47:27.700,919,CC BY-SA 2.5,
2054,1895,3,An amusing observation: all your benchmark times are multiples of 2^-12 (=1/4096).,2010-08-19T13:48:59.023,919,CC BY-SA 2.5,
2055,1909,1,The www.r-project.org/useR-20xx  URLs are preferable.,2010-08-19T13:56:00.550,334,CC BY-SA 2.5,
2056,1912,0,"sorry if it is my english (unfortunatly I'm french) but is it proper english ? what do you mean by ""losing power"" is it about testing? if yes, testing what ?",2010-08-19T14:22:29.410,223,CC BY-SA 2.5,
2057,1912,1,"There's nothing unfortunate about being French.  :)  Regretfully, I don't entirely know what Gary was talking about (hence the question).  I might assume that he meant ""loss of information"", but would be happy to get guidance in that respect.  I would say that this is correct english, but questionable statistics.",2010-08-19T14:28:46.447,5,CC BY-SA 2.5,
2058,1901,0,"Interesting... I'm a bit busy at the moment, but I'll definitely have a look at that!",2010-08-19T14:35:21.343,582,CC BY-SA 2.5,
2059,1912,0,"Perhaps, you can email him and ask? You could use the opportunity to point to this thread seeking his permission to post his answer here. (indirect promotion of the site)",2010-08-19T14:41:44.793,,CC BY-SA 2.5,user28
2060,1912,0,@Srikant: Done; we'll see what he says.,2010-08-19T14:54:49.207,5,CC BY-SA 2.5,
2061,1636,0,"Just for those of you not familiar with Phi (\phi), http://en.wikipedia.org/wiki/Contingency_table#Measures_of_association",2010-08-19T15:14:29.700,196,CC BY-SA 2.5,
2062,1915,0,"sorry for the graphical-techniques tag, but my brand new account doesn't let me tag with 'graph' and 'partitioning' as I intended to.",2010-08-19T16:10:09.153,1007,CC BY-SA 2.5,
2063,1871,0,"Thanks all.

The behavior that lead to the accuracy and time measures was very controlled. I'm looking at the results from an exploratory perspective, while trying to ask why the inaccuracies occurred.

I'm unsure about ""Each participant provides 6 values, therefore it is not unbalanced""? When subjects are correct, I have a different number of time measures between the levels, e.g. subject A got 23 correct in level 1, 16 in level 2, 10 in level 3. If I take the mean correct per level per subject can I just ignore the number of correct answers & the difference in the number of correct answers?",2010-08-19T16:25:38.313,993,CC BY-SA 2.5,
2065,1852,2,"(1) I think you meant to write that the *sample*, not the *population* mean, is 112 with 95% *probability*, not *confidence.*
(2) Your point is well taken--it could apply to any question--but isn't it stated a little extremely? First, the question doesn't ask for an inference about the population mean: we are told it is $\$$200. Thus, conditional on the assumptions, we can estimate the population mean with certainty! Second, even if we were asked to estimate the population mean from the sample, there's still some trivial information we could offer (e.g., it doesn't exceed $10^11 per annum).",2010-08-19T16:56:09.480,919,CC BY-SA 2.5,
2066,1852,1,"(1) good catch. (2), yes, I can make the problem setup asymptotically perverse for fixed results, _post hoc_. my bad.
however, I am no longer sure what the OP is trying to test. If they know the population mean is 200, why are they trying to test it?",2010-08-19T17:13:03.993,795,CC BY-SA 2.5,
2067,1852,1,"BTW, evidently a CEO salary/least paid salary ratio of 400 is not considered extreme in the US. 800 is a bit perverse, though.",2010-08-19T17:15:45.953,795,CC BY-SA 2.5,
2068,1919,0,May fit better under: http://stats.stackexchange.com/questions/1906/data-mining-conferences?,2010-08-19T17:16:13.207,5,CC BY-SA 2.5,
2069,1920,1,Already noted here: http://stats.stackexchange.com/questions/1906/data-mining-conferences,2010-08-19T17:21:43.103,5,CC BY-SA 2.5,
2070,1919,0,"@Shane, I guess so. I hadn't seen the other two conference questions before I answered.",2010-08-19T17:28:27.027,11,CC BY-SA 2.5,
2071,1912,0,"I saw this as well, and assumed it was a warning against blindly studentizing data.",2010-08-19T17:47:36.353,795,CC BY-SA 2.5,
2072,1916,0,"Thanks. I fitted a no nugget model to the likelihood using proc mixed, I guess I should compare the AICC with that of a model with nugget.",2010-08-19T18:08:06.140,1004,CC BY-SA 2.5,
2073,1844,0,"Cox model incorporates covariates. The Kaplan-Meier, Nelson-Aalen, Mantel-Haenszel methods model hazard as a function only of age.",2010-08-19T18:54:53.113,795,CC BY-SA 2.5,
2074,1871,0,"those raw answers... 23, 16, etc. cannot go into a repeated measures ANOVA.  You need to get their means and enter one value of each (6 total) for each S.",2010-08-19T19:52:32.937,601,CC BY-SA 2.5,
2075,1924,0,Hm...following the derivations under asymptotic normality link I'm a bit confused...what's the difference between I and H?,2010-08-19T20:14:36.283,511,CC BY-SA 2.5,
2076,1924,0,"There is no difference. See the [definition of fisher information i.e., I](http://en.wikipedia.org/wiki/Fisher_information#Definition) where it is re-written as H. I guess the variance expression can be simplified on the wiki as $H^{-1} I$ is an identity matrix.",2010-08-19T20:32:39.393,,CC BY-SA 2.5,user28
2077,1924,0,"Hm....it says that H becomes I when the model is correctly specified, this makes me wonder what's the difference when it is not",2010-08-19T20:50:44.980,511,CC BY-SA 2.5,
2078,1928,1,Nice reference: thanks.  It comprehensively covers everything mentioned here.,2010-08-19T21:03:18.633,919,CC BY-SA 2.5,
2079,1884,0,I am not convinced -- is P(X_n != X_(n+1)) independent from n?,2010-08-19T21:36:15.600,573,CC BY-SA 2.5,
2080,1874,0,"Just a good image reconstruction algorithm(The regions involved are often not convex). It's just that there are a lot of different ones, and I'm not familiar which ones are best in the context I'm approaching the problem in. (Easy to parametrize estimated surfaces, easy to compute volume, and easy to find minimum distance from a given point)",2010-08-19T21:55:29.840,996,CC BY-SA 2.5,
2081,1924,0,Where does it say that? I am a bit unsure what the model correctness has got to do with the relationship between H and I.,2010-08-19T22:36:29.543,,CC BY-SA 2.5,user28
2082,1791,0,"Well that's a bunch of hints. I'll have a look at those, thx!",2010-08-19T23:02:21.863,979,CC BY-SA 2.5,
2083,1924,0,"At the bottom of the asymptotic normality derivation from your link. It says ""Finally, the information equality guarantees that when the model is correctly specified, matrix H will be equal to the Fisher information I, so that the variance expression simplifies to just I^‚àí1""",2010-08-19T23:19:04.473,511,CC BY-SA 2.5,
2084,1924,0,I thought about it and I fail to see the connection to the model being the correct one for the data. I do not have a standard textbook to see what the issue is. If I can think of something I will update my answer or add a comment.,2010-08-20T01:03:51.620,,CC BY-SA 2.5,user28
2085,1909,0,Thanks Dirk. I added the link to R conferences on R-project,2010-08-20T01:44:32.523,183,CC BY-SA 2.5,
2087,1925,1,"Would you like to explain your reasons?
I can see how such a model might provide a more precise model of observed responses. However, in the typical practical research situations that I have seen, researchers are interested in whether the two groups differ in terms of the mean (e.g., did the training group report greater performance than the control; was student satisfaction higher one year to the next). The proportional odds ratio model does not test this question exactly as far as I am aware.",2010-08-20T01:53:50.553,183,CC BY-SA 2.5,
2088,1888,0,"I assume that the mean of the sample responding to a Likert item is generally a meaningful summary of the group's position on the underlying dimension.
It's interesting to think about when the meaning of a Likert item would vary systematically between groups. Of course, this issue extends beyond just Likert items, probably to any subjective measurement procedure.",2010-08-20T02:03:10.717,183,CC BY-SA 2.5,
2089,1836,4,Thanks - any chance this is the Rodriguez paper you're thinking of?  http://arxiv.org/abs/bayes-an/9504001,2010-08-20T06:07:24.607,251,CC BY-SA 2.5,
2090,1849,0,Thanks for the thoughts Aniko. I'll take a look at this in the next few days.,2010-08-20T06:18:29.590,991,CC BY-SA 2.5,
2091,1844,0,"@shabbychef: with Cox PH, use a single binary covariate, i.e. coded 0/1 for reference/comparison groups, then exp(beta) = HR.",2010-08-20T06:47:38.413,251,CC BY-SA 2.5,
2092,1944,4,maybe `standardized` ?,2010-08-20T08:34:49.683,339,CC BY-SA 2.5,
2094,1946,0,"no, it is called z-score (small letter), big letter Z-scores refer to mean = 100 and sd = 10.",2010-08-20T08:46:51.370,442,CC BY-SA 2.5,
2096,1946,0,"@Henrik I have never heard of something like this; nevertheless I've made the change, it seems lowercase z is used more often.",2010-08-20T09:25:39.137,,CC BY-SA 2.5,user88
2097,1946,0,could be that it is a psychology specific distinction. browsing the web i realized that it could even be specific German. At least I only found it in the German wikipedia (not surprisingly i learned it in a German university): http://de.wikipedia.org/wiki/Normskala,2010-08-20T09:50:21.083,442,CC BY-SA 2.5,
2099,1853,0,Multivariate or univariate ?(it makes a huge difference in this context),2010-08-20T10:09:52.970,603,CC BY-SA 2.5,
2102,1922,0,"upvoted for the reference to the paper and the book, I'll read them",2010-08-20T11:54:36.750,840,CC BY-SA 2.5,
2103,1862,0,"this might be too much, if management asks 'how did you get the aggregated numbers?' they'll want a simpler technique so they can (feel they) understand it. Alas, the real world :-( Thanks, though.",2010-08-20T11:57:37.980,840,CC BY-SA 2.5,
2104,1869,0,I'll mark this as the answer; there are several good suggestions in it so I'll think how to apply them.,2010-08-20T11:58:40.817,840,CC BY-SA 2.5,
2105,1943,0,Having had a brief look at that paper i'm not sure the estimates they consider are the same as those in the questioner's equations. I agree with the comments under the question - maybe in 1981 approximate methods were useful but these days there's no obvious reason not to use Cox regression.,2010-08-20T12:42:10.483,449,CC BY-SA 2.5,
2106,1844,0,"The log-rank is a more powerful test than Cox PH when the proportionbal Hazards assumption is satisfied. So with a single 2-level covariate, a log-rank or Mantel-Haenszel test is preferable.",2010-08-20T12:54:43.443,521,CC BY-SA 2.5,
2107,1844,0,see below for answer...,2010-08-20T13:11:51.667,521,CC BY-SA 2.5,
2108,1949,0,Interesting... I guess analytical chemists are very much behind the times! Mind telling me how both of these became discredited? I will look into your reference and see how the algorithms for these look like.,2010-08-20T13:31:16.067,830,CC BY-SA 2.5,
2109,1836,0,"@ars: Yes, it is: the URL in the subtitle confirms it.  Thanks for finding it.",2010-08-20T14:16:48.697,919,CC BY-SA 2.5,
2110,1941,2,"Thank you very much for this thought.  Indeed, this is a standard and well-documented approach to multiple censoring.  One difficulty lies in its intractability: those integrals are notoriously difficult to compute.  There's a modeling problem lurking here, too: the value of *d* is usually positively correlated with *Y*, as implied by the first paragraph of my description.",2010-08-20T14:22:58.607,919,CC BY-SA 2.5,
2111,1872,0,"Thank you Kevin - I went with this option, went a head and ordered his book.",2010-08-20T14:37:13.133,253,CC BY-SA 2.5,
2112,1879,0,"Great answer Jeromy, I will make use of the resources you offered. thanks!",2010-08-20T14:38:01.977,253,CC BY-SA 2.5,
2113,1912,0,"He says:
""not stat'l power, but scale invariance loses the power that can be extracted from knowledge of the substance""
http://twitter.com/kinggary/status/21624260888",2010-08-20T14:40:01.723,449,CC BY-SA 2.5,
2116,1912,0,"Thanks @onestop: now I understand what he meant.  Even though I know the answer, I'll update the question for future searches.",2010-08-20T15:21:20.860,5,CC BY-SA 2.5,
2117,1956,1,You can use the HMM package: http://cran.r-project.org/web/packages/HMM/index.html.,2010-08-20T15:26:12.730,5,CC BY-SA 2.5,
2118,1943,0,"@onestop: hmm, think definition of O/E == LR with the log forgotten above? I agree with what you say about Cox PH -- that's not the question I was trying to answer, but your advice is better in the broader context.",2010-08-20T15:27:21.653,251,CC BY-SA 2.5,
2119,1946,1,Let's not confuse the z-score with the process of standardization.  The former is described in ars' answer: it is a statistic.  Standardization consists of replacing each of the X[i] by (X[i]-m)/s for further analysis.  That's what nico was asking about.,2010-08-20T15:58:49.197,919,CC BY-SA 2.5,
2120,1844,0,@Thylacoleo: the log-rank test is equivalent to the Score test under Cox PH. Think you're confusing PH *tests* with HR *estimators* here.,2010-08-20T16:10:02.463,251,CC BY-SA 2.5,
2121,1961,2,"In the context of categorical variables, one usually talks of a ""measure of association"" instead of ""correlation"". Searching for this term should lead you to lots of options.",2010-08-20T17:20:03.630,279,CC BY-SA 2.5,
2122,1958,0,"Thanks for the suggestions. I'll have to look into them in more detail, but one thing that occurs immediately is that the series do not share a time frame, so cointegration and VAR are probably not relevant. I've edited the question to try to clarify this, apologies for being misleading before. Noise filtering brings up some other issues, but those are for another question and another day!",2010-08-20T17:44:04.463,174,CC BY-SA 2.5,
2123,1958,0,"Ok.  Well, looking at the distributions may be the way to go.  You can also try a distribution test such as Anderson‚ÄìDarling (as in this question: http://stats.stackexchange.com/questions/1645/appropriate-normality-tests-for-small-samples).",2010-08-20T18:01:16.977,5,CC BY-SA 2.5,
2124,1961,0,Thanks Aniko. I'll correct my question here (and also look for it),2010-08-20T18:04:46.707,253,CC BY-SA 2.5,
2125,1947,0,"Nice answer, Robin!",2010-08-20T18:07:46.560,251,CC BY-SA 2.5,
2129,1895,0,@whuber: must be the maximum resolution - a quarter of a microsecond ain't bad! 22-bits resolution-ish?,2010-08-20T19:51:40.860,1001,CC BY-SA 2.5,
2130,1963,0,Thank you @shane for creating the meta-analysis tag: I wanted to but couldn't.,2010-08-20T20:38:57.143,919,CC BY-SA 2.5,
2133,1965,0,"You're correct for the case beta = 0, but not for positive values of beta: you lost everything by throwing away the -beta*n*log(n) term.",2010-08-20T21:41:12.633,919,CC BY-SA 2.5,
2134,498,0,"I'm with Peter and Srikant. While I generally support the policy change of being more broadly accepting of computing questions, this one seems far removed from statistical content and more like a question for tech support. One here and there doesn't bother me, but I think it would be seriously detrimental if the main page became cluttered by such questions.",2010-08-20T22:32:00.287,71,CC BY-SA 2.5,
2135,498,0,"Keeping the majority of questions about statistical analysis will encourage experts to check out questions that might lie outside of the immediate domain.  If we get to the point where tags have to be used to sort past all of the questions about interfacing with statistical programs, I suspect we'll lose valuable cross-pollination.",2010-08-20T22:33:43.843,71,CC BY-SA 2.5,
2136,498,0,"For example: the GIS Exchange (http://gis.stackexchange.com/) is very permissive about what kinds of questions can be asked there, and as a result, the main page is almost completely dominated by technical questions.  That's fine and clearly what the community there wants, but it makes it tough to find questions that grapple with interesting problems or applications - so I largely ignore most of the questions and overall participate much less than I do here.  Interesting theoretical questions and applications are the majority of what I see here, and I'd like to keep it that way.",2010-08-20T22:47:36.910,71,CC BY-SA 2.5,
2137,498,0,(Sorry for the big block o' text),2010-08-20T22:48:20.417,71,CC BY-SA 2.5,
2139,1965,0,"uhh, no, not really. I assumed the OP was interested in an asymptotic (in $n$) bound. the worst case for my lower bound is $\beta = 1$. Then for, say, $n > 35$, we have $\log{n} < 0.1 n$, and thus $\log{L} = -n\log{n} + n^2 > 0.9 n^2$. Using L'Hopital's rule, you can show that $\log{L} \ge (1 - \epsilon) n^2$, asymptotically in $n$, for every $\epsilon >= 0$.",2010-08-21T00:47:02.553,795,CC BY-SA 2.5,
2140,1972,0,Articles directly about this would be hard to find... perhaps look up least significant difference scores... which is equivalent to what you ware trying to do.,2010-08-21T02:09:24.930,601,CC BY-SA 2.5,
2141,1961,1,"After a second thought - what I am looking at here is ordered, so looking at three levels as categorical would not fit here...",2010-08-21T03:40:05.790,253,CC BY-SA 2.5,
2142,1962,2,Quick note: Phi and Pearson correlation give the same value. Phi is just a simpler calculation procedure.,2010-08-21T05:10:33.107,183,CC BY-SA 2.5,
2145,1844,0,"ars, you are probably correct. I wrote my comment from memory and it doesn't seem correct on reflection.",2010-08-21T09:48:28.893,521,CC BY-SA 2.5,
2146,1962,1,And Cramer's V gives the same value as Phi for 2x2 tables.,2010-08-21T11:02:27.203,1356,CC BY-SA 2.5,
2147,1984,1,"No, not formally -- LaTeX submission is encourages but if you look at the [instructions page](http://www.jstatsoft.org/instructions) it does not contain the word Sweave.  Authors do use it and/or ship the R code with the paper, but to me this echos Shane's point about package vignettes.",2010-08-21T11:44:07.067,334,CC BY-SA 2.5,
2148,1984,1,"Ok, still most submitters do use it (also journal style includes Swave.sty); the main problem is that there are no Rnws published, still papers made by Sweave come with Stangle output.",2010-08-21T13:21:08.277,,CC BY-SA 2.5,user88
2149,1965,0,"oops, should be $\epsilon > 0$; letting $\epsilon = 0$ is right out.",2010-08-21T16:03:09.020,795,CC BY-SA 2.5,
2150,1884,10,"You ought to credit the person who *really* answered this question (George Lowther), especially because you copied his answer with only trivial changes.  See http://math.stackexchange.com/questions/2763/what-is-the-expected-number-of-runs-of-same-color-in-a-standard-deck-of-cards .",2010-08-21T17:17:44.323,919,CC BY-SA 2.5,
2152,1359,1,"@Brett There are a few book recommendations on my [related question](http://stats.stackexchange.com/questions/1815/recommended-books-on-experiment-design) and some others may accrue in time. Not sure how well they fit your criteria, but might be worth a glance.",2010-08-21T18:18:12.117,174,CC BY-SA 2.5,
2154,1986,0,"I know about Tukey's HSD. I think I was unclear in my question. While i'm doing ANCOVA, i'm not trying to generate CIs from post-hoc pairwise comparisons. I'm just generating them by bootstrapping the samples in each level of the factor (also tried using the t-distribution). The CIs are for the mean of each individual group (factor level). I agree with Tal's comment that this does not allow differences between groups to be assessed. It's easy to do a Bonferroni correction on these CIs, is it possible to do the Hochberg (or other) method? (p.adjust requires a vector of p values I don't have).",2010-08-21T23:26:12.157,1029,CC BY-SA 2.5,
2155,1868,0,Your blog is chock full of useful nuggets; thanks!,2010-08-22T00:21:34.117,251,CC BY-SA 2.5,
2156,1998,0,"The following question on [""intuitive explanation of multicollinearity""](http://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-re) may be of some help in the above context.",2010-08-22T01:30:24.227,,CC BY-SA 2.5,user28
2157,1997,3,"I second this answer and would just like to add another great reference on this topic:  Singer's Applied Longitudinal Data Analysis text <http://gseacademic.harvard.edu/alda/>.  Though it is specific to longitudinal analysis, it gives a nice overview of MLM in general.  I also found Snidjers and Bosker's Multilevel Analysis good and readable <http://stat.gamma.rug.nl/multilevel.htm >.  John Fox also provides a nice intro to these models in R here <http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-models.pdf>.",2010-08-22T03:17:00.480,485,CC BY-SA 2.5,
2158,2002,0,Notice that the span measure would mean different window size for different number of observations.,2010-08-22T04:10:17.800,253,CC BY-SA 2.5,
2161,2008,6,"If your aim is to judge how much that citing article is ""important"" or ""good"" by itself, you could always try reading it...",2010-08-22T07:47:05.030,449,CC BY-SA 2.5,
2165,2008,2,"Well, reading takes time and you may not even have the article available to begin with, so trying to estimate the relevance and quality vs time and maybe even money investment required is crucial.",2010-08-22T10:40:55.350,979,CC BY-SA 2.5,
2166,1763,0,stat579: Where exactly are the video lectures?,2010-08-22T11:52:22.617,339,CC BY-SA 2.5,
2167,1763,0,@gd047 That's sad. stat579 videos appear to have been removed. I've removed the link from my answer.,2010-08-22T12:07:15.790,183,CC BY-SA 2.5,
2168,1884,0,"@whuber Yes definitely, it's not my credit. I merely reproduced it here. Thanks for putting up that note.
@Karsten It is shown by a counting argument - once you have fixed $X_n$ in 52 ways, there are 26 ways you can select non-matching $X_{n+1}$. So $P(X_n\ne X_{n+1})=\frac{52*26}{52*51}$.",2010-08-22T12:35:10.773,994,CC BY-SA 2.5,
2169,1884,0,"@Karsten If you are asking if the events $X_n \ne X_{n+1}$ are independent of $n$, they are not. But that doesn't matter since expectation of sum = sum of expectations even if the quantities are dependent.",2010-08-22T12:38:02.077,994,CC BY-SA 2.5,
2172,1763,0,"Perhaps because this ""newer version"" is available. http://www.public.iastate.edu/~hofmann/stat480/",2010-08-22T14:32:49.147,339,CC BY-SA 2.5,
2173,2023,0,"Thanks. I don't have easy access to the full-text, but from the opening page it does look useful regarding point 1 (i.e., combining two correlations).",2010-08-22T14:55:24.623,183,CC BY-SA 2.5,
2174,2024,0,I alfred.  What model (on your data) do you use to estimate the mean and SE ?,2010-08-22T14:59:10.047,253,CC BY-SA 2.5,
2175,1763,0,The videos appear to still be available online. I added the links.,2010-08-22T15:36:08.213,183,CC BY-SA 2.5,
2176,2024,0,Can you post an example graph?,2010-08-22T16:10:03.547,,CC BY-SA 2.5,user88
2177,1763,0,@Jeromy Anglim Ahhh! You did a great job! (I wonder how). Is there any video from stat480 online also?,2010-08-22T16:20:18.073,339,CC BY-SA 2.5,
2179,2013,1,you could add Red-R to the list (kind of a clone of Orange in R): http://www.red-r.org/,2010-08-22T19:10:44.310,170,CC BY-SA 2.5,
2180,2027,0,"Thank you John & Tai

I guess that covers any potential avenue of my poorly formulated question.

regards

Alfredo",2010-08-22T19:23:31.523,,CC BY-SA 2.5,Alfred
2182,2032,0,Possible duplicate: http://stats.stackexchange.com/questions/652/best-books-for-an-introduction-to-statistical-data-analysis,2010-08-22T22:43:40.670,5,CC BY-SA 2.5,
2183,2027,0,"You're welcome Alfred.  Please choose Johns answer so to verify you found your answer :)  Cheers,
Tal",2010-08-22T23:12:32.077,253,CC BY-SA 2.5,
2185,949,6,"I assume you mean ""when are the MLEs of the parameters in closed form?""",2010-08-23T00:43:52.180,805,CC BY-SA 2.5,
2186,203,3,"A related question: People often use the nonparametric Mann-Whitney test for this kind of data. Since there are only five possible values, there will be lots of tied ranks. The Mann-Whitney test adjusts for tied ranks, but does this adjustment work when there are a huge number of ties?",2010-08-23T01:22:52.757,25,CC BY-SA 2.5,
2188,707,0,"@mbq: I say the ""usual"" choice. Does not mean every choice",2010-08-23T02:52:22.497,188,CC BY-SA 2.5,
2189,2034,0,"Good point. However, in research settings this may or may not be possible. Means and standard deviations are typically known. However, in some cases a different measurement tool is used in the two studies. E.g., Imagine a study looking at the correlation between intelligence and job performance across jobs and two studies varied in the measurement of job performance or in the particular intelligence test used. Thus, the observed variables are on different metrics, even though the theorised latent variables are the same.",2010-08-23T03:40:13.407,183,CC BY-SA 2.5,
2190,1853,0,univariate. but now you've made me curious about the multivariate case. ;),2010-08-23T03:49:11.310,795,CC BY-SA 2.5,
2191,2032,0,I collected some links here: http://meta.stats.stackexchange.com/questions/6/what-should-our-faq-contain/361#361,2010-08-23T04:47:35.687,251,CC BY-SA 2.5,
2195,2013,0,I've downloaded R and I am playing with it now.,2010-08-23T11:52:45.863,1026,CC BY-SA 2.5,
2198,2034,0,"A necessary condition for combining these would be some quantitative relationship between the two forms of measurement, Jeromy.  Absent that, it's difficult to see how any combination of the two study results could be adequately defended.",2010-08-23T13:59:41.453,919,CC BY-SA 2.5,
2199,2037,0,"I changed the tag from ""truncated-gaussian"" to ""truncation"" because most answers will be potentially useful in situations involving other distributions.",2010-08-23T14:21:10.707,919,CC BY-SA 2.5,
2202,1997,0,"Thank you all for your responses :) As a follow up question, couldn't most data be conceptualized as being naturally hierarchical/nested? For example, in most psychological studies the there are a number of dependent variables (questionnaires, stimuli responses, etc...) nested within individuals, which are further nested within two or more groups (randomly or non-randomly assigned). Would you agree that this represents a naturally hierarchical and/or nested data structure?",2010-08-23T16:20:47.613,835,CC BY-SA 2.5,
2203,1988,1,"Carlos, thank you for your contribution, but the level of scientific validation that I need should, at least, give me a ""probability"" value of that claim. Thanks, anyway.",2010-08-23T16:20:59.517,990,CC BY-SA 2.5,
2204,1999,0,"Thank you for your detailed reply Srikant :) I am not currently familiar with Bayesian analyses, but i is one of the topics that I have been meaning to investigate. Is Hierarchical Bayesian analysis different from the other multilevel/hierarchical analyses discussed on this page? If so do you have a recommended resource for interested parties to learn more?",2010-08-23T16:24:56.717,835,CC BY-SA 2.5,
2205,1997,0,"If any of you multilevel/hierarchical gurus could spare a few minutes I would be very grateful if you could weigh in on the analysis questions posed in a different post (http://stats.stackexchange.com/questions/1799/recommendations-or-best-practices-for-analyzing-non-independent-data-specifi). Specifically, do you think that the pain perception data outlined in that post would be better analyzed by hierarchical analyses than non-hierarchical analyses? Or would it not make a difference or even be inappropriate? Thanks :D",2010-08-23T16:31:20.017,835,CC BY-SA 2.5,
2206,1999,0,"From an analytical perspective HB analysis = multi-level models. However, the term multi-level models is used when you have different levels that occur naturally (See the example of @ars). The term HB models is used when you do not necessarily have different levels in the situation. For example, if you are modeling a consumer's response to various marketing variables (e.g., price, adv spend etc) then you may have the following structure at the consumer level: $Œ≤_i \sim N(\bar{\beta},\Sigma)$ and $\bar{\beta} \sim N(.,.)$ at the population level. For references: See the other answers.",2010-08-23T16:35:38.700,,CC BY-SA 2.5,user28
2207,1943,0,"Bernstein et. al. show some reasons (small n, ties) that cause the two methods to be inaccurate or different. But all of the discrepancies they showed are small. So I don't think anything in that paper explains the three fold discrepancy I saw that prompted this question. See below for the answer I came up with.",2010-08-23T17:38:22.250,25,CC BY-SA 2.5,
2209,1965,0,"The point is that O(n^2) is too crude.  You obtained the next term, -beta*n*log(n), but then threw that away.  So your answer, O(exp(n^2)), is correct but it's not best.",2010-08-23T17:57:47.570,919,CC BY-SA 2.5,
2210,2013,0,"@Amro Thanks! However, it is not available on Mac platform, unless I'm mistaking?",2010-08-23T18:37:46.093,930,CC BY-SA 2.5,
2213,2036,0,"According to reference, the distribution function is: $g(y) = C\sum_{k=1}^\infty\frac{\delta_k y^{\rho+k-1}e^{-\frac{y}{\beta_1}}}{\Gamma(\rho+k)\beta_i^{\rho+k}} $",2010-08-23T20:53:17.600,1043,CC BY-SA 2.5,
2216,2036,0,where $C = \sum_{i=1}^n\left(\frac{\beta_1}{\beta_i}\right)^{\alpha_i}$,2010-08-23T20:59:32.750,1043,CC BY-SA 2.5,
2217,2036,0,And $\gamma_k = \sum_{i=1}^n\alpha_i\frac{(1 - \frac{\beta_1}{\beta_i})^k}{k}$,2010-08-23T21:19:17.980,1043,CC BY-SA 2.5,
2218,2036,0,"with
$\rho = \sum_{i=1}^n\alpha_i$",2010-08-23T21:19:36.560,1043,CC BY-SA 2.5,
2219,2036,0,"And finally 
$\delta_k = \frac{1}{k+1}\sum_{i=1}^{k+1}i\gamma_i\delta_{k+1-i}; \delta_0=1, k=0,1,2,3,\ldots$",2010-08-23T21:19:54.530,1043,CC BY-SA 2.5,
2220,386,2,"This is a nice idea, but if I have understood it correctly, it seems to enlarge the idea of ""outlier"" to include *any* value in a dataset that is distant from the others.  For example, in the batch {-110[1]-90, 0, 90[1]110} of 43 integers, wouldn't your procedure identify the 0 (which is the *median* of these numbers!) as the unique ""outlier""?",2010-08-23T22:36:55.787,919,CC BY-SA 2.5,
2221,1965,0,"It is not clear what kind of bound the OP is looking for. Because the free variable was given as $n$, I assume a bound asymptotic in $n$ is desired. In my answer I note that this is an asymptotic bound. (the big-O notation is another hint that this is the case.) The bound that I showed is an asymptotic *lower bound*. I am not 'throwing away' the log term: $\Omega{(n^2 - n\log{n})}$ is $\Omega{(n^2)}$. Then since this lower bound is the same as the trivial upper bound, the trivial upper bound is optimal.",2010-08-23T22:47:04.807,795,CC BY-SA 2.5,
2222,2059,2,"EM is _not_ a heuristic, it is an algortihm for maximizing likelihood with known properties. The error you are looking for probably has nothing to do with EM, but rather the efficiency of the maximum likelihood estimate. This will surely depend on the unknown parameters, and not just the sample size.",2010-08-24T00:00:20.510,279,CC BY-SA 2.5,
2223,2059,0,"Yep, my question refers to those ""known properties"". It is used as a heuristic in the sense that I see people apply it on ""good faith"" beyond what is explained by provable bounds. I changed the wording of the question, since this is really not the issue I'm interested in.",2010-08-24T00:59:35.717,1072,CC BY-SA 2.5,
2224,2060,0,"@Moritz : In reply to your comment about EM being a heuristic, EM does have strong theoretical foundations and it is known to provably converge to a local maximum.",2010-08-24T01:13:51.697,881,CC BY-SA 2.5,
2225,2061,0,Great question!!,2010-08-24T02:54:22.507,253,CC BY-SA 2.5,
2229,1762,0,I think I'm going to simply write a simulator and tabulate results.,2010-08-24T06:20:45.883,937,CC BY-SA 2.5,
2230,2066,1,Do you want to know the mathematics or simply a code solution in R or some such?,2010-08-24T10:40:50.777,601,CC BY-SA 2.5,
2231,2066,0,"I need to implement this in C#, so the mathematics would be good. A code sample would be fine too, if it doesn't rely on some builtin R/Matlab/Mathematica function I can't translate to C#.",2010-08-24T10:55:38.190,956,CC BY-SA 2.5,
2232,1873,4,He's clumsy enough to change his name in the middle of a question at least! ;),2010-08-24T11:10:19.190,229,CC BY-SA 2.5,
2233,2056,1,"Interesting, I'll have to look that reference up. Are you sure you mean ""fourth"" approach? The first approach I list seems to describe ""simply resampling the subjects"".",2010-08-24T11:17:48.503,364,CC BY-SA 2.5,
2234,2067,1,"Can you clarify the predicted variable, ""mean endorsement""?. Is this a 0-100 scale that participants used for response, or is this a measure of the proportion of trials on which participants said ""yes, I endorse"" (vs. ""no, I do not endorse""). If the latter, then it is inappropriate to analyse this data as proportions. Instead, you should be analyzing the raw, trial-by-trial data using a mixed effects model with a binomial link function.",2010-08-24T11:36:06.837,364,CC BY-SA 2.5,
2235,2067,0,"Sorry, for omitting this: it is a 0-100 response scale.",2010-08-24T11:39:40.877,442,CC BY-SA 2.5,
2236,1898,0,"what is FWE and FDR? I suppose FWE to be family wise error, but the other??",2010-08-24T11:56:34.820,442,CC BY-SA 2.5,
2237,2069,0,"I am not quite sure I understand your recommendation in 1. As actual SE [i.e., SD/sqrt(n)] and estimated SE are both model based, you recommend using the model-based. So which one?
Or do you mean: go with the more complicated model (here: ANOVA) cause both models are reasonable.",2010-08-24T12:07:14.580,442,CC BY-SA 2.5,
2238,2069,0,agree with point 1 completely,2010-08-24T12:09:29.227,601,CC BY-SA 2.5,
2239,2067,0,"Do you have many 0's or 100's? If not, I'd consider dividing by 100 and performing a logit transform to take into account the restriction of range at the extremes. This is essentially what is achieved by the binomial link function when you have binary data, but is useful if you only have proportion-like data as you seem to have here. However, you can't logit transform 1 or 0, so you'd have to toss any responses of 100 or 0.",2010-08-24T12:16:05.193,364,CC BY-SA 2.5,
2240,2068,1,"I have good (non-statistical) reasons to plot the graph as it is: You directly see the answer to the research question. Furthermore, I am not looking for a error bars for inferential purposes as I know about the within-between problems. But, thanks to pinpointing me back to Mason & Loftus, I must have forgotten that they had a mixed example. I have to think on whether or not it serves my purpose.",2010-08-24T12:16:29.943,442,CC BY-SA 2.5,
2241,2067,0,"Oops, just realized that my first comment was not 100% correct. Each plotted mean represents the mean of two responses on a 0-100 scale. In this data there are a lot values very near to 100, and some directly on 100, but actually very little at 0 and around 0. You have some literature for justifing your recommendation?",2010-08-24T12:28:31.263,442,CC BY-SA 2.5,
2242,2066,0,"PDF, CDF or inverse CDF?",2010-08-24T12:29:48.187,830,CC BY-SA 2.5,
2243,604,0,"@Srikant Because the RSS would vanish in this case, attached is a toy example: http://gist.github.com/547494 (I did not investigate the DA results though).",2010-08-24T12:59:06.567,930,CC BY-SA 2.5,
2244,2013,0,"I'm not a Mac user, but I think the Linux build could work for you (you need to manually install all python dependencies): http://www.red-r.org/forum/topic.php?id=22#post-76",2010-08-24T13:03:57.150,170,CC BY-SA 2.5,
2245,2067,0,"As a side note, because the conditions are not varying along continuous dimensions some data visualization people might claim this data is better represented by bar graphs.",2010-08-24T13:25:06.667,196,CC BY-SA 2.5,
2246,2070,0,"Mike, in the package languageR the pvals.fnc function does an MCMC to evaluate the hypotheses of the lmer model - however it does not handle designs with random slopes - that lead me to suspect that there was some reason doing MCMC with random slopes was in someway problematic, do you know definitively that there is no such problem?",2010-08-24T13:32:04.997,196,CC BY-SA 2.5,
2247,1898,0,"Well, this thread is subjective, so who knows...  Now seriously - FDR stands for false discovery rate (wikipedia it)",2010-08-24T13:45:03.633,253,CC BY-SA 2.5,
2248,2070,0,"I have to admit I still haven't figured out how MCMC works, which is one of the reasons I opted for bootstrapping instead. While bootstrapping should be possible with random slopes, as you intimated, it may be that  pvals.fnc doesn't let you do CIs for models with random slopes because this is for some reason invalid, and further it may be that this invalidity extends to bootstrapping such models. I don't intuitively think there would be any problem with bootstrapping, but that may be a function of my limited expertise.",2010-08-24T14:28:02.140,364,CC BY-SA 2.5,
2249,2073,0,"When alpha and beta are not too far apart (i.e., alpha/beta are bounded above and below), the SD of Beta[alpha, beta] is proportional to 1/Sqrt(alpha).  E.g., for alpha = beta = 10^6, the SD is very close to 1/Sqrt(8) / 1000.  I think there will be no problem with the representation of l and r even if you're only using single precision floats.",2010-08-24T15:25:46.810,919,CC BY-SA 2.5,
2250,2071,0,"Stupid question: How did you do that graphical experiment? I tried to plot the distribution for alpha/beta around 100, but I couldn't see anything due to underflow errors.",2010-08-24T15:57:41.523,956,CC BY-SA 2.5,
2251,2073,0,which is to say that $10^6$ is not 'sufficiently large' ;),2010-08-24T16:25:59.780,795,CC BY-SA 2.5,
2252,931,0,"No, although there are plug-ins (for Firefox, at least) that help with downloading Flash.  Not easy in all cases, but possible in most.",2010-08-24T16:57:58.680,71,CC BY-SA 2.5,
2253,2073,1,"Yeah, it's a crazy number for a beta application.
BTW, those inequalities won't produce good intervals at all, because they are extremes over all distributions (satisfying certain constraints).",2010-08-24T17:19:34.647,919,CC BY-SA 2.5,
2254,2071,0,"You don't want to plot the integrand: you want to plot the integral.  However, you can get the integrand in many ways.  One is to enter ""plot D (beta(x, 1000000, 2000000), x) / beta(1, 1000000, 2000000) from 0.3325 to 0.334"" at the Wolfram Alpha site.  The integral itself is seen with ""Plot beta(x, 1000000, 2000000) / beta(1, 1000000, 2000000) from 0.3325 to 0.334"".",2010-08-24T17:27:44.750,919,CC BY-SA 2.5,
2255,2074,0,"Perfect! I had the NR book on my desk all the time, but never thought to look there. Thanks a lot.",2010-08-24T17:34:37.830,956,CC BY-SA 2.5,
2256,2067,1,Other data visualization people might claim that bar graphs are a crime against humanity :Op,2010-08-24T17:52:21.827,364,CC BY-SA 2.5,
2257,2065,0,"I figured out the answer two days ago, and posted it here as a new answer. I also then expanded and updated the web page at graphpad.com that you found. I just edited that page again to include a link to an Excel file with the problem data (http://www.graphpad.com/faq/file/1226.xls). I couldn't do that until I got permission from the guy who generated the data (he wants to be anonymous, and the data is vaguely labeled).",2010-08-24T18:28:08.800,25,CC BY-SA 2.5,
2258,1944,0,"btw, this is special case of ""whitening"" which takes data and makes it's empirical covariance matrix identity",2010-08-24T19:30:40.407,511,CC BY-SA 2.5,
2259,1931,0,Any measure that coincides with the usual median when restricted to R1 is a candidate generalization. There must be a lot of them.,2010-08-24T19:37:24.870,1011,CC BY-SA 2.5,
2260,2073,0,"@whuber: You're right, they are crazy numbers. With my naive algorithm, the ""sane"" numbers were easy and worked well, but I couldn't imagine how to calculate it for ""crazy"" parameters. Hence the question.",2010-08-24T19:52:59.853,956,CC BY-SA 2.5,
2261,1963,0,"Thank you @propofol and @Jeromy Anglim for your excellent contributions: after skimming DeCoster and looking over the Amazon reviews of the books, I have passed them all along to my colleague.  It is a real shame one cannot checkmark multiple answers; in this case I literally flipped a coin: tails, it went to propofol.",2010-08-24T22:29:17.460,919,CC BY-SA 2.5,
2262,2081,0,"Yes, it looks like it needs to be solved numerically in general. I really just needed a practical way of plotting epsilon as a function of n, and ended up using a version of Chernoff inequality for fair coin from which expression for epsilon can be obtained in closed form",2010-08-24T23:00:01.933,511,CC BY-SA 2.5,
2263,2069,0,"Hi Henrik,
Simple example - compare two groups (x1, x2) assumed ND.
Assumptions & models:

1) Independently sampled, different variance. SEs for x1, x2 estimated separately. This is implicitly the assumption in many graphical presentations. The estimated SEs differ.
 
2) Indep., same var. Usual ANOVA assumption. Estimate SEs using pooled RSS. Estimate is more robust IF assumptions correct.

3) Each x1 has an x2 pair. SEs estimated from x1-x2. To effectively plot them you need to plot the difference x1-x2.

Once you mix 1) and 2) you have a real problem plotting meaningful SEs or CIs.",2010-08-25T00:42:31.133,521,CC BY-SA 2.5,
2265,2069,0,"Henrik, a comment on the plot. How many subjects do you have? I would strongly recommend plotting the data individually and use line segments to link individuals. (Line segments linking means is deceptive.) There is no need to plot SEs. The idea is to visually support your statistical analysis. Provided the plot does not become too cluttered, a reader should see (for example) that the clear majority of scores go up from MP-valid-implaus to AC-inval-plaus for the Inductive group & down for the Deductive group. See:
http://www.jstor.org/stable/2685323?seq=1
Especially Figs 1 & 9 bottom panels.",2010-08-25T02:01:21.397,521,CC BY-SA 2.5,
2266,2087,1,"Thanks Tal, I'll try corrplot now. I also wish I knew how to simplify your solution (which I linked to in the question) but I'm just a newbie in R so you know more than me. I'll update the question to clarify the solution looks complicated *to me*",2010-08-25T04:04:52.400,840,CC BY-SA 2.5,
2267,2086,2,You sound like R is inferior to propriety software. :),2010-08-25T05:49:51.640,144,CC BY-SA 2.5,
2268,2076,0,Consider this posting as a stand-alone question. I would be very interested what gurus have to say about this.,2010-08-25T06:02:31.277,144,CC BY-SA 2.5,
2269,2093,0,Do you want to generate normal random variables truncated to range between 0 and 1 or uniform random variables between 0 and 1? The Marsagla polar method will return numbers over the entire real line as it generates standard normal random variables.,2010-08-25T09:46:57.313,,CC BY-SA 2.5,user28
2271,2086,0,"For me it sounds totally reasonable to use the pearson product-moment-correlation (assuming continuous data) in your case (assuming enough points on your scale and not a don't know midpoint). Whole fields within psychology (e.g., personality or social psychology) rest (successfully) on the assumption that answers to a single item on an e.g., five-point (or seven-point) scale ranging from very un-X to very X can be treated as continuous. See also this thread: http://stats.stackexchange.com/questions/539/does-it-ever-make-sense-to-treat-categorical-data-as-continuous",2010-08-25T09:59:53.510,442,CC BY-SA 2.5,
2272,2093,0,"I don't know if this solves your problem, but you might wanna take a look at this question where some transformations that squash everything between 0 and 1 are discussed: http://stats.stackexchange.com/questions/1112/how-to-represent-an-unbounded-variable-as-number-between-0-and-1/1113#1113",2010-08-25T10:40:47.987,442,CC BY-SA 2.5,
2273,2086,0,@romunov: Not sure how you got the impression that I believe R is inferior to other s/w. But it's not the case at all.,2010-08-25T12:45:32.550,840,CC BY-SA 2.5,
2274,2071,0,"I plotted the integrand, i.e. the pdf of the beta distribution, in Stata - it has a builtin function for the pdf. For large alpha and beta you need to restrict the range of the plot to see it's close to normal.


If I was programming it myself i'd compute its logarithm then exponentiate at the end. That should help with the underflow problems. The beta function in the denominator is defined in terms of gamma functions, equivalent to factorials for integer alpha and beta, and many packages/libraries include lngamma() or lnfactorial() instead/as well as gamma() and factorial() functions.",2010-08-25T12:51:07.750,449,CC BY-SA 2.5,
2275,2090,1,"Although most of them don't seem relevant for the OP questions, still, this is a nice link - thanks Jeromy!",2010-08-25T13:20:17.817,253,CC BY-SA 2.5,
2276,2057,0,"That was a great insight! All your assumptions can be considered, and your rationale is sound! I do have some doubts though, if you don't mind: 

(I) Shouldn't d2[j,i] = h[j,i+2] ...?

(II) Why do you consider the increment as y[j,i+2] - 2*y[j,i+1] + y[j,i] and not just y[j,i+1]-y[j,i]? Is it a kind of derivative (i.e., to calculate the ""slope"")?",2010-08-25T13:25:00.443,990,CC BY-SA 2.5,
2277,2088,0,"The Sunflower is a fun solution.  Using a jitter is what I tried when first I looked at the topic, but I found it do be not effective enough for the plotting of correlation matrixs...",2010-08-25T13:26:17.820,253,CC BY-SA 2.5,
2278,2057,0,"(1) Yes, good catch.  I fixed that typo.

(2) We're really looking at *second* derivatives: the increments d[j,i] = h[j,i+1] - h[j,i] act like slopes (and are slopes when times are equally spaced).  I am proposing to distinguish ""exponential"" from ""sublinear"" behavior in terms of changes in the slopes.  Whence we want to consider the signs of

d2[j,i] := d[j,i+1] - d[j,i] = (h[j,i+2] - h[j,i+1]) - (h[j,i+1] - h[j,i]) etc.",2010-08-25T13:32:12.270,919,CC BY-SA 2.5,
2279,2087,0,"The corrplot looks good. It gives a great visual snapshot of size and direction of correlations. In the case of 5-point ordered categorical variables, it might be useful to supply some other measure of association besides Pearson's correlation: e.g., polychoric correlations. The size of standard Pearson's correlations of ordered categorical variables is influenced somewhat by the mean of the two variables.",2010-08-25T13:38:35.783,183,CC BY-SA 2.5,
2280,2088,0,"Yeah, jitter could get pretty messy with a scattermatrix with lots of variables. I suppose the benefit of jitter and sunflower is that you get to see the raw data (albeit perturbed in the jitter case).",2010-08-25T13:40:47.777,183,CC BY-SA 2.5,
2282,2085,4,"R and S-Plus are very similar, but definitely not the same (I know from painful experience!).",2010-08-25T13:44:10.357,5,CC BY-SA 2.5,
2284,40,1,"Retagged ""random-variate"" to ""random-variable"" for consistency with similar questions.",2010-08-25T14:14:06.740,919,CC BY-SA 2.5,
2285,111,0,"Are you sure about your second point, generating normal random variables by inverting the CDF?  The inverse of the normal CDF is a fairly expensive function to evaluate.  I imagine Box-Muller's method would be faster.  Faster still would be Marsaglia's ziggurat method for generating normals.",2010-08-25T14:53:59.127,319,CC BY-SA 2.5,
2286,2073,2,"OK, you're right: once alpha+beta exceeds 10^30 or so, you will have difficulties with doubles :-).  (But if you represent l and r as differences from the mean of alpha/(alpha+beta), you'll be fine until alpha or beta exceed about 10^303.)",2010-08-25T15:34:21.457,919,CC BY-SA 2.5,
2287,2106,0,"Thank you for taking the time to answer my question. however, your response doesn't answer my question. I already mentioned in my post that I already came across the ezANOVA function. I tried it without success. I am also already familiar with the aov notation you metnion.",2010-08-25T15:55:28.673,1084,CC BY-SA 2.5,
2289,2106,1,"Do you get any error messages when using ezANOVA? Whenever I use it, I get a nice table of sphericity tests and GG/HF corrections.
It does not work for ANCOVA models, for this you will need to use the lme4 package, and there are other things to account for variance.",2010-08-25T16:05:43.847,966,CC BY-SA 2.5,
2290,2106,0,"It would probably help if you put the output of ""str(p12bl)"" or ""str(subset(p12bl, exp2==1))"" here.",2010-08-25T16:16:23.930,966,CC BY-SA 2.5,
2291,111,0,"I also find this suspicious. Marsaglia's Ziggurat is the default in Matlab, and I can't imagine Matlab being better than R in the field of random number generation.",2010-08-25T16:21:20.663,795,CC BY-SA 2.5,
2292,2088,0,"Agreed (I love jitter, simply not for this :) )",2010-08-25T16:32:54.063,253,CC BY-SA 2.5,
2293,2103,0,+1 A better answer than mine as it explicitly takes into consideration the range over which the pdf is non-zero.,2010-08-25T16:36:18.603,,CC BY-SA 2.5,user28
2295,2104,1,Do you want to look at sphericity or a factor to correct F-values because of violations in sphericity?,2010-08-25T16:50:57.357,601,CC BY-SA 2.5,
2296,2108,0,"This may be a question more appropriately asked on the stackoverflow forum, also you may be able to get a more concise answer if you describe what type of file the url is pointing to.",2010-08-25T17:38:06.030,1036,CC BY-SA 2.5,
2297,2103,0,"@Srikant: Thanks; you are generous.  Did you notice the connection with the theory of copulas?  Maybe we should even add a ""copula"" tag.",2010-08-25T17:39:22.930,919,CC BY-SA 2.5,
2298,2094,9,Ok this makes it clear. Exponential pdf can be used to model waiting times between any two successive poisson hits while poisson models the probability of number of hits. Poisson is discrete while exponential is continuous distribution. It would be interesting to see a real life example where the two come into play at the same time.,2010-08-25T18:03:50.560,862,CC BY-SA 2.5,
2299,2093,1,"@Ants: ""Scaling down"" from the real line to an interval will necessarily make the resulting generator non-Normal.",2010-08-25T18:14:27.713,919,CC BY-SA 2.5,
2300,2112,1,I was just about to recommend this.  The ez package relies on car for some analysis.,2010-08-25T18:47:42.030,601,CC BY-SA 2.5,
2303,2104,0,@John I need to see if I need the correction,2010-08-25T18:55:22.780,1084,CC BY-SA 2.5,
2304,2112,0,"Thank you, the R newsletter is where I picked the mlm direction from. alas, I am still figuring how to choose idata idesign for a nested within subject design. I will try to figure out Anova().",2010-08-25T19:14:54.643,1084,CC BY-SA 2.5,
2305,2111,1,"Are you refering to the following paper?
Kuk, A.Y.C. (1984) All subsets regression in a proportional hazards model. Biometrika, 71, 587-592",2010-08-25T19:22:05.237,930,CC BY-SA 2.5,
2306,2013,0,@Amro I'll give it a try; in the past I've been testing RAnalyticFlow (http://j.mp/bYF8xs) but did not get convinced: I am basically a CLI user :-),2010-08-25T19:52:42.210,930,CC BY-SA 2.5,
2307,2114,2,"BTW, the penalized R package (http://j.mp/bdQ0Rp) includes l1/l2 penalized estimation for Generalized Linear and Cox models.",2010-08-25T20:07:49.780,930,CC BY-SA 2.5,
2308,2105,0,I definitely want a normally distributed variable. I'll update the question with the specific problem I'm trying to solve.,2010-08-25T20:08:50.327,937,CC BY-SA 2.5,
2309,2098,0,"For rejection sampling, what will I use as my g(x) and M value to simulate the normal distribution? For inverse transform sampling, do I use D Ibbetson's Algorithm 209 (http://old.sigchi.org/~perlman/stat/doc/z.c) to compute the cdf? What do I use as the inverse?",2010-08-25T20:21:40.643,937,CC BY-SA 2.5,
2310,2117,0,"Not bad at all. The format is static, but the information is very useful.",2010-08-25T20:32:56.373,840,CC BY-SA 2.5,
2311,2102,0,"I want to get a ""normal"" distribution with the appropriate hump at 0.5.",2010-08-25T21:03:41.687,937,CC BY-SA 2.5,
2312,219,0,Maybe also L'Ecuyer's RNG with multiple streams (http://j.mp/bzJSlm)?,2010-08-25T21:08:09.673,930,CC BY-SA 2.5,
2313,2098,0,"@Ants If I am right, you don't need a cannon like ITS.",2010-08-25T21:25:22.547,,CC BY-SA 2.5,user88
2314,111,0,"@John Indeed, the polar method is available in R, see the setRNG package.",2010-08-25T21:28:18.160,930,CC BY-SA 2.5,
2315,2098,0,"For rejection sampling set f(x) ~ standard normal pdf I(0<x<1), g(x) ~ standard normal and and M=1. Your condition will then become select x if u < I(0<x<1) which is basically what I stated in my one line sentence above under rejection sampling.",2010-08-25T21:34:16.550,,CC BY-SA 2.5,user28
2316,45,0,"e.g., Python (http://j.mp/9TVyhv), Perl (http://j.mp/by85El), Octave (http://j.mp/aVQ5Xz), Clojure (http://j.mp/dkj9Z9, not the default one), Haskell (http://j.mp/aWK7kK), lua (http://j.mp/bSD2vO), or even SQL (http://j.mp/aOPJMW, for an overview) :-)",2010-08-25T21:41:15.657,930,CC BY-SA 2.5,
2317,2103,0,"@whuber My familiarity with copulas does not go very far. In any case, I am not sure how many will see the connection. So, a copula tag may just confuse some.",2010-08-25T21:56:16.427,,CC BY-SA 2.5,user28
2318,2111,0,"yes indeed. I guess I'll have to dig up that paper somehow. It seems old, however.",2010-08-25T22:56:20.283,795,CC BY-SA 2.5,
2319,2114,0,"stuck in matlab land, implementing it myself...",2010-08-25T23:23:39.040,795,CC BY-SA 2.5,
2321,2098,0,@mbq: I don't get the ITS reference. I've been missing out on a lot of pop culture and current events references the past few months.,2010-08-26T00:37:51.657,937,CC BY-SA 2.5,
2323,2093,0,@Henrik: Thanks. I'll have to remember some of those tricks for future reference.,2010-08-26T00:41:23.443,937,CC BY-SA 2.5,
2324,2097,0,"Thanks for the pointer. LOL! My new implementation for NormalDistribution looks a lot like I'd taken a slice of that code project and focused on just normal random numbers. I've got the same abstract classes for RNG's and Distribution's. I've got just one concrete RNG that uses the .NET Random class, and a concrete NormalDistribution class.",2010-08-26T00:48:57.593,937,CC BY-SA 2.5,
2325,2103,0,@Srikant: Granted.  But isn't the point of tags to facilitate searches and create semantic ties among threads?  I'm sure plenty of existing tags also have the potential to confuse the uninitiated already.  Maybe this is a discussion for the meta site?,2010-08-26T01:05:29.610,919,CC BY-SA 2.5,
2326,2102,1,"Then you won't be able to restrict it to the interval [0, 1).  As a practical matter, normal probabilities further out than about seven standard deviations are too small to be worth computing, anyway, so you could set the standard deviation to 1/2 / 7 = 1/14 = about 0.07.  This isn't too far from the value you found empirically.  With your value and enough draws, you are likely to get a number outside [0, 1); with 1/14, it's extremely unlikely.  Thus, my recommendation is to take a value from the Marsaglia algorithm, divide by 14, and add 1/2.  Adjust the 1/14 to suit your application.",2010-08-26T01:08:43.993,919,CC BY-SA 2.5,
2327,1991,0,"But what does this have to do with statistical ""power"" in the King quotation?  (I would ask the same question of stormygeorge, too.)",2010-08-26T01:15:10.923,919,CC BY-SA 2.5,
2331,2098,0,@Ants ITS=Inverse Transform Sampling,2010-08-26T06:53:50.287,,CC BY-SA 2.5,user88
2332,2097,0,"But as I can see they are not overloading Microsoft's original, so they can break the [0,1) limit.",2010-08-26T06:55:11.193,,CC BY-SA 2.5,user88
2333,2131,0,This asymmetry does not look *so* bad; are you sure you don't have an error somewhere else? Also can you give some more details about the data? There is not a general solution to this.,2010-08-26T07:05:53.020,,CC BY-SA 2.5,user88
2334,2089,0,"Another great answer! PyIMSL Studio sounds interesting, too bad it isn't open source. There's some overlap with NumPy/SciPy, though. In any case, I think these were good tips for anyone wanting to assemble their own Python statistics workbench!",2010-08-26T07:29:27.057,890,CC BY-SA 2.5,
2335,2097,0,"Yes, that is correct.",2010-08-26T08:30:42.673,937,CC BY-SA 2.5,
2337,2110,0,"Thanks Shane, it's actually an XML web service. I had a quick look at the XML package documentation, am I right in thinking that it just does the parsing? I'm still unsure of how to actually request the data, from the web service. Thanks",2010-08-26T10:07:01.410,428,CC BY-SA 2.5,
2338,2136,1,"Actually, ""conditional logit"" is a very ambiguous term. It some contexts (mainly when dealing with panel data), it is equivalent to Chamberlain's estimator, but it is very unfrequent. Most of the times, it refers to a cross-sectional model where the outcome variable can take more than 2 values. All your proposals actually refers to packages that consider this last possibility. The same with mixed-logit: it is not a fixed-effect logit. I've already taken a look at Farnsworth's overview, but it is not exhaustive enough to speak about this estimator. Anyway, thank you for your answer !",2010-08-26T10:11:31.553,1091,CC BY-SA 2.5,
2339,2110,0,"@Matt As Shane wrote; connect with RCurl, get the output and parse it with XML. We would be able to give you the code if you'd give more details about this web service.",2010-08-26T10:21:34.030,,CC BY-SA 2.5,user88
2340,2135,2,+1; still ksvm from kernlab seems to be the preferred R SVM implementation. For instance it scales variables on its own and has a nice (=working very well) heuristic procedure for picking good $\gamma$ for RBF.,2010-08-26T10:49:51.093,,CC BY-SA 2.5,user88
2342,2138,0,"Excellent Henrik. I also prefer the ""points with means dislocated"". Linking subjects with line segments may look too cluttered. Pity. As to homogeneity of variance I am a little more sanguine. The variance problem may not be as bad as it looks in the raw data. For the most part I suspect you will be comparing contrasts - within group differences. Contrast variances will be more homogeneous that the variances of the raw data. If raw measures with different variances are  compared (eg. Inductive vs Deductive in the MP-valiad & plausible group) a non-parametric test could be used as a back-up.",2010-08-26T12:58:52.157,521,CC BY-SA 2.5,
2343,2093,1,"Your clarification completely confused me. You plan to generate a random normal variable `x1` truncated to the 0-1 range, then plug it into the `x1 * w * stdDev + mean` formula (what is `w`?) to get a normal with mean `mean` and standard deviation `stdDev`? If so, that's totally wrong. Just take a standard normal `z`, and calculate `mean + z*stdDev` to get a non-standard normal variable.",2010-08-26T13:01:13.707,279,CC BY-SA 2.5,
2344,2136,0,"""Conditional logit"" does *not* refer to having more than two outcome levels. Some functions might extend it to that situation, but that is not the point.",2010-08-26T13:09:34.070,279,CC BY-SA 2.5,
2345,2112,0,"@ John Anova() is supposed to take my aov() object as a model, but for some reason I am having problems making it work.",2010-08-26T13:21:49.427,1084,CC BY-SA 2.5,
2347,733,1,"This remind me of a poster I saw at the Human Brain Mapping 2010 conference. Tom Nichols used parametric bootstrap to assess the stability of the cophenetic correlation and silhouette in hierarchical clustering, but see his poster: http://j.mp/9yXObA.",2010-08-26T13:27:22.553,930,CC BY-SA 2.5,
2348,2112,1,"@Adam In the on-line help for Anova(), you will find a working example with within- and between-subjects factors; the main caveats of Anova() is that you need to pass idata/idesign parameters in addition to your lm object. Also see this thread on the r-help mailing list, http://j.mp/9lphud.",2010-08-26T13:39:37.537,930,CC BY-SA 2.5,
2349,2103,0,"@whuber True. But, right now the connection to copulas is a bit obscure for those who do not know what they are. Perhaps, you can add a line or two to make the connection in which case the tag 'copula' may make sense.",2010-08-26T13:54:52.310,,CC BY-SA 2.5,user28
2350,2141,0,"In R that's var.test(q1, q2)",2010-08-26T14:08:55.743,601,CC BY-SA 2.5,
2352,1991,1,"@whuber Please see in the question where Gary King explicitly says that he is not talking about statistical power, but about ""the power that can be extracted from knowledge of the substance"".",2010-08-26T14:23:36.120,666,CC BY-SA 2.5,
2353,2100,0,"Thanks, Jeromy. Actually this is even better than what I asked.",2010-08-26T14:25:10.797,666,CC BY-SA 2.5,
2354,2093,0,"@Aniko: I was just using 'w' because that's what code that I grabbed had. I should have renamed it 'r2' to represent r-squared during the looping, and then added a new variable 'transform' for the 'w' in the second to the last line in the original code. When I gave up trying to get a return value from 0-1, I resorted to <code>x1 * w * stdDev + mean<code> but the code is not used in the context of overriding Random.Sample() anymore. How do I get <code>z<code> given the Marsaglia polar method?",2010-08-26T14:39:12.727,937,CC BY-SA 2.5,
2355,2145,0,"Does that mean we can always get 
$$\frac{d^kf(a)}{dx^k} = \gamma_k$$?",2010-08-26T14:57:54.883,1043,CC BY-SA 2.5,
2357,2093,0,"Sorry, I did not scroll up to the code. Now I see what `x1` and `w` are. Mea culpa. Then using `x1 * w * stdDev + mean` is correct (`z` is `x1*w`). I still don't get the part about having to have the result to be between 0 and 1, but I see that you are happy with mbq's answer, so I am happy to live with my confusion.",2010-08-26T15:40:23.063,279,CC BY-SA 2.5,
2358,2145,0,Almost: replace *a* by 0 and divide by *k!*.,2010-08-26T15:49:48.783,919,CC BY-SA 2.5,
2359,733,0,@chl Thanks; indeed recently I have seen similar thing done in my lab; the result was that the clusters are not significant though :-/,2010-08-26T15:50:29.943,,CC BY-SA 2.5,user88
2360,1991,0,"Ah, thanks: I was focusing on the first quotation only.  But that leaves us wondering specifically what King means by ""power.""  Evidently it's something that is gained by not ""shirking responsibility"" ;-).",2010-08-26T15:52:58.737,919,CC BY-SA 2.5,
2361,2131,0,"my dataset have 17 predictors(3 continuous and 14 categorical), obviously 1 class variable, and total 1000 obsrvations. The frequency dist. for class var. with train and test is train(bad) = 197, test(bad)= 103, trian(good)= 446, test(good)= 254",2010-08-26T15:57:34.693,861,CC BY-SA 2.5,
2363,2136,1,"Yup, but the conditional logit model can (as I said) take more than 2 values, which differentiates it easily from Chamberlain's model, just like the fact that Chamberlain is designed specifically for panel data. This is thus a relevant information ; precise description of the usual conditional logit isn't (and the description of both would take more than 600 chars).",2010-08-26T16:07:32.973,1091,CC BY-SA 2.5,
2364,2127,0,"1) yes, the question is somewhat ambiguous; there are, as you mention, many definitions of 'optimal': via information criterion, cross validation, etc. Most of the heuristic approaches I have seen to the problem proceed by stepwise predictor addition/removal: single pass forward addition or subtraction, etc. However, Hosmer & Lemeshow make reference to this method (a variant of work by Lawless & Singhal), which somehow 'magically' selects predictors by a single computation of an MLR (modulo some other stuff). I am very curious about this method...",2010-08-26T16:55:58.027,795,CC BY-SA 2.5,
2365,2114,0,"LARS is great, BTW. very cool stuff. not sure how I can jam it into the framework of Cox Proportional Hazards model, tho...",2010-08-26T16:56:36.527,795,CC BY-SA 2.5,
2367,2089,0,"It is free as in beer (for non-commercial use), but alas, not free as in speech.",2010-08-26T18:08:07.593,1080,CC BY-SA 2.5,
2370,2151,0,Is dimension reduction an option?,2010-08-27T01:59:57.453,5,CC BY-SA 2.5,
2371,2151,3,"we have 4 classes, not really",2010-08-27T03:03:06.800,,CC BY-SA 2.5,CLOCK
2372,2152,2,I would say the same providing another link http://www.google.gr/url?sa=t&source=web&cd=1&ved=0CB8QFjAA&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.98.4088%26rep%3Drep1%26type%3Dpdf&ei=nlx3TIq-LJCk4Abn_Li3Bg&usg=AFQjCNHO-_yjWAJrRVnJms7MbcqaJkd8eg&sig2=sqERq2v68UvOhJDMviOklg,2010-08-27T06:40:29.287,339,CC BY-SA 2.5,
2373,2152,1,"And here is another one, directly related to multi-class problem: Multi-class ROC analysis from a multi-objective optimisation perspective, Pattern Recognition Letters 2006 27(8): 918-927 (http://j.mp/9AMgzq).",2010-08-27T07:38:55.643,930,CC BY-SA 2.5,
2374,2112,0,"@Adam Did you mean the example included in the help file for Anova? In this case, I grabbed the R transcript and put it there, http://j.mp/9pGtKY. HTH",2010-08-27T08:30:41.030,930,CC BY-SA 2.5,
2375,2154,0,"Better than naming it Part 2 (or n), make a link to the previous question inside this.",2010-08-27T09:10:00.560,,CC BY-SA 2.5,user88
2376,2114,2,"The Glmnet software has a lasso'd Cox PH model:
http://cran.r-project.org/web/packages/glmnet/index.html
there is also a MATLAB version (not sure if it does a cox model though):
http://www-stat.stanford.edu/~tibs/glmnet-matlab/",2010-08-27T09:29:41.577,495,CC BY-SA 2.5,
2377,1817,1,"I had a look at this, and it's perfectly fine but has a bit too much of the feel of an undergraduate text book for my tastes. Not that there's anything wrong with that...",2010-08-27T09:44:20.223,174,CC BY-SA 2.5,
2378,2086,0,I was just being a smart ass. I hope there's no hard feelings. :),2010-08-27T10:41:42.997,144,CC BY-SA 2.5,
2379,2126,1,"Note that in actual computations, one should not be forming $A^T A$ directly; exploit the QR or singular value decomposition of $A$ for this.",2010-08-27T10:45:22.963,830,CC BY-SA 2.5,
2380,1817,1,"Very nice book indeed! I've started long ago to reproduce his analysis (with Design Expert and SAS) using R, but never find time to finish it. If you like to check it out, http://www.aliquote.org/articles/tech/dae/",2010-08-27T11:17:11.820,930,CC BY-SA 2.5,
2381,2115,0,"Thanks. It worked - this way I can plot both the Class random effects (level=1), and the student (level=2).",2010-08-27T11:30:46.447,108,CC BY-SA 2.5,
2382,1544,0,Thanks for your response John.  I'm not sure how I would go about using mixed-effects modelling to test from equivalence - do you have any examples?  Thanks!  Sam.,2010-08-27T12:27:35.097,867,CC BY-SA 2.5,
2383,2152,1,"Thanks for the other links, apparently that whole issue is popular, and its archive on science direct can be found here
http://www.sciencedirect.com/science?_ob=PublicationURL&_tockey=%23TOC%235665%232006%23999729991%23621242%23FLA%23&_cdi=5665&_pubType=J&view=c&_auth=y&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=ece739626e7c50b228e85480cb2407e3",2010-08-27T13:17:58.810,1036,CC BY-SA 2.5,
2384,2146,0,I think it highly unlikely that you will get answered without clarifying this question.,2010-08-27T13:48:09.280,5,CC BY-SA 2.5,
2385,1233,0,"I agree that this is the simplest method and will probably do the job. I think I would just suggest a minimum level for the c-line. If one or two cases are raising too many false positive alerts, just change the threshold to something higher.",2010-08-27T13:54:44.227,1036,CC BY-SA 2.5,
2386,2157,3,"P-values are not appropriate measures for control charts, because of the serial multiple comparisons involved and the possibility (likelihood, in this case) of serial correlations.  Control chart properties are better assessed in terms of expected lengths of out-of-control (OOC) runs and in-control runs under the null and alternative hypotheses, respectively.  On this basis, letting even one case trigger an OOC is almost never a good procedure.",2010-08-27T14:12:07.123,919,CC BY-SA 2.5,
2387,1959,0,"Could you tone down the criticism in this a little?  Your point is fair, but I would love to see Gary get involved in the site (if at all possible)...",2010-08-27T14:13:51.740,5,CC BY-SA 2.5,
2388,2112,0,@chl thank you! for some reason I couldn't find an on-line help for car Anova(). but this example is already in the R off-line help at ?Anova. I am starting to understand this method.,2010-08-27T14:19:11.353,1084,CC BY-SA 2.5,
2389,2112,0,"@Adam Ah ok, my answer was thus a little bit out... What I call on-line is indeed the help() or ? call under R. For external references, you can try http://www.rseek.org/ or http://search.r-project.org/ (much better than directly goggling about R). Also I just this post with a nice illustration: http://j.mp/dnJgW0.",2010-08-27T14:42:32.210,930,CC BY-SA 2.5,
2390,1934,0,Ditto with your second paragraph. If you know 2 infections is too low to be bothered with you can probably reasonably form a minimum estimate in which you want an alert raised.,2010-08-27T14:49:44.803,1036,CC BY-SA 2.5,
2393,2157,0,"I was talking about the p-value of one case, so I think it is correct. Of course I agree with a general claim that this can (and should) be done better, still from the voice of the article lockhart cited I can conclude he seeks for statistics ""light"".",2010-08-27T15:04:17.417,,CC BY-SA 2.5,user88
2394,2154,2,"The answer I supplied to your previous question suggests a version of the ""c-chart"" that is more rigorously justified, quite simple, and accordingly is easy to interpret. (It also raises the logically earlier question of whether control charting is an appropriate technique for you.) You did not respond to this, but now you re-raise the question, so I am wondering why: why do you think this solution will not work for you?",2010-08-27T15:10:33.780,919,CC BY-SA 2.5,
2395,2157,0,"I agree that your p-value calculation is correct, but the issue I am raising concerns whether that's the right way, or even a good way, to think about and design control charts.  For those familiar with adjusting for large numbers of multiple comparisons, a single p-value can be an excellent guide, but for others this way of thinking might be misleading.",2010-08-27T15:14:16.603,919,CC BY-SA 2.5,
2398,1959,0,"Normally I am reluctant to criticize (for example, I have yet to vote down any contribution in these forums), but the criticism seems warranted in light of the tone of that tweet.  Nevertheless, I share your interests and will delete the unnecessary parts of the response.",2010-08-27T15:19:51.207,919,CC BY-SA 2.5,
2399,1959,0,Thanks.  I'll delete my comments here too now so that this all just looks like a coherent train of thought.  :),2010-08-27T15:22:12.710,5,CC BY-SA 2.5,
2400,2112,1,"Here is the correct link, http://j.mp/c6nDvn.",2010-08-27T15:30:48.977,930,CC BY-SA 2.5,
2404,1544,0,equivalence of what?  Your question doesn't mention this.  Perhaps edit the question to be more precise.,2010-08-27T16:52:11.227,601,CC BY-SA 2.5,
2407,2167,2,"BTW, kernels are not really essential for SVM's. The ""heart"" of SVM is the principle of soft margin maximization. Going to kernel representation makes your problem dimensionality O(m^2) instead of O(d) where m is the number of examples and d is the dimension of your feature space, so if m^2 is more than d, you might be better off doing away with kernels
http://jmlr.csail.mit.edu/papers/v6/keerthi05a.html",2010-08-27T17:53:21.080,511,CC BY-SA 2.5,
2411,2169,0,What is the distribution of $x$? I suppose we can assume that the weights are known.,2010-08-27T18:34:51.940,,CC BY-SA 2.5,user28
2413,2169,1,"right, sorry, the weights are independent from $x$. The restrictions on the weights characterize, evidently, the class of 'coherent' risk measures. I have not seen any suggestions about how to set them, though, and this appears to be a black art. However, perhaps one would choose them to get the best discriminatory power (i.e. smallest standard error)...",2010-08-27T18:56:19.233,795,CC BY-SA 2.5,
2417,2168,0,"@Yaroslav, @chl, @Sahne I've started this topic on chat, I think it is more comfortable place to discuss than comments ;-) http://chat.meta.stackoverflow.com/rooms/170/stats",2010-08-27T22:39:08.553,,CC BY-SA 2.5,user88
2419,2146,0,"Cautious +1 to the edit. In truth, I think the question is still not perfectly clear, but I know I'd love to read the answers!",2010-08-27T23:14:00.557,174,CC BY-SA 2.5,
2422,2169,0,"@Glen_b: I am looking for a non-parametric estimator. I just found 2 papers by Stigler which, though very dense, appear to give a form of the asymptotic variance for the parametric case.  I am now hunting down papers by Downton, Stillito, as referred to in Elamir & Seheult's papers on L-moment standard errors.",2010-08-27T23:35:58.400,795,CC BY-SA 2.5,
2423,2173,5,"1. IMHO time spent with R is well invested if you plan to do anything serious.
2. Also consider KNIME http://www.knime.org/ as RapidMiner alternative.",2010-08-28T00:01:20.573,22,CC BY-SA 2.5,
2426,2169,0,"Oh, okay, you mean asymptotically. But you will still need certain assumptions about the distribution for the results to apply, and the results will depend on characteristics of the distribution that are unknown (such as the variance).",2010-08-28T03:24:53.343,805,CC BY-SA 2.5,
2427,2172,1,"Awesome, great links! I already use R and ggplot2, but the visualizations from there seem more of the ""graphics for a report""-variety, than of the ""eye candy/visualization interesting in of itself""-variety I'm looking for. (ggplot2 is super beautiful, but it's not really meant to allow unlimited creativity.) Am I wrong?, or do you sometimes use R/ggplot2 as an input into another visualization tool?",2010-08-28T04:28:46.970,1106,CC BY-SA 2.5,
2428,2159,2,"Again some interesting tools. I knew about Numpy, Matplotlib and ReportLab, but Pyspread seems like an interesting idea. At least I would like to type Python expressions in spreadsheet cells. While it doesn't solve all possible problems, it could be good for prototyping and playing around with data.",2010-08-28T06:21:35.973,890,CC BY-SA 2.5,
2429,2168,0,"When you say ""can only be applied to linear models"" do you mean that you can prove that no application exit to something else than linear models?",2010-08-28T08:14:56.387,223,CC BY-SA 2.5,
2430,2168,0,"What do you mean by ""examples"" ? as you state it is seems to be something that can be interpreted as a dot product.",2010-08-28T08:16:42.597,223,CC BY-SA 2.5,
2432,2174,0,"`igraph` works in R also; for 3D openGL accelerated vis in R, use `rgl` & `misc3d` packages.",2010-08-28T09:41:15.630,,CC BY-SA 2.5,user88
2433,2106,0,@Matt thank you for the very helpful edit. I was trying other things after the ~ except 1 - as in your answer. Thanks again!,2010-08-28T16:41:11.930,1084,CC BY-SA 2.5,
2435,2106,0,"Ah, it seems that there's a bug in the current version of ez (v1.6.1). I've tracked it down to the fact that in converting to the formats that Anova() likes, ez was failing to turn something into a factor. I'm not sure whether this was always a bug, or whether Anova() recently changed in a way that broke ez. Regardless, I'm preparing ez v2.0, and this is one of the bug fixes. Stay tuned...",2010-08-28T17:16:34.920,364,CC BY-SA 2.5,
2436,2183,0,"@Srikvant.  Assume 5% is acceptable significance.  I'm seeking a more precise answer, one which exposes the idea that ""A leads B"" is a new statistic, the difference of pA and pB, and that it's corresponding confidence interval is not simply 2*MOE.",2010-08-28T23:39:37.843,,CC BY-SA 2.5,somebody
2439,2106,0,"Thanks Mike, it seems to have happened fairly recently. I just finished an analysis last month using your package and haven't noticed any problems until this question came up.",2010-08-29T07:12:59.430,966,CC BY-SA 2.5,
2440,1720,1,"Sometimes we are interested in problems that come from from formulations of the total quantity of something. If you know the mean, you can go from the mean to the total (multiplying by the number of the observations). There is no way to go from the median to the total!",2010-08-29T08:22:41.133,339,CC BY-SA 2.5,
2441,2180,2,Has any one of those algos been implemented in R or other software?,2010-08-29T09:26:32.740,339,CC BY-SA 2.5,
2442,2180,6,"Yes, take a look at the R penalizedSVM package. Other packages of interest are : penalized, elasticnet, ppls, lars, or more generally: http://cran.r-project.org/web/views/MachineLearning.html",2010-08-29T09:33:19.063,930,CC BY-SA 2.5,
2443,2187,0,do you recommend to read all of them ? :),2010-08-29T10:41:03.577,223,CC BY-SA 2.5,
2444,2187,3,"In Psychology the Tabachnik & Fidell Book has a pretty good reputation. It is very understandable and applied and not too mathematic. However, examples are only in SPSS or SAS (no R!). But if your problem is covered in there, you will definitely solve it with the book. I recommend it as a good starting point. I don't like the Hair book (same level as Tabachnik & Fidell, but worse). And you gotta LOVE the Gelman. However, it is more complicated.",2010-08-29T12:00:38.657,442,CC BY-SA 2.5,
2445,2187,3,"HAIR et al is good if you don't like math and you want a step by step process. It's popular in management and business schools. If you can handle math, Hair et al can seem verbose.

Tabachnick and Fidell is popular in psychology. It's clearly written and does contain some mathematics. However, if you want a rigorous mathematical treatment, I'd look for an additional book to complement it.",2010-08-29T13:09:37.017,183,CC BY-SA 2.5,
2446,2181,1,"To what extent do you want: (a) mathematical rigour; (b) applications in particular software (e.g., R, SPSS, SAS, etc.); (c) domain-specific applications?",2010-08-29T13:12:27.157,183,CC BY-SA 2.5,
2447,2181,0,"Jeromy, allow me to take all of these with one blow: I'm a psychology student. And I reckon you're familiar with required statistical background... So there... =)
(I'm good with R and SPSS... but R has a greater priority)",2010-08-29T15:26:47.500,1356,CC BY-SA 2.5,
2449,2190,1,"Can you expand on this question a little?  What else to you know about the ""great prediction""?",2010-08-29T17:00:13.033,5,CC BY-SA 2.5,
2451,2190,3,"If you mean reading the forest by looking at each tree and trying to understand what is going on, don't do it, you'll fail to see anything in such a complex, randomized structure. RF is just a black box method, its models are not intended to describe data (as usual in machine learning).",2010-08-29T17:33:21.067,,CC BY-SA 2.5,user88
2452,2138,1,I like the points with mean centrally.  It has a truer representation of the lines.  You could make the points smaller.,2010-08-29T19:18:56.073,601,CC BY-SA 2.5,
2453,2193,0,As I understand it is possible to reconstruct the structure but it is impossible to assign class specific lineage of nodes. Here is my problem. I have a binary problem (presence/absence) of variable that can be predictive of one of 4 classes (as single and as joint for example combination of 5 variables). I am getting high prediction relying on top 20 by VIMP out of 1500 variables). I have to extract the specific combinations at least some of them,2010-08-29T23:18:37.163,,CC BY-SA 2.5,CLOCK
2454,2177,0,"Thanks for the response, I did not include specific examples of methodologies such as LOESS smoothing line, or model selection types (like BIC or using an F-test) because they are non-central to my question. Thanks for the clear up and I was definitely referring to partial regression plots (the first article I had been exposed to it in had called them added variable plots). Although it is not really central to my question either so I just referred to the wikipedia page. I also edited multi-variate as indeed I am only predicting 1 variable. Thanks again for the response.",2010-08-30T12:32:58.930,1036,CC BY-SA 2.5,
2455,783,0,"Anyway, I can give a citation of Lehman about that: ""There is some convenience in such standardization since it permits a reduction in certain tables needed for carrying out various tests"".",2010-08-30T12:37:28.153,223,CC BY-SA 2.5,
2456,1332,0,depending on what you call 'the thruth'... I don't think I give so much value to statistical truth :),2010-08-30T12:39:31.627,223,CC BY-SA 2.5,
2458,1360,0,"Do you think that by ""those"" he means ""everyone"" ? what was the context of this citation ? it seems a bit strong like this :)",2010-08-30T12:42:22.667,223,CC BY-SA 2.5,
2459,2072,0,Can you clarify what you mean by irregular? I had initially taken it to mean that you had two series of different discrete time intervals.,2010-08-30T12:42:39.313,1036,CC BY-SA 2.5,
2460,2206,0,I'll run with the idea that it's Monte Carlo -- thanks for the response.,2010-08-30T12:44:02.773,1117,CC BY-SA 2.5,
2461,2072,1,"Yes, I mean two time series with different random arrival times (not regularly sampled).",2010-08-30T12:56:30.667,5,CC BY-SA 2.5,
2462,2177,0,"Also I think my last statement sufficiently summarizes what I am interested in, and I don't see it as being entirely consumed by formal inference.",2010-08-30T13:37:58.507,1036,CC BY-SA 2.5,
2463,2201,0,Another useful link: http://stats.stackexchange.com/questions/1385/techniques-for-handling-incomplete-missing-data,2010-08-30T13:49:03.360,339,CC BY-SA 2.5,
2465,646,0,"I'm not sure how many of these will have the scope of ""marketing"" research, but I would also suggest the OP check out ICPSR. It is also good to know the quality of the sampling and data record keeping, which I imagine could be haphazard if you get data from a proprietary, non-education related source.",2010-08-30T15:32:19.930,1036,CC BY-SA 2.5,
2466,2200,0,"what? the question says 'blah blah blah would be $min(a,b)$, how do I find the min?' I answered the question.",2010-08-30T16:38:00.527,795,CC BY-SA 2.5,
2467,2145,0,"In fact, I got another form

$$ (n+1)!\delta_{n+1} = \gamma_{n+1}+\sum_{k=1}^nk\gamma_k\cdot[(n-k)!\delta_{n-k}] $$",2010-08-30T17:27:09.630,1043,CC BY-SA 2.5,
2468,2215,0,"Re: ""I think of Monte Carlo as any method that uses probabilistic sampling of data as input into a computation..."" this is a useful way for me to think of the problem. Thank you.",2010-08-30T17:46:11.527,1117,CC BY-SA 2.5,
2469,766,0,"In addition, `n!` = `Gamma(n+1)` for n >= 0. So try to look for a function called `Gamma` if you need to calculate the factorial (or log Gamma if you're calculating the log likelihood)",2010-08-30T18:43:09.860,961,CC BY-SA 2.5,
2470,2221,1,This is also why the area of a circle is *not* equal to $\int_0^{2\pi} \int_0^R 1 d\theta dr$.,2010-08-30T19:03:38.453,795,CC BY-SA 2.5,
2475,2222,0,Thank you. Is it ok to generate the samples using `sample` and `replace=TRUE`? Is there any reason to use a package like `boot`?,2010-08-30T20:36:33.120,339,CC BY-SA 2.5,
2476,2222,0,"Typically it's done with replacement so you want to set that to TRUE.  As for why... the package is optimized so it should run faster.... never timed it.  That can be a problem if you set R large.  And, as you can see, the code is nice and concise.  It also has lots of features you wouldn't get easily rolling your own.",2010-08-30T23:10:18.733,601,CC BY-SA 2.5,
2479,2202,0,"your last explanation on finding the smallest intersection can be applied to all cases. Min `A‚à©B` is `max(a+b-1,0)`",2010-08-31T03:57:00.773,862,CC BY-SA 2.5,
2480,2224,0,I think it is better to put a comment and vote for closing the question. Indeed it may be a bit confusing to have an answer wich is a link to a question on the same site... I voted to close the question.,2010-08-31T07:38:31.393,223,CC BY-SA 2.5,
2481,2222,0,"boot.ci returns the confidence interval. Is there any (boot) function that gives the p.value? (as the ratio of the number of differences at least as high as the one observed, over the total number of generated samples)",2010-08-31T08:49:08.873,339,CC BY-SA 2.5,
2482,2222,0,"ok, I found a way to get it: `sum(b$t>=b$t0)/b$R`",2010-08-31T09:02:29.920,339,CC BY-SA 2.5,
2483,2234,0,"Do we have to assume that you are considering a fixed set of predictors, i.e. you are interested in getting a reliable prediction given $k$ predictors, or are you also interested in some kind of penalization on the $X_j\quad (j=1\dots k)$?",2010-08-31T11:29:14.603,930,CC BY-SA 2.5,
2484,2234,0,"I admit that for my personal interest, penalization wouldn't be necessary, and for the sake of knowledge here I would say both are relevant answers :)",2010-08-31T11:46:39.227,253,CC BY-SA 2.5,
2485,2234,0,For future reference: you might have been able to phrase this question in such a way that we would have allowed it as a non-CW question.  See http://meta.stats.stackexchange.com/questions/290/what-is-community-wiki,2010-08-31T13:07:31.287,5,CC BY-SA 2.5,
2486,2234,0,"Thank you for the link Shane.  A very interesting discussion you opened there.  After reading Thomasas answer, I believe this should still be a community wiki, since my intention was to find as many alternatives as possible (something i doubt any one person would be able to supply).
   Yet, again, thank you for directing me to that thread!",2010-08-31T13:22:00.373,253,CC BY-SA 2.5,
2487,2200,0,@shabbychef I agree; bad questions should be punished with no answers.,2010-08-31T14:21:09.380,,CC BY-SA 2.5,user88
2488,2224,0,"@Robin: Thank you for the suggestion.  I do not see how it would be any more or less confusing to place a link in an answer or a comment, though.  I, too, have voted to close the question, but I first waited a day for the poster to respond to Shane's suggestion that this question be edited.",2010-08-31T14:22:26.660,919,CC BY-SA 2.5,
2489,2174,0,Also `matplotlib` plots are ugly; they may be nice for a long-years gnuplot user.,2010-08-31T14:23:30.763,,CC BY-SA 2.5,user88
2490,2106,0,"@ Matt, I still get the same EZanova warning msg. for Anova I get another error msg: 
Note: model has only an intercept; equivalent type-III tests substituted.
Error in linearHypothesis.mlm(mod, hyp.matrix, SSPE = SSPE, idata = idata, : The error SSP matrix is apparently of deficient rank = 0 < 1",2010-08-31T14:43:45.307,1084,CC BY-SA 2.5,
2491,2227,1,What's the source?,2010-08-31T15:02:25.087,5,CC BY-SA 2.5,
2493,2224,1,my view is that answering a question is a step toward accepting it in the site.,2010-08-31T15:16:04.040,223,CC BY-SA 2.5,
2494,2106,0,"This may help with the type-III thing https://stat.ethz.ch/pipermail/r-help/2007-October/144240.html ; and I've edited my answer to reflect the updated version of ez, which has changed ""sid"" to ""wid"". I don't get your problem with the data you've given me.",2010-08-31T15:24:33.373,966,CC BY-SA 2.5,
2495,2106,0,"#Try this with the code I've written for the car Anova
#transfer data back to long to perform ezANOVA    
df2<-melt(df1, id.vars=""observ"", measure.vars=names(df1)[6:21])
df2$suc<-substr(df2$variable,1,2)
df2$cit<-substr(df2$variable,3,4)
df2$observ<-factor(df2$observ)
ez1<-ezANOVA(data=df2,
  within=.(suc, cit),
  wid=.(observ),
  dv=.(value)
  )",2010-08-31T15:28:08.107,966,CC BY-SA 2.5,
2496,2222,0,@gd047 : take into account that this is a one-sided p-value you're calculating.,2010-08-31T15:58:54.067,1124,CC BY-SA 2.5,
2497,2227,1,It was taken from Mike West's website: http://www.stat.duke.edu/~mw/fineart.html,2010-08-31T16:23:32.740,881,CC BY-SA 2.5,
2498,2240,1,Can you give an explanation of why permutation tests should not be used on datasets that you can't calculate all possible permutations?,2010-08-31T16:27:10.973,1036,CC BY-SA 2.5,
2499,2240,0,"@Andy W : First define ""permutation test"". for me, permutation tests are exact tests, using every permutation possible. That is impossible on larger datasets. The ""approximate permutation tests"" are in fact the simples Monte Carlo method around, and should be adressed that way. Next to that, the central limit theorem assures in most cases that the assumptions regarding the distribution of the test statistics are met when using large datasets. In complex testing, the use of permutation tests on large datasets makes calculation times unbearably long without adding any significant value. my2cents",2010-08-31T16:42:08.420,1124,CC BY-SA 2.5,
2500,2200,0,"No, that's not what he's asking for. He asked for the min of the intersection, as can be deducted by careful reading of the first of the two sentences in the opening post.",2010-08-31T16:51:40.677,1124,CC BY-SA 2.5,
2502,2224,0,@Robin: Thank you; that makes sense and it's a principle that will help in future cases.,2010-08-31T17:12:55.590,919,CC BY-SA 2.5,
2503,2222,0,"@John Christie : Could you show me how your last bootstrap code should be modified in order to draw resamples of size 5 with replacement from the first sample and separate resamples of size 6 from the second sample (instead of resampling from the total number of 11 values). As I read on page 18 of the linked pdf, this should be the correct way to draw the bootstrap samples.",2010-08-31T18:12:56.903,339,CC BY-SA 2.5,
2504,2240,0,"I didn't say anything like bootstrapping a permutation test. I came into this question after reading the last paragraph of [SECTION 14.5 | Summary], in the linked pdf.",2010-08-31T18:33:03.053,339,CC BY-SA 2.5,
2505,2238,0,+1 it is always nice to bootstrap something. On the other hand every challenge is easy with R: http://stat.ethz.ch/R-manual/R-devel/library/boot/html/tsboot.html,2010-08-31T19:47:47.827,,CC BY-SA 2.5,user88
2506,2249,0,"Thank you very much for that.  I thought of the Mann-Whitney test, but many (most?) of the values will result in ties.  Won't this reduce the power of the analysis?",2010-08-31T21:38:04.753,,CC BY-SA 2.5,John
2507,2249,2,"Yes, of course it will; but as you're working with 10 observations per group, don't expect very much from this setting; check for yourself at http://statpages.org/#Power",2010-08-31T21:53:56.843,930,CC BY-SA 2.5,
2508,2186,1,why the Tinsley book? No reviews on Amazon suggests it ain't a big seller or particularly good.,2010-08-31T23:18:05.303,74,CC BY-SA 2.5,
2509,2238,0,Thanks Dirk and mbq. I have one time series I collected. I am not using a data generating process to produce different series. So in this case I understand that I cannot get std deviations? Please correct me if I'm wrong. Thanks again!,2010-08-31T23:51:38.377,443,CC BY-SA 2.5,
2510,2238,0,You could try 'chunked' resampling. Google for block bootstrap.,2010-09-01T01:14:32.647,334,CC BY-SA 2.5,
2511,2245,2,great question: see also this related question on correlation and causation http://stats.stackexchange.com/questions/534/under-what-conditions-does-correlation-imply-causation,2010-09-01T05:14:43.280,183,CC BY-SA 2.5,
2512,2200,0,@mbq - whats wrong with the question?,2010-09-01T06:00:17.303,862,CC BY-SA 2.5,
2513,2257,0,"thanks for this. I would like to higlight the groups using color such that all points from the same group have the same color. Can quite figure out what the convert command does, let me check the documentation though a layman explanation can also help. 
Thanks a lot!",2010-09-01T06:37:16.597,18462,CC BY-SA 2.5,
2514,2258,0,"LaTeX works, you must just stop editing for a while so that it will be applied to preview window.",2010-09-01T07:26:03.240,,CC BY-SA 2.5,user88
2515,2258,0,"For LaTeX, enclose terms in $ signs.",2010-09-01T07:26:12.747,159,CC BY-SA 2.5,
2516,2258,0,Got it. Which tag can I use for multiline latex block?,2010-09-01T07:33:25.310,862,CC BY-SA 2.5,
2517,2256,1,You may also check out `rgl`.,2010-09-01T07:35:39.570,,CC BY-SA 2.5,user88
2518,2258,0,"The align environment will work, but use \newline instead of \\.",2010-09-01T07:45:25.050,159,CC BY-SA 2.5,
2519,2200,0,@saminny You could have formulated it better so that it would be less ambiguous and easier to understand. shabbychef just pointed it out with his answer.,2010-09-01T07:46:30.797,,CC BY-SA 2.5,user88
2521,2240,1,"@gd047 Then I have misread your question. But you should really keep confidence intervals and p.values strictly separated. The confidence interval is estimated based on bootstrapping within each sample (although it is biased by definition), the permutation test is done by permutations over the complete dataset. Those are two completely different things.",2010-09-01T08:15:40.293,1124,CC BY-SA 2.5,
2523,2260,2,"Interesting point about the means. I suppose differences between means would suggest that the groups are different in some respect. And empirically I'd expect that where there's large differences between means, there's more likely to be differences in factor structure. Differences in means might also have implications for how the findings regarding the factor structure could be generalised. It might also increase correlations between items in general due to an effect the opposite of range restriction.",2010-09-01T08:35:28.633,183,CC BY-SA 2.5,
2524,2258,0,how do I use align environment? Is it a tag?,2010-09-01T08:41:23.433,862,CC BY-SA 2.5,
2525,2262,1,Could you explain what a 'peak' stand for in the context of your data?,2010-09-01T10:03:25.763,930,CC BY-SA 2.5,
2526,2262,1,Also do you have access to the underlying data? How are the curves drawn?,2010-09-01T10:09:50.720,8,CC BY-SA 2.5,
2528,2258,0,"Several points: 1. See my edits as to how to use Latex. 2. You may also want to see this thread: [Vote early, vote often](http://meta.stats.stackexchange.com/questions/314/vote-early-vote-often). 3. If this is homework could you please tag it as such so that people can provide hints instead of complete answers.",2010-09-01T11:00:54.530,,CC BY-SA 2.5,user28
2529,2265,1,Thanks for the comprehensive response. OpenMX does look like a good open source R package option for multi-group CFA. The IRT options also look interesting. Good point about the problems of hypothesis testing in CFA. Thanks for the references.,2010-09-01T11:12:05.397,183,CC BY-SA 2.5,
2530,2257,0,"`s3d <- scatterplot3d(x.mds$points[,1:3], color=as.numeric(gl(2,50)))` would highlight the first 50 points in black, the remaining ones in red; you can pass any logical vector or factor to suit your data. HTH",2010-09-01T11:12:43.320,930,CC BY-SA 2.5,
2531,2265,1,"I'd recommend lavaan as well, stupid I didn't think of it earlier.",2010-09-01T11:28:17.313,1124,CC BY-SA 2.5,
2532,2250,0,"With only 10 observations in each group, ordinal logistic regression is very unlikely to fit, let alone give correct estimates of the variances. I'd suggest the permutation test.",2010-09-01T11:54:28.100,1124,CC BY-SA 2.5,
2533,2264,0,"although not the same, you may want to see this related post: http://stats.stackexchange.com/questions/1848/bayesian-rating-system-with-multiple-categories-for-each-rating",2010-09-01T11:55:58.223,183,CC BY-SA 2.5,
2534,2269,2,This is an inappropriate place to ask this question. You need to deal with SAS directly or other forums directly related to utilizing SAS software.,2010-09-01T12:15:22.400,1036,CC BY-SA 2.5,
2535,2253,0,"Thanks everyone.  I went back and tried the Mann-Whitney test, using the method of listing the score of each individual as a column.  (FWIW, I use GraphPad Prism for the analysis.)  The results do appear reasonable, given what we already know about the biology.  Thanks very much for your help.",2010-09-01T12:15:46.770,,CC BY-SA 2.5,John
2536,2269,0,"Agree, now closed.",2010-09-01T12:42:06.853,,CC BY-SA 2.5,user88
2537,2268,0,"Thanks for the link. I agree that the number of ratings for the product compared to average number for all products is significant. But the variance must be significant too. Suppose every product had zero variance in its ratings, then a single product rating would be sufficient to provide the ev.",2010-09-01T12:50:30.447,1134,CC BY-SA 2.5,
2538,2266,0,"As a newbie student maybe I shouldn't be too picky, but I'm not very happy with this answer. Why can't an ev be calculated? Can you prove that statement? Also 95% CI seems plain arbitrary.",2010-09-01T12:57:18.917,1134,CC BY-SA 2.5,
2540,2263,0,"Hello Joris.

Thanks for the input.

All the peaks are gaussians - having a nice bell shape. The only thing that varies is the amplitude (height) and the SD (width).

I actually don't really know about this density function. COuld you explain it more or give me link towards it?

Thanks.",2010-09-01T13:48:46.960,1133,CC BY-SA 2.5,
2541,2263,0,"@chl: 'peak' is simply any area above my baseline. All peaks have gaussian distributions.

@csgillespie: I have access to the original data that plotted the histogram and the curves are drawn in GraphPad Prism using a 'Sum of three gaussians' fit.",2010-09-01T13:52:02.093,1133,CC BY-SA 2.5,
2542,2263,0,This is how the whole graph looks like: http://imgur.com/ipBiL.png,2010-09-01T13:53:16.993,1133,CC BY-SA 2.5,
2544,2263,0,"@Cadu Ok, so this does not resemble what is commonly refered to as AUC, since this term is more often used in reliability studies or when assessing performance of classifier, i.e. when you plot sensibility against (1-specificity).",2010-09-01T13:58:26.763,930,CC BY-SA 2.5,
2545,2263,0,"In full, the probability density function or pdf : http://en.wikipedia.org/wiki/Probability_density_function . The one for the normal distribution, which I use, is thoroughly described. Google will give you tons of hits on that. As you used the sum of gaussians, the approach I gave you should work. Mind you, it gives you the total area for each gaussian curve. So if you add the AUC's for the three curves, you end up with a higher area than under the combined curve.",2010-09-01T14:03:54.363,1124,CC BY-SA 2.5,
2546,2262,1,Maybe change your title because AUC is definitively misleading there...,2010-09-01T14:10:20.980,930,CC BY-SA 2.5,
2550,2232,3,"Thanks, I like the convergence of infinite series point-of-view!",2010-09-01T14:20:24.957,1106,CC BY-SA 2.5,
2551,2263,0,"Sounds pretty good for me, Joris.

However i am still unsure which one of those formulas is the correct one. If it is not too much, can you point it out which one you think would be the best one for this situation?

And, yes, i'll keep in mind that it is not the total AUC.

Thanks for your time again.",2010-09-01T14:23:11.117,1133,CC BY-SA 2.5,
2552,2250,0,"@Joris Good point! I forgot the sample size issue when I initially wrote my answer. The GLM would certainly yields poor estimates... Re-randomization test should be better, you're right. Thanks!",2010-09-01T14:26:30.210,930,CC BY-SA 2.5,
2553,2274,0,"Hey Chl. I will try this mclust package and see what it can do.

Thanks for the hint :)",2010-09-01T14:32:47.650,1133,CC BY-SA 2.5,
2558,2253,1,"Mann-Whitney can be a good choice, but be aware that it makes some specific assumptions about the relationships among the two groups.  It is good when results appear reasonable, but that cannot be the sole reason to adopt a statistical test: that would be getting into a circular argument (""we chose this test because it gives reasonable answers; our answers are significant because this test says so"").  At a minimum, make sure to graph all the data in a way that promotes visual comparison of the groups.",2010-09-01T14:56:58.837,919,CC BY-SA 2.5,
2559,2249,0,"@John: I suspect the KS test I proposed would be more powerful than Mann-Whitney because it can detect various alternatives that M-W cannot (such as a change in spread) and otherwise is doing a mathematically similar calculation.  This is difficult to study in full generality--in your case, each group's results has four degrees of freedom, so there are eight dimensions of differences to explore--but if you could be more specific about exactly *how* the two groups might differ (which is matter for science, not statistics, to decide), you would have a better basis for selecting a test.",2010-09-01T15:01:46.097,919,CC BY-SA 2.5,
2560,2260,0,"Good point I hadn't thought of it that way.  Thank you.  FYI I'm not sure how common it is to use this term, but Rosenthal (of Rosenthal and Rosnow, 2007) refers to the effect opposite of restriction of range as hypertrophy of range.",2010-09-01T15:08:25.563,196,CC BY-SA 2.5,
2565,2167,0,"@Yaroslav: Thanks for the reference.  Are you aware of any implementations of that ""Modified Finite Newton Method""?",2010-09-01T15:23:11.533,5,CC BY-SA 2.5,
2566,2266,0,E[X] is a property of the pop that you are drawing inferences about.  x-bar is an unbiased estimator of that value.  You don't have to use 95% -- adjust t accordingly.  But note the SE of x-bar is s/sqrt(n) not s/n. I'll see if I can edit this.,2010-09-01T15:31:26.750,1107,CC BY-SA 2.5,
2568,2266,0,"@Bart : an expected value is about the population, and a theoretical value. It can be seen as the limit of the sample mean when the sample size goes to infinity. You need to estimate it using the mean, but you can't calculate it unless you know the complete population. Which you don't. See : http://en.wikipedia.org/wiki/Expected_value",2010-09-01T15:40:19.450,1124,CC BY-SA 2.5,
2569,2263,0,"@Cadu : What's the software you're using? If it's merely for calculation, you shouldn't use the theoretical functions. In R, there is a function called dnorm() that does the calculation for you. On this wiki page you get more information on the probability density function for the normal distribution : http://en.wikipedia.org/wiki/Normal_distribution",2010-09-01T15:46:02.810,1124,CC BY-SA 2.5,
2570,2277,0,"Hmm, you got a point there, whuber.

In the graph that i posted i got the mean, which indeed it nothing more than the position in the X axis of the peak. The SD deviation should be half of the width of the peak. And i also got the amplitude of these peaks which is the height.

If i got it right, your suggestion is that i could calculate the area of the peaks simply by multiplying the height (amplitude) by the width (2*SD). And due to the fact that i am only interested in the ratios between the peaks, this should be enough.

Did i get you right?",2010-09-01T15:51:11.850,1133,CC BY-SA 2.5,
2571,2263,0,"@chi: I think you are mixing up two acronyms. 
ROC curve = Receiver Operator Characteristic curve
AUC = Area Under Curve",2010-09-01T16:03:26.103,25,CC BY-SA 2.5,
2572,2277,0,"Yes, you are correct.  You don't need to worry about the factor of 2, either--it's meaningless--because you just want ratios.  The problem with this approach is that when the peaks are close together, the tails of a peak's neighbors contribute somewhat to the peak's own height.  Thus, it's best to use a peak-estimation procedure that compensates for this.  That's what chl, for example, was referring to by a ""mixture of gaussians.""",2010-09-01T16:06:28.773,919,CC BY-SA 2.5,
2573,2282,1,"@Tal: I am basically happy to allow any R questions on here, but I just don't see how this has any relevance to the subject of data analysis.  This is a meta question about R itself, not even about using R.  We'll see what others think, but this seems more appropriate for stackoverflow, r-help, twitter, or a blog post.",2010-09-01T16:12:52.953,5,CC BY-SA 2.5,
2574,2275,0,"I think it would be ideal if you could present some of the data you finally have. At least for me, that would help a lot in thinking about an answer.",2010-09-01T16:13:12.703,442,CC BY-SA 2.5,
2575,2282,0,Regarding how to do this: I would suggest making your own version of the `library` function that would track each package when you load it.,2010-09-01T16:13:56.367,5,CC BY-SA 2.5,
2576,2282,0,Related meta questions: http://meta.stats.stackexchange.com/questions/1/how-to-answer-r-questions and http://meta.stats.stackexchange.com/questions/169/is-it-possible-to-integrate-data-from-so-com-questions-tagged-r-here.,2010-09-01T16:16:57.347,5,CC BY-SA 2.5,
2577,2277,0,"Okay then :)

Actually i was trying to use chl's approach but nothing so far.

On the other hand, i used Jori's approach to calculate the area and then i calculated the ratios between them. Afterward i did the same thing with this easy procedure of yours - and i must say that although the areas were different, the ratio was pretty much similar.

I guess this solves my issues.

Thanks everybody for the help :)",2010-09-01T16:18:06.783,1133,CC BY-SA 2.5,
2578,2282,0,"Lastly: regarding how to ftp; not sure how to do this from within an R function, but you can do it easily with `system(""ftp ..."")`.",2010-09-01T16:20:45.077,5,CC BY-SA 2.5,
2579,2266,0,"@Joris The wiki entry is much too advanced for me. Maybe I'm not asking for the right quantity. If by definition you need the entire population to calculate ev, I'd like a similar quantity that allows me to incorporate priors when my data is incomplete. So I can update the question, what would I call that?",2010-09-01T16:22:26.057,1134,CC BY-SA 2.5,
2582,2282,0,"Thanks Shane. I'll go through the threads.  You are correct, this is more of a R community topic then a statistical one - I'll vote to close the question (I don't erase it since it is an example of a ""bad"" question for this website)",2010-09-01T16:56:23.747,253,CC BY-SA 2.5,
2583,2282,0,"Thanks @Tal.  This is similar in nature to this question: http://stats.stackexchange.com/questions/2269/access-tables-created-in-sas-enterprise-guide-client-into-sas-enterprise-miner-cl, which was also closed.",2010-09-01T17:00:20.163,5,CC BY-SA 2.5,
2585,2282,0,You should also reach out to @johnmyleswhite: http://twitter.com/johnmyleswhite/status/22395563278,2010-09-01T17:03:41.573,5,CC BY-SA 2.5,
2587,2282,1,"If you were looking for gathering data on _installed_ packages, the crantastic package with its function crantastic.submitInstalledPackages would be worth looking at..",2010-09-01T17:09:46.867,573,CC BY-SA 2.5,
2589,2275,0,"The actual probability after the fact is either 1 or 0.  So for example For test 1: I say 75% of a 1, and ends up a 1, test 2: 35% ends up 0, etc.  How can I quantify the accuracy of my estimate?",2010-09-01T18:14:36.757,1137,CC BY-SA 2.5,
2591,2284,1,The Federalist paper is very interested and describes similar to above answer.  Thanks.,2010-09-01T18:30:29.947,1137,CC BY-SA 2.5,
2592,2269,5,I wholeheartedly disagree. We should cover statistics software here too. How many R questions are there?,2010-09-01T18:38:27.443,74,CC BY-SA 2.5,
2593,2280,0,Thanks. I've looked at some of your suggestions and they seem to be along the lines I was thinking. But can you recommend anything more specific and suitable for the lay person that I am.,2010-09-01T18:56:59.057,1134,CC BY-SA 2.5,
2594,2266,2,"@bart, think of this question: what is the expected value of the height of an American? The only way to know this value exactly would be to ask every single person his height and take the average. That's the expected value---it's a property of the full population. That's really hard. Instead, we get a sample, a subset of our population, ask them their heights and take the average. This is the sample mean---a property of our sample and an estimate of the expected value. As our sample gets larger, we should get a sample mean that gets closer to the true expected value (the law of large numbers).",2010-09-01T19:03:47.900,401,CC BY-SA 2.5,
2595,2263,0,"@Joris: Since dnorm(avg, avg, sd) = C * 1/sd, this formula for ""AUC"" simply proposes computing heights * sd / C (where of course C = 1/sqrt(2 pi) for Gaussians but can be some other constant for other scale-location families).  The constant factor disappears when computing ratios (which is what the question asks for), whence one might just as well compute heights * sd and be done with it.",2010-09-01T19:32:58.400,919,CC BY-SA 2.5,
2596,2277,0,"@Cadu: the ratios shouldn't be ""pretty much similar""; they should be *identical* (to within floating point error).",2010-09-01T19:34:48.583,919,CC BY-SA 2.5,
2598,2285,1,could you elaborate on how the bin counts would be compared to the probabilities?  I presume you have something quantitative in mind.,2010-09-01T20:35:54.763,919,CC BY-SA 2.5,
2599,2262,0,"The title should say ""peak height"" rather than ""peak mean"".",2010-09-01T20:46:40.303,25,CC BY-SA 2.5,
2601,2285,3,"@whuber: the probabilities can be plotted against the empirical frequencies to get what's called a calibration plot; the points should be roughly along the diagonal. I don't know if one could do a more quantitative analysis, this isn't an area I know a whole lot about, just a few things i've picked up here and there.",2010-09-01T21:10:28.863,495,CC BY-SA 2.5,
2602,2280,1,@bart -- I wish I could offer a safe automated way to do this but I don't think it exits.  See above for edits w/ a little more detail.,2010-09-01T22:54:38.690,1107,CC BY-SA 2.5,
2604,2292,0,"Point taken.

The problem appeared because the estimates of the smoothing parameters of the best fitting model (an ETS(M,A,N) ) are 0.0001 and 0.9999 which I think are the set limits for the possible values the estimates can take. So I was trying to find out what was going on. 

I was looking for standard errors because I'm used to finding out about a model's parameters' distributions whenever I have to estimate one..:) thinking that if they vary too much they are not too reliable.",2010-09-02T01:06:57.013,443,CC BY-SA 2.5,
2605,2292,0,I did bootstrap as advised earlier. I found that the standard deviation was not that useful because the distribution was very skewed.,2010-09-02T01:08:07.847,443,CC BY-SA 2.5,
2606,2279,1,Yes its trivial. I was able to come up with a simple proof using sigma algebra.,2010-09-02T01:39:48.033,862,CC BY-SA 2.5,
2607,2298,0,p.s. maybe this should be a community wiki? I'm not sure how that works...,2010-09-02T03:32:49.573,795,CC BY-SA 2.5,
2608,2167,0,"no, but Keerthi and Langford's pages have links to some software that may be related, since they both worked at Yahoo Research",2010-09-02T03:53:17.880,511,CC BY-SA 2.5,
2609,2292,0,"@noworries. Yes, they are the boundaries of the parameter region. A value close to zero is not a problem -- it indicates the associated state variable is extremely stable and essentially does not change over time. A value close to one usually indicates the model is trying to adapt quickly to each new observation which probably means the model is mis-specified.",2010-09-02T05:24:17.683,159,CC BY-SA 2.5,
2610,2298,0,if you don't give us more info on $f$ and on what type of characteristic then it seems to be community wiki but I am not sure neither :),2010-09-02T05:47:49.420,223,CC BY-SA 2.5,
2611,2298,0,"The fact that $f$ in undefined makes it a feed of a different multivariate visualization methods, so it should be CW. And now it is.",2010-09-02T07:20:09.280,,CC BY-SA 2.5,user88
2613,2261,1,"Regarding your last point about GLM, this is basically how IRT models work: they can be viewed as generalized mixed-effects model with a logit function link and individuals treated as random effets (not items!). This yields the so-called marginal likelihood approach, where subjects' parameters are assumed to follow a standard normal distribution, for model identification purpose. The problem is that it has to be done on each subscale which are hypothesized to measure a single construct.",2010-09-02T08:32:10.840,930,CC BY-SA 2.5,
2614,2268,0,Good point. I just wanted to flag that the Bayesian option seems like the way to go. Of course @Kingsford has now provided a more rigorous explanation.,2010-09-02T08:49:29.270,183,CC BY-SA 2.5,
2615,338,1,"Still, none of those books claim that there is more than one entropy.",2010-09-02T10:25:52.500,,CC BY-SA 2.5,user88
2616,2306,0,I should say for full disclosure that I have already posted this to the bioconductor list,2010-09-02T10:26:34.950,1150,CC BY-SA 2.5,
2617,2280,0,"You forget that some products don't have a score, or only very few. Lme is very likely to get convergence problems, and the estimates of the standard errors cannot be trusted.",2010-09-02T11:00:59.217,1124,CC BY-SA 2.5,
2618,2261,1,"Good point. And in fact, It would be better to use GEE models or mixed effect models instead of GLM, a standard GLM doesn't allow for random effects.",2010-09-02T11:04:04.780,1124,CC BY-SA 2.5,
2619,2266,0,"@Bart : You want to estimate the expected value, and you do that by modelling the sample means in any way. It sounds trivial, but it is crucial to understand what exactly you're doing. I see far too many students struggle with statistics because they don't understand the underlying concepts. I don't want to be rude, but it might be a good idea to look for a thorough introduction into statistics. It will help you tremendously in understanding what is going on.",2010-09-02T11:09:12.697,1124,CC BY-SA 2.5,
2620,2269,0,There is an ongoing discussion about this question on meta: http://meta.stats.stackexchange.com/questions/400/is-this-question-really-not-related-to-statistical-analysis,2010-09-02T11:11:45.983,,CC BY-SA 2.5,user28
2621,2307,0,"Thanks for that.  I'm not particular sold on SVM, just using that as an example.  So if you used random trees, you don't have to do cross-validation? Is that right.",2010-09-02T11:13:59.020,1150,CC BY-SA 2.5,
2622,2269,1,"of course you need some software/tool and can't do all statistics by just using calculator. If this is not a valid question then why do tags like ""R"", ""SAS"" exists here?

Are you guys planning a new site for ""Statistical Analysis Softwares""?",2010-09-02T11:26:25.350,263,CC BY-SA 2.5,
2623,2307,7,"yes, RFs include a random sampling of variables (typically $\sqrt{p}$) when growing a tree and each tree is based on a boostraped sample of the individuals; variable importance is computed on so-called out-of-bag samples (those not used for building the decision tree) using a permutation technique. The algorithm is repeated for m trees (default m=500) and results are averaged to compensate uncertainty at the tree level (boosting).",2010-09-02T11:34:20.303,930,CC BY-SA 2.5,
2624,2281,2,"A 95% confidence interval by definition covers the true parameter value in 95% of the cases, as you indicated correctly. Thus, the chance that your interval covers the true parameter value is 95%. You can sometimes say something about the chance that the parameter is larger or smaller than any of the boundaries, based on the assumptions you make when constructing the interval (pretty often the normal distribution of your estimate). You can calculate P(theta>ub), or  P(ub<theta). The statement is about the boundary, indeed, but you can make it.",2010-09-02T11:44:24.477,1124,CC BY-SA 2.5,
2625,2307,3,It is important that it is called Random Forest not Random Trees; you may have problems with Google.,2010-09-02T11:52:11.267,,CC BY-SA 2.5,user88
2626,2263,0,"@Harvey Yes, you're right: these are ROC that are used to show classification accuracy, but we use their corresponding AUC (or, AUC wrt. first bisector) to compare one classifier to another (or one predictive instrument to another). Now let's try ""AUC"" as a keyword in Pubmed and look at the results: How much deal with clinical evidence-based medicine or biomedical studies?",2010-09-02T12:12:39.073,930,CC BY-SA 2.5,
2627,2261,1,"FYI, this article from Tuerlinckx et al. 2006 provides a nice review of GLMMs: http://bit.ly/9k8vJs. Also, about the use of GEE as an alternative marginal model, it was proposed by Feddag, Grama, and Mesbah (2003) to estimate parameters of the Rasch Model (one-parameter IRT model), or loglinear multidimensional IRT models as defined by Kelderman and Rijkes (1994). It is currently available in the Stata `raschtest` package.",2010-09-02T12:33:14.857,930,CC BY-SA 2.5,
2638,2146,0,"Do I understand it right that you have two sets of time series, and you want to check whether or not they differ significantly?",2010-09-02T13:59:52.023,1124,CC BY-SA 2.5,
2639,2281,12,"Joris, I can't agree. Yes, for any value of the parameter, there will be >95% probability that the resulting interval will cover the true value. That doesn't mean that after taking a particular observation and calculating the interval, there still is 95% conditional probability given the data that THAT interval covers the true value.

As I said below, formally it would be perfectly acceptable for a confidence interval to spit out [0, 1] 95% of the time and the empty set the other 5%. The occasions you got the empty set as the interval, there ain't 95% probability the true value is within!",2010-09-02T14:21:34.080,1122,CC BY-SA 2.5,
2641,2269,0,Reopened the question per this discussion: http://meta.stats.stackexchange.com/questions/400/is-this-question-really-not-related-to-statistical-analysis,2010-09-02T14:36:22.687,5,CC BY-SA 2.5,
2643,2281,0,"@ Keith : I see your point, although an empty set is not an interval by definition. The probability of a confidence interval is also not conditional on the data, in contrary. Every confidence interval comes from a different random sample, so the chance that your sample is drawn such that the 95%CI on which it is based does not cover the true parameter value, is only 5%, regardless of the data.",2010-09-02T15:02:43.540,1124,CC BY-SA 2.5,
2645,2280,0,"@Joris-Without a prior you can't include products with 0 information, but because there are thousands of products, having few observations per product is not a problem (even many with 1 obs will be OK). But still, without being familiar with the data, the context of the problem, and desired output/inferences we are only speculating as to what is appropriate.",2010-09-02T15:44:48.880,1107,CC BY-SA 2.5,
2646,2306,2,Please summarize any bioconductor results back here?,2010-09-02T16:01:50.647,5,CC BY-SA 2.5,
2649,2266,0,@Joris You are not being rude. I've tried studying maths and stats in a conventional way and I have to say it hasn't been very successful. What seems to work for me is to reason the problems out in my own special way and then see how that matches up with what others do.,2010-09-02T17:07:36.433,1134,CC BY-SA 2.5,
2650,2314,0,"I think there's still a generalization issue when using the same sample (1) to assess the classifier classification/prediction performance while tuning its parameters (eventually, with feature selection) and (2) use in turn its predictions on the whole data set. In fact, you are breaking the control exerted on overfitting that was elaborated using cross-validation. Hastie et al. provide a nice illustration of CV pitfalls, esp. wrt. feature selection, in their ESL book, ¬ß 7.10.2 in the 2nd edition.",2010-09-02T17:16:12.673,930,CC BY-SA 2.5,
2651,2298,0,"it's fairly easy for me to fit $f$ to linear model, so we should assume that has already been done, and we are looking at the _residual_ from that fit. As I mentioned in the question, I am looking for interesting facts about the function, so I don't yet know what I might find. Because of this open-ended nature, I thought maybe it should be a CW. I am still not sure on how one decides whether a Q should be CW.",2010-09-02T17:25:09.440,795,CC BY-SA 2.5,
2653,2281,3,"Joris, I was using ""data"" as a synonym for ""sample,"" so I think we agree. My point is that it's possible to be in situations, after you take the sample, where you can prove with absolute certainty that your interval is wrong -- that it does not cover the true value. This does not mean that it is not a valid 95% confidence interval.

So you can't say that the confidence parameter (the 95%) tells you anything about the probability of coverage of a particular interval after you've done the experiment and got the interval. Only an a posteriori probability, informed by a prior, can speak to that.",2010-09-02T17:46:47.777,1122,CC BY-SA 2.5,
2654,2316,1,You should delete this 'answer' and add it to your question as you have not really found the answer as evident by the last line.,2010-09-02T17:48:29.817,,CC BY-SA 2.5,user28
2655,2316,0,"Nice find, but when Simon writes

""view that single observation as a draw from a true probability distribution who's average you want...and you can view that true average itself as having been drawn from a probability distribution of averages""

he is describing a mixed or multilevel model.  I don't think there's a need to guess that ""K=25 seems to work well"" because the best linear unbiased predictor equations were worked out more than 60 years ago (BLUP). Excellent Bayesian estimators exist as well.  As Brad Efron said, ""Those who ignore Statistics are condemned to reinvent it""",2010-09-02T18:00:39.740,1107,CC BY-SA 2.5,
2656,2318,2,"I would even recommend the entire book from I. Guyon and coworkers, http://j.mp/anblwx. The ESL book from Hastie et al., http://j.mp/bW3Hr4, provides also interesting discussions around this 'hot' topic.",2010-09-02T18:24:46.450,930,CC BY-SA 2.5,
2657,2318,0,"I disagree with your claim; FS is interesting on its own for some explanatory information that it delivers (marker/SNPs selection is an example when it is a main aim of analysis). The feature selection overfit is of course a problem, but there are methods to omit it.",2010-09-02T18:25:58.000,,CC BY-SA 2.5,user88
2658,2316,0,@Kingsford What's worong with K = Vb/Va?,2010-09-02T18:57:20.117,1134,CC BY-SA 2.5,
2659,2266,0,@Joris How do I calculate the ev from the CI? How does this approach make use of the prior information provided by the other products?,2010-09-02T19:06:21.267,1134,CC BY-SA 2.5,
2660,2316,0,"@Srikant It's the best ""answer"" I've got so far. I think I'll be trying it out because:
1. As currently stated Jorly's doesn't actually provide the ev, and there is no indication of how prior information is utilised.
2. Kingsford's answer could suffer convergence problems apparently, and there is a lot of work for me to figure out what it all means.
3. Simon's solution worked well for him, and I trust Simon.
4. I think in time I'll get a proof of his method
5. The method uses all the available data and is correct at the extrema when individual variance is small and large",2010-09-02T19:28:35.520,1134,CC BY-SA 2.5,
2661,2298,0,"At first blush this question seems purely mathematical: visualizing the function has nothing to do with the distribution of its arguments.  However, the reference to ""heteroskedasticity"" suggests you're really trying to visualize an object that is more explicitly represented as $f(x_1, ..., x_k) + \epsilon$ or more generally as  $g(f(x_1, ..., x_k), \epsilon)$ where $epsilon$ is a zero-mean random variable and $g(z,0) = z$ for all z.  Isn't this just a response-surface analysis?",2010-09-02T19:42:16.897,919,CC BY-SA 2.5,
2662,2314,0,"@chl: who said anything about tuning parameters? If additional things are performed, they should be repeated during cross-validation as well. Clearly modifying your algorithm until you get good cross-validated error rates is ""cheating"". BTW, I agree that cross-validation, especially leave-one-out, is not all that great.",2010-09-02T20:39:28.647,279,CC BY-SA 2.5,
2663,2316,0,"@bart - I don't think you'll run into convergence problems, but you're right that it takes awhile to learn the methods.  As for what's wrong with Vb/Va, I'm not sure how to answer because it's not clear to me how those values are being calculated.  But under the nested Gaussian assumption described, one good way to define 'best' is in terms of minimizing mean squared error (MSE).  This is what the BLUP equations do.  See, for example, [here](http://tiny.cc/4zfl1).",2010-09-02T21:00:07.593,1107,CC BY-SA 2.5,
2664,2321,1,"The inputs are assumed to be iid Gaussian, implying there is no direction of maximal variance.  But perhaps I have misinterpreted this assumption?",2010-09-02T22:07:06.990,919,CC BY-SA 2.5,
2665,2298,0,"@whuber I don't know enough RSM to say for sure (and the wikipedia page is not helping me). I am not necessarily trying to optimize the function (gasp), but am trying to find interesting facts about it. What makes this a statistical question is that I'm not terribly interested in the case where any of the $x_i$ are far from zero (assumed standard normal), and so what constitutes 'interesting' should somehow be weighted by relevance.",2010-09-02T22:54:51.437,795,CC BY-SA 2.5,
2666,2304,0,"ggobi looks interesting, but I am repelled by the `xml` data format. maybe I'll get over that, but it is not high priority.",2010-09-02T22:55:42.977,795,CC BY-SA 2.5,
2667,707,0,"@mbq: I have read the paper; Shao reports on a simulation study with only 40 observations, and shows that LOOCV underperforms the Monte-Carlo CV except in the case where no subselection is appropriate (the full feature set is optimal). 100 is way more than enough, at least for subset selection in linear models.",2010-09-02T23:14:06.780,795,CC BY-SA 2.5,
2669,2322,2,"Ok, to be clear -- I'm not saying LOOCV is a good idea for a big number of objects; obviously it is not, but Shao is not applicable here. Indeed in most of cases rules for LMs does not hold for ML.",2010-09-03T00:17:35.580,,CC BY-SA 2.5,user88
2670,2316,0,@bart It is not driven by sound statistical principles as is evident by the hand waving he does. If you want a statistically sound solution you should really explore multi-level models that Kingsford refers to in his comments. See my [answer](http://stats.stackexchange.com/questions/1822/test-for-poolability-of-individual-data-series/1827#1827) to another question which illustrates the basic idea.,2010-09-03T00:22:41.180,,CC BY-SA 2.5,user28
2671,538,6,"Er... I'm not familiar with real stats by a long shot... but isn't ""exposing all hidden variables"" by definition impossible? How do you know when there's no more ""hidden"" variables?",2010-09-03T01:54:54.713,1170,CC BY-SA 2.5,
2672,1287,0,"Chapter 17 covers:
17 An overview of the GLM
17.1 linearity and the GLM
17.2 bivariate to multivariate statistics and overview
17.2.1 bivariate form
17.2.2 simple multivariate form
17.2.3 full multivariate form
17.3 Alternative research strategies

I hope this can help
Regards",2010-09-03T02:01:23.580,10229,CC BY-SA 2.5,
2673,2304,1,@shabbychef Then check out the interface from R called rggobi http://www.ggobi.org/rggobi/,2010-09-03T02:30:47.927,183,CC BY-SA 2.5,
2674,2326,1,Many questions: 1. Is it an experiment or an observational study? 2. State clearly what are your predictors and what is your outcome variable. 3. Try to be more specific. 4. What do you mean by sorting? 5. What is your sample? Is it n=100 pieces?,2010-09-03T03:44:30.893,183,CC BY-SA 2.5,
2675,1136,1,"A good model is never solely based on statistical tests and criteria. It should be a combination of literature review, experience, statistics and common sense.",2010-09-03T03:46:11.643,10229,CC BY-SA 2.5,
2677,2328,1,Do you really need NN? This method is rather considered obsolete (partially because it is very hard to generally answer your questions).,2010-09-03T06:36:53.673,,CC BY-SA 2.5,user88
2678,707,0,"@shabbychef You've got me here; the second argument in my first comment is of course a junk, I had some other works in mind and overgeneralized. Nevertheless, I will still argue that Shao's paper is not a good reference for general ""LOO fails for large N"" since its scope is reduced to linear models.",2010-09-03T06:43:08.900,,CC BY-SA 2.5,user88
2679,2314,0,"not it is not cheating, since CV shows you the approximation how algorithm will perform on new data. You only need to be sure that you haven't settled on something based on the whole set (this is a leak of information about the structure of the full set, so it can immediately bias all train parts).",2010-09-03T07:15:11.827,,CC BY-SA 2.5,user88
2680,2318,0,"I was making the point that FS doesn't necessarily improve predictive importance, and can make it worse.  If finding the informative features is of intrinsic importance, then of course FS should be used, but it may well be that predictive performance is compromised if over-fitting the feature selection criterion occurrs (which happens rather easily).  For tasks like micro-array analysis, I would use (bagged) ridge regression for predictions and something like the LASSO for determing the key features (for gaining understanding of the biology).  There is no need to do both in the same model.",2010-09-03T07:28:20.430,887,CC BY-SA 2.5,
2681,2314,0,"@mbq - Ankino is correct, tuning your model to minimise a CV statistic is ""cheating"" and the CV statistic of the final model will have a substantial optimistic bias. The reason for this is that the CV statistic has a non-negligible variance (as it is evaluated on a finite set of data) and thus if you directly optimise the CV statistic you can over-fit it and you can end up with a model that generalises less well than the one you started with.  For a demonstration of this, in a machine learning context, see

http://jmlr.csail.mit.edu/papers/v11/cawley10a.html

Solution: Use nested XVAL",2010-09-03T07:40:01.317,887,CC BY-SA 2.5,
2683,2337,0,"Ideally, the question should be self-contained. Could you please give the relevant formulas as well? By the way, I may not have understood your question properly but won't euclidean distance give the same weight to all the variables? That metric seems to be one of the distance measures offered by the library you are using.",2010-09-03T10:42:31.263,,CC BY-SA 2.5,user28
2684,2326,1,To add to Jeromy's comment. It would be nice if you could state your research question. So far I am pretty confused.,2010-09-03T10:50:09.453,442,CC BY-SA 2.5,
2686,2338,0,"Hello Mike. Thank you for the answer, and for your package - it is really great!",2010-09-03T11:05:41.337,253,CC BY-SA 2.5,
2689,2281,0,"@ Keith : I see your point. So in the Bayesian approach, I take a diffuse prior to construct the same interval and call it a credible interval. In a Frequentist approach, if I can prove with absolute certainty that the interval is wrong, I have either violated assumptions, or I know the true value. In either case, the 95% confidence interval is not valid any more. The assumptions involved imply a diffuse prior, i.e. a complete lack of knowledge about the true parameter. If I have prior knowledge I shouldn't calculate a confidence interval in the first place.",2010-09-03T11:23:47.263,1124,CC BY-SA 2.5,
2691,2266,0,"@Bart. The expected value is a theoretical concept. Statistics is used to approximate it. If you can calculate the true expected value, there is no need for a confidence/credibility interval any more, because (Frequentist) the chance the true expected value is the true expected value, is 1, or (Bayesian) your true expected value is not a random variable. I gave you the frequentist approach, no strict priors involved. You use the information of the other products by calculating a common standard deviation. So if you want a Bayesian approach, my answer is indeed not what you're looking for.",2010-09-03T11:50:30.047,1124,CC BY-SA 2.5,
2692,2335,0,Could you be more precise about what you mean by cross-tabulating the ORs? Your factors have more than two levels?,2010-09-03T12:50:06.580,930,CC BY-SA 2.5,
2693,2335,0,"Yes, the factors have 3 and 6 levels respectively. I'm wanting a table of what the predicted odds are for each possible combination of `fac1` and `fac2`.",2010-09-03T13:22:42.680,229,CC BY-SA 2.5,
2695,2342,0,"Excellent! This is exactly what I was looking for, thanks!",2010-09-03T14:01:30.963,229,CC BY-SA 2.5,
2696,2298,0,"@Shabbychef: Right, your stipulation of standard normals establishes you want to know $f$ in a neighborhood of the origin that doesn't extend more than a few units away. RSM occurs to me because it includes methods that *efficiently* explore the behavior of a function, so if evaluating $f$ is expensive, RSM methods may be helpful. If evaluating it is not expensive, you have a purely mathematical problem. If each evaluation for a given set of arguments can produce a variety of results, you need the $\epsilon$ term--but maybe you can eliminate it by averaging multiple evaluations at each point?",2010-09-03T14:15:19.337,919,CC BY-SA 2.5,
2697,2337,0,"instead of ""(multiply each dimension with 1/average of that dimension)"" don't you mean to multiply by the reciprocal of the *standard deviation* of the corresponding coordinates?  This is not a bad choice but it's sensitive to outliers.  Rescaling by a resistant measure of spread, such as the IRQ, would protect you from outliers and still accomplish what you intend.",2010-09-03T14:19:10.310,919,CC BY-SA 2.5,
2698,2345,2,Maybe it should be a comment? (even if it is a good comment :) ),2010-09-03T14:24:08.980,223,CC BY-SA 2.5,
2699,2344,2,There is not a single tree... But see @Shane's response for plotting one of them for illustrative purpose.,2010-09-03T14:48:41.973,930,CC BY-SA 2.5,
2700,2335,0,"Ok, @Bernd's answer is fine with me. Maybe have a look at the `Design` package from Franck Harrell; it has very nice functions along `lrm()` for GLMs and related stuff.",2010-09-03T14:52:41.900,930,CC BY-SA 2.5,
2701,2340,1,Thanks for your answer. I just found a useful paper on that issue: http://dx.doi.org/10.1007/BF01897163. It compares the usefulness of different normalization approaches with each other and with no normalization.,2010-09-03T15:05:22.917,977,CC BY-SA 2.5,
2702,2337,0,"@Srikant Vadali: I wanted to post the formulas, but I couldn't find a way to embed them here, and ASCII-formulas would have made things worse instead of better. Also, the book I referenced is freely available as an ebook, so everyone can look it up.",2010-09-03T15:07:17.560,977,CC BY-SA 2.5,
2703,2337,0,Check out http://meta.stats.stackexchange.com/q/386/930 for LaTeX typesetting,2010-09-03T15:09:58.353,930,CC BY-SA 2.5,
2704,2348,0,"Perhaps, CW? The question seems like a poll of what people use. How would you define the 'best' answer? I admit that I am a bit fuzzy when to apply CW. So, feel free to ignore this comment if you feel otherwise.",2010-09-03T15:22:57.730,,CC BY-SA 2.5,user28
2705,2281,2,"No, I'm afraid you still haven't quite got it. There is no requirement for a ""diffuse prior"" in either case. It is fine to calculate a confidence interval whether you have prior knowledge or not -- the point is that the confidence interval just doesn't care. A confidence interval guarantees its coverage probability absolutely, even in the worst case. It will not be ""the same interval"" as a credibility interval informed by a prior, at least not in general.",2010-09-03T15:47:27.160,1122,CC BY-SA 2.5,
2706,2281,2,"And as I said, it is perfectly acceptable, formally speaking, that at the end of your experiment you arrive at a particular confidence interval that you can prove does not cover the true value. This does NOT mean the interval was invalid or that it's not a 95% confidence interval. Of course if you reran the same experiment 100 times you must expect to get such a nonsense result less than 5 of those times, but the fact that you get provable nonsense 5% of the runs is formally okay as long as the confidence interval covers the value the other 95% of the outcomes.",2010-09-03T15:50:52.437,1122,CC BY-SA 2.5,
2707,2281,1,"And the transpose is true for a credibility interval -- it is perfectly acceptable to have values of the parameter that produce a credibility interval that's always wrong! As long as your prior says those values are rare.

Imagine a bag containing a trillion weighted coins -- one of which has heads probability 10%, and the rest are fair coins. Your experiment is: draw a coin from this distribution, flip it ten times, count the discrete # of heads, then state 95% credible interval on heads prob. If you get the ""10%"" coin the interval will ALWAYS FAIL TO COVER. Again, doesn't make it invalid.",2010-09-03T15:57:56.950,1122,CC BY-SA 2.5,
2708,2348,1,"I wouldn't mind leaving this as non-CW, especially if Colin can rephrase it slightly to allow for the possibility of one best answer.  That said, I can't envision how to do that...",2010-09-03T16:05:09.193,5,CC BY-SA 2.5,
2709,707,0,"@mbq I'm curious enough about this that I think it should be a question in its own right. I would guess that Shao's work extends to variable selection under logistic regression or Fisher LDA. possibly to classification with kernel methods for finite kernel space (e.g. polynomial kernels), b/c the model is linear in the kernel space. but I have no good intuition for whether it is appropriate for comparing models, using different kernels, say. I hope somebody has tested the results in these arenas. The fact that LOO is _impractical_ for large $n$ is another issue...",2010-09-03T16:07:06.473,795,CC BY-SA 2.5,
2710,2281,4,"In one of Jaynes papers

http://bayes.wustl.edu/etj/articles/confidence.pdf

He constructs a confidence interval and then shows that for the particular sample you can be 100% sure that the true value does not lie in the ""confidence interval"".  That doesn't mean that the CI is ""wrong"", it is just that a frequentist confidence interval is not an answer to the question ""what is the interval that contains the true value of the statistic with probability 95%"".  Sadly that is the question we would like to ask, which is why the CI is often interpreted as if it was an answer to that question. :-(",2010-09-03T16:24:03.463,887,CC BY-SA 2.5,
2712,2200,0,"before the question was edited to have proper LaTeX, when I answered it, it was very hard to parse. Look at this in plaintext: 'Max value of intersection would be min(a,b). how do I find the min?' 
With the proper markup, it is much easier to parse. (Though it looks like a homework question to me...)",2010-09-03T16:30:58.677,795,CC BY-SA 2.5,
2714,2298,0,"@whuber: as these things go, eventually $k$ will be sufficiently large, and the code will be sufficiently encumbered with bells and whistles that some sampling technique will have to be employed. I am not there yet, though.",2010-09-03T17:25:45.633,795,CC BY-SA 2.5,
2715,2298,0,"@Shabbychef: That points away from answers proffered by eng-carlin (because dimensionality reduction won't accomplish anything) and Josh Hermann (because a tornado chart looks purely at one coordinate at a time and further reduces the graph of $f$ from one dimension to zero) and favors answers akin to Jeromy Anglim's.  Along those lines, Mathematica offers great utilities to interpolate and visualize functions, given either as formulas or as sampled data.",2010-09-03T17:37:41.017,919,CC BY-SA 2.5,
2717,2352,0,There must be something in chemometrics books; the only man I know that uses LOO is also doing it.,2010-09-03T17:51:55.360,,CC BY-SA 2.5,user88
2718,42,0,how's the performance of this? (I run screaming whenever I see the word 'Java'...),2010-09-03T18:01:28.507,795,CC BY-SA 2.5,
2719,2350,2,"Just to be clear: Are you applying RFs in an iterative manner, that is by selecting the top-ranked features (according to Gini index or decrease in MSE) from the whole input space? I know that RFs may not necessarily improve with increasing trees number, but what you are depicting seems to make sense (the best features allow to predict with 100% accuracy the OOB samples), although there is clearly a risk of overfitting when proceeding this way.",2010-09-03T18:08:07.913,930,CC BY-SA 2.5,
2720,2326,0,"Okay, more detail - you got it ;)",2010-09-03T18:10:36.163,1114,CC BY-SA 2.5,
2721,2345,0,"@robin-girard Perhaps it should. Actually I started it as a comment, but then decided that it does answer one of the questions: is the probability of the tree 0.25 or 0.5.",2010-09-03T18:12:36.690,279,CC BY-SA 2.5,
2722,42,0,"@shabbychef Seem quite good from what I've heard, but generally Weka is used as a first step to test several algorithms and look at their performance on given data sets (or a subset thereof), then one recode part of the core program to optimize its efficiency (e.g. with high-dimensional data calling for cross-validation or bootstraping), sometimes in C or Python.",2010-09-03T18:14:12.297,930,CC BY-SA 2.5,
2723,9,1,"@James + http://j.mp/9fVmtV, http://j.mp/aNDyf2, http://j.mp/9Gzzri :-)",2010-09-03T18:19:39.867,930,CC BY-SA 2.5,
2724,2304,0,Bump for rggobi. I just downloaded it after reading @shabbychef's comment and it changed/rocked my world.,2010-09-03T18:39:20.043,1076,CC BY-SA 2.5,
2725,2356,28,"Under rather mild assumptions, (a) Bayesian estimation procedures are admissible and (b) all, or almost all, admissible estimators are Bayesian with respect to some prior.  Thus it's no surprise that a Bayesian confidence interval ""yields the same or better results.""  Note that my statements (a) and (b) are part of the *frequentist* analysis of rational decision theory.  Where frequentists part company with Bayesians is not over the mathematics or even the statistical procedures, but concerns the meaning, justification, and correct use of a prior for any particular problem.",2010-09-03T18:48:20.173,919,CC BY-SA 2.5,
2727,2251,3,"A huge plus one for point 2.  Other than going through human subjects protection training, I've never received the tiniest bit of training on data collection and storage.  Getting the data collection right is vastly more important than the analysis.",2010-09-03T18:54:34.447,71,CC BY-SA 2.5,
2728,2346,4,Small extension about plots: `plot.randomForest` shows how OOB error and in-class OOB error evolved with increasing number of trees; `varImpPlot` shows attribute importance measures for top attributes and `MDSplot` all objects plotted on the 2D projection of RF object proximity measure.,2010-09-03T19:25:18.740,,CC BY-SA 2.5,user88
2730,2356,3,"So, does the above comment imply that the answer to the OP's question is 'No such examples can be constructed.'? Or perhaps, some pathological example exists which violates the assumptions behind admissibility?",2010-09-03T19:44:08.477,,CC BY-SA 2.5,user28
2731,2346,0,+1 for citing the `MDSplot()` function. I must admit I often use RFs as a way to highlight clusters of individuals (based on the RF proximity measure) rather than selecting the best features. Clinicians often read much easily such plots than dotplot of var. importance...,2010-09-03T20:07:49.980,930,CC BY-SA 2.5,
2732,2365,6,"It is possible to be interested in the differences between Bayesian and frequentist statistics without it being a quarrel. It is important to know the flaws as well as benefits of ones preferred approach.

I specifically excluded priors as that is not a problem with the framework, per se, but just a matter of GIGO.  The same thing applies to frequentists statistics, for example by assuming and incorrect parametric distribution for the data.  That wouldn't be a criticism of frequentist methodology, just the particular method.

BTW, I have no particular problem with improper priors.",2010-09-03T20:52:49.227,887,CC BY-SA 2.5,
2733,2221,0,"Thanks for this response. To follow-up: is the solution to the the ""other formulation"" (wind vectors), 1/3, correct given the issues that you raise?",2010-09-03T21:56:55.473,401,CC BY-SA 2.5,
2734,2365,3,"Jaynes first example: Not one statistician in his right mind will ever use an F-test and a T-test on that dataset. Apart from that, he compares a two-tailed test to P(b>a), which is not the same hypothesis tested. So his example is not fair, which he essentially admits later on. Next to that, you can't compare ""the frameworks"". What are we talking about then? ML, REML, LS, penalized methods,...? intervals for coefficients, statistics, predictions,...? You can as well ask whether Lutheran service is equivalent or superior to Shiite services. They talk about the same God.",2010-09-03T22:22:11.210,1124,CC BY-SA 2.5,
2735,2365,0,"Could you clarify what is your data and what are the parameters you  would be estimating in your model? I am a bit confused on this point. Also, could you please use $$ instead of $ to center the formula? The font size is very small right now.",2010-09-03T22:23:09.330,,CC BY-SA 2.5,user28
2736,2356,4,"@Srikant: Good question.  I think the place to begin investigating is a situation where there are non-Bayes admissible estimators--not necessarily a ""pathological"" one, but at least one that provides some opportunity to find a ""contrary example.""",2010-09-03T22:29:08.203,919,CC BY-SA 2.5,
2737,2365,0,"@Srikant: The example in Felsensteins book is based on a Jukes-Cantor model for DNA evolution. Data is DNA sequences. You want to estimate the probability of change in your sequence, which is related to your branch length based on the mentioned formula. Branch lengths are defined as time of evolution : the higher the chance for changes, the more time that passed between the ancestor and the current state. Sorry, but I can't summarize the whole theory behind ML and Bayesian phylogenetic inference in just one post. Felsenstein needed half a book for that.",2010-09-03T22:29:34.647,1124,CC BY-SA 2.5,
2738,2365,0,I guess I just wanted you to clarify what variables in your equation was data and which ones were the parameter as it was not clear from your post especially to someone like me who is an outsider. I am still lost but I guess I would need to read the book to find out more.,2010-09-03T22:44:20.240,,CC BY-SA 2.5,user28
2739,2365,0,"@Srikant : I tried to clarify a bit more. Actually, P is the parameter that is used in the likelihood function for optimization, and the formula merely gives its relation to t, which can alternatively be used in the likelihood function. Sorry I can't be more clear. If Phylogeny interests you, I can surely recommend Felsensteins book, it's a gem. http://www.sinauer.com/detail.php?id=1775",2010-09-03T23:04:10.970,1124,CC BY-SA 2.5,
2740,2281,0,"@Keith: I'm not getting it. If you mean that the 10% coin only gives head 1 in 10 times, and you end up with 0 heads, you cannot compute a confidence interval. If you have 1 head in ten times, your interval will indeed not cover 50%. But I never claimed it covered. I just claimed it is unlikely it doesn't cover. I do NOT know the true value. Plus, all CI (Wald, score,Pearson,...) have a bad coverage on the edges of the probability space, definitely with only 10 cases. So I wouldn't state anything based on that CI. I'd use probability calculation to come to a conclusion. Like Bayes did.",2010-09-03T23:35:53.847,1124,CC BY-SA 2.5,
2741,2365,0,"It isn't clear to me why it is a problem that a flat prior on p implies an exponentially decreasing prior on t.  If that is inconsistent with biological knowledge, it simply means that a flat prior on p does not reflect actual prior knowledge.

I also don't see why it is a problem to use an improper flat prior on t (other than I would have thought it inconsistent with prior knowledge; the branch time can't be say a billion years, if it were we wouldn't be here yet, so it is inappropriate to use a flat prior).

Note that flat priors don't necessarily imply ignorance.",2010-09-03T23:45:34.603,887,CC BY-SA 2.5,
2742,2281,0,@Keith : but I got your point - the true value is not a random variable - I agree. My bad.,2010-09-03T23:47:27.230,1124,CC BY-SA 2.5,
2743,2365,0,"@Dikran : it's not a problem. It is a fact. The problem is that p and t are strictly related, and hence should give exactly the same model. That happens in an ML approach, but that doesn't happen in the Bayesian approach. In Felsensteins example, a truncation of the t-prior at 700 or larger makes that the credible interval doesn't cover the true value any more. In this particular case, i.e. the lack of prior knowledge, Bayesian inference just isn't feasible. There is no sensible ""uninformative"" prior that can be used.",2010-09-04T00:08:47.533,1124,CC BY-SA 2.5,
2744,2365,0,"@Dikran : Regarding the flat t-prior: the prior gets truncated. When truncated at 5(!), most of the mass of the prior on p is concentrated around the maximum p-value. With larger truncation values, this effect is even more pronounced. The point is-again- that it's impossible to find a sensible prior when you have no prior knowledge in phylogenetic inference.",2010-09-04T00:11:10.677,1124,CC BY-SA 2.5,
2745,2281,1,"Joris, my last comment was about a ""95% credible interval"" -- not confidence interval! If you have a bag with one trillion fair coins and a single 10%-heads coin, and your experiment has you draw a coin uniformly at random from the bag, flip it ten times and then state a credibility interval on the heads probability, your credibility interval will always be [0.5, 0.5] no matter what. Thus if you happened to draw the unfair coin, the credibility interval will always be wrong.",2010-09-04T03:59:07.460,1122,CC BY-SA 2.5,
2746,2281,0,"Also I can't agree that ""all CI"" have bad coverage on the edges. Any exact CI, and some approximate CIs, will guarantee that the coverage is always greater than the confidence parameter (e.g. the 95%), even in the worst case. This is true of the Blyth-Still-Casella and Clopper-Pearson intervals for a proportion.",2010-09-04T04:02:57.267,1122,CC BY-SA 2.5,
2747,2367,1,I believe Shao showed that k-fold CV is inconsistent if $k$ is fixed while $n$ grows.,2010-09-04T05:18:10.490,795,CC BY-SA 2.5,
2748,2367,1,The BIC has k growing with n.,2010-09-04T05:56:26.920,159,CC BY-SA 2.5,
2749,2365,1,"Joris, I think you are missing the point, a flat prior is not necessarily non-informative.  It is completely reasonable for the same state of knowledge/ignorance to be expressed by a flat prior on p and (say) a flat prior on log(t) (which is a very common Jeffrey's prior) rather than a flat prior on t.  Does the book investigate ideas of MAXENT and transformation groups for this problem? 

There isn't enough detail in your example, but from what I can tell even a truncated flat prior on t is likely to be inconsistent with prior knowedge about t.",2010-09-04T08:39:52.407,887,CC BY-SA 2.5,
2750,2365,0,"@Joris, also in your original comment you suggest the flat prior on t must be truncated, because otherwise the area under the density function is infinite.  This is not true, there are plenty of problems where improper priors work very well, so there is not necessarily a need to truncate the flat prior.",2010-09-04T08:49:51.587,887,CC BY-SA 2.5,
2752,2369,7,"Food for thought, however the particular example is unfair as the frequentist approach is allowed to consider the relative costs of false-positive and false-negative costs, but the Bayesian approach isn't.  The correct thing to do according to Bayesian decision theory is to give an interval of [0,1] as there is no penalty associated with false-negatives.  Thus in a like-for-like comparison of frameworks, none of the Bayesians would ever be beheaded either.

The issue about bounding false-positives though gives me a direction in which to look for an answer to Jaynes' challenge.",2010-09-04T09:10:21.210,887,CC BY-SA 2.5,
2753,2369,1,"Note also that if the selected coin is flipped often enough, then eventually the Bayesian confidence interval will be centered on the long run frequency of heads for the particular coin rather than on the prior.  If my life depended on the interval containing the true probability of a head I wouldn't flip the coin just once!",2010-09-04T09:57:59.440,887,CC BY-SA 2.5,
2754,2348,0,"I've tried to change the question to make it less CW - not sure if I have succeeded :( @Shane @Srikant if you still think that it should be a CW, feel free to change it.",2010-09-04T10:50:46.140,8,CC BY-SA 2.5,
2755,2374,0,"If I understand correctly, your question encompasses at least two different topics: (1) use of FA in attitude or motivational scales, and (2) how to handle 'extreme' patterns of responses (ceiling/floor effects) in such scales?",2010-09-04T11:32:56.387,930,CC BY-SA 2.5,
2756,2372,0,"Technically, you are correct but do note that the confidence interval gives the set of parameter values for which the null hypothesis is true. Thus, ""are the observed data x reasonable given our hypothesis about theta?"" can be re-phrased as ""What true values of theta would be a compatible hypothesis given the observed data x?"" Note that the re-phrased question does not necessarily imply that theta is being assumed to be a random variable. The re-phrased question exploits the fact that we perform null hypothesis tests by inspecting if the hypothesized value falls in the confidence interval.",2010-09-04T15:32:22.633,,CC BY-SA 2.5,user28
2757,2186,0,"Just because it is the only book I know which combines exploratory MV analysis, statistical modeling, and psychometrics. Maybe not the best one actually, but interesting on its own.",2010-09-04T16:59:16.127,930,CC BY-SA 2.5,
2758,2367,1,"I will just silently remind that *IC <--> *CV correspondence from Shao paper works **only** for linear models, and BIC is equivalent only to k-fold CV with certain k.",2010-09-04T17:26:08.060,,CC BY-SA 2.5,user88
2759,2111,2,"Find this article in the meantime, The lasso method for variable selection in the cox model, from Tibshirani (Stat. Med. 1997 16: 385-395), http://j.mp/bw0mB9. HTH",2010-09-04T21:30:44.700,930,CC BY-SA 2.5,
2760,2111,1,"and this more recent one (closely linked to the `penalized` R package), http://j.mp/cooIT3. Maybe this one too, http://j.mp/bkDQUj. Cheers",2010-09-05T08:29:41.960,930,CC BY-SA 2.5,
2761,2385,2,"The aim of visualization is not to show the data, but to show a claim that this data supports -- so first try to focus on what you want to show, to make this question answerable. Or, if you just want a pool of visualization ideas, make it a community wiki.",2010-09-05T13:08:37.680,,CC BY-SA 2.5,user88
2762,2385,0,"@mbq I don't think it is that ambiguous. He wants to compare means of the scales between different groups, as well as the response rates between different groups. Visualization can be exploratory (the fact that he is making a report to senior management doesn't mean to me that it can't be exploratory).",2010-09-05T13:45:01.203,1036,CC BY-SA 2.5,
2763,2380,2,May I suggest that you edit your main question to include this information?,2010-09-05T15:50:49.080,196,CC BY-SA 2.5,
2764,2377,0,Do you require that the standard deviation remain constant with this transformation as well?,2010-09-05T16:04:35.903,196,CC BY-SA 2.5,
2765,2385,0,@Andy so my suggestion for CW; even exploratory visualization is made to check *something*.,2010-09-05T19:32:13.847,,CC BY-SA 2.5,user88
2766,2365,0,"@Dikran : Guess you are missing the point : using the same uninformative prior gives two different models with Bayesian statistics on the same dataset. Not so with ML. The Bayesian can be very biased due to the nature of the model and the incompatibility of that model with infinite priors. You don't have to believe me. Felsenstein is the authority on phylogenetic inference, and his book explains you better than I will be able to. Reference in a previous comment.",2010-09-05T20:02:47.387,1124,CC BY-SA 2.5,
2767,2397,0,What exactly do you mean by limiting distribution in this case?,2010-09-05T21:24:39.593,352,CC BY-SA 2.5,
2768,2397,0,"ie, the one you get from Central Limit Theorem, let me update details",2010-09-05T21:28:01.960,511,CC BY-SA 2.5,
2769,2398,1,"but the covariance matrix of multinomial in question is singular, you showed it yourself...",2010-09-05T22:26:32.703,511,CC BY-SA 2.5,
2770,2398,0,"Oh, I see your problem! One of the elements, say the $d$th is completely dependent on the others. Probably if you chop off the last row and column of $C$ you will get that the $[p_1,p_2,\dots,p_{d-1}]$ are normally distributed, but I'll have to have a think about it.  Surely this is solved somewhere already!",2010-09-05T22:41:08.137,352,CC BY-SA 2.5,
2772,2365,0,"@Joris, as I said a flat prior is NOT NECCESSARILY UNINFORMATIVE. Consider this, if two priors give different results, then the must logically represent a different state of prior knowledge (see early chapters of Jaynes book that set out desiderata for Baysian inference).  Therefore the ""flat p"" prior and ""flat t"" prior cannot both be uninformative.  Felsenstein may be an expert on phylogenetic inference, but it is possible that he is not an expert on Bayesian inference. If he states that two priors giving different results are both uninformative, he is at odds with Jaynes (who certailny was).",2010-09-05T23:09:15.037,887,CC BY-SA 2.5,
2773,2398,0,"One suggestion I found is to still use a Gaussian, but use pseudo-inverse instead of inverse and ""product of non-zero eigenvalues"" in place of determinant. For d=2 this seems to give the correct density form, but the normalization factor is off",2010-09-05T23:26:10.103,511,CC BY-SA 2.5,
2775,2400,1,"So then, sigma is the entropy of N(0,sigma) corresponding to squared error, and min(p,1-p) is the entropy of Bernoulli(p) corresponding to 0,1 prediction loss? Seems like quite a generalization!",2010-09-06T00:03:45.560,511,CC BY-SA 2.5,
2777,2401,2,"Do you know of any published research that looks at the relative merits of using likert scales as intervals vs ordinal data? Perhaps, the flaws of treating them as interval level scales are not serious enough to warrant a complex approach. If that is the case then your approach may simply be a wild goose chase.",2010-09-06T01:12:06.877,,CC BY-SA 2.5,user28
2778,2377,0,"no, I expect it will not, but the excess kurtosis should remain fixed. I would expect the transform to be monotonic, however, and preferably deterministic.",2010-09-06T02:23:41.060,795,CC BY-SA 2.5,
2779,2392,0,"I had been doing something like this: rank, then use the g-and-h transform to get a fixed kurtosis and skew. However, this technique assumes I actually know the population kurtosis, which I can estimate, but I am interested, philosophically, if there is a transform which preserves the kurtosis without me having to know what it is...",2010-09-06T02:26:04.253,795,CC BY-SA 2.5,
2780,2392,0,"@shabbychef:  Oh, well then sorry for not adding anything new. However, you added something new, I hadn't heard of the g-and-h formula before.  Do you have a freely accessible citation that provides it?  I stumbled onto one paper with it spelled out (http://fic.wharton.upenn.edu/fic/papers/02/0225.pdf) but the notion is a bit foreign to me (in particular is that e^Z^g or something else)?  I tried it like this... but the results seemed odd... a+b*(e^g^z-1)*(exp((h*z^2)/2)/g).",2010-09-06T05:04:41.457,196,CC BY-SA 2.5,
2781,2377,1,Yikes - woe unto the person that wants to prove a non-deterministic function is monotonic.,2010-09-06T05:05:54.120,196,CC BY-SA 2.5,
2782,2386,2,"There is nothing wrong with a scientist ""believing"" in something, especially as a Bayesian probability represents the degree of belief or knowledge regarding the truth of some proposition.",2010-09-06T07:25:56.310,887,CC BY-SA 2.5,
2783,2386,2,"...The problem arises only when a scientist cannot distinguish between a belief and a fact.  There is nothing unscientific in the belief that Bayesian or frequentist statistics are superior, as there is no objective test that can decide the answer (AFAIK), so the choice is largely subjective and/or a matter of ""horses for courses"".",2010-09-06T07:36:00.367,887,CC BY-SA 2.5,
2785,2378,0,I realise that there are countless variants of this algorithm. What I was interested in was which ones do people routinely use.,2010-09-06T07:50:24.937,8,CC BY-SA 2.5,
2788,2369,1,"Having though about this a bit more, this example is invalid as the criterion used to measure success is not the same as that implied by the question posed by the king.  The problem is in the ""no matter which coin is drawn"", a clause that is designed to trip up any method that uses the prior knowledge about the rarity of the biased coin.  As it happens, Bayesains can derive bounds as well (e.g. PAC bounds) and if asked would have done so, and I suspect the answer would be the same as the Clopper-Pearson interval.  To be a fair test, the same information must be given to both approaches.",2010-09-06T08:29:18.367,887,CC BY-SA 2.5,
2789,2345,0,"Thanks, I added some suggestion on one possible equivalence relation such that formula (2) would apply (I would have liked to add this as a comment but the number of characters was too high).",2010-09-06T08:46:29.947,1185,CC BY-SA 2.5,
2790,2406,1,"I would put it: ""Of the X% American/Australian/French/etc. adults who intend to buy a television, Y% intends to buy brand A, Z% intend to buy brand B,..."" So you immediately see both information in one sentence.",2010-09-06T08:49:23.627,442,CC BY-SA 2.5,
2791,2350,0,Yes that's what I am doing,2010-09-06T08:56:44.447,1150,CC BY-SA 2.5,
2793,2397,1,"What you're referring to is the asymptotic distribution of the **maximum likelihood estimator** of a multinomial. Also, the first equation should be n^{-1}, not n^{-1/2}.",2010-09-06T10:12:10.567,495,CC BY-SA 2.5,
2794,2406,0,@Henrik. Thanks. I agree. I guess I was trying to put it in slightly more general terms.,2010-09-06T10:16:50.120,183,CC BY-SA 2.5,
2796,2388,0,The use of box-plots seems doubtful there. I just try to plot 190 bxp on the same page and this doesn't seem to carry out so much information as can be seen here: http://j.mp/925AY8. P.S. I didn't even use the R base graphics but relied on the `grid` package for the layout. @Shane's suggestion might be better.,2010-09-06T14:23:22.810,930,CC BY-SA 2.5,
2797,2388,0,@chl Your right that is difficult to envision. The thinner you make the plots the more it just turns into a trend (like a time series). I don't think that would be a problem except that there is likely no logical ordering to the groups in this posters problem.,2010-09-06T14:47:28.517,1036,CC BY-SA 2.5,
2798,1297,3,"Also, isn't it possible to get CI (for the predictions) using bootstrapping ?",2010-09-06T14:58:08.747,253,CC BY-SA 2.5,
2800,2412,2,"Not to spoil the party, but how are random forest supposed to provided robustness to contamination by outliers is a mystery.",2010-09-06T15:27:04.850,603,CC BY-SA 2.5,
2801,2411,0,"Thank you kwak.  This article seems to be talking about boosting methods.  Do the results they present hold for the simple classifier case of a CART model? (on the surface it sounds like it, but I didn't go through the article enough to really know)",2010-09-06T15:28:18.840,253,CC BY-SA 2.5,
2802,2411,0,"The result they present holds for any convex loss function, and was initially discussed by Tukey. To sum things up, the measure of spread (Gini or entropy) used to quantify the quality of a node is sensitive to contamination by outliers (i.e. observations that are miss-labeled in the dataset).  This problem affects both the building and the prunning stage. Contamination of a dataset by observation with wrongly imputed label will typically cause the resulting tree to be much too complex (you can check this rather easely by yourself).",2010-09-06T15:38:22.293,603,CC BY-SA 2.5,
2803,2362,0,So @mbq when doing a CV is it valid to firstly do a random forest with all samples selected; doing it twice once with all and secondly with the top 10 variables (which can be quoted in a paper).  Then do a leave-one out cross-validation (selecting the 10 top genes each try) and quote the CV error from that?,2010-09-06T15:47:14.057,1150,CC BY-SA 2.5,
2804,2365,0,"@Dikran : The point is not whether a flat prior is uninformative. The point is that an satisfying uninformative prior cannot be defined due to the nature of the model. Hence rendering the whole method unusable if you don't have prior information, and thus leading to the conclusion that Bayesian inference in this case is inferior to the ML approach. Felsenstein never said a flat prior was uninformative. He just illustrated why an uninformative prior cannot be determined, using the example of a flat prior.",2010-09-06T16:03:44.807,1124,CC BY-SA 2.5,
2805,2411,0,Thank you Kwak!  And is there no loss function that is robust?,2010-09-06T16:11:35.113,253,CC BY-SA 2.5,
2806,2411,1,"no *convex* loss function. See this article ""A fast algorithm for the minimum covariance determinant estimator"" for an example of what can be done with non-convex loss functions (although not related to classification, the article is worth a read).",2010-09-06T16:14:31.003,603,CC BY-SA 2.5,
2808,2365,0,"@Joris - it may be that an uninformative prior cannot be constructed in this case, but nothing that you have written so far establishes that to be the case.  What does Felsenstein write about MAXENT and transformation groups (the two main techniques used to determine an uninformative prior for a particular problem)?  If he has not investigated those methods, how can he know an uninformative prior is impossible?  It looks to me that a flat prior on p corresponds a flat prior on log(t), which is a well known Jeffreys' prior.  Can you demonstrate that the flat log(t) prior is informative?",2010-09-06T16:22:09.630,887,CC BY-SA 2.5,
2809,2411,0,It is very enjoyable to ask questions when getting answers like the ones you give - thank you!  Last questions: Do you know of an easy way to implement such a loss function in R's CART? (rpart),2010-09-06T16:29:18.837,253,CC BY-SA 2.5,
2810,2411,0,The problem you point out seems easier to solve for regression than classification trees. I do not know how to change the spread criterion (from s.d. to Qn) in R but since Qn (and MAD) are implemented it should not be difficult. Best (let us know whether you had any success with a 'robustified' version).,2010-09-06T16:40:08.137,603,CC BY-SA 2.5,
2811,2394,3,"Good answer, John.  Note that kurtosis usually isn't worth looking at when a distribution is skewed: the skew already tells you there's going to be substantial kurtosis.  Thus, good applications are with datasets known a priori to be (approximately) symmetric.  Residuals often are this way: the kurtosis can be a quick numerical means to assess the shape of their distribution.  An additional point is that the sampling *variance* of the standard deviation is a function of the kurtosis, so if you're estimating SDs or variances, you might want to glance at the kurtosis to assess the precision.",2010-09-06T17:11:40.827,919,CC BY-SA 2.5,
2812,2379,6,Thank you for posting this.  It's an important (and potentially inspiring) discussion to have.,2010-09-06T17:27:30.127,919,CC BY-SA 2.5,
2813,2362,1,"@danielsbrewer I would do this in some other way (paying more attention to feature selection), but this is correct; yet it is more on the topic of benchmarking RF feature selection than on selecting best markers for your biological problem.",2010-09-06T17:39:35.890,,CC BY-SA 2.5,user88
2814,2411,2,"@Tal CART is an equivalent to boosting, of a ""pivot classifier"" (the criterion that sits in each tree node, like some attribute grater than something or some attribute value in set something).",2010-09-06T17:47:23.223,,CC BY-SA 2.5,user88
2815,2411,0,"True.  Since I was intending to try this on regression trees, I'll have a go at it in the next few weeks and will come back to report if something nice will be formalized.  Best.",2010-09-06T17:49:10.917,253,CC BY-SA 2.5,
2816,2412,3,"@kwak Still, this is a good answer; trees in RF don't see the entire set, so many of them will not be contaminated. Even better -- tracking in which leafs do OOB cases land can be used to find mislabeled objects and eliminate them. (As I recall now, this is mentioned in Breiman's paper about RF).",2010-09-06T17:50:57.683,,CC BY-SA 2.5,user88
2817,2397,1,"In notation above, for d=2, X_n is the number of heads after n coin throws, so it's X_n/sqrt(n) that approaches Normal, not X_n/n, no?",2010-09-06T18:11:51.080,511,CC BY-SA 2.5,
2819,2397,1,"Yes, you're right. I was just confusing myself.",2010-09-06T18:37:09.383,495,CC BY-SA 2.5,
2820,2408,0,"What's a bit unsatisfactory is the freedom of choice, to get a valid density I need to ask for distribution of A x where A is some d-1 rank (d)x(d-1) matrix. Will the error of CLT approximation for finite n be equivalent for all choices of A? That's not clear to me",2010-09-06T18:39:07.037,511,CC BY-SA 2.5,
2821,2406,0,He should make sure the sample size from the new group is not too small however,2010-09-06T18:45:49.710,74,CC BY-SA 2.5,
2822,2408,1,"Yes, the error should always be the same. Keep in mind that the last element of the vector is functionally dependent on the other (d-1) elements (in both the finite sample and asymptotic cases).",2010-09-06T18:46:39.127,495,CC BY-SA 2.5,
2823,2382,1,"I think statistics is a separate area from probability theory, though.",2010-09-06T19:31:01.530,1106,CC BY-SA 2.5,
2824,2419,3,"Rpy, of course ;-)",2010-09-06T19:31:49.947,,CC BY-SA 2.5,user88
2825,2415,1,What are ways in which people are trying to solve these problems?,2010-09-06T19:42:19.960,1106,CC BY-SA 2.5,
2826,2408,0,"It's not that the `last' element is dependent, Yaroslav's problem is that he doesn't like the idea of getting to choose which element to drop.  I agree with the answer you have given but I also think that a bit more thought and care is required here.",2010-09-06T21:30:33.580,352,CC BY-SA 2.5,
2827,2408,0,"@Yaroslav: Perhaps it would be good to have an idea of what application you have in mind here, because at this stage there are potentially lots of answers to your question.",2010-09-06T21:33:57.927,352,CC BY-SA 2.5,
2828,2415,3,"@grautur: That's four excellent questions (plus many more, because your response applies to every answer in this thread).  They all deserve elaborate answers, but obviously there's no space for that here: one question at a time, please!",2010-09-06T21:58:17.927,919,CC BY-SA 2.5,
2829,2427,7,Monte Carlo methods can give very accurate answers provided you do enough simulations.,2010-09-06T22:59:43.747,159,CC BY-SA 2.5,
2830,2056,1,"Yes, it does, but it describes resampling the subject means.  D&H advocate resampling the subjects and fitting the original model.",2010-09-06T23:00:55.163,187,CC BY-SA 2.5,
2832,2408,1,"Robby -- application I had in mind is here http://mathoverflow.net/questions/37582/how-big-is-the-sum-of-smallest-multinomial-coefficients  Basically integrals of Gaussian suggested by CLT give extremely good approximation to sums of binomial coefficients (for small n, even better than integrating Gamma representation directly!), so I was seeing if I can do something similar to get approximate sums of multinomial coefficients, which I need to get non-asymptotic error bounds for various fitters (like, maximum likelihood)",2010-09-06T23:23:45.367,511,CC BY-SA 2.5,
2834,2427,3,If you want a programming solution a monte carlo is the only approach. I do not see any reason why you will not get accurate answers using monte carlo. A mathematical/analytical solution may not be easy.,2010-09-06T23:49:37.943,,CC BY-SA 2.5,user28
2835,2427,0,"I have seen discussion about Monte Carlo and people said if you want to achieve 6 decimal places, it will take too long, or perhaps I'm confused with other similar problems. Since it's fairly easy to code a Monte Carlo approach, I guess it will be worthwhile to give it a try first.",2010-09-07T01:14:38.150,18,CC BY-SA 2.5,
2836,2420,2,"Technically speaking, a histogram is used to show summary information about a frequency distribution. I think you want to ask ""Is a bar chart satisfactory?"", to which I would answer Yes. If there are repeated measures on the categorical variables, then you can capture variability with a box plot, as mentioned in csgillespie's comment.",2010-09-07T02:22:19.663,1080,CC BY-SA 2.5,
2837,2406,0,@Neil. Good point.,2010-09-07T03:15:54.313,183,CC BY-SA 2.5,
2838,2423,1,see also question on mathoverflow regarding books on mathematical statistics http://mathoverflow.net/questions/31655/statistics-for-mathematicians,2010-09-07T03:18:37.553,183,CC BY-SA 2.5,
2839,2431,1,"I don't think that's been around for a while, and it's not very comprehensive either...one reason I started this post was because I found an error there, and also couldn't find some things I needed, like density formula for functions of random vectors",2010-09-07T03:30:55.853,511,CC BY-SA 2.5,
2840,2369,1,"Dikran, there needn't be ""Bayesians"" and ""Frequentists."" They're not incompatible schools of philosophy to which one may subscribe to only one! They are mathematical tools whose efficacy can be demonstrated in the common framework of probability theory. My point is that IF the requirement is an absolute bound on false positives no matter the true value of the parameter, THEN a confidence interval is the method that accomplishes that. Of course we all agree on the same axioms of probability and the same answer can be derived many ways.",2010-09-07T04:55:38.120,1122,CC BY-SA 2.5,
2842,2432,1,"discontinuities at known x-values, or at unknown x-values? (known x is easy enough)",2010-09-07T07:04:27.317,805,CC BY-SA 2.5,
2843,2432,0,@glen I updated the question: I'm interested in situations where timing of discontinuities is not known apriori.,2010-09-07T07:08:14.553,183,CC BY-SA 2.5,
2844,2433,1,"Re LOESS: Perhaps my terminology is not quite right. By LOESS I'm referring to models that predict Y from X using some form of localised curve fitting. e.g., as seen in most of these graphs: http://www.google.com/images?um=1&hl=en&biw=1076&bih=569&tbs=isch:1&sa=1&q=lowess+graph&aq=f&aqi=&aql=&oq=&gs_rfai=",2010-09-07T07:12:49.097,183,CC BY-SA 2.5,
2845,755,0,"Like it, though it does remind me of http://en.wikipedia.org/wiki/Swiss_Toni",2010-09-07T07:18:02.573,449,CC BY-SA 2.5,
2846,2420,0,@Josh - see updated question -> aren't bar charts for displaying quantities? : are dates quantities ? -,2010-09-07T07:51:35.607,414,CC BY-SA 2.5,
2847,750,11,I think this quote is more of a cynical but realistic view of how statistical data is mostly used in debates (i.e. selected to support a preconceived notion rather than produced to test a hypothesis),2010-09-07T09:12:43.227,127,CC BY-SA 2.5,
2848,2362,0,"@mbq, sorry to go on, but I am very new to the machine leaning classification area.  So if you wanted to select the best discriminating markers for my sort of problem how would you go about it? and what error would you report?  I appreciate all the input.",2010-09-07T10:18:10.480,1150,CC BY-SA 2.5,
2852,2434,2,"Your answering a slightly different question than the question asks. The question is asking the expected number of cells that would be empty after 50 jumps. Correct me if I am wrong, but I see no direct path from the probability a flea ends up in a certain square after 50 jumps to the answer how many cells would be expected to be empty.",2010-09-07T13:24:19.570,1036,CC BY-SA 2.5,
2853,2431,0,"It may be useful if you email the author. Perhaps, it is a misunderstanding and if not it will help in correcting the error in the next edition.",2010-09-07T13:53:27.133,,CC BY-SA 2.5,user28
2854,2419,0,I agree with mbq. Is there a very good reason why you have to do that in Python? Otherwise I'd use the workhorse R as a back-end as well.,2010-09-07T13:54:16.430,1124,CC BY-SA 2.5,
2855,2281,0,"@Keith. I should specify ""bad"" coverage. Too much coverage is also bad coverage. I'll state it differently : on the edges, the exact coverage does not coincide with the chosen coverage.",2010-09-07T13:57:23.110,1124,CC BY-SA 2.5,
2857,2434,1,@Andy W -- great comment; yet Monte Carlo can be used to do this last step ;-),2010-09-07T14:19:51.443,,CC BY-SA 2.5,user88
2858,2423,3,"You could specify whether you need an introduction on applied statistics, or one on (theoretical) statistical inference. I.e., do you want the framework of testing, regression and ANOVA explained or do you want to know what the central limit theorem and the inequality of Chebiyshev have to do with the weak law of large numbers?",2010-09-07T15:11:25.727,1124,CC BY-SA 2.5,
2859,2430,3,You should keep in mind that achieving what you want might be impossible.,2010-09-07T15:33:30.707,279,CC BY-SA 2.5,
2860,2420,1,"@Tom - Yes, bar charts are used to display quantities, so you could use a simple dot plot or line chart too. Some would argue that bar charts have a low ""information-to-ink"" ratio, but it is a perfectly reasonable choice here. And yes, I would consider dates quantities, especially in this setting where the dates are ""days since some event"". My original comment was admittedly nitpicking: your original question asked ""For example is a histogram satisfactory..."", but histograms are for when the independent variable is discrete/continuous over some interval.",2010-09-07T15:41:06.107,1080,CC BY-SA 2.5,
2862,2427,4,"I don't dispute any of the three previous answers, but the (simple) analysis in the answer I have offered puts these remarks in perspective: if you want six decimal place accuracy for an estimate of a number that will be in the hundreds, the Monte Carlo simulation will take at least a year on a machine with 10,000 CPUs running in parallel.",2010-09-07T15:53:14.987,919,CC BY-SA 2.5,
2863,2429,1,"+1: i too learned from Lehman (very good reference), but this one is not mentionned nearly enough",2010-09-07T16:05:51.123,603,CC BY-SA 2.5,
2865,2420,0,"One more thing: you have some votes for box plots but can you clarify if you have a single date for each category, or if you have multiple dates?",2010-09-07T16:10:03.143,1080,CC BY-SA 2.5,
2867,2430,3,'You should keep in mind that achieving what you want might be impossible.' I will stick this quote on the door of my office.,2010-09-07T16:21:08.647,603,CC BY-SA 2.5,
2869,2392,1,"@drnexus: I did not want to bias the results by mentioning my technique. I learned about the g-and-h and the g-and-k distributions from Haynes et. al, http://dx.doi.org/10.1016/S0378-3758(97)00050-5 , and Fisher & Klein, http://econstor.eu/bitstream/10419/29578/1/614055873.pdf",2010-09-07T16:32:29.343,795,CC BY-SA 2.5,
2870,2431,0,"I have confirmation from author, so it'll probably be corrected in next edition",2010-09-07T16:40:39.233,511,CC BY-SA 2.5,
2871,2367,0,"Actually, I believe Shao shows that CV is inconsistent unless $n_v/n \to 1$ as $n \to \inf$, where $n_v$ is the number of samples in the test set. Thus $k$-fold CV is always inconsistent for variable selection. Have I misunderstood? By $k$-fold CV I mean dividing the sample into $k$ groups and training on $k-1$ of them, and testing on 1 of them, then repeating $k$ times. Then $n_v/n = 1/k$ for $k$-fold CV, which never approaches 1.",2010-09-07T16:46:57.707,795,CC BY-SA 2.5,
2872,2431,2,"Could you, with the permission of the author, post the error briefly as a comment here? That will be useful to everyone.",2010-09-07T17:04:10.053,,CC BY-SA 2.5,user28
2873,2423,0,see also question http://stats.stackexchange.com/questions/414/introduction-to-statistics-for-mathematicians,2010-09-07T17:29:02.283,223,CC BY-SA 2.5,
2877,2442,2,"Just for fun, I did a brute-force calculation in Mathematica.  Its answer is a ratio of a 21,574 digit integer to a 21,571 digit integer; as a decimal it's comfortably close to 900/e as expected (but, since we're asked not to post a solution, I'll not give any more details).",2010-09-07T18:43:28.077,919,CC BY-SA 2.5,
2878,1136,2,"You can not eliminate human judgment -- nor would you want to. That said, making a model explicit does help reveal assumptions and open them up for discussion.",2010-09-07T19:12:07.600,660,CC BY-SA 2.5,
2879,2369,0,"I agree with the first point, it is a matter of ""horses for courses"", but examples which show where the boundaries lie are interesting and provide insight into the ""courses"" best suited to each ""horse"".  However, the examples must be fair, so that the criterion for success matches the question as posed (Jaynes is perhaps not completely immune to that criticism, which I will address in my answer which I will post later).",2010-09-07T19:47:20.120,887,CC BY-SA 2.5,
2880,2369,0,"The confidence interval only provides a bound on the *expected* number of false positives, it is not possible to put an absolute bound on the number of false positives for a particular sample (neglecting a trivial interval of [0,1]).  A Bayesian would determine an interval such that the probability of of more than five beheadings is less than some threshold value (e.g. 10^-6).  This seems at least as useful as a bound on the expected number of beheadings and has the advantage of being a (probabilistic) bound on what happens to the actual sample of courtiers.  I'd say this one was a clear draw.",2010-09-07T19:57:02.983,887,CC BY-SA 2.5,
2881,2447,0,"That's a neat idea: thanks for the reference.  However, the residuals to that particular fit look pretty bad, which makes me wonder how well it identifies potential changepoints.",2010-09-07T20:40:12.780,919,CC BY-SA 2.5,
2882,2447,0,whuber: i do not know how much you are familiar with the theory of quantile regression. These lines have a major advantage over splines: they do not assume any error distribution (i.e. they do not assume the residuals to be Gaussian).,2010-09-07T20:58:39.387,603,CC BY-SA 2.5,
2883,2420,0,@Josh - no problem - I understood your distinction - I think you are right about dates being a quantity and in fact that's what I was struggling to see and hence the original question.,2010-09-07T21:33:26.767,414,CC BY-SA 2.5,
2884,2420,0,"Yes, I have single date (last access time) per category",2010-09-07T21:35:16.523,414,CC BY-SA 2.5,
2885,2431,0,"Theorem 14.6, Sigma is singular so density isn't defined, this is fixed by instead considering distribution of $\hat{p_i}$ which is $\hat{p}$ with $i$'th component dropped (covariance matrix will be $\Sigma$ with i'th row/column removed)",2010-09-07T21:50:26.463,511,CC BY-SA 2.5,
2886,2423,0,"Joris: well, internet is already pretty good for explanations, my motivation is having something to check against when I need a statistics related formula. For instance, recently I needed a formula for P(X=x|v'x=a) where X is multivariate gaussian and v is some vector, and none of my statistics books had it",2010-09-07T21:57:05.253,511,CC BY-SA 2.5,
2887,2412,4,"The problem is that outliers will make some 'bad' (i.e. contaminated) tree look better than good (uncontaminated) ones. This is called, masking effect and is easy to replicate with simulated data. The problem comes about because the criterion you use to evaluate trees is not in itself robust to outliers. I know i'm starting to sound like a fundamentalist mullah, but unless each and every tool you use is made robust, your procedure can be shown to be sensitive (at one level or another) to outliers (and hence not robust).",2010-09-07T22:20:44.310,603,CC BY-SA 2.5,
2888,2434,4,"@Andy W: Actually, the hard part was getting all those probabilities.  Instead of adding them at each cell, multiply their complements: that's the probability that the cell will be empty.  The sum of these values over all cells gives the answer.  Glen_b's approach beats simulation by seven or eight orders of magnitude ;-).",2010-09-07T22:36:18.643,919,CC BY-SA 2.5,
2889,2446,1,I don't think this is quite the same thing as ecological regression - that's about trying to infer something about the within-group slopes when you only have the group means. Here you have the within-group slopes.,2010-09-07T22:42:22.317,449,CC BY-SA 2.5,
2890,2056,2,"You also might like to see the recently-published: Ren, Shiquan , Lai, Hong , Tong, Wenjing , Aminzadeh, Mostafa , Hou, Xuezhang and Lai, Shenghan(2010) 'Nonparametric bootstrapping for hierarchical data', Journal of Applied Statistics, 37: 9, 1487 ‚Äî 1498",2010-09-07T23:28:17.007,187,CC BY-SA 2.5,
2891,2362,2,"The main problem is that it is really hard to compare two models (model=learning method+feature selection method), but for simplicity you can just assume something (like I'll use RF and select top 10 attributes) and admit that you know that this may be suboptimal, but you agree on that while you are for instance satisfied with the accuracy. In that case your only problem is to remove bias of attribute selection. tbc.",2010-09-07T23:53:19.617,,CC BY-SA 2.5,user88
2892,2362,2,"So, I would do a simple bagging: you create 10 (or 30 if you have a good computer) random subsamples of objects (let's say by random picking with replacement), train RF on each, get it's importance and return a rank of each attribute averaged over all repetitions (best attribute gets rank 1, second best 2 and so on; it can be averaged so the attribute that was 12 times 1st and 18 times 2nd have rank of 1.6), finally select 10 with best ranks and call them your markers. Then use a CV (LOO, 10-fold or preferably random sampling) to obtain an error approximation of RF using your markers. tbc.",2010-09-08T00:00:02.157,,CC BY-SA 2.5,user88
2893,2362,2,"Report the ranks (hopefully they should be pretty near 1,2,3...), CV error with its deviation (just count standard deviation of results of each CV round) and OOB error (probably will be identical to CV error). DISCLAIMER: This is not a method for selecting optimal number of attributes -- you need RFE and nested CV to do that. DISCLAIMER2: I haven't worked with such a data, so I don't guarantee that your referees will be happy with it (though I believe they should).",2010-09-08T00:11:27.630,,CC BY-SA 2.5,user88
2895,2446,0,onestep:> Good point                                            Adam>: do you have access to the underlying data ? Or at least to the standard errors arround each slope  ?,2010-09-08T02:08:40.083,603,CC BY-SA 2.5,
2896,2463,2,excellent: +1 for the details,2010-09-08T02:30:48.797,603,CC BY-SA 2.5,
2898,2447,0,@kwak This looks interesting. Not assuming a normal error distribution would be useful for one of my applications.,2010-09-08T02:36:46.173,183,CC BY-SA 2.5,
2899,2445,3,I found the bcp R package http://www.jstatsoft.org/v23/i03/paper which implements the Barry & Hartigan algorithm,2010-09-08T02:39:44.990,183,CC BY-SA 2.5,
2900,2447,0,"Indeed, what you get out of this estimation are actual conditionnal quantiles: in a nutshell, these are to splines/LOESS-regressions what boxplots are to the couple  (mean, s.d.): a much richer view of your data. They also retain there validity in non gaussian context (such as assymetric errors,...).",2010-09-08T02:51:52.027,603,CC BY-SA 2.5,
2901,4,2,"Are you interested in whether the distributions are of a different form (e.g., normal, poisson, etc.) or whether parameters are different (e.g., mean or sd of a normal distribution) or both?",2010-09-08T03:16:45.867,183,CC BY-SA 2.5,
2902,2448,0,"Hi Kwak, The Huettmann paper is interesting but I have a few questions. 1) Under Assessing the link Functions, what is meant by the ""mean coefficients derived from the 1000 comparisons"" (p44)? 2) What do the ""Predictions"" in Table2 refer to? Predicted probability or predicted number of nesting sites, or what? 3) The marbled murrelet data are claimed to be public domain. Do you know the source? Thanks.",2010-09-08T03:44:33.633,521,CC BY-SA 2.5,
2904,2419,0,the only reason being that I have used R only very few times a year or so ago and python I'm using every day...,2010-09-08T06:03:21.287,961,CC BY-SA 2.5,
2910,2193,0,"Yes, so you just look at the top 20 you rely on, and find the patterns of similarity for the joint. The joint combinations giving you the prediction contain the information of the variable associations.",2010-09-08T10:22:18.927,1098,CC BY-SA 2.5,
2911,2193,0,Oh yeah and you just do correlation analysis like a,2010-09-08T10:23:08.177,1098,CC BY-SA 2.5,
2912,2146,0,@Joris Meys- exactly,2010-09-08T10:25:40.360,1098,CC BY-SA 2.5,
2914,2478,0,"Thanks. Does LDA require you to know the topics upfront? In our case we do not know to which topic each Document belongs and we will be using the similarity measure to perform clustering (EM- G-Means, or GAAC)",2010-09-08T10:59:12.803,1212,CC BY-SA 2.5,
2915,2478,0,"@ebony1 Nice reference to LSA, I made a similar answer some time ago at http://stats.stackexchange.com/questions/369/working-through-a-clustering-problem/2196#2196",2010-09-08T11:00:17.393,930,CC BY-SA 2.5,
2916,135,0,"In SAS, KS test is available in `proc npar1way`. In R, in addition to `ks.test()`, there is the `nortest` package which provides several other adjustment tests.",2010-09-08T11:15:20.670,930,CC BY-SA 2.5,
2917,2367,1,I probably shouldn't have said k-fold. The BIC is equivalent to leave-v-out CV where v=n[1-1/(log(n)-1)] and so it satisfies the Shao consistency criterion. I've updated my answer accordingly.,2010-09-08T11:23:26.587,159,CC BY-SA 2.5,
2918,2478,1,"@Joel: No, LDA doesn't assume that you know the topics for each document beforehand. BTW, just to be clear, LDA represents each document as a mixture of topics, not by just a single topic. So each topic will contribute to some fraction in the documents (and the individual fractions will sum to 1). Basically, LDA assumes that each word in the document is generated by some topic.",2010-09-08T11:37:44.257,881,CC BY-SA 2.5,
2920,2367,0,@mbq. I've now added the linear caveat.,2010-09-08T11:42:56.870,159,CC BY-SA 2.5,
2921,2482,3,Highlight your code and click on the button that looks like binary numbers...,2010-09-08T11:52:22.867,5,CC BY-SA 2.5,
2922,2478,0,"@ebony - thanks! At risk of a rephrasing the question and repeating myself, does LDA require you to know the number of discreet topics?",2010-09-08T12:02:01.140,1212,CC BY-SA 2.5,
2930,2209,8,And it is available for free download at the authors page: http://www.inference.phy.cam.ac.uk/mackay/itila/book.html,2010-09-08T12:37:47.323,247,CC BY-SA 2.5,
2931,2483,0,"This is not an answer, but the question reminds me of a presentation by Max Khesin on ""graphical models"" that he gave at a recent NY meetup: http://www.slideshare.net/xamdam/graphical-models-4dummies",2010-09-08T12:47:15.413,5,CC BY-SA 2.5,
2933,26,10,"Agreed, it's a valid question. It's also well stated as it asks for example usage and calculation. Surely the purpose of the site is to create a repository for ALL questions statistical.",2010-09-08T13:03:15.660,1212,CC BY-SA 2.5,
2934,2481,0,Kolmogorov test is for distributions hence object lying in infinite dimensional space... not obvious to get asymptotic power=1. t-test is about testing the mean of a real valued random variable?,2010-09-08T13:10:01.210,223,CC BY-SA 2.5,
2935,2481,0,"I haven't claimed that it is obvious, I just said that I have found this statement. So the natural question for me would be: what about other statistical tests - no matter what they are designed for.",2010-09-08T13:33:17.817,1215,CC BY-SA 2.5,
2936,2486,0,"Thank you, Joris. So ""my definition"" was just partially correct. ;) Do you probably know the asyptotic power of Friedman test (non-parametric repeated measures ANOVA)?",2010-09-08T13:43:04.720,1215,CC BY-SA 2.5,
2941,2459,1,"That's a nice observation.  However, I found it more bother than it's worth to exploit this explicitly.  Most of the programming amounts to setting up the transition matrix.  Once you do that, just square it and work with that.  By using sparse matrices, removing half the zeros doesn't save any time anyway.",2010-09-08T14:17:25.013,919,CC BY-SA 2.5,
2945,2447,0,"@kwak: The residuals are heavily correlated with the x-coordinate.  For example, there are long runs of negative or small positive residuals.  Whether they have a Gaussian distribution or not, then, is immaterial (as well as irrelevant in any exploratory analysis): this correlation shows that the fit is poor.",2010-09-08T14:30:58.283,919,CC BY-SA 2.5,
2946,2445,0,@Jeromy: Thank you for the R package and for inserting the links to the references.,2010-09-08T14:32:07.280,919,CC BY-SA 2.5,
2951,2478,0,Yes. But there are variants of LDA (HDP-LDA) that do not require specifying the number of topics. See this paper: http://www.cse.buffalo.edu/faculty/mbeal/papers/hdp.pdf,2010-09-08T14:55:48.960,881,CC BY-SA 2.5,
2953,2473,0,I'll come back with results.,2010-09-08T15:18:49.043,1154,CC BY-SA 2.5,
2954,2474,0,Thanks for your answer and reference. I will check differents models and do cross validation. Bayesian is a good idea.,2010-09-08T15:19:33.783,1154,CC BY-SA 2.5,
2956,2465,0,"Its a within subject design. With 4 slope values for each subject. Each slope value was calculated, in a different condition, over 5 independent variable values - and there are actually 4 measurements for each of these iv values.",2010-09-08T15:44:02.333,1084,CC BY-SA 2.5,
2957,2446,0,kwak:> I have the data! Not sure what is the best way to share data here.,2010-09-08T15:46:51.223,1084,CC BY-SA 2.5,
2959,2441,1,"I like the idea of creating a chain of indicators, but in your description it sounds like you still have to maintain full counts of fleas in all cells and run that, too, as a Markov chain.  So what is accomplished with the additional complexity of tracking the $n_{ij}$?",2010-09-08T15:53:12.723,919,CC BY-SA 2.5,
2960,2394,2,@whuber: actually the range of possible values of skew and _excess kurtosis_ allows distributions with some skew and zero excess kurtosis.,2010-09-08T16:49:40.733,795,CC BY-SA 2.5,
2962,2367,0,"ah, OK. much more sensible now.",2010-09-08T17:27:11.190,795,CC BY-SA 2.5,
2963,2355,0,"Beyond proofs, I'm wondering if there have been simulation studies of any of the five cases I list, for example.",2010-09-08T17:27:57.630,795,CC BY-SA 2.5,
2964,2459,0,"@whuber: I suspect the point of these problems is to learn problem solving techniques, rather than consume a lot of computational cycles. Symmetry, parity, etc, are classic techniques from Larson's book on problem solving.",2010-09-08T17:31:12.197,795,CC BY-SA 2.5,
2965,2447,0,"Whuber: the line you see on the plot is an estimate of the 90% conditionnal quantile (i.e. the function $f(x):p(y_i<f(x_i))\approx 0.9\; forall i$. This is true assymptotically for $f(x)$, not for each of the piecewise linear function taken individually.",2010-09-08T17:39:55.470,603,CC BY-SA 2.5,
2967,2355,0,Wanna make some?,2010-09-08T17:58:26.340,,CC BY-SA 2.5,user88
2969,2492,0,See http://meta.stats.stackexchange.com/questions/290/what-is-community-wiki,2010-09-08T18:03:57.193,5,CC BY-SA 2.5,
2970,2492,6,"In a certain sense, this is true of all test of a finite number of parameters. With $k$ fixed (the number of parameters on which the test is caried) and $n$ growthing without bounds, any difference between the two groups (no matter how small) will always break the null at some point. Actually, this is an argument in favor of bayesian tests.",2010-09-08T18:07:28.977,603,CC BY-SA 2.5,
2971,2473,0,Do you know of any papers comparing scores computed based on PLS-DA or PLS-1/2 with regression scores coming from a factor model (which account for measurement errors)?,2010-09-08T18:42:31.890,930,CC BY-SA 2.5,
2972,2492,2,"For me, it is not a valid argument. Anyway, before giving any answer you need to formalize things a little bit. You may be wrong and you may not be but now what you have is nothing more than an intuition: for me the sentence ""In the era of cheap memory, big data, and fast processors, normality tests should always reject the null of normal "" needs clarifications :) I think that if you try giving more formal precision the answer will be simple.",2010-09-08T19:01:08.107,223,CC BY-SA 2.5,
2973,2441,0,"@whuber No, you need not maintain a flea position as a markov chain. Think of what I am proposing as a random walk for a cell. A cell initially is at position '1' from where it can go to 0, 1, 2, 3, 4, or 5. The probability of state transition depend on the states of the adjacent cells. Thus, the proposed chain is a on a re-defined state space (that of cell counts for each cell) rather than on the flea position itself. Does that make sense?",2010-09-08T19:41:35.480,,CC BY-SA 2.5,user28
2974,2483,0,"Insights into the warning process would help in precise answers: Are your events of interest (i.e., weather in your example) continuous variables which are discretized or are they truly discrete? How does the warning system work? In other words, what triggers a warning?",2010-09-08T19:58:44.217,,CC BY-SA 2.5,user28
2975,2493,1,"Do you have a the raw data from the GPS devices?  If so, you might consider sharing it here and we might be able to give some more explicit answers.",2010-09-08T19:59:54.037,5,CC BY-SA 2.5,
2977,2446,0,"Ok Adam, i don't need the data, but if you have access to all 150 data points, then, it is a completely different problem statistically (you should metion it in the question). The good news is this becomes much simpler to solve. I will respond in the answer zone.",2010-09-08T21:17:19.227,603,CC BY-SA 2.5,
2978,2446,0,"I erased my previous comment, as i had missunderstood the nature of your question.",2010-09-08T21:24:18.250,603,CC BY-SA 2.5,
2979,1433,21,"In statistics, bias is not the same as error. Error is purely random, bias is not. You have bias when you know that the expected value of your estimate is not equal to the true value.",2010-09-08T21:30:57.787,1124,CC BY-SA 2.5,
2980,2251,0,"I'd love to answer too, but I'm afraid there's nothing left to add to what Kingsford said.",2010-09-08T21:43:55.307,1124,CC BY-SA 2.5,
2981,2441,1,"It makes sense, but it seems like a step backwards, because isn't the number of states now much larger? In one model there are 900 states--the position of a single flea--and no more than four transitions out of each one. The computation only needs to be done for a single flea because they all move independently.  In yours it seems a state is described by a cell's occupancy along with the occupancy of its up to four neighbors. That would be an extremely large number of states and also a very large number of transitions among the states. I must be misunderstanding what your new state space is.",2010-09-08T21:50:10.693,919,CC BY-SA 2.5,
2982,2459,1,"That's a good point.  Ultimately some judgment is needed.  Project Euler appears to emphasize tradeoffs between mathematical insight and computational efficiency.  Glen_b mentioned symmetries that are worth exploiting first because there is more to be gained from them.  Moreover, by using sparse matrix arithmetic you will achieve the twofold gain automatically (whether you're aware of the parity or not!).",2010-09-08T21:56:59.060,919,CC BY-SA 2.5,
2983,2498,2,this is great! I'm slapping myself for not doing the experiments myself...,2010-09-08T22:35:17.087,795,CC BY-SA 2.5,
2985,2498,48,"On a side note, the central limit theorem makes the formal normality check unnecessary in many cases when n is large.",2010-09-08T23:19:31.450,1124,CC BY-SA 2.5,
2986,2497,0,"Firstly, would you be able to point me at the GIS forums where you've seen this dicussed? When it comes to sub-sampling I take it you mean sampling from the existing (sampled) data. By extrapolating a fit what kind of fit are you talking about? Lastly, I don't think the 2D kernel smooth approach you describe would work well for me as I would like the process to be automated. Is it not possible to directly combine both datasets and then kernel smooth them into one line?",2010-09-09T01:21:20.200,1227,CC BY-SA 2.5,
2987,2434,0,"@whuber , Thanks for the explanation. Indeed getting those probabilities in under a minute would be challenging. It's a fun puzzle and thanks for your input.",2010-09-09T01:33:24.747,1036,CC BY-SA 2.5,
2993,515,2,I think your first example refers to Bertrand's paradox. Very nice illustration of the different ways to define a probabilistic space!,2010-09-09T07:48:50.480,930,CC BY-SA 2.5,
2994,2473,1,"Sorry for delay. You might find useful the paper ""Importance of data structure in comparing two dimension reduction methods for classification of microarray gene expression data"" from Caroline Truntzer (BMC Bioinformatics). Actually, it is not about factor model, but PCA model.",2010-09-09T08:17:17.417,609,CC BY-SA 2.5,
2995,2499,3,"Not to be rude, but isn't this a question that is already answered ad nausea on Wikipedia and the likes? Google gave me the answer within 15 seconds...",2010-09-09T08:32:42.190,1124,CC BY-SA 2.5,
2996,694,10,That's definitely my favorite. I always have to stop on this and laugh when scrolling over this page. It's just so bad!!,2010-09-09T09:02:48.803,442,CC BY-SA 2.5,
2997,2498,38,"yes, the real question is not whether the data are actually distributed normally but are they sufficiently normal for the underlying assumption of normality to be reasonable for the practical purpose of the analysis, and I would have thought the CLT based argument is normally [sic] sufficient for that.",2010-09-09T09:37:22.440,887,CC BY-SA 2.5,
2998,2509,0,I should point out that I have left out any scale factors (e.g. $\sqrt{N}$ is common in these problems) but they can be added without trouble. What you choose for a scale factor will depend on the distribution of $g'(p_0)$.,2010-09-09T12:31:31.280,352,CC BY-SA 2.5,
2999,2510,0,"It would help immensely to clarify what you mean by ""globally"" and ""approximate""!  For example, h itself is a fine approximation to g according to many measures.  If you require that g and its approximator be relatively close pointwise for all of R^2, then you're not going to get any better than that.",2010-09-09T12:54:22.783,919,CC BY-SA 2.5,
3000,2497,0,"Check out the forums (both the old and new) at esri.com.  By ""fit"" I mean any reasonable (nonlinear) fit to the points.  A direct combination of two paths is highly problematic.  It would be tempting to just intersperse the union of all GPS readings in the two tracks, but there is no obvious unique way of connecting them up or of averaging them out somehow.  That's the basic problem to be solved here and I'm not aware that anybody actually has a general-purpose solution.",2010-09-09T13:03:13.443,919,CC BY-SA 2.5,
3002,2456,0,"Ok, indeed I rewrote it completely to have different scales on both sides, but it seems there is nothing in R that can do it natively.",2010-09-09T13:24:54.540,,CC BY-SA 2.5,user88
3003,2456,0,"I'm sure that you could do it with ggplot2, but it would require a little effort.",2010-09-09T13:44:54.943,5,CC BY-SA 2.5,
3004,1646,0,"I'm having some trouble understanding the dependent measure in your question.  How is team effectiveness being measured?  When you say ""the measure of team effectiveness"" it sounds as though you are talking about your measure of ""team functioning"".  If so, isn't the dependent measure is necessarily criteria contaminated?",2010-09-09T14:43:33.053,196,CC BY-SA 2.5,
3005,2513,1,"Please may I suggest a new tag along the lines of ""artefacts""? (Or ""artifacts"" if you happen to be of the American persuasion).",2010-09-09T15:06:00.053,266,CC BY-SA 2.5,
3007,2507,0,"I presume you are referring to a situation where one first does a normality test, and then uses the result of that test to decide which test to perform next.",2010-09-09T16:07:23.570,25,CC BY-SA 2.5,
3008,2511,5,"this is a very good point. however, don't most hypothesis tests have arbitrarily low power against some alternative? _e.g._ a test for zero mean will have very low power when given a sample from a population with mean $\epsilon$ for $0 \lt |\epsilon|$ small.  I am still left wondering whether such a test can be sanely constructed at all, much less whether it has low power in some cases.",2010-09-09T16:11:25.107,795,CC BY-SA 2.5,
3009,2511,2,"also, 'polluted' distributions like the one you cite always seemed to me to be at odds with the idea of being 'identically distributed'. Perhaps you would agree. It seems that saying samples are drawn i.i.d. from some distribution _without stating the distribution_ is meaningless (well, the 'independently' part of i.i.d. is meaningful).",2010-09-09T16:14:21.530,795,CC BY-SA 2.5,
3010,2506,0,This is a nice way of putting it. I am wondering if you can generalize this description to also apply to the kernel of 'kernel density estimation'.,2010-09-09T16:18:52.630,795,CC BY-SA 2.5,
3012,2510,0,I may be missing something but I agree with @whuber. I do not see why you need an approximation g(x) when you have h(x)^+ and in what sense will any other function g(x) be better than h(x)^+?,2010-09-09T16:36:47.557,,CC BY-SA 2.5,user28
3017,2506,2,"In a way, yes. One way to understand kernel density estimation is that you approximate the density of a point from some distribution as a weighted average of its similarities with a set of points from the distribution. So the notion of similarity does play a role here as well.",2010-09-09T17:14:44.687,881,CC BY-SA 2.5,
3020,2511,2,"(1) You're right about the low power, but the problem here (it seems to me) is that there is no gradual step from ""finite"" to ""infinite"": the problem seems not to have a natural scale to tell us what constitutes a ""small"" departure from the null compared to a ""large"" departure. (2) The distributional form is independent of considerations of iid.  I don't mean that, say, 1% of the data will come from a Cauchy and 99% from a Normal.  I mean that 100% of the data come from a distribution that is almost normal but has Cauchy tails. In this sense the data can be iid for a contaminated distribution.",2010-09-09T17:39:07.823,919,CC BY-SA 2.5,
3022,2510,0,"In light of your clarification, are you asking for a polynomial approximation to the entire function g or to the data (x_i, g(x_i))?  (The solutions to those questions are somewhat different...)",2010-09-09T17:42:00.877,919,CC BY-SA 2.5,
3024,2499,54,"I absolutely hate wikipedia answers for stats. There are rambling, symbolic messes. I am looking for a gem of an answer that can explain the answer in plain English, as I believe that that shows a deeper level of understanding than a math equation. There are many popular ""plain English"" questions on here, and for good reason.",2010-09-09T18:04:48.773,74,CC BY-SA 2.5,
3025,2510,0,"@ Sri. Indeed i do have h(x) (the alpha are known) and therefore h(x)^+. But h(x)^+ is not a polynome. For reason pertaining to computational issues and ease of derivation of some properties, i need to approximate this function by a polynomial. Since h(x) [and therefore h(x)^+] are convex functions, i though, maybe, there should be a way to approximate them by some polynome, over the entire domain (R^2).",2010-09-09T18:43:51.903,603,CC BY-SA 2.5,
3027,2510,0,@ Whuber: Im really looking for an approximation to the entire function g(x_i). The criterion is such that we have somethin practical to work with (i.e. the criterion is essentially L2 distance between g and \hat{g}). See also my response to Sri's comment above.,2010-09-09T18:45:31.370,603,CC BY-SA 2.5,
3029,2519,9,"+1: indeed, all three answers here are logically consistent with one another.",2010-09-09T19:03:23.623,603,CC BY-SA 2.5,
3030,2516,11,+1: this question usually exposes some interesting point of views.,2010-09-09T19:05:58.943,603,CC BY-SA 2.5,
3031,2520,0,"This will get the ball rolling. $g(x)$ has indeed as simple analytical form, but not a polynomial one. It will make my problem simpler if i could re-express $g(x)$ as a (or even several) polynome(s), even very complicated one(s). Wiggly is okay, so long as i can make it more precise by adding more terms.",2010-09-09T19:12:43.580,603,CC BY-SA 2.5,
3032,2518,0,"That's weird... intuitively, this seems to contradict the Law of Large Numbers.",2010-09-09T19:12:50.343,666,CC BY-SA 2.5,
3033,2518,0,Carlos:> can you be more specific ?,2010-09-09T19:25:45.360,603,CC BY-SA 2.5,
3034,2521,1,"Fantastic. Although your contribution answers my problem, i would like to use it to ask the following question: assuming now that we only consider a compact $B\in\mathbb{R}^2$. What is not the non-trivial answer?",2010-09-09T19:31:59.050,603,CC BY-SA 2.5,
3035,2441,0,@whuber I see the problem now. I think my proposal amounts to defining a markov chain on the state of the grid. Define the state of the grid as $\{n_{ij}\}$. Then we have a markov chain on the grid's state as the cell counts of all the cells at any one time period are dependent on the cell counts of all the cells in the previous time period. I agree with you this is an impractical proposal as the state space increases dramatically.,2010-09-09T19:45:14.293,,CC BY-SA 2.5,user28
3036,2521,2,@kwak: Least squares!  The standard example of a compact subset is a finite set of points and the L^2 norm sums the squares of residuals.  With measurable compact subsets you need to *integrate* the squares of residuals.  There's also a classical L-infinity (sup norm) theory of approximating functions by polynomials (Weierstrass' Theorem) or even with sets of arbitrary real powers (Muntz-Szazs Theorem).  Rudin's textbook on real and complex analysis has an accessible account (I'm looking at a 1974 printing).  I believe these extend from R^1 to higher dimensions.,2010-09-09T19:49:03.323,919,CC BY-SA 2.5,
3037,2521,0,"@kwak (continued): The L^2 theory constructs orthonormal sets of polynomial functions (such as Legendre Polynomials and Chebyshev Polynomials).  You obtain the coefficients of good approximations simply by taking the inner product with the basis elements, exactly as in finite-dimensional linear algebra.  Different bases have different approximation properties.  Arbitrary (measurable) subsets B likely require variants of these systems adapted to their specific geometry.",2010-09-09T19:51:26.597,919,CC BY-SA 2.5,
3038,2521,0,"Ok, thanks very much. In some odd way this solves my problem (limits what i can do).",2010-09-09T19:53:31.580,603,CC BY-SA 2.5,
3039,2518,0,"The LLN basically states that the larger your sample is, the better it represents the ""real"" probability distribution. In your example, the more house numbers I examine, the closer to 50% the number of odd-numbered houses will be. So it sounds weird that it becomes easier for you to break through the band, since it shrinks in proportion to the square root of $n$. (Am I making sense here?)",2010-09-09T19:54:59.470,666,CC BY-SA 2.5,
3044,2492,10,"The thread at ""Are large datasets inappropriate for hypothesis testing"" discusses a generalization of this question.  (http://stats.stackexchange.com/questions/2516/are-large-data-sets-inappropriate-for-hypothesis-testing )",2010-09-09T20:17:48.403,919,CC BY-SA 2.5,
3046,2473,0,"Thx, I will look at that paper.",2010-09-09T20:52:41.140,930,CC BY-SA 2.5,
3047,1537,0,"Because (n+k)/2 is not necessarily integral, consider rewriting the probability as Pr(S_n = 2^{2k-n}) = 2^-n Comb(n,k), 0 <= k <= n.  (There's also something fishy about your equating 2^k x = k.)",2010-09-09T21:08:15.373,919,CC BY-SA 2.5,
3048,2355,2,"I do; I'm going to have to learn a lot more R, though, to share the results here, though.",2010-09-09T21:16:44.830,795,CC BY-SA 2.5,
3052,56,52,"It should be pointed out that, from the frequentists point of view, there is no reason that you can't incorporate the prior knowledge *into* the model. In this sense, the frequentist view is simpler, you only have a model and some data. There is no need to separate the prior information from the model.",2010-09-09T22:29:26.867,352,CC BY-SA 2.5,
3055,471,6,"Back when this was a \$3.95 and then a \$4.95 paperback, I bought copies by the dozen and gave them away to friends, clients, and anyone else who might be interested.",2010-09-09T22:54:01.843,919,CC BY-SA 2.5,
3056,370,12,"Both are worth a periodic re-reading, maybe once a decade, just to refresh the ideas.  Concerning Tukey: it's great to sit down just with pencil and paper once in a while and do a deep analysis of an interesting dataset.",2010-09-09T22:55:49.750,919,CC BY-SA 2.5,
3057,2518,1,@Carlos -- but convergence does not mean equality; this is guaranteed only for unreachable limit of infinity. So there is no contradiction ;-),2010-09-09T23:32:56.983,,CC BY-SA 2.5,user88
3058,93,0,when you say very small how small are you talking about? Unfortunately is is common to see people using sophisticated methods applied to a handful of data. If the problem is not having a good study to start with one should look at alternatives and make a case for a better study or data collection,2010-09-09T23:58:29.500,10229,CC BY-SA 2.5,
3059,2526,0,"This demonstration seems to assume the expected amount of money is zero, but it's not.  Furthermore, you appear to assume that the net of doublings minus halvings must be nonnegative, which is also incorrect.  Collectively these errors yield the correct limit, but that's an accident.",2010-09-10T02:15:23.513,919,CC BY-SA 2.5,
3060,2447,0,"@Kwak: Thank you for the clarification.  But if the estimate is true only asymptotically and globally, why should we expect it to perform at all well in identifying changepoints?  Is there some additional property the broken line regression has, of which I am unaware, that provides some comfort in this respect?",2010-09-10T02:36:03.867,919,CC BY-SA 2.5,
3061,720,1,"I am confused by the remark about changing units, because by definition AIC is unitless (it's an adjusted maximum log likelihood).  A change in the data units would not change the maximized likelihood at all and therefore would not change the AIC either.  (Regardless, your recommendation to pay attention only to the difference is not in question.)",2010-09-10T02:41:22.407,919,CC BY-SA 2.5,
3062,1646,0,"@drknexus In typical team studies, each team member is asked about for example their level of agreement with statements like ""my team is performing well"", ""team members get along well with each other"", ""my team handles conflict constructively""... The question is thus the degree to which such beliefs are shared by team members.",2010-09-10T02:52:57.690,183,CC BY-SA 2.5,
3063,1537,0,should your variable `money` be `x`?,2010-09-10T03:15:35.293,183,CC BY-SA 2.5,
3064,2034,0,"The situation I mention above is standard practice in meta analytic studies of correlations. The ""combining apples and oranges"" critique is of course commonly raised regarding meta analysis. However, in defence of the practice, there is often substantial theoretical and empirical evidence that different measurements of the same construct (e.g., different intelligence measures) are highly correlated.",2010-09-10T03:31:32.300,183,CC BY-SA 2.5,
3065,2034,0,"@Jeromy: Let's see whether I have understood. X and Y could be different measures of ""intelligence."" In study A, 100 people randomly drawn from a special school are tested. They all have comparable (low) levels of intelligence and r = 0.50. In study B, 50 gifted students are tested; they all have comparable (high) levels of intelligence and r = 0.45. If the raw data were to be combined, r would be almost 1.00. In study C, 50 more low-level people are studied and r = 0.45. The r from the combination of studies A and C would lie close to 0.45 or 0.50. How would a meta-analysis handle this?",2010-09-10T04:02:06.290,919,CC BY-SA 2.5,
3066,2511,1,"I don't follow (2). It seems you are flipping a heavily biased coin, and usually drawing a normal random variate, and very rarely a Cauchy. Since you specified this distribution, the words 'identically distributed' are meaningful. However, to say that some observations are identically distributed _without_ giving the distribution seems meaningless, because that distribution could be: first uniformly select an integer $i <= 1000$, then draw a random variable from distribution $i$ out of 1000 different distributions. my Q: how could one have a sample which is *not* 'identically distributed'?",2010-09-10T04:36:19.893,795,CC BY-SA 2.5,
3068,2034,0,"typically X would be one variable (e.g., intelligence) and Y would be a different variable (e.g., average grade at university). Each study might use a slightly different measure of intelligence and a slightly different measure of average grade. Some meta analytic studies apply corrections for range restriction,  reliability of measurement, and other factors. Of course in the ideal situation the same measure would be used in all studies, and the raw data would be available, but meta analysis aims to synthesise results in the published literature as best as possible.",2010-09-10T07:14:41.730,183,CC BY-SA 2.5,
3069,2517,1,I do not know if Mark and Shern had your view in mind but just to re-phrase your point- if the model for the data under the null is 'wrong' then you will reject the null hypothesis for a large enough data.,2010-09-10T08:40:53.683,,CC BY-SA 2.5,user28
3070,1537,0,"@Jeromy: Yes, I've changed it. @whuber: You're correct, I've tried to make the probability a bit clearer. BTW, integral->integer in your comment.",2010-09-10T08:43:23.247,8,CC BY-SA 2.5,
3073,2507,4,"I refer to the general utility of normality tests when used as  method to determine whether or not it is appropriate to use a certain method. If you apply them in these cases, it is, in terms of probability of committing an alpha error, better to perform a more robust test to avoid the alpha error accumulation.",2010-09-10T10:42:59.330,442,CC BY-SA 2.5,
3074,2447,0,@Whuber:> i appended my answer with some mathematical details and a much needed scientific citation. Best.,2010-09-10T11:24:13.183,603,CC BY-SA 2.5,
3075,2537,0,"Are you working with R or Stata? Are the items locally independant (i.e. the probability of choosing any one item does not depend on response to other items)? Could you confirm that what you call ""conditional fractions"" are just the % of males/females that choosed a given item, adjusting for other effects in your model?",2010-09-10T12:56:18.630,930,CC BY-SA 2.5,
3077,2537,0,@chl : that's exactly what I want to achieve.,2010-09-10T13:37:41.697,1124,CC BY-SA 2.5,
3078,2500,2,"Very nice! I'm going to use your example with the circle to explain kernel methods, as it is the best visualization I met up til now. Thanks!",2010-09-10T13:47:06.480,1124,CC BY-SA 2.5,
3079,2447,0,"@Kwak: Thank you!  I see how the case \tau = 0.5 might be effective.  (BTW, you probably meant to write \tau instead of \alpha in the formula.)  It would be interesting to compare its performance to methods specifically designed to identify changepoints.",2010-09-10T14:02:16.820,919,CC BY-SA 2.5,
3080,2483,0,"I think it would be ideal if you tell us your real question. Then we can give a lot better answers. The changed example also needs more clarification which will be specific to this example and will probably lead us away from your real question. I know that revealing your real question comes with the potential cost of somebody ""stealing"" it. But I think that is the price for getting good answers here.",2010-09-10T14:24:10.020,442,CC BY-SA 2.5,
3082,2538,0,"@Joris Maybe I will have to rework my response because it seems to me I'm not entirely answering your question; indeed, I just talked about estimating main effects of your covariates, but we can also imagine model with random slope or random intercept, or a combination thereof to derive proper estimates for each gender.",2010-09-10T15:01:07.663,930,CC BY-SA 2.5,
3083,2538,0,"this is already more than I hoped for. I've seen a comparison between gee and glmm before, and the output differed sometimes quite substantially, hence my choice for gee. I'll check out the gee package. I suppose I could use the same approach more or less for the interaction terms, as I want the odds per item and per gender (hence the interaction terms). Or am I wrong there?",2010-09-10T15:25:13.743,1124,CC BY-SA 2.5,
3084,2538,0,"@Joris Ok, I've somewhat UPDATED my response. Under the GLMM, you can fit separate intercept and/or slope for both gender, which is not exactly the same as an interaction term (which as formulated there would also be viewed as a fixed effect). Whatever the solution you choose, you need to account for a different probability of endorsing any one of your 5 items depending on the gender of the respondent. More generally, you will useful references by just looking at *Differential Item Functioning* (DIF) on the web. I just remember that the `difR` package has interesting functionalities for this.",2010-09-10T15:43:54.527,930,CC BY-SA 2.5,
3085,2538,0,"indeed, I want to see gender and item as a fixed effect. The research hypotheses are formulated that way. The only randomness in the whole thing is the personId. But I'll read in on the topic using your links. Thanks for the valuable help.",2010-09-10T15:47:28.390,1124,CC BY-SA 2.5,
3086,2538,1,"@Joris Other links that might be useful: http://j.mp/bzetkQ, http://j.mp/ac20UQ, http://j.mp/a1UNRb. HTH",2010-09-10T15:52:20.437,930,CC BY-SA 2.5,
3087,2507,0,"Hello Henrik, you bring an interesting case of multiple comparisons which I never thought of in this case - thanks.  (+1)",2010-09-10T16:59:19.503,253,CC BY-SA 2.5,
3088,2511,1,"*Q: how could one have a sample which is not 'identically distributed'?* an example of non-iid sample would be in time series (GARCH), where the observations are generated by a mixture of Gaussian distributions whose parameter are themselves random variables. In the context of the example above, a non iid sample would be one generated by \alpha percent Gaussian + (1-\alpha) percent Cauchy, where \alpha is a r.v. in (0,1). Then draws from this mixture would be non-iid.",2010-09-10T17:48:27.150,603,CC BY-SA 2.5,
3089,2447,0,"@Whuber:> with the caveat that this is not an 'online' algorithm. If you add observations, the locations of the change points will vary (although this behavior is fairly easy to 'fix', for some reasons, the author have never explored that direction).",2010-09-10T17:59:16.093,603,CC BY-SA 2.5,
3091,2541,4,"Without reference to the number of parameters you try to estimate, or equivalently the kind of model your are working with, it seems rather difficult to give you a clear answer.",2010-09-10T18:36:26.123,930,CC BY-SA 2.5,
3093,2509,0,"Yup, and page 51 of van der Vaart's ""Asymptotic Statistics"" gives the distribution of that quantity as Gaussian with covariance matrix $$E[g'^2]/E[g'']^2$$ I don't see why consistency is that important here...Taylor expanding around the biased estimate p* instead of p0 seems to produce Gaussian with the same variance by this argument",2010-09-10T18:43:49.833,511,CC BY-SA 2.5,
3094,2535,1,"Cronbach's alpha is essentially a measure of internal consistency, that it is of inter-items correlations. It suffers from many pitfalls though, especially the fact that it increases with increasing the # of items (it may even be negative, although it is a ratio of variances!). I think we shall focus here at describing variations/homogeneity at the level of individuals within teams, so thinking of mixed-effects modeling or hierarchical models is a good starting point.",2010-09-10T19:10:51.390,930,CC BY-SA 2.5,
3095,1198,0,"Thank you, Carlos, for the recommendation.  It is indeed a great read, despite (or perhaps because of) its age.  I'm especially impressed that the authors (Box, Hunter, & Hunter) appeal to *permutation* distributions, rather than arguing for normality, as the ""ultimate"" justification for the classical tests (t, F, etc.).",2010-09-10T19:50:39.127,919,CC BY-SA 2.5,
3096,2531,0,isn't there a copy/past typo in the first equation ?,2010-09-10T19:53:23.747,603,CC BY-SA 2.5,
3097,730,27,This sentence itself is a model (an epistemological one),2010-09-10T20:00:18.917,603,CC BY-SA 2.5,
3098,2547,4,It seems like you already had a reasonable answer on the other site?,2010-09-10T20:40:51.857,5,CC BY-SA 2.5,
3100,1520,1,"Probability of observing value above median is one half, but your quantity is log-normally distributed for large n, which has different mean and median, so you shouldn't expect that probability to approach 1/2.",2010-09-10T21:33:02.290,511,CC BY-SA 2.5,
3102,2548,9,Breakdown isn't an issue for a descriptive statistic of an entire population.,2010-09-10T22:06:33.870,919,CC BY-SA 2.5,
3103,2546,4,Beautiful reference--and spot on relevant.  Thank you.,2010-09-10T22:21:10.390,919,CC BY-SA 2.5,
3104,2509,0,"Sure, a biased estimate converging to $p^*$ is going to have a similar central limit theorem, i.e. $p^* - \hat{p}$ must then be assumed to converge. This is simply replacing $p_0$ by $p^*$.",2010-09-10T23:37:24.213,352,CC BY-SA 2.5,
3105,2549,0,"Actually I think nowadays the skew in a lot of countries is more towards seniors, not tots.",2010-09-11T00:09:34.637,830,CC BY-SA 2.5,
3106,2549,0,"Perhaps, it is skewed the other way but the general point stands. For skewed distributions a median may make more sense than the mean.",2010-09-11T00:21:30.863,,CC BY-SA 2.5,user28
3108,2550,1,"I think you meant ""The median is more resistant to such errors than the mean"". I agree with your comments though, and I believe the US census typically reports median's for many categories in official reports (not just the age) for basically all of the same reasons. Income is maybe even a better example than age to illustrate such points.",2010-09-11T01:54:37.943,1036,CC BY-SA 2.5,
3112,2539,0,"@srikant: Thanks very much. This is DEFFINITELY going in the right direction. Plus you have clearly outlined your thought process, and explained things as you are going along. Very, very good. Give me a little time to reflect on your answer and make sure that it addresses all the issues I need to address.",2010-09-11T07:22:13.680,1216,CC BY-SA 2.5,
3113,2539,0,"@srikant: The more I read your answer, the more I love it - its a shame that I can't vote it up, whilst I'm doing a little bit more reading around the subject area. I have a couple of further questions for you. 1). In the Bayesian model you described above, are there any underlying assumptions of the observed variables regarding: i). normality ii). independence (i.e. zero auto correlation)?  If there are any assumptions what are they and how does that affect the ""appropriateness"" of bayesian model, if one (or both) of the underlying assumptions are ""violated"" ?",2010-09-11T07:33:08.507,1216,CC BY-SA 2.5,
3114,2539,0,"Second question: You have (understandably), assigned a nominal scale to the observed variable (which light bulb is switched on), which you called 'Severity'. Will the Bayesian model still be an appropriate one, if instead of using a nominal scale, I use an ordinal one - i.e. there is no 'ranking' between the signals given. For example Level1, .. LevelN merely indicate the level of a building in which the attact event is predicted to occur.",2010-09-11T07:38:53.547,1216,CC BY-SA 2.5,
3115,2558,0,"A question though (on other polynomials): my aim is to approximate a multivariate gaussian by polynomial expansions: which of the different approaches (Chebyshev, Hermite,  Laguerre) is considered best for approximation ?",2010-09-11T07:47:15.700,603,CC BY-SA 2.5,
3116,2539,0,"Srikant: Could you please elaborate on this statement: ""In order to compute the required probability, P(S=s|W=w) you need to have some sense of P(S=s) which you can estimate or guess based on previous experience.",2010-09-11T07:52:20.207,1216,CC BY-SA 2.5,
3117,2539,0,"@Srikant: Could you please elaborate on the following statement (i.e. how do I do what you suggested, in practise): ""You can then assess the accuracy of the device by looking at the following probabilities: P(S=s|W=s).""",2010-09-11T07:53:49.087,1216,CC BY-SA 2.5,
3118,730,6,"but see a nice discussion around this quote on Gelman's blog, http://j.mp/9SgIBO",2010-09-11T10:21:25.927,930,CC BY-SA 2.5,
3119,2549,0,"I just updated my answer on math.stackexchange to emphasize just that point.  People look for symmetry and can incorrectly impose symmetry when it isn't there.  When you report the median, you give an answer that is symmetric -- the median splits the population in half -- even though the distribution is not symmetric.",2010-09-11T14:11:38.210,319,CC BY-SA 2.5,
3120,2542,17,Here's a page on exactly how good the normal approximation of the t distribution is for n=30.  http://www.johndcook.com/normal_approx_to_t.html,2010-09-11T14:25:20.370,319,CC BY-SA 2.5,
3121,2545,8,"Your example illustrates the value of robust statistics.  The *sample median* estimates the location parameter of a Cauchy distribution well.  One could argue that the weakest link in using a t-test with 30 samples is the t-test, not the 30 samples.",2010-09-11T14:35:20.043,319,CC BY-SA 2.5,
3123,2547,1,@Shane: But perhaps different sites hold the potential to garner different answers from different points of view?,2010-09-11T14:56:20.507,919,CC BY-SA 2.5,
3124,2561,0,"Edward Tufte is a statistician.  Started his career with BA and MS in statistics from Stanford, taught and wrote books about statistics for political scientists and is a fellow of the ASA.",2010-09-11T15:13:09.837,1107,CC BY-SA 2.5,
3125,2563,2,I am a bit unclear as to what exactly you are asking and how data analysis enters the picture? Please elaborate.,2010-09-11T16:37:35.250,,CC BY-SA 2.5,user28
3126,2539,0,Reg assumptions- Normality is irrelevant as the variables are discrete. Using the empirical proportions to estimate P(W=w|S=s) is equivalent to assuming independent bernoulli trials for the events; an assumption which may or may not be reasonable given your context. Your estimates for this probability will be 'off' if this assumption is violated. And consequently your assessment of the reliability of the device will also be affected.,2010-09-11T16:45:16.483,,CC BY-SA 2.5,user28
3127,2539,0,"Reg nominal vs ordinal- you have your terms switched. Ordinal implies ranking and nominal does not imply ranking. Yes, the model will hold for both ordinal and nominal data. Do note that if you have ordinal data we can develop a better model by exploiting the ordering of the data. Currently, for ordinal data the model is not as powerful as it can be as it does not use the ordering information present in the data.",2010-09-11T16:49:20.407,,CC BY-SA 2.5,user28
3128,2539,0,Reg P(S=s): This is application specific but perhaps you could take a random sample of your target population to which you wish to generalize and assess P(S=s),2010-09-11T16:50:19.267,,CC BY-SA 2.5,user28
3129,2539,0,"Reg criteria for P(S=s|W=s): I do not know of any explicit criteria. Obviously, this should be as close to 1 as possible. So, perhaps compute how far the vector {P(S=s|W=s)} is from a vector of 1s by computing the Mean Squared Error. That way you have a scalar measure of the performance of the device which you can then compare across several potential designs of the warning system.",2010-09-11T16:52:43.130,,CC BY-SA 2.5,user28
3130,1406,0,"Unfortunately it is popular among those that don't understand or trust statistics.  Once had it quoted to me when I gave a scientist an estimate of the number of hours it would take me to design and analyze his experiment.  He didn't seem to appreciate the fact that noise can look like effects, and effects of interest can be hidden from intuition by noise.",2010-09-11T17:05:56.130,1107,CC BY-SA 2.5,
3131,1330,6,Love this one -- a wonderful bonus of being a statistician.,2010-09-11T17:13:07.987,1107,CC BY-SA 2.5,
3134,2563,0,"How is ranking not related to data analysis?  You're trying to combine user data (up/down votes) to generate a rank statistic that conveys information.  Admittedly, more work on this has been done in machine learning than statistics, but that's on topic, right?",2010-09-11T18:56:10.023,251,CC BY-SA 2.5,
3136,2563,0,@ars It *seems* on topic but the question was/is a bit unclear to me. An algorithm can mean many different things depending on context.,2010-09-11T19:47:49.887,,CC BY-SA 2.5,user28
3137,2574,1,"@Harpreet You are not estimating the shape of the PDF since as @Dirk indicated it has closed form, you just specify its parameters (e.g. $\mu$ and $\sigma^2$ for a gaussian). It will not necessarily ""fit"" the data. Now, there exist several kind of non-parametric density estimates, where you only use the data at hand (plus some kernel specifications or window span, etc.); see e.g., online help for the `density` R function.",2010-09-11T19:57:24.090,930,CC BY-SA 2.5,
3138,2575,3,"I must admit I never fully understand why Gelman advocates the use of histogram with small bin width; why not using stripchart plot or raw data with superimposed kernel density estimates, which much better convey the empirical distribution of the observed data?",2010-09-11T20:06:31.440,930,CC BY-SA 2.5,
3139,2575,2,"@chl: There are of course other good visualization methods to get a sense of sampling variability.  But on the narrower comparison of histogram v. pdf under discussion here, I think his point is well made.",2010-09-11T20:18:52.423,251,CC BY-SA 2.5,
3140,2577,0,"It's not absolutely necessary; I was hoping whether there was such a technique so there's more rigour than eye-balling. If nothing exists, well, this is it, but perhaps there is.",2010-09-11T21:04:52.813,840,CC BY-SA 2.5,
3141,2561,0,"@Kingsford My fault! I was initially thinking of another citation, not from Tufte and didn't remove my first words... I UPDATED my response. Many thanks!",2010-09-11T21:11:41.153,930,CC BY-SA 2.5,
3144,2573,0,"Could you please clarify whether this question concerns data (whose distribution could be represented by a histogram) or theoretical constructs (such as a pdf, which describes a probability distribution).",2010-09-11T21:42:33.967,919,CC BY-SA 2.5,
3146,2577,0,"Oh sorry, I didn't mean to imply there weren't any techniques, only that you might be better served with collapsing into categories rather than being misled by a distinction between ranks x and x+1.  For example, I think the scoring method you propose is a fine way to start.",2010-09-11T22:01:53.480,251,CC BY-SA 2.5,
3147,2573,4,"But where does the pdf come from?  By definition, a pdf describes a theoretical probability distribution.  Do you perhaps mean the edf (empirical distribution function)?",2010-09-11T22:38:37.003,919,CC BY-SA 2.5,
3148,2563,0,@Srikant: there was a comment earlier about *closing* the question which I was responding to.,2010-09-11T22:56:18.683,251,CC BY-SA 2.5,
3149,2367,3,"@mbq: No --  the AIC/LOO proof by Stone 1977 does *not* assume linear models.  For this reason, unlike Shao's result, it's widely quoted; see for example the model selection chapters in either EOSL or the Handbook of Computational Statistics, or really any good chapter/paper on model selection.  It's only a bit more than a page long and worth reading because it's somewhat neat for the way he avoids having to compute the Fisher information/Score to derive the result.",2010-09-12T02:45:40.563,251,CC BY-SA 2.5,
3150,2367,0,"@Rob: right, it's not k-fold.  In fact, Shao devises the BICV procedure (Balanced Incomplete CV) in the referenced paper because the LKO procedure is so computationally expensive (C(n, k) subsets).",2010-09-12T02:48:17.893,251,CC BY-SA 2.5,
3151,2583,0,"I don't think it is relevant whether I know the true model in the wild. If there is a 'true' model, I would prefer a method which is more likely to find it.",2010-09-12T03:49:05.380,795,CC BY-SA 2.5,
3152,200,0,"+1 for this answer. I'll embellish it by saying that ""meaningful"" should also be considered in the context of decision making. If a given pattern is reliably extracted from the data generating process, is there a business/research decision that will be made differently by its presence? If so, you can estimate the Value of Information with respect to the effort required to do the data mining and the potential (monetary) gain from confidently making a different decision.",2010-09-12T03:56:17.867,1080,CC BY-SA 2.5,
3153,2583,2,"@shabbychef: I don't disagree.  But note: ""If there is a 'true' model"" *and* it's under consideration .. how would you know this a priori?",2010-09-12T04:08:01.743,251,CC BY-SA 2.5,
3154,430,3,"ditto. I usually put it in terms of 52 cards, and the goal is to find the ace of spades.",2010-09-12T04:08:32.313,795,CC BY-SA 2.5,
3155,2583,1,"Note also that my second paragraph actually makes the point in your comment.  This is a nice property, but it's not all clear how applicable it is in the wild; even though it's comforting in some sense, it may be misguided.",2010-09-12T04:13:58.460,251,CC BY-SA 2.5,
3156,2400,0,"Yes. The entropy for square loss is constant and the entropy for 0-1 loss is min(p,1-p). What's also interesting is that these have strong correspondences to divergences too. The square loss to the Hellinger divergence and 0-1 loss to variational divergence. Since entropies defined like this they are necessarily concave functions and it turns out the f-divergence built using f(p) = -entropy(p). Bob Williamson and I have explored some of this in our paper: http://arxiv.org/abs/0901.0356 . It's fun stuff.",2010-09-12T05:39:17.243,1201,CC BY-SA 2.5,
3157,2580,0,do you know the probability distribution of the values in **each bin** ? The y axis label makes me think that this could be Poissonian or Multinomial ? (assuming a model gives you the mean in each bin),2010-09-12T07:47:55.857,961,CC BY-SA 2.5,
3158,2574,0,"@Harpreet This is just Markdown syntax, as for editing a post through the online editor: `*ab*` gives *ab* (italic) `**ab**` gives **ab** (bold) `$\sqrt{2}$`=$\sqrt{2}$",2010-09-12T08:00:35.593,930,CC BY-SA 2.5,
3159,2585,1,"Should this be community wiki? I know that more than one reasonable answer could be given, but it seems like a question where reputation should be awarded.",2010-09-12T08:43:32.080,183,CC BY-SA 2.5,
3160,2531,0,I don't see it?,2010-09-12T09:54:53.347,1124,CC BY-SA 2.5,
3162,2581,0,"Say, you have a portfolio of assets and you want to estimate the sensitivity of the diversification benefit to a severe change in correlations (therefore, the need to stress correlation matrix). Stressing the eigenvalues seems like a mathematically attractive idea, however, it doesn't necessarily reveal that much - wouldn't be more useful to stress actual correlation because you can express a view about specific variables.",2010-09-12T10:59:48.907,1250,CC BY-SA 2.5,
3163,2580,0,"The data is essentially drawn from two Poisson processes, but there are hidden variables that I can't correct for, leading to overdispersion. Thus, a negative binomial is definitely a better model. (see the new image/text I added above). I need to show that my nb model fits better quantitatively.",2010-09-12T11:36:40.610,54,CC BY-SA 2.5,
3164,2580,1,How about a metric like Mean Squared Error between actual vs predicted values?,2010-09-12T11:42:44.200,,CC BY-SA 2.5,user28
3165,2478,0,"@ebony - thanks for your input. I'll leave the question open for a while longer, to encourage other answers.",2010-09-12T15:48:10.660,1212,CC BY-SA 2.5,
3168,2558,1,"I'm doesn't matter so much _which_ polynomial class you use (up to numerical issues), but you can get better results by the choice of nodes where you sample the function (in this case the bivariate gaussian density) in order to compute the coefficients in front of the polynomials. For this problem, the Chebyshev nodes are optimal, at least when interpolating on a finite interval. see http://en.wikipedia.org/wiki/Chebyshev_nodes or any decent numerical analysis text (e.g. Cheney & Kincaid) or http://pages.cs.wisc.edu/~amos/412/lecture-notes/lecture09.pdf",2010-09-12T16:33:13.880,795,CC BY-SA 2.5,
3169,2558,0,should say 'it doesn't matter'...,2010-09-12T17:20:49.867,795,CC BY-SA 2.5,
3171,2580,0,"hrmm - I like that idea, Srikant. It's a lot simpler than what I was thinking, but still makes sense. Throw into a an answer below so I can credit it and send some rep your way.  I'm still interested in hearing other methods, but this may work for now.",2010-09-12T18:13:42.983,54,CC BY-SA 2.5,
3172,2588,0,"I found that page too, but it does not mention any relationship to the upper tail, so I was unsure.",2010-09-12T18:21:26.733,977,CC BY-SA 2.5,
3174,2307,1,"+1, good answer and serendipitous for me -- much thanks for the paper references, especially the review.",2010-09-12T18:36:13.703,251,CC BY-SA 2.5,
3175,2367,0,"@ars But AIC must exist, convergence is poor, and so on. My whole crusade about this topic is against using it mindlessly to support some strange claims like that LOOCV is better/worse than 10-fold on 10 object set or that it works in case of machine learning.",2010-09-12T18:39:25.957,,CC BY-SA 2.5,user88
3176,2367,0,"@mbq: I was only objecting to the strongly worded comment on ""*IC"" / ""*only* for linear models"", not your crusade where I happily cheer you on.  Your latest comment is spot on.  This is the essence of the objection in my answer, i.e. false sense of comfort through stronger claims than supported by theory.  Model selection is far from a settled science -- and for good reason: as the model space becomes more complex, it's quite hard to prove widely applicable results.",2010-09-12T19:24:38.847,251,CC BY-SA 2.5,
3177,2599,0,Thanks! The Wikipedia article on hierarchical clustering does not link to that one.,2010-09-12T20:32:07.187,977,CC BY-SA 2.5,
3178,2599,2,"Oh right.  Fixed now under ""see also"" links, thanks for pointing that out!",2010-09-12T20:38:19.667,251,CC BY-SA 2.5,
3179,2588,1,Well upper tail is by definition P(X>x).,2010-09-12T22:18:59.907,,CC BY-SA 2.5,user28
3180,2585,0,"I am more or less looking for opinions, so there's no real ""solution"" to my question. Though I would be happy to award reputation to good responses...if it's possible.",2010-09-12T23:19:32.113,1253,CC BY-SA 2.5,
3181,2585,0,Doesn't look like I can change it back into a regular question - could a moderator help with this?,2010-09-12T23:28:15.407,1253,CC BY-SA 2.5,
3182,2545,3,"John:> ""One could argue that the weakest link in using a t-test with 30 samples is the t-test, not the 30 samples"". Very true, and also the assumption that the data is *iid*. Also, the median is MLE for Cauchy distributed random variables (and hence efficient), but in general you could need more than 30 observations.",2010-09-12T23:54:36.057,603,CC BY-SA 2.5,
3183,2558,0,"Yes, indeed. But do the Chebyshev node generalize to multivariate functions ? I have tried to search the web and the only class of polynomials that is considered in the multivariate case seems to be the Hermite.",2010-09-12T23:58:37.637,603,CC BY-SA 2.5,
3184,2605,0,@kwak I would be interested in knowing the exact solution as my proposal is more of a hack rather than a principled approach. Could you provide a pointer like a reference/ a term to search for?,2010-09-13T00:15:42.883,,CC BY-SA 2.5,user28
3185,2605,0,"@Srikant:> i don't think it's an 'hack' v.s. principled approach difference. Your answer is in term of eigenvalues whereas Edward's question specifically mentions individual correlation coefficients. As a statistician, i understand that eigenvalues  have better statistical properties and are easier to work with than individual correlation coefficients (in this case the e.V.'s   will have the positive side effect of 'pooling' the correlation coefficients). But your simulation will no longer be in terms of individual assets, which may (or may not) be important for Edward.",2010-09-13T00:28:03.433,603,CC BY-SA 2.5,
3186,2605,0,@kwak Can you shed some light on the structure of the objective function? Why is it formulated the way it is?,2010-09-13T01:51:40.237,,CC BY-SA 2.5,user28
3187,2560,0,"While I accepted the other answer, I'd like to say thanks for this one too (which is also quite useful)!",2010-09-13T02:08:49.993,386,CC BY-SA 2.5,
3189,2605,0,"IMO, the above is a really helpful answer. +1 from me.",2010-09-13T02:23:13.673,,CC BY-SA 2.5,user28
3190,2604,0,"Thanks. That clarifies a bunch of things and opens up a whole slew of new questions which I'll have to do some research on. I guess my main question is, does what you're saying mean that something more simple, like just taking root mean squared error, isn't a valid way to approach this problem? I'll grant that it's probably not as robust and won't give me a p-value, but it's something I could do quickly while I try to track down a copy of the book you reference. Any thoughts would be appreciated.",2010-09-13T02:58:32.163,54,CC BY-SA 2.5,
3191,520,0,"I'll give you +1 (so that you now have your 2 original upvotes :), just because I agree that with many data points, Python may be a good option; now the minor point, IMO, is that there're not so many clustering algorithms available in SciPy compared to R.",2010-09-13T06:16:10.947,930,CC BY-SA 2.5,
3194,2607,0,"@ kwak - that you! The sign of a small correlation can become important in certain optimisation problems. For example, a small negative correlation (which, statistically might have been positive just as likely due to insignificance) can be combined with 2 large volatilities (and as a result large negative covariance), which will lead to misleading optimisation results. Had the sign been positive, the results would have been very different. Zero correlation would at least not give preference to either.",2010-09-13T07:34:38.583,1250,CC BY-SA 2.5,
3195,2607,0,"By the way, I agree that the matrix might become indefinite. However, wouldn't it be feasible to simply make the negative eigenvalues = 0 and recalculate and rescale the matrix so that the diagonal values are 1? Example of the procedure can be found here: http://comisef.wikidot.com/tutorial:repairingcorrelation",2010-09-13T07:39:20.513,1250,CC BY-SA 2.5,
3196,2605,0,"@kwak that was very helpful! It is possible to estimate the limits to stress testing numerically by increasing the stress levels until the minimum eigenvalue of the matrix no longer is >= 0. I have also been exploring stress testing the entire matrix by a single parallel shift (ie each correlation +/- x stress level). Again, it is easy to estimate the limits to stress testing numerically. Rule of thumb: max upward stress is roughly equal to the smallest eigenvalue of a pos-def matrix. Max downward stress is more complicated. Is there a convenient way to solve this problem analytically?",2010-09-13T07:50:26.030,1250,CC BY-SA 2.5,
3197,2610,0,"Thanks for this excellent answer! In fact, the hierarchical clustering module you showed is already part of scipy. Also, scipy provides an implementation of k-means, so I could easily use that.",2010-09-13T08:36:32.897,977,CC BY-SA 2.5,
3198,2530,1,"Thanks.  I've done some more searching and your formula appears to be more commonly used (and using a ratio of variances seems simpler and more intuitive to me).  I tried both formulas on two data sets; the results were qualitatively similar in one case and quantitatively identical in the other, so I guess the results of the two versions of the test converge at some point.  I'm going to proceed using your formula.  If anyone else has any insight into the two formulae, it would be great to hear it.",2010-09-13T09:01:33.390,266,CC BY-SA 2.5,
3199,2610,0,"Ok, I didn't look in details into this. For k-means, you need to pay attention to the fact that we generally need two outer loops for validating cluster solution (one where you vary the # of clusters and another for varying the seed -- the objective being to minimize the RSS); then you can use the Gap statistic to choose the optimal # of clusters.",2010-09-13T09:18:19.767,930,CC BY-SA 2.5,
3201,2605,0,"@Srikant:> The solution is a k dimensional ellipse centered at $a_0$ (the actual observed correlation coefficients). This is like asking 'how far i can walk in the direction given by -w until i meet the boundary of the ellipse (which is represented by the first constraint). The actual distance is $\max.\; a_0'w-a'w$, but since the first term is a constant, it drops from the objective function. So this is given by min. a'w",2010-09-13T10:30:55.607,603,CC BY-SA 2.5,
3202,2605,0,"@Edward:> the numerical approach is okay when $p$ is small. What you do there is that you take a bunch of points at random inside the $k$ dimensional $(-1,1)$ hypercube and then you test each of your random points you test (using the value of the smallest eigenvalue of the resulting matrix) whether that point is inside the $k$ dimensional ellipse that i describe which bound the zone of admissibility. When $k$ increases you approach is no longer feasible because the ellipse will occupy an ever smaller part of the hypercube (this is known as the curse of dimensionality) meaning that the odds...",2010-09-13T10:39:07.837,603,CC BY-SA 2.5,
3203,2605,0,...of having a randomly selected point falling inside this ellipse (your acceptance rate) will become ever smaller. You would have to make exponentially more tries to have a fixed number of PSD matrices. THat's why it's better to have an explicit formulation of the boundary of the ellipse.,2010-09-13T10:43:18.390,603,CC BY-SA 2.5,
3204,2605,0,"@Edward:> your second question (analytical solution). If you want to know by how what quantity $\delta$ you can shrink all of your correlation coefficients towards 0 while still having a PSD matrix, this is easy. If you want to know by how what quantity $\delta$ you can shrink all of your correlation coefficients towards -1 while still having a PSD matrix, this is more complicated. Which one you want?",2010-09-13T10:49:54.630,603,CC BY-SA 2.5,
3205,2607,0,Edward:> your second point: i need to think about this but i have the impression that what you propose amounts to shrinking all your correlation coefficients by a small amount towards 0.,2010-09-13T10:57:46.457,603,CC BY-SA 2.5,
3206,2605,0,I am interested in the 2nd. Allow me to describe the problem in more detail. I have some initial correlation matrix. I want to stress each correlation in the matrix by the same constant simultaneously (except the diagonal; lets call this global parallel stress since it affects the entire matrix by the same constant). I start with adding 0.01% to each correlation and check if the matrix is still PSD. I continue increasing the stress levels by small increments. Eventually I encounter a stress level above which the matrix would no longer be PSD. Let's call this the upper stress boundary.,2010-09-13T11:42:06.463,1250,CC BY-SA 2.5,
3207,2605,0,I repeat the same procedure for negative stresses and find the lower stress boundary. My general observation was that the minimum eigenvalue initial correlation matrix is roughly equal to the upper stress boundary. Do you think that an analytical solution for this problem is too complicated?,2010-09-13T11:46:04.117,1250,CC BY-SA 2.5,
3208,2605,0,"Generally, I am more interested in the upper stress boundary. I stress the entire matrix at the same time because it is a simple way to reduce diversification benefit across all market variables.",2010-09-13T11:55:24.447,1250,CC BY-SA 2.5,
3209,2605,0,"Edward:> maybe open a new threat (that links to this one). Since you marked this one as solved, you're not getting a lot of traffic here (and i think other people in this forum have alot of experience with issues similar to yours). I think your problem is now different enough from the initial one and specific enough  that it deserves a threat of its own.  Also you might want to cross post it in http://www.or-exchange.com/",2010-09-13T12:26:13.267,603,CC BY-SA 2.5,
3210,2598,0,"As @Edward pointed out, please add some details.",2010-09-13T12:40:02.730,,CC BY-SA 2.5,user88
3212,2612,0,"Thanks a lot for your reply! In principle i'm also interested in how to solve a concrete problem like the one i described (thus thanks for the WinBUGS tip). But at the moment i'm trying to do a simulation study for a seminar paper in which i'd examine the performance (coverage rates etc.) of MI under model misspecification. I suppose i'll just forget about the variance components if I can't find a solution and focus on the fixed effects, but it's frustrating to give up.",2010-09-13T13:15:38.563,1266,CC BY-SA 2.5,
3213,2612,0,@Rok Great idea for the simulation! I'll look forward for this particular issue. I suppose you already search on the r-sig-mixed mailing and Gelman's book on multilevel regression...,2010-09-13T13:20:52.210,930,CC BY-SA 2.5,
3214,2605,0,Good idea! Will do.,2010-09-13T13:22:51.997,1250,CC BY-SA 2.5,
3215,2613,4,"The McNemar test is designed for paired data, isn't it? So you cannot compare its output with that of Pearson or Fisher which assumes independent samples...",2010-09-13T13:40:15.897,930,CC BY-SA 2.5,
3216,2613,0,"OK, can you put your comment as answer?",2010-09-13T14:08:18.777,603,CC BY-SA 2.5,
3217,2617,1,"There are many ways to interpret (or misinterpret) your description, Daniel, so let me know where this attempt at a clarification might be wrong: You have a set of (x,y) coordinates.  You apply a function to them to obtain a set of (x,y,z) values, a ""terrain map.""  The z values can in principle lie in [-2.25,2.25] but actually they lie in [-1.75,1.75] with the middle 40% of them in [-0.4,0.4].  You seek a transformation f so that the collection of f(z) is uniformly distributed in [-1,1].  (If this characterization is correct, there's a simple solution involving a linear transform of the rank.)",2010-09-13T14:58:25.503,919,CC BY-SA 2.5,
3218,2612,0,"I looked now, tanks for the references! Unfortunately, there's nothing on MI in the r-sig-mixed archives; and Gelman only gives the basic formula on how to combine inferences from MI when we have variation within and between imputations given (¬ß25.7).",2010-09-13T15:14:20.333,1266,CC BY-SA 2.5,
3219,2607,2,"@kwak: Have I misinterpreted that last statement?  The eigenvalues of a 3 x 3 correlation matrix, for instance, are determined by three correlation coefficients (a,b,c).  For (a,b,c) = (-0.9, 0.5, -0.1), the ev's are (2.07415, 0.91532, 0.0105346).  Changing c from -0.1 to -0.01 changes the ev's to (2.03383, 0.99151, -0.0253402).  Thus, a relatively small *decrease* in absolute value of a single coefficient can destroy the positive definiteness of the matrix.",2010-09-13T15:16:41.043,919,CC BY-SA 2.5,
3220,2619,1,I would recommend that you to split this across three separate questions.,2010-09-13T15:20:50.180,5,CC BY-SA 2.5,
3221,2619,0,"Actually, now that I read this a little closer, I see that the answer for each is very closely related.",2010-09-13T15:25:22.933,5,CC BY-SA 2.5,
3222,2615,0,You may want to explicitly link to the previous question on this issue as @kwak's answer on that question may be relevant here.,2010-09-13T15:35:27.260,,CC BY-SA 2.5,user28
3223,2615,0,Good idea! Added.,2010-09-13T15:51:14.947,1250,CC BY-SA 2.5,
3224,2615,0,"Also, on re-reading your question it is not clear what you mean by ""minimum eigenvalue initial correlation matrix"". Could you clarify what you mean by that term?",2010-09-13T15:57:32.873,,CC BY-SA 2.5,user28
3225,2617,0,@whuber - This sounds like a perfect interpretation.,2010-09-13T16:07:00.277,1267,CC BY-SA 2.5,
3226,2619,0,"I felt that the heart of the question is comparing two different distributions, I just happen to list three different ways to do it.",2010-09-13T16:14:58.533,559,CC BY-SA 2.5,
3227,2607,0,+1 counter example: i had assumed that making the ellipse more spherical was a transformation that preserved the convexity of the associated quadratic form. Corrected (i.e. strike in the response).,2010-09-13T16:19:00.680,603,CC BY-SA 2.5,
3228,2622,0,I actually have the Agresti textbook on my desk right now and I have been using it. The problem is that I didn't know what specific methodology I should be using.,2010-09-13T16:32:54.710,559,CC BY-SA 2.5,
3229,2558,1,"ah, yes, for some reason I thought the function was separable.  In higher dimensions, I only have experience in spline interpolation, e.g. for finite-element method compuations. (oddly enough, my thesis was on the topic). the big takeaway in planar meshing to minimize interpolation error is to minimize the maximum angle appearing in the triangulation. the margin does not permit me to be more detailed than this ;) you can look up any of the papers by J. Shewchuk (see 'finite element quality' at http://www.cs.cmu.edu/~jrs/jrspapers.html) or N. Walkington & G. Miller, etc. etc. hth.",2010-09-13T16:34:25.813,795,CC BY-SA 2.5,
3230,2615,0,"Minimum eigenvalue initial correlation matrix: 1) calculate the eigenvalues of the correlation matrix prior to stress testing (i.e. initial matrix), 2) select the smallest eigenvalue, 3) this eigenvalue will be roughly equal to the upper stress boundary.",2010-09-13T17:02:22.853,1250,CC BY-SA 2.5,
3231,2623,1,"if your series is non stationnary, the ACF will decline very slowly, to the point of being useless (it essentially a constant). What do you mean by 'have any meaning' ?",2010-09-13T17:02:54.663,603,CC BY-SA 2.5,
3232,2603,0,The location changes after each attempt,2010-09-13T17:47:26.360,64,CC BY-SA 2.5,
3233,2607,0,@kwak: Thank you.  Issues involving positive definiteness can be subtle.,2010-09-13T17:48:35.510,919,CC BY-SA 2.5,
3234,2613,2,"Don't know if your udpate calls for a new answer, anyway I'll continue to comment on. For a given # of cases, power for detecting an effect can be increased by selecting more controls than cases (which also come into play in CIs width). Power increases as one moves from a 1:1 to a 1:2 matching but the benefits are less interesting above that ratio. However, it is still useful to have more controls in case of exclusion during sensitivity analysis or things like that. If you're interested in CC design, look at this Lancet series, http://j.mp/aZWRrg",2010-09-13T18:29:50.643,930,CC BY-SA 2.5,
3235,2625,0,"The problem with this method is that I never actually have a complete map. The my noise function generates [x,y] independently from [x+n,y+m], and should have, on its own, a uniform distribution. I'm trying to make the function uniform. A uniform distribution over the map is simply a byproduct. (Granted, I didn't really make this clear in the question; +1 for the idea.)",2010-09-13T19:07:22.503,1267,CC BY-SA 2.5,
3236,2622,2,"@Elpezmuerto Very briefly, to complement @ars answer, question 1 can be answered with a conditional or trellis plot, e.g. sth like `dist ~ occ | isLand` using Lattice, or see the `coplot()` function in the `vcd` package -- this is for exploratory purpose; question 2 calls for a prediction model; depending on the variable you consider as your outcome, it may be logistic regression (e.g. if Y=isLand), a linear regression (e.g. if Y=distance), or directly a log-linear model providing you categorize your continuous measurement; question 3 is clearly a log-linear model as suggested by @ars.",2010-09-13T19:10:03.573,930,CC BY-SA 2.5,
3237,2622,1,"@Elpezmuerto @ars Thanks to the work of Laura Thompson, Agresti's book is available in R too, http://j.mp/9fXheu :-)",2010-09-13T19:12:25.597,930,CC BY-SA 2.5,
3238,2620,0,"This is looking good, but I'm having trouble interpreting my results (which I edited in above.) I think it means my starting distribution isn't exactly normal, but I can't seem to tweak it to work. I'm using `x = (1 - erf(x))/2`. Am I missing a constant scalar of some sort?",2010-09-13T19:17:12.150,1267,CC BY-SA 2.5,
3239,2622,2,@chl: that's a great find! Thank you.  @Elpezmuerto:  There's a series of examples in Agresti concerning crabs -- I'm pretty sure there's a continuous variable (size of crab?) along with a color (range) and a boolean (can't recall).  So fairly close to your case -- it's probably instructive to read through those examples which span at least 2 chapters (one chapter is logistic regression I believe).,2010-09-13T19:33:44.110,251,CC BY-SA 2.5,
3241,2603,0,"as @Edward said. I could ask such a thing in a homework actually... Now Martin, can you figure out what's the chance you never find that treasure?",2010-09-13T19:47:02.617,1124,CC BY-SA 2.5,
3242,2622,0,"@ars These are esp. chapters 4 and 5, with carapace width and weight as continuous variables and spine condition as another categorical (ordinal) variable, used in Poisson and Logistic regression :)",2010-09-13T19:51:01.130,930,CC BY-SA 2.5,
3243,2620,1,"@Daniel: Don't think you're missing a constant, but scaling might help.  Think your intuition about normality is correct though.  I updated my answer with some suggestions.",2010-09-13T19:56:09.467,251,CC BY-SA 2.5,
3244,2622,0,"@Ars, I actually have been using the crab examples before you mentioned it. So atleast I know I am on the right track",2010-09-13T20:18:52.840,559,CC BY-SA 2.5,
3245,2625,0,"@Daniel: Hmm, your actual situation is a little unclear.  Maybe you could edit the question to say more about what your doing.  Anyway, I'm operating on the assumption that the references to normality may be a bit of a red herring.",2010-09-13T20:23:28.970,919,CC BY-SA 2.5,
3246,2617,0,"@Daniel: Which probability integral are you using?  If you used the one appropriate for the upper (green) curve, then the lower (red) curve should be perfectly horizontal.",2010-09-13T20:24:27.740,919,CC BY-SA 2.5,
3247,2617,0,@whuber - I'm using cubic interpolation between integral points generated from a hash function.,2010-09-13T20:32:53.783,1267,CC BY-SA 2.5,
3248,2496,0,"Thank you, I will look into this, I already know how to do a regression analysis of this data, but maybe the chapter also has a solution to my problem.",2010-09-13T20:43:56.600,1084,CC BY-SA 2.5,
3249,2617,0,"@Daniel: Although I'm quite familiar with all those terms, without more details it's hard to say whether the problems with the lower red curve are to be expected (e.g., if you're using a spline with a small number of knots) or are actually serious.  I'm startled by the amount of error in the tails, though.",2010-09-13T20:46:57.097,919,CC BY-SA 2.5,
3250,2617,0,"@whuber - I'm taking the X,Y coordinates of the four corner points, running them and a seed value through MurmurHash2, and using cubic interpolation calculate get the points between the corners. Since all this can be done independently for each point, I want the *function* to be uniform, not only the map.",2010-09-13T20:57:52.330,1267,CC BY-SA 2.5,
3251,2632,0,I think this may be the way I'm going to go. I fear you're right about the hash function not being completely normal.,2010-09-13T20:59:45.713,1267,CC BY-SA 2.5,
3252,2632,0,"@Daniel: I suspect it doesn't matter if it's normal or not. If you want to know how normal it is, generate N numbers from a normal distribution, sort it, and plot against the numbers in your table. If your table is normal you'll get a straight line (QQ plot). If not, it will not be straight. Useful thing.",2010-09-13T21:14:12.867,1270,CC BY-SA 2.5,
3253,2632,0,@Daniel: There's a really brain-dead simple way to generate a pretty good normal random number. Just add together 12 uniform numbers from the range -0.5 to 0.5.,2010-09-13T21:16:56.877,1270,CC BY-SA 2.5,
3254,2496,0,"It'll still be a regression analysis, just one that takes unto account (i.e. corrects) the fact that 4 measurements on the same observations are correlated with one another.",2010-09-13T21:18:45.787,603,CC BY-SA 2.5,
3257,2617,0,"@Daniel: Aha!  Do you want the function to have a uniform distribution on each rectangle, or does it have to be uniform *globally*?  (Until now the latter was my interpretation, but your comment suggests it's the former.)  The first interpretation has a definite mathematical answer.",2010-09-13T21:34:56.557,919,CC BY-SA 2.5,
3258,750,7,"There are three kinds of lies: Lies, damned lies and fake statistics.",2010-09-13T22:01:51.430,1124,CC BY-SA 2.5,
3259,2617,0,"@whuber - The *limit* of the distribution (if that makes any sense) should be uniform. But any single point should be also. So if I pick, say (45.32,-12.1), it should have equal probability of being any number [-1,1]. (45.32,-12.2) should also have equal probability of being any number [-1,1], though in reality it will be very very close to the number selected before. Any integral point [(0,0),(1,4),etc] already has uniform distribution. But interpolation between points skews the distribution.",2010-09-13T22:15:47.930,1267,CC BY-SA 2.5,
3260,2558,0,"*the big takeaway in planar meshing to minimize interpolation error is to minimize the maximum angle appearing in the triangulation.* meaning Delaunay triangularization ? this is nice because its quiet simple to do. Thanks for the papers, i will have a look.",2010-09-13T22:25:20.970,603,CC BY-SA 2.5,
3261,2617,0,"Interestingly enough, the distribution of points generated from the hash and 1 dimensional cubic interpolation seems to not be normal at all. It looks almost uniform, but after generating a billion values, there is still noticeable noise in the PDF.",2010-09-13T22:25:57.757,1267,CC BY-SA 2.5,
3262,2617,0,"@Daniel: I'm still lost, but I'm getting there.  Let's see: you are generating hash values (which, if it's a good hash, should act like independent uniformly distributed random numbers) at points with *integral* coordinates.  Then you're interpolating those values to all other coordinates.  You find that the distribution of all values is not uniform.  You would like to make it so by transforming the interpolated values.  Am I close now?  If so, should the transformation be predetermined or can it depend on the hashes at the integral coordinates?",2010-09-13T22:48:34.983,919,CC BY-SA 2.5,
3263,2558,0,"A small maximum angle is implied by a large lower bound on minimum angle ($\pi - 2\alpha$ where $\alpha$ is the lower bound on angle); Delaunay Triangulations maximize this lower bound over all triangulations. Shewchuk explains this fairly well, IIRC. My thesis less so ;) You are more free however, because you are not constrained to include certain edges (meshing algorithms are usually applied to weird shapes), and can put your mesh points anywhere. Practically speaking, you want to use Strang & Persson's meshing code, in Matlab, or use Shewchuk's Triangle, which is a high quality standalone.",2010-09-13T23:12:24.730,795,CC BY-SA 2.5,
3264,2619,0,"For `occupants` what you've got is an ordinal variable, so I wouldn't think of it as categorical. Especially with 8 values, it's almost continuous.",2010-09-14T00:07:09.600,1270,CC BY-SA 2.5,
3265,2617,0,"@whuber - You seem to be much less lost than you make yourself out to be. If I understand your question, I believe I want a predetermined transformation. I know how I could transform any given map so that it's uniform. But I need a transformation that generally works all the time. So the I can add on to my map indefinitely, and it will retain uniformity (and probably get more and more uniform, too.)",2010-09-14T00:37:28.267,1267,CC BY-SA 2.5,
3266,2636,5,The distribution of age is certainly not log normal.,2010-09-14T00:51:17.043,159,CC BY-SA 2.5,
3267,2622,0,"@chl: totally impressed by your consistent thoroughness in all responses! You're setting a high bar though.  :)  @Elpezmuerto: totally on the right track.  I'm guessing you're more or less aware of the methodology and just need to start applying it to your data.  Then ask specific questions if you hit a wall or are unsure.  It's the only way to learn modeling, I think.",2010-09-14T01:25:27.807,251,CC BY-SA 2.5,
3268,2633,0,"Oops, you're using agglomerative methods, so the ""1 or 2 clusters"" part doesn't make sense -- the ""inverse"" applies, you don't want n-1 or n-2 singletons, etc, i.e. letting clustering work for a bit before applying validity criteria shouldn't be problematic.",2010-09-14T01:29:13.960,251,CC BY-SA 2.5,
3269,2036,1,"@Gong-Yi: nice, thanks for updating with the answer!",2010-09-14T01:32:35.497,251,CC BY-SA 2.5,
3271,2639,0,Do you really mean you have 1 observation from A? It seems like you are likely to make an error with so little data.,2010-09-14T03:17:16.217,795,CC BY-SA 2.5,
3274,2639,0,"So say a sample has size n = 40000.  Then you have one such sample of random variates from A, and 100 replicates each for B, C and D?",2010-09-14T03:55:52.653,251,CC BY-SA 2.5,
3275,2641,28,"Great question. I would add ""odds"" and ""chance"" in there too :)",2010-09-14T05:28:41.807,74,CC BY-SA 2.5,
3276,2641,7,I think you should take a look at this question http://stats.stackexchange.com/questions/665/whats-the-difference-between-probability-and-statistics/675#675 because Likelihood is for statistic purpose and probability for probability.,2010-09-14T06:04:20.573,223,CC BY-SA 2.5,
3279,2650,2,"If you need a command to do that, you better provide us at least with the language/package you use. Are we talking SAS, SPSS, Stata, SPlus, Minitab, R, ... ?",2010-09-14T09:36:46.557,1124,CC BY-SA 2.5,
3280,2604,2,"imagine that you had a set of points (x,y) and you were considering whether you might fit a straight line or a quadratic. If you compared the RMSE, the quadratic *would always beat the straight line*, because the line is a quadratic with one parameter set to zero: if the least squares estimate of the parameter is exactly zero (which has zero probability for continuous response), it's a tie, and in every other case the line loses. It's the same with the Poisson vs the negative binomial - a free Negative Binomial can always fit at least as well as a free Poisson.",2010-09-14T10:28:42.733,805,CC BY-SA 2.5,
3281,2651,0,"Thanks a million for your valuable answer, it really does help, for your previous remark, i'm a stata user.",2010-09-14T10:53:58.400,1251,CC BY-SA 2.5,
3282,1315,0,"You're getting lots of useful ideas for different distributions. To answer your question ""I want to know what statistical distribution this is"", the usual method is a QQ plot, easy to generate in R.",2010-09-14T11:25:03.753,1270,CC BY-SA 2.5,
3283,1315,1,"Of course, if all you want is a smooth curve, take all your data, sort it to make an empirical CDF, smooth it, and take the first derivative numerically. That's your curve.",2010-09-14T11:29:03.463,1270,CC BY-SA 2.5,
3284,2651,1,"In Stata, I wouldn't know a command to check that easily. When using implemented goodness of fit tests, you have to be aware you cannot compare the values across non-nested models in many cases, and definitely not across different frameworks.",2010-09-14T11:54:18.837,1124,CC BY-SA 2.5,
3285,2651,0,"@Ama Maybe worth looking at the UCLA Resources Center for Stata, http://j.mp/9xBk6U",2010-09-14T12:14:36.697,930,CC BY-SA 2.5,
3286,2654,0,"Thanks for the suggestion. Perhaps, my description makes it sound like a sigmoidal process, but the data is typically not as smooth as a sigmoidal process might suggest. The theory and the data suggest that some form of relatively abrupt change occurs, which leads to the commencement of the rise from values around 0 to values around 1. Thus, it's something like a segmented nonlinear regression with unknown change points.",2010-09-14T13:26:14.757,183,CC BY-SA 2.5,
3287,1234,0,"In quiet periods price volatility tends to be lower, so prices closer to the mean can be considered outliers. However, because you use MAD for (presumably) an entire trading day (or even longer) these outliers are less than 3 MAD away from the median and will not be filtered. The reverse is true for busy periods with higher price movements (acceptable price movements will be filtered). Thus the problem reduces to properly estimating the MAD at all times, which is the issue to begin with.",2010-09-14T13:39:17.310,127,CC BY-SA 2.5,
3288,2604,0,"Nice explanation - I get what you're saying now. I think my case is a little different, because I'm not doing regression to get a fit, but rather, I'm basing the extra NB parameter on outside information (I expect the var/mean ratio to be N).  Since Poisson is the special case where N=1, what I'm really comparing is the choice of N. I agree that if I was doing regression, the NB would always be able to find a better fit, because it's less constrained. In my case, where I'm choosing a value for N up front, it would certainly be possible to choose some crazy value of N that makes the fit worse.",2010-09-14T13:40:22.313,54,CC BY-SA 2.5,
3289,2654,1,"The sigmoidal functions become step functions with appropriate prarameters, but are you meaning you are wanting something like a level shift in time series analysis?",2010-09-14T13:47:30.800,229,CC BY-SA 2.5,
3290,2622,0,"@Ars, I felt I hit an information wall but you guys are getting me on the right track. I am also considering using a generalized linear model because max occupants is a discrete response variable that is a count as a possible outcome. Agresti (page 74) suggests generalized linear model then.",2010-09-14T13:51:06.417,559,CC BY-SA 2.5,
3291,2496,0,Then I think think I have explained my question properly. I already understand how to do a regression analysis in a within subject design. What I don't understand is how to adjust df when comparing means (and correlating) slope values acquired in a repeated design.,2010-09-14T14:06:42.027,1084,CC BY-SA 2.5,
3292,2496,0,"In fact a simple regression line, that minimizes distance the vertical distance, is not appropriate for finding the relation between two dependent variables, one has to calculate the minimized euclidean distance.",2010-09-14T14:08:04.477,1084,CC BY-SA 2.5,
3295,2496,0,"Adam1:> once you control (by fixed or random effect) for the correlation between measurement, you will also increase the number of parameters of your model, hence decrease the number of df.",2010-09-14T14:17:13.840,603,CC BY-SA 2.5,
3297,2648,0,Does $Y$ increases monotoneously ? (it's not clear from your description whether the first derivative of $Y$ is always $\geq0$),2010-09-14T14:31:52.953,603,CC BY-SA 2.5,
3298,2496,0,"Kwak:> not sure how to explain myself better. I think you are trying to help me do a regression, but that is not my question - I already understand that, what I am trying to do is to apply a t-test or a correlation to individual slope values. I am asking if I am loosing df treating each slope value as 1 df even though it was calculated over repeated measurements.",2010-09-14T14:39:53.830,1084,CC BY-SA 2.5,
3299,2651,0,"Joris, Chl, many many thinks to both of you, your advices are really so valuable!!!!",2010-09-14T14:40:53.923,1251,CC BY-SA 2.5,
3300,2615,1,Apparently a solution had been founded on the OR exchange forum (http://www.or-exchange.com/questions/695/analytical-solutions-to-limits-of-correlation-stress-testing). Edward:> Maybe you should repost mark this question as solved,2010-09-14T14:53:02.267,603,CC BY-SA 2.5,
3301,2617,0,"@Daniel: I don't think your hope is going to play out exactly. A fixed transformation can be devised to uniformize the data within any given cell.  That distribution will, of course, depend on the hash values at the four corners.  The whole grid thereby becomes an equally-weighted mixture of uniforms which have varying ranges (and those ranges are correlated among neighboring cells).  There will be tendency for fewer values to be generated near the extreme end of the z-interval.  However, for any given set of (x,y) coordinates, you can generate several independent sets of z's... (continued).",2010-09-14T15:05:16.690,919,CC BY-SA 2.5,
3302,2617,0,"... With this set, you can either create a lookup table (as suggested by Mike Dunlavey) or, more simply, fit a spline to the edf for an efficient approximation to the rank transform I suggested.  That empirically-determined, simulation-based transform should work well for all future realizations of z's using the same grid (or, I believe, grids of similar size and shape).",2010-09-14T15:07:41.647,919,CC BY-SA 2.5,
3304,2604,0,I'm certainly going to read up on the smooth tests of goodness of fit that you suggested though.  Thanks for the informative answers.,2010-09-14T15:31:55.557,54,CC BY-SA 2.5,
3305,2647,58,The distinction between discrete and continuous variables disappears from the point of view of measure theory.,2010-09-14T15:48:30.070,919,CC BY-SA 2.5,
3306,2639,0,"ok, I'm going to delete my 'answer', because I have no idea what your problem is and what you are trying to solve.",2010-09-14T16:31:54.597,795,CC BY-SA 2.5,
3307,2660,0,"Just in case it helps for further reading, link to the wikipedia article for KS test: http://en.wikipedia.org/wiki/K-S_Test",2010-09-14T16:59:56.223,251,CC BY-SA 2.5,
3309,2661,0,Is your question related to some extent to assessing model goodness of fit or the comparison of two nested models?,2010-09-14T19:28:38.110,930,CC BY-SA 2.5,
3311,2661,0,"@chl, both, i have data and need to determine the proper fit and how well it fits",2010-09-14T19:43:01.677,559,CC BY-SA 2.5,
3312,2657,2,"I don't really understand how the conclusion you reached about the issue of tossing a coin is related to the question asked. Maybe you could expand a little bit on that point? Statistical tests seem to be relevant to the extent they help to infer observed results to a larger population, whether it be a reference or general population. The question here seems to be: Given that the sample is close to the population of test takers for a fixed period of time (here, one year), is classical inference the right way to reach a decision about possible differences at the individual level?",2010-09-14T19:43:49.150,930,CC BY-SA 2.5,
3313,2660,0,"Thank you, ars.  I figure anyone can search Wikipedia themselves, but you have made it more convenient by providing the link.",2010-09-14T20:07:26.530,919,CC BY-SA 2.5,
3314,2647,37,@whuber yes but an answer using measure theory is not that accessible to everyone.,2010-09-14T20:09:06.713,,CC BY-SA 2.5,user28
3315,2647,25,"@Srikant: Agreed.  The comment was for the benefit of the OP, who is a mathematician (but perhaps not a statistician) to avoid being misled into thinking there is something fundamental about the distinction.",2010-09-14T20:36:32.933,919,CC BY-SA 2.5,
3316,2663,3,"+1 for the sensible discussion; a few points though.  Inferential machinery is unavailable for population analysis, but in many modeling cases, I'd question whether one ever has *the* population data to begin with -- often, it's not very hard to poke holes.  So it's not *always* an appeal to a super population as the means to deploy inference.  Rather than ""super population"", the better way is to assume a data generating process yielding, for example, the year to year test taking cohorts in question.  That's where the stochastic component arises.",2010-09-14T20:41:59.020,251,CC BY-SA 2.5,
3317,2664,0,"This was an OR question to begin with :), i'm glad an elegant solution was found.",2010-09-14T21:10:16.017,603,CC BY-SA 2.5,
3318,2539,0,"Thanks for the response. Apologies for the delay in accepting your answer, I had not realized you had answered my questions.",2010-09-14T21:15:23.703,1216,CC BY-SA 2.5,
3319,2668,0,Here I am thinking of all of frequentist/bayesian probability and all of descriptive/exploratory/inferential statistics.,2010-09-14T21:57:29.740,,CC BY-SA 2.5,user1108
3321,2663,2,"I don't think there as any disagreement here, except for the lack of inferential machinery for population analysis.  Randomization tests are applicable to populations and can reasonably test whether the data generating process is likely due to a random generating process versus a systematic generating process.  They do not assume random sampling and are a rather direct test of chance versus systematic variation.  Our traditional tests happen to stand in pretty well for them.",2010-09-14T22:45:07.637,485,CC BY-SA 2.5,
3322,2663,0,"That's true re: ""lack of inferential machinery"".  Careless wording on my part, especially since I liked the point you made about randomization tests in your answer.",2010-09-14T23:02:42.550,251,CC BY-SA 2.5,
3325,2667,0,"I'm not immediately sure how to solve this, but it seems reasonable that an estimate of the cdf can still be obtained. It might not be continuous, but I don't see that as a huge problem.",2010-09-15T00:11:30.037,352,CC BY-SA 2.5,
3327,2641,5,"Wow, these are some _really_ good answers.  So a big thanks for that!  Some point soon, I'll pick one I particularly like as the ""accepted"" answer (although there are several that I think are equally deserved).",2010-09-15T01:13:24.733,386,CC BY-SA 2.5,
3329,2664,0,when you get a chance please update your answer with the proof for the benefit of future readers.,2010-09-15T03:23:10.553,,CC BY-SA 2.5,user28
3330,2670,0,"You're assuming independence and a probability of 1/12 of being born in January. Neither are quite correct (e.g twins, triplets etc exist and have a tendency to be found together after birth, and the proportion of births that are in January isn't actually 1/12, though it's pretty close).",2010-09-15T03:24:38.067,805,CC BY-SA 2.5,
3331,2635,1,Your question is not clear. Could you please edit the text and add a bit more context?,2010-09-15T03:25:47.133,,CC BY-SA 2.5,user28
3332,2604,0,"Sorry about not realizing that the data didn't come into the choice of overdispersion parameter. There may be some argument for doing it your way, but if the external estimate is likely to reflect what you actually observe, the NB still may have some advantage depending on circumstances.",2010-09-15T03:28:14.007,805,CC BY-SA 2.5,
3334,2606,1,"This was the right answer for my particular situation, even though Glen_b's response helped me learn more.  So more upvotes for him, accepted answer for Srikant. Everybody wins - thanks all.",2010-09-15T04:35:43.363,54,CC BY-SA 2.5,
3335,162,3,"Spring force is proportional to the deformation, so this is not a least squares regression!",2010-09-15T05:31:36.913,795,CC BY-SA 2.5,
3336,162,1,"Nice try! Depends on the spring. For example, if the spring constant is 1/sigma, works great ;)",2010-09-15T05:54:10.083,74,CC BY-SA 2.5,
3337,2648,0,@kwak The data itself varies from time point to time point. The line of best fit is generally either stable or monotonically increasing.,2010-09-15T06:52:37.680,183,CC BY-SA 2.5,
3338,2670,0,@stat-teacher LaTeX works here. Just enclose formulas in dollar signs $...$,2010-09-15T07:08:27.080,183,CC BY-SA 2.5,
3339,2657,1,"@chl Yes, but it seems to OP is trying to infer an underlying probability of success. The tests compare the observed proportions to the theoretical distribution to determine if there is a difference for a given level of confidence. You are testing for any form of randomness, not just sampling error randomness.",2010-09-15T09:02:35.930,229,CC BY-SA 2.5,
3340,2675,2,"A little bit more detail would be ideal to get the question. At least, how does your data look like, who are the subscribers, what do they have to do with average time, and how is 'average time in service' defined?",2010-09-15T11:05:08.787,442,CC BY-SA 2.5,
3342,2675,0,"I agree with Henrik. The more precise your question is, the better the answer will be.",2010-09-15T12:12:02.503,8,CC BY-SA 2.5,
3345,2654,0,"My assumption is that a latent change occurs that triggers the initial rising process. I'm also interested in identifying the exact time point when this occurs. Thus, I see this as a true step function, whereas I would have thought a sigmoidal function could be parameterised to approximate this, but actually, it would still be continuous in the sense that it has a second derivative.",2010-09-15T14:10:04.080,183,CC BY-SA 2.5,
3347,2680,0,@Srikant: Would you mind explaining which two independent variables are being summed?,2010-09-15T14:39:47.490,919,CC BY-SA 2.5,
3348,2681,1,"@G. Jay: Could you elaborate a little on what ""standard and important result"" this is and how it applies in this situation?",2010-09-15T14:40:37.410,919,CC BY-SA 2.5,
3349,2673,0,"Great point!  Note, however, that this report would only be directly relevant for a room full of six and seven year olds ;-).  Seriously, if you are concerned about the statistics of the question (which I think is great), then it gets interesting once you realize people control birth month and even day of the week (doctors induce labor, etc.) and that over time the degree of control as well as what is favored can change.  But please don't take these remarks as quibbles: I'm only pointing out some intriguing consequences of the path you have indicated here.",2010-09-15T14:47:47.887,919,CC BY-SA 2.5,
3350,2648,0,"Jeromy:> then, i don't think a Sigmoid function is what you want (because of symmetry assumption built unto it).",2010-09-15T15:02:56.853,603,CC BY-SA 2.5,
3351,2680,1,I'd assume red and non-red cars.,2010-09-15T15:23:24.290,1287,CC BY-SA 2.5,
3352,2680,0,"@whuber @msalters yes that is correct. My answer addresses the OP's first qn. No of cars per hour is poisson. No of cars per hr = No of red cars per hr + No of non-red cars per hr. Hence, No of red cars per hr is poisson.",2010-09-15T15:26:18.467,,CC BY-SA 2.5,user28
3353,2682,0,"Sorry, I had thought it was clear that the color here should be taken as red or non-red. The chance of a car being red obviously doesn't depend on a previous car being blue or green, but does depend on the previous car being red.",2010-09-15T15:29:43.007,1287,CC BY-SA 2.5,
3354,2680,0,"Hadn't realized it this explicitly, but I can indeed get at the results I need by treating my ""all cars"" not as an independent variable, but as the sum of two variables with related distributions. Then œÉ(cars) = œÉ(red cars) + œÉ(non-red cars) = œÉ(red cars) * (1/p(red))",2010-09-15T15:37:19.087,1287,CC BY-SA 2.5,
3355,1924,0,"The difference between $I$ and $H$ is that $H$ is expected second derivative, whereas I is expected product of first derivatives. $I$ is not always the same as $-H$, for instance, for uniform distribution",2010-09-15T15:39:12.473,511,CC BY-SA 2.5,
3356,2681,0,"Sure.  I said ""standard"" because I have seen it usually covered in the first few chapters of books on stochastic processes. (Didn't mean to seem snooty;  hope it didn't come off that way).  I said ""important"" because a) Poisson processes form the foundation for renewal processes in general (where interarrival times are not necessarily exponentially dist'd but are independent with dist'n F), and b) decomposition of Poisson processes in particular gives, for instance, a handy way to  simulate continuous time Markov chains with a computer.  Durrett has a nice discussion of this point.",2010-09-15T15:45:31.417,,CC BY-SA 2.5,user1108
3357,2681,0,"By the way, as whuber noticed, the process determining which cars are ""red"" and which ones ""not-red"" should be independent of the Poisson process;  otherwise, all bets are off.  :-)  The OP did mention independence, but it can be tricky to make things perfectly clear.",2010-09-15T15:48:58.180,,CC BY-SA 2.5,user1108
3358,2680,0,@Srikant: Cool!  Thanks for the reference to Raikov's Theorem.  (I might have spent a lot of time trying to think of a counterexample otherwise...),2010-09-15T15:50:26.520,919,CC BY-SA 2.5,
3359,2673,0,"@whuber I agree with you, my post was rather simplistic (I just found it interesting!) Two other points of interest. First, in the report it gives different birth rates classified by race. Secondly, I'm a bit confused why the Monday birth rate isn't much higher (since the rate is low over the weekend). Instead, it seems Tuesday is the day of choice.",2010-09-15T15:54:05.950,8,CC BY-SA 2.5,
3361,162,2,"no, no, the point is that in static equilibrium, the sum of forces would be zero; assuming equal spring constants, you would be minimizing the sum of absolute deviations, i.e. $L_1$ regression, not least squares. This ignores the fact that the springs would have to be freely floating on the stick, so they would shift so that deformation would not be entirely in the $y$ direction, resulting in something like a Principal Components fit, but with absolute errors.",2010-09-15T16:19:10.160,795,CC BY-SA 2.5,
3363,2667,0,"Like Robin says, definition of entropy depends on choice of measure. For discrete distributions it's clear-cut, but for continuous there's no agreement on the right measure to use, so you have a spectrum of different entropies. Perhaps you should ask why you need entropy in a first place, ie, what exactly do you expect to learn from it?",2010-09-15T17:06:25.230,511,CC BY-SA 2.5,
3369,2673,0,"would *you* (as a hypothetical obstetrician) like to come in on Monday mornings to deliver babies, lol?",2010-09-15T18:14:19.843,919,CC BY-SA 2.5,
3375,2106,0,"updated ez, but still doesn't work 2: Anova() failed for some reason (maybe there are too few Ss?). should having too few Ss matter?",2010-09-15T19:17:40.950,1084,CC BY-SA 2.5,
3378,2644,0,"I'm using a hash function to determine the integral points, which is why they have uniform distribution already. Continuity is important, however, between the points. I don't want completely random noise.",2010-09-15T19:26:33.490,1267,CC BY-SA 2.5,
3379,2688,0,"What R package did you use for HC3 estimation? `sandwich`, `contrast`?",2010-09-15T19:46:25.407,930,CC BY-SA 2.5,
3380,2617,0,"I generated 10 billion numbers, found the best fit Normal distribution, and applied the inverse probability transform. It seems to work, but my data's not quite normal.",2010-09-15T19:46:28.213,1267,CC BY-SA 2.5,
3381,2688,0,"Another question while we are in: What is the design you are considering, I mean is there any clustering or multiple predictors, or is it a simple linear regression? This may help the reader to better understand the context of your study.",2010-09-15T20:47:17.963,930,CC BY-SA 2.5,
3382,2695,6,That's very layman ;-),2010-09-15T21:24:57.753,,CC BY-SA 2.5,user88
3383,2688,0,Have you tried re-expressing the dependent variable to stabilize variance?,2010-09-15T21:34:23.450,919,CC BY-SA 2.5,
3384,2682,0,@MSalters: thank you for the clarification.  I'll leave my answer up for amusement value ;-).,2010-09-15T21:35:11.593,919,CC BY-SA 2.5,
3385,2691,119,"Good question. I agree with the quote as well. I believe there are many people in statistics and mathematics who are highly intelligent, and can get very deep into their work, but don't deeply understand what they are working on. Or they do, but are incapable of explaining it to others.I go out of my way to provide answers here in plain English, and ask questions demanding plan English answers.",2010-09-15T21:43:29.863,74,CC BY-SA 2.5,
3387,333,1,Looks like Louis Bachelier started it all in 1900 http://en.wikipedia.org/wiki/Louis_Bachelier,2010-09-15T21:58:06.177,74,CC BY-SA 2.5,
3388,2699,1,They have ratings break downs by gender and age groups as well.  I suspect that they may average within those groups and then taking the average of the group means for their reported score.,2010-09-15T22:04:57.613,196,CC BY-SA 2.5,
3389,2699,0,"@drnexus Yes, good point about age, but I'm not sure about gender...",2010-09-15T22:07:21.353,8,CC BY-SA 2.5,
3390,2686,1,Hint: being more concise will probably help you get an answer.,2010-09-15T22:09:49.073,,CC BY-SA 2.5,user88
3395,2636,1,"I do not think you can infer age is log-normally distributed just from the fact that it is always positive. The gamma and the Weibull distributions are also always positive, so why not choosing those ones?",2010-09-15T23:08:27.347,582,CC BY-SA 2.5,
3396,2695,4,"It's a little mathy, but the best way to understand something is to derive it.",2010-09-16T01:08:28.433,7,CC BY-SA 2.5,
3397,2695,34,You have an exceptionally well-educated grandmother :-).,2010-09-16T01:44:58.330,919,CC BY-SA 2.5,
3398,2703,2,"Welcome to the stats site @Joel!  If you get a chance, you might also contribute to the discussion on our proposed distributed StackExchange data analysis project: http://stats.stackexchange.com/questions/2512/what-should-be-our-first-polystats-project.",2010-09-16T02:11:33.470,5,CC BY-SA 2.5,
3400,2687,1,"The mean at different levels of the repeated measures factor could change, even when r = 1.0 between scores at different time points.",2010-09-16T03:14:23.857,183,CC BY-SA 2.5,
3402,2674,0,very good. I'll sum the two and see how that works!,2010-09-16T04:45:18.490,795,CC BY-SA 2.5,
3403,2691,10,"This was asked on the Mathematics site in July, but not as well and it didn't get many answers (not surprising, given the different focus there).  http://math.stackexchange.com/questions/1146/intuitive-way-to-understand-principal-component-analysis",2010-09-16T05:03:44.287,919,CC BY-SA 2.5,
3404,2688,0,I¬¥m using the sandwich package for the HC3 estimator. I¬¥m using just simple linear regression. Intuitively I feel more comfortable with the bootstrap version and I guess I¬¥ll stick with that. //thx for the input,2010-09-16T05:19:58.830,1291,CC BY-SA 2.5,
3405,2701,0,"Note that it's often not possible to calculate the mean (without making parametric assumptions about the distribution) as the longest observed time is usually too short for the survival curve to reach zero, often it's not even close. You can estimate some of the quantiles, such as the median time, however, though which ones depends on the proportion who experience the event.",2010-09-16T06:09:56.287,449,CC BY-SA 2.5,
3408,2654,0,"@Jeromy I'm still not sure why a sigmoidal function isn't appropriate without seeing the shape of the data, but you might find this paper useful: http://www.unc.edu/~jbhill/tsay.pdf",2010-09-16T09:23:34.137,229,CC BY-SA 2.5,
3409,2698,2,"To add to this, when you have (near-)equal semiaxes (i.e. the ellipsoid has a (near-)circular slice), it indicates that the two pieces of data corresponding to those axes have (near-)dependency; one can talk about principal axes for an ellipse, but circles only have one radius. :)",2010-09-16T09:43:26.663,830,CC BY-SA 2.5,
3410,2712,0,"Thanks, but I'm trying to avoid 'confidence interval'. I seek for some formal distance metric to interpret error. I know what I'm gone pick, I just need metric 'how sure am I' (in %).",2010-09-16T09:50:22.803,1313,CC BY-SA 2.5,
3412,2712,1,"It seems you need some since I'm not familiar with, so I hope someone else will give you a more satisfying answer; nevertheless getting/assuming distribution would be a inevitable starting point for such an analysis.",2010-09-16T11:46:28.047,,CC BY-SA 2.5,user88
3413,2717,1,You should consider adding the information that you have already tried UPGMA (and others that you may have tried) :),2010-09-16T11:49:07.717,977,CC BY-SA 2.5,
3418,854,2,"I agree that computing resources allow to easily perform permutation and exact tests, or long runs of Monte Carlo simulations, but here is a paper (from *Statistics in Medicine*) where the authors argued against the systematic use of Fisher's exact test and rather recommended choosing the test according to the question that is posed: http://bit.ly/9qflht.",2010-09-16T12:20:34.290,930,CC BY-SA 2.5,
3420,2680,0,Pardoxically the Central Limit Theorem states that if you keep adding independent identically distributed Poisson variables the distribution will converge to a normal distribution. Go figure... http://en.wikipedia.org/wiki/Central_limit_theorem,2010-09-16T13:11:34.167,521,CC BY-SA 2.5,
3421,2712,0,"I have already used mine metric.
Thanks in any case.",2010-09-16T13:41:34.053,1313,CC BY-SA 2.5,
3422,2680,0,@thylacoleo I do not think there is a paradox because of two reasons. CLT applies to a suitable normalization of the sum and not to the sum itself. You need to control the growth in the numerator by a suitable scale factor as otherwise there is no convergence.,2010-09-16T13:46:44.937,,CC BY-SA 2.5,user28
3426,2675,0,"Average time in service is simply the number of days from when they first appear in our system, to when they choose to leave.  The data available is the subscriber identifier, the current state (i.e. whether or not they've left yet) and the date they joined (and also the date they left if applicable)",2010-09-16T14:08:10.393,12517,CC BY-SA 2.5,
3428,2698,6,"I would be more cautious here, J.M.  First, just to clarify, by ""near-dependency"" you must mean ""nearly independent.""  This would be true for a multinormal variate, but in many cases PCA is performed with data that are markedly non-normal.  Indeed, the clustering analyses that follow some PCA calculations can be viewed as one way to assess a strong form of non-normality.  Mathematically, circles *do* have principal axes, but they are just not uniquely determined: you can choose any orthogonal pair of radii as their principal axes.",2010-09-16T14:11:11.427,919,CC BY-SA 2.5,
3429,2712,0,"Consider adding your solution as an answer; it is fair to do so, users that will read this in future will get the solution and you could earn some reputation.",2010-09-16T14:18:51.097,,CC BY-SA 2.5,user88
3430,2711,7,"Requests to ""avoid explanation"" and a reluctance to use probability statements suggest an intention to use statistics to sanctify a result rather than understand or improve it.  That can make for a difficult conversation, because statisticians do not share this point of view.",2010-09-16T14:20:47.210,919,CC BY-SA 2.5,
3431,2678,0,Is there any software around to assist in the analysis?,2010-09-16T14:26:11.403,12517,CC BY-SA 2.5,
3432,2729,0,pam actually gives that out of the box (when using plot on the pam).  I'm still open for other solutions :),2010-09-16T15:53:27.270,253,CC BY-SA 2.5,
3433,2729,1,"@Tal I know, but you haven't mentioned it.",2010-09-16T16:07:23.477,,CC BY-SA 2.5,user88
3434,2698,1,"Yes, sorry, I suppose ""the principal axes of a circle are indeterminate"" would have been a better way of putting it.",2010-09-16T16:13:22.370,830,CC BY-SA 2.5,
3435,2701,0,"@onestop: thanks, I fixed my answer to note the mean/censored issue.",2010-09-16T16:23:59.730,251,CC BY-SA 2.5,
3436,1653,4,"@AndyF Not `lm()`, unless you move to mixed-models with the `nlme` or `lme4` package, but there's a handy way to handle repeated-measurements through appropriate specification of the `Error` term in `aov()`, see more details on Baron & Li tutorial, ¬ß6.9, http://j.mp/c5ME4u",2010-09-16T17:25:46.703,930,CC BY-SA 2.5,
3437,1723,0,"@aL3xa I think the randomization approach is better appropriate given your research field; notwithstanding the fact that usual parametric tests provide good approximation to exact permutation tests, non-parametric tests also imply some kind of assumption (e.g. on shape of the distribution). I even wonder how we might really define what is a deviation from normality in small-sample study. I think you should ask for further discussion about this point in a separate question.",2010-09-16T17:35:55.230,930,CC BY-SA 2.5,
3439,2663,0,sorry. I have difficulties to understand how i would compute permutations and what conclusions I'll be able to made for them.,2010-09-16T18:53:16.250,1154,CC BY-SA 2.5,
3441,2686,1,I agree with you entirely. My question is long and rambling but that is my problem. I do not really know what my question is. Can you help me?,2010-09-16T19:11:24.753,104,CC BY-SA 2.5,
3444,2729,0,"True, I simply forgot:  +1  :)",2010-09-16T19:20:34.237,253,CC BY-SA 2.5,
3445,2741,0,"So a set of ""If-Then"" rules is basically a forest of single-level decision trees (i.e., single binary splits on a single predictor)?",2010-09-16T19:30:28.643,1121,CC BY-SA 2.5,
3446,2741,1,"@user1121 Yes, but only in case of binary classification.",2010-09-16T19:35:45.173,,CC BY-SA 2.5,user88
3447,2729,2,Also I still think that this is a very good method.,2010-09-16T19:39:16.683,,CC BY-SA 2.5,user88
3448,2716,0,"Hey Peter! Good to see you here. This is a really good, simple, no math answer.",2010-09-16T19:40:02.957,29,CC BY-SA 2.5,
3450,2748,1,It might help to explain more about your study design: how are cases and controls sampled?,2010-09-16T20:58:19.957,449,CC BY-SA 2.5,
3453,2738,3,"@Tal Galili You are welcome Tal! If you find out some other method for choosing the number of clusters, share it with us.",2010-09-16T21:12:05.827,339,CC BY-SA 2.5,
3454,2751,0,"Well, we answer quite at the same time. Nice reference for `metan`!",2010-09-16T21:12:52.347,930,CC BY-SA 2.5,
3458,796,3,Meta-response: Good point.  But your data might not behave like mine! :-),2010-09-16T23:56:56.533,919,CC BY-SA 2.5,
3459,2751,0,Thansk everyone - I only have one group - I'm trying to calculate Orwin's (1983) modification of Rosenthal's Failsafe N and it seems like this is only possible using Cohen's d. Any suggestions?,2010-09-17T00:07:51.743,,CC BY-SA 2.5,Jay
3461,2761,0,"I'm sorry, what distinction are you making between the large sample formula and the small sample formula?  Are you calculating the variances using a population formula in large samples rather than using a sample estimate of the population variance?",2010-09-17T05:53:05.863,196,CC BY-SA 2.5,
3463,2739,1,Do this question and the answer refer solely to machine-learning?,2010-09-17T08:12:33.733,442,CC BY-SA 2.5,
3465,2765,0,"Howell is very good, but not very mathematical.",2010-09-17T08:30:35.273,442,CC BY-SA 2.5,
3468,2780,0,"+1 Great link, I missed this reference -- so I guess now it can be handled within R without any problem :) I put the PDF at this address in case someone want to read it: http://j.mp/bnnwsu",2010-09-17T09:08:23.827,930,CC BY-SA 2.5,
3469,2781,2,Thanks. The idea of using a clustering procedure had occurred to me. I imagine the challenge would be to adequately capture and weight the possible individual-level curve features in a theoretically meaningful way. I'll have a look at see how it works in kml.,2010-09-17T09:08:49.560,183,CC BY-SA 2.5,
3470,2781,1,"Well, it works pretty well although the interface is awful (and I know the guy who build it :) -- I used it two months ago for separating clinical groups based on individual profiles on developmental measurements (Brunet-L√©zine).",2010-09-17T09:12:09.407,930,CC BY-SA 2.5,
3474,2775,0,"If the entries are generated from a Normal distribution rather than a uniform, the decomposition you mention ought to be SO(n) invariant (and therefore equidistributed relative to the Haar measure).",2010-09-17T11:47:03.640,919,CC BY-SA 2.5,
3475,2781,1,Here's another primary reference for FDA: http://www.psych.mcgill.ca/misc/fda/,2010-09-17T12:02:37.287,364,CC BY-SA 2.5,
3476,2787,1,What would be the goals of qualitative and quantitative testing?,2010-09-17T13:25:28.547,,CC BY-SA 2.5,user28
3477,2789,2,"I have never heard of this approach, but it seems somehow reasonable. But, ""experiment"" should rather be a random factor, than a fixed one. These two experiments are just two randomly drawn from the population of possible experiments with this design. They are in no way fixed.",2010-09-17T13:32:51.167,442,CC BY-SA 2.5,
3478,2789,0,"Ah, you're indeed correct that experiment should be a random factor. I'll update the answer to reflect this. Thanks!",2010-09-17T13:35:03.570,364,CC BY-SA 2.5,
3479,2792,4,"Baayen's textbook, *Practical Data Analysis for the Language Sciences with R*, may still be found at http://bit.ly/aQ1KGb",2010-09-17T13:59:15.447,930,CC BY-SA 2.5,
3480,2775,0,Interesting. Can you provide a reference for this?,2010-09-17T14:39:46.903,30,CC BY-SA 2.5,
3481,2786,0,M. : Nice reference: This appears to be the most efficient solution (asymptotically).,2010-09-17T14:46:17.283,919,CC BY-SA 2.5,
3483,2792,0,"Thanks. I just had it in a kind of unusable Version with ""Draft"" in the background.",2010-09-17T17:34:43.930,442,CC BY-SA 2.5,
3484,2765,0,It's plenty mathematical. More computational.,2010-09-17T18:30:26.650,1334,CC BY-SA 2.5,
3487,2786,3,"@whuber: Heh, I picked it up from Golub and Van Loan (of course); I use this all the time to help in generating test matrices for stress-testing eigenvalue/singular value routines. As can be seen from the paper, it's essentially equivalent to QR-decomposing a random matrix like what kwak suggested, except that it is done more efficiently. There's a MATLAB implementation of this in Higham's Text Matrix Toolbox, BTW.",2010-09-17T19:14:47.173,830,CC BY-SA 2.5,
3488,2772,2,"0  down vote
 

Gappy:> this is not an answer to your question, but an alternative, more recent, and often more potent in out of sample forecasting, approach to this problem:

Large Bayesian VARs, see this recent paper http://ideas.repec.org/p/cpr/ceprdp/6326.html",2010-09-17T19:56:02.003,603,CC BY-SA 2.5,
3489,2786,0,M.:> Thanks for the matlab implementation. Would you by any chance know of a Haar pseudo-random matrix generator in R?,2010-09-17T19:58:44.687,603,CC BY-SA 2.5,
3490,2786,0,"@kwak: No idea, but if there's no implementation yet, it shouldn't be too hard to translate the MATLAB code to R (I can try to whip one up if there really isn't any); the only prerequisite is a decent generator for pseudorandom normal variates, which I'm sure R has.",2010-09-17T20:06:23.177,830,CC BY-SA 2.5,
3491,2786,0,"M.:> yes i'll probably translate it my self. Thanks for the links, Best.",2010-09-17T20:13:19.893,603,CC BY-SA 2.5,
3493,2740,0,"OK - I think this is the right conceptual answer - I hadn't appreciated the difference between the effect of the correlation across repeated measures on the within group power (improved) and between groups power (degraded).  Of course, given that this is a clinical trial, what I really care about is the power to detect a group x time interaction.  I believe I should be using in g*power the ""repeated measures, within-between interaction"" model, which does show increasing power with increasing correlation over the repeated measure.",2010-09-17T23:11:20.400,1290,CC BY-SA 2.5,
3495,527,2,"@robin: ""matrix"" in this context is standard analytical chemistry terminology; it refers to the medium where the entities to be analyzed for (the ""analytes"") can be found. For instance, if you're analyzing for the concentration of lead in tap water, lead is the analyte, and water is the matrix.",2010-09-18T00:49:04.920,830,CC BY-SA 2.5,
3496,2806,2,"There are loads of public SVD algorithms. See http://en.wikipedia.org/wiki/Singular_value_decomposition#Implementations. Can't you use or adapt one of them? Also, R is open-source, and under a GPL licence, so why not borrow its algorithm if it does the job?",2010-09-18T03:16:10.637,159,CC BY-SA 2.5,
3497,2806,0,"@Rob:  I'd like to avoid practically writing a linear algebra library, and I also want to avoid the GPL's copyleft.  Also, I've looked at bits and pieces of the R source code before and it's generally not very readable.",2010-09-18T03:25:47.253,1347,CC BY-SA 2.5,
3498,2810,0,The Wikipedia algorithm cites this and is equivalent to this for the case of finding one principal component at a time.,2010-09-18T03:26:12.870,1347,CC BY-SA 2.5,
3499,2806,4,Am I missing something? You have >10K features but <1K samples? This means the last 9K components are arbitrary. Do you want  all 1K of the first components?,2010-09-18T03:59:16.640,795,CC BY-SA 2.5,
3500,2806,2,"In any event, you can't escape having to implement SVD, though thanks to much numerical linear algebra research, there are now a lot of methods to choose from, depending on how big/small, sparse/dense your matrix is, or if you want just the singular values, or the complete set of singular values and left/right singular vectors. The algorithms are not terribly hard to understand IMHO.",2010-09-18T05:39:04.433,830,CC BY-SA 2.5,
3501,1921,1,"Nice reference! In the same vein, Categorical data analysis: Away from ANOVAs. (transformation or not) and towards logit mixed models. T. Florian Jaeger (JML 2008 59), http://j.mp/a8h763.",2010-09-18T07:42:54.967,930,CC BY-SA 2.5,
3503,2680,0,@Srikant I'm pretty sure the CLT applies to sums. http://cnx.org/content/m16948/latest/,2010-09-18T08:28:43.107,521,CC BY-SA 2.5,
3504,2680,0,"@ Srikant I'm pretty sure the CLT applies to sums. http://cnx.org/content/m16948/latest/  I think the paradox is resolved because the mean of the sum of n iid Poisson distributions is n*Œª, whilst the SD  is sqrt(n*Œª). See ""Sums of Poisson-distributed random variables"" http://en.wikipedia.org/wiki/Poisson_distribution#Properties  And for large  Œª (say >1000) the Poisson can be approximated as the normal distribution with mean Œª and variance Œª. http://en.wikipedia.org/wiki/Poisson_distribution#Related_distributions",2010-09-18T08:36:02.370,521,CC BY-SA 2.5,
3506,2782,5,"Is it a quotation? I'm just wondering how trying alternative testing procedures (not analysis strategies!) may not somewhat break control of Type I error or initial Power calculation. I know SAS systematically returns results from parametric and non-parametric tests (at least in two-sample comparison of means and ANOVA), but I always find this intriguing: Shouldn't we decide before seeing the results what test ought to be applied?",2010-09-18T09:57:47.090,930,CC BY-SA 2.5,
3507,1701,0,I used the kmeans R function with a random initialization so the results were different for each clustering attemp. Now I am reading the paper suggested above by Suresh.,2010-09-18T10:19:05.840,221,CC BY-SA 2.5,
3509,2636,0,@Rob: @nico: I'm sure you're right. It was a poor choice of example. Typically we model pharmacometric parameters like volume and clearance.,2010-09-18T12:53:10.543,1270,CC BY-SA 2.5,
3510,2820,1,"Hi Tal, thanks! Bonferroni does not seem appropriate to me - if one of my SNPs *is* causal and others are associated with it, there should be a signal, and Bonferroni has always looked too conservative to me (I usually prefer Holm's stepwise correction). The FDR you link to and p.adjust do not consider *combined* evidence (and the FDR still requires me to understand the correlation of my tests, the original question). multcomp may help, though at first glance it seems like it deals more with multiple tests within a *single* model, whereas I have *multiple* models. I'll dig deeper...",2010-09-18T14:12:53.830,1352,CC BY-SA 2.5,
3511,2796,0,"Lot's of things are known about Fisher Information, it's integrals of fisher information that I'm not sure about. I'm not familiar with what you say about Fisher Information turning into some known divergence on integration",2010-09-18T14:43:09.403,511,CC BY-SA 2.5,
3512,2822,1,"Wow, thanks for going to all this trouble! I understand your qualms about bootstrapping, and I'm almost convinced. I think my main complication is the numerical covariate I have that will certainly be necessary (either by itself or in interaction with genotype), and that seems to rule out mt.maxT and plink, although I may need to look into plink again. But I will certainly dig through the references you provided!",2010-09-18T15:56:52.983,1352,CC BY-SA 2.5,
3513,2822,0,"You can always work with the residuals of your GLM to get ride of your covariates, though you lost some Df that will be difficult to account or reintroduce afterwards (e.g. for computing p-value).",2010-09-18T16:08:45.203,930,CC BY-SA 2.5,
3514,604,0,I also don't get how you get perfect fit with a line -- what if your points don't lie on any line?,2010-09-18T18:06:19.457,511,CC BY-SA 2.5,
3515,2822,0,"Hm, residuals from my logistic regression? Would that be legitimate?",2010-09-18T18:09:35.097,1352,CC BY-SA 2.5,
3516,2822,0,"Yes, why not? It is not uncommon to remove the variance accounted for by other covariates and then move on 2nd-level analysis with your residualized data. It is often faster (for instance, plink is pretty slow with categorical covariates, while it's ok with continuous ones; `snpMatrix` or simply `glm()` performs quite better on this point but you cannot embed a lot of SNPs within `glm()`...); the problem is that getting the corrected p-value at the end of your 2nd analysis is rather tricky (because you have to account for the parameters already estimated).",2010-09-18T18:27:33.777,930,CC BY-SA 2.5,
3517,2822,0,"For an illustration of how people are working with residuals, see for example p. 466 of Heck et al. Investigation of 17 candidate genes for personality traits confirms effects of the HTR2A gene on novelty seeking. *Genes, brain, and behavior* (2009) vol. 8 (4) pp. 464-72",2010-09-18T18:31:55.277,930,CC BY-SA 2.5,
3520,2822,0,"I think the Nyholt's paper is worth looking at. I'm just re-reading it and it seems to provide good reference and a well-founded framework. Maybe you'll have to check if there isn't more recent publications on this topic, e.g. http://j.mp/9jHTpI but I just look at the abstract.",2010-09-18T18:39:35.820,930,CC BY-SA 2.5,
3521,2822,0,"Last but not least, Nicodemus has been very active for promoting the use of Random Forests when working with SNPs, and RFs are great because you get measure of variable importance and % of classification error on resampled subjects, see e.g. http://j.mp/cstgeO. There is also a paper on how to use haplotype block instead of PCA like suggested in the aforementioned paper, http://j.mp/c4V45i",2010-09-18T18:46:08.160,930,CC BY-SA 2.5,
3522,2826,1,"Thanks. Unfortunately, few of my data are numeric, but this is a useful phenomenon to know about. Incidentally, there was a nice programme about this on BBC Radio 4 a few years ago - looks like it's still available in RealAudio: http://www.bbc.co.uk/radio4/science/further5.shtml",2010-09-18T19:24:44.963,1343,CC BY-SA 2.5,
3523,2778,0,"I have found the book by Maxwell and Delaney, and having read already 20-30 pages, I must say it is very nice... I'll keep on reading and I think I'll find the answers I'm looking for, thanks!",2010-09-18T19:58:22.763,1320,CC BY-SA 2.5,
3524,2820,0,"Hello Stephan.  I understand, sorry for not helping more.  Good luck!  Tal",2010-09-18T20:14:01.537,253,CC BY-SA 2.5,
3525,2810,0,"OK, I see the link now.  This is a fairly simple approach, and like Wikipedia mentions, there are advances upon this basic idea.  On reflection though, you're going to have to deal with some kind of trade-offs (convergence in this case).  I wonder if you're asking the right question here.   Are there really no good bindings to linalg libraries for D?",2010-09-18T20:26:54.510,251,CC BY-SA 2.5,
3526,2828,0,can you give more information about what attributes are available about the models?,2010-09-18T20:42:21.607,795,CC BY-SA 2.5,
3527,1815,4,See also this question  (asked with specific reference to designining clinical trials): http://stats.stackexchange.com/questions/2770/good-text-on-clinical-trials/,2010-09-18T21:02:27.720,266,CC BY-SA 2.5,
3528,2828,0,"This is kind of an open question, so my question would be -- what kind of attributes do I need to be able to measure complexity? At the most basic level, a probability model is a set of probability distributions, and I fit the model to data by picking the best fitting member",2010-09-18T21:32:21.587,511,CC BY-SA 2.5,
3529,2835,0,you don't need to tweak it. Quantreg will happily digest irregularly spiced time series.,2010-09-18T21:57:54.807,603,CC BY-SA 2.5,
3530,2214,0,We need a finite mean (first moment) to compute an L-estimator (don't we?). Levy distributed r.v. do not come with such niceties. Correct me if i'm wrong.,2010-09-18T22:02:30.867,603,CC BY-SA 2.5,
3531,2835,0,"No doubt about that. But does it do *local* quantile regression? - If not, one can play around with the weights and just calculate lots of models",2010-09-18T22:06:46.603,1352,CC BY-SA 2.5,
3532,2830,0,"Hm...the paper is all about regression, I wonder if this can be used for discrete probability estimation. Also, I don't really understand the motivation he gives for it -- gdf is a degree of sensitivity of parameters to small changes in data, but why is it important? I could choose a different parameterization where small changes in parameters in original parameterization correspond to large changes in the new parameterization, so it'll seem more sensitive to data, but it's the same model",2010-09-18T22:07:39.080,511,CC BY-SA 2.5,
3533,2830,0,"Yaroslav:> *I could choose a different parameterization where small changes in parameters in original parameterization correspond to large changes in the new parameterization, so it'll seem more sensitive to data * can you give an example (involving an affine equivariant estimator)? Thanks,",2010-09-18T22:24:39.893,603,CC BY-SA 2.5,
3534,2830,1,"DoF in linear regression work out to the trace of the hat matrix or the sum of sensitivities -- so the motivation/concept aren't all that far out.  Tibshirani & Knight proposed Covariance Inflation Criterion which looks at covariances of model estimates instead of sensitivities.  GDF seems to have been applied in a number of model procedures like cart and wavelet thresholding (Ye's paper on adaptive model selection has more details), and in ensemble methods to control for complexity, but I don't know of any discrete estimation cases.  Might be worth trying ...",2010-09-18T23:11:29.817,251,CC BY-SA 2.5,
3535,2830,0,"Don't know about ""affine equivariant estimators"", but suppose we rely on maximum likelihood estimator instead. Let q=f(p) where f is some bijection. Let p0,q0 represent MLE estimate in corresponding parameterization. p0,q0 are going to have different asymptotic variances, but in terms of modeling data, they are equivalent. So the question comes down to -- in which parameterization is the sensitivity of parameters representative of expected risk?",2010-09-18T23:15:55.140,511,CC BY-SA 2.5,
3536,2833,0,"Good advice most of the time, but what about the case of large datasets, where you can't feasibly look through all the data manually?",2010-09-19T00:51:01.033,1347,CC BY-SA 2.5,
3538,2817,5,Good grief... constructing the covariance matrix is never the best way for SVD. I displayed an example of why explicitly forming the covariance matrix is not a good idea at math.SE: http://math.stackexchange.com/questions/3869/3871#3871 .,2010-09-19T03:30:04.673,830,CC BY-SA 2.5,
3539,2782,4,"@chl good point. I agree that the above rule of thumb can be used for the wrong reasons. I.e., trying things multiple ways and only reporting the result that gives the more pleasing answer. I see the rule of thumb as useful as a data analyst training tool in order to learn the effect of analysis decisions on substantive conclusions. I've seen many students get lost with decisions particularly where there is competing advice in the literature (e.g., to transform or not to transform) that often have minimal influence on the substantive conclusions.",2010-09-19T05:45:50.497,183,CC BY-SA 2.5,
3540,2106,0,What's the error message?,2010-09-19T07:06:42.420,966,CC BY-SA 2.5,
3541,2811,0,"Good point about latent groups. I've seen several applications of latent class analysis & cluster analysis where it seems to be just carving up a continuous variable int categories such low & high (http://jeromyanglim.blogspot.com/2009/09/cluster-analysis-and-single-dominant.html). However, I do have some individual-level longitudinal data which visually look like they are coming from  categorically distinct data generating processes (e.g., always high, always low, gradual increasing, low-then-abrupt-increase, etc.) and within categories there is more continuous variation of parameters.",2010-09-19T09:22:23.023,183,CC BY-SA 2.5,
3542,2844,0,"For the 2nd question, you mean the eigenvectors or the component loadings summary?",2010-09-19T09:40:04.747,930,CC BY-SA 2.5,
3543,2844,0,"Hi chl - I mean the output from ""summary(pc.cr)"" - for some reason, I can't find it. (doing something like summary(pc.cr)[[1]] will get me only part of the table)",2010-09-19T09:44:07.483,253,CC BY-SA 2.5,
3544,2782,1,@chl no it's not a quotation. But I thought it was good to demarcate the rule of thumb from its rationale and caveats. I changed it to bold to make it clear.,2010-09-19T10:11:22.950,183,CC BY-SA 2.5,
3545,2781,1,"I found this introduction to FDA link by Ramsay (2008), particularly accessible http://gbi.agrsci.dk/~shd/public/FDA2008/FDA_Sage.pdf",2010-09-19T10:27:29.060,183,CC BY-SA 2.5,
3546,2782,1,"Ok, it makes sense to me to try different transformations and look if it provides a better way to account for the studied relationships; what I don't understand is to try different analysis strategies, although it is current practice (but not reported in published articles :-), esp. when they rely on different assumptions (in EFA vs. PCA, you assume an extra error term; in non-parametric vs. parametric testing, you throw away part of the assumptions, etc.). But, I agree the demarcation between exploratory and confirmatory analysis is not so clear...",2010-09-19T10:29:29.263,930,CC BY-SA 2.5,
3548,2814,0,"Indeed this is the same proof as the one provided by Paul Rubin on or-exchange.com. Multiplying each correlation by a constant has a very significant drawback though - it is a very very small stress test. It is possible to apply a much larger stress test by adding a constant to each matrix, not multiplying. However, it seems the analytical solutions for that are a little less obvious.",2010-09-19T12:15:13.890,1250,CC BY-SA 2.5,
3550,2807,2,Electrical potential difference is an example of a real-world quantity that can be negative.,2010-09-19T13:03:21.280,582,CC BY-SA 2.5,
3551,2833,1,"@dsimcha It also depends of sample size per group. It is known, for example, that when the samples are of equal size the t-test  is robust against departure from the homoscedasticity assumption; if $n_1\neq n_2$, then the probability of a type I error will be $<\alpha$ if the larger $\sigma^2$ is associated with the larger sample, and *vice versa*. See Zar, JH *Biostatistical Analysis* (4th Ed., Prentice Hall, 1998) for further references.",2010-09-19T13:17:37.500,930,CC BY-SA 2.5,
3552,2828,3,"What, precisely, is ""complexity""?  (This is not a flippant question!)  In the absence of a formal definition, we cannot hope to make valid comparisons of something.",2010-09-19T13:30:33.050,919,CC BY-SA 2.5,
3553,2415,3,"Concerning the first bullet (drug trials): even people who otherwise might not be interested in medical experimentation should read the NYTimes article *New Drugs Stir Debate on Basic Rules of Clinical Trials* (http://www.nytimes.com/2010/09/19/health/research/19trial.html?pagewanted=1&_r=1&th&emc=th ).  The statistically literate reader will immediately see the unstated implications concerning experimental design and using p-values for decision making.  There is a statistical resolution, somewhere, to the life-and-death conundrum described in this article.",2010-09-19T13:35:16.840,919,CC BY-SA 2.5,
3554,2852,0,Could you explain how data were collected and what they intend to measure? Are the different columns of your data matrix independent measurements?,2010-09-19T14:23:14.670,930,CC BY-SA 2.5,
3555,2849,0,what's the other dimension of the panel (i.e. N)?,2010-09-19T14:41:27.093,603,CC BY-SA 2.5,
3556,2852,0,Thank you. These are biological data and each column are indepent data under the same measure method. I only want to know the difference between the first column band each one of others.,2010-09-19T15:52:51.437,,CC BY-SA 2.5,Chuangye
3557,2849,0,"thanks Jeromy for your reply,my second dimension is N=16.",2010-09-19T16:27:19.520,1251,CC BY-SA 2.5,
3559,2849,0,Sorry i mean Kwak,2010-09-19T16:36:57.930,1251,CC BY-SA 2.5,
3560,2849,0,"Ama>: imho that's much too little. Even if the observations were drawn from a single process (i.e. a single run of 4*16=64 *differentiated* observations), the granger test for causality would probably require more data points. Now in your case you would also have to account for the loss of degrees of freedom to control for the fact that your observations are coming from different process.",2010-09-19T16:52:24.340,603,CC BY-SA 2.5,
3561,2811,0,"@Jeromy, I don't think the work I cited would discourage people from using such methods to identify latent groups. I would say the point of the work is that you can't use such methods to solely infer the existence of groups, because you will always find groups, even in random data. It is up to more subjective interpretation whether those groups you find are real or are simply artifacts of the method. You could identify some logical theories that generate such processes and then see if the groups identified fit within those theories.",2010-09-19T16:59:23.633,1036,CC BY-SA 2.5,
3562,2849,0,"sure, you are right, but is there any possibility to bypasse this issue??",2010-09-19T16:59:44.570,1251,CC BY-SA 2.5,
3563,2846,0,Great topic - You'll get many +1's for this question...,2010-09-19T17:07:02.247,253,CC BY-SA 2.5,
3564,1653,0,"@AndyF `aov()` is built on top of the `lm()` function but include additional argument, called *Special* terms, like `Error`.",2010-09-19T17:44:29.677,930,CC BY-SA 2.5,
3566,2828,0,That's what I'm asking essentially,2010-09-19T17:49:14.097,511,CC BY-SA 2.5,
3568,2847,0,"+1 I also use FactoMineR but I remember that when I tried it's PCA method on a really large dataset, I never got results.",2010-09-19T18:11:01.953,339,CC BY-SA 2.5,
3569,2828,2,"But can't you at least give us a hint as to what aspect of a model you're trying to capture in the word ""complexity""?  Without that, this question is just to ambiguous to admit one reasonable answer.",2010-09-19T18:14:42.803,919,CC BY-SA 2.5,
3570,2859,0,"Nice remark! Apart from looking for a plausible interpretation of the data, do you think the use of an exact permutation test is not an option there?",2010-09-19T18:22:24.360,930,CC BY-SA 2.5,
3571,2847,0,"@gd047 It failed for me too, though it is based on an SVD (might be optimized to handle large data set :)",2010-09-19T18:26:52.620,930,CC BY-SA 2.5,
3572,2834,0,"would you mind clarifying what you mean by ""(a) our conclusions should not depend on the particular sample used for the comparison""?  I'm having trouble due to the ambiguity of ""sample"" in this context: does it mean ""statistical sample"" (a set of data presumed to represent a process or population) or ""environmental sample"" (a bit of water, soil, air, or tissue, typically).  With either meaning I can't quite draw the logical line to your conclusion that this ""precludes any method based on correlations.""",2010-09-19T18:36:54.130,919,CC BY-SA 2.5,
3573,2859,2,"@chl: I think an exact permutation test would work fine; I implicitly applied two of them in my response.  The reason why it should work is that the *actual* test statistic itself will not be affected by ties, because none of the data in the first column is tied within anything, so that their ranks are all uniquely determined.  For the same reason, a Wilcoxon test that does *not* adjust for ties should give correct results!",2010-09-19T18:40:38.190,919,CC BY-SA 2.5,
3574,1653,0,"aov() is simply a wrapper to lm().  It does some contrast coding behind the scenes and packages the result in the ANOVA style.  All of it is modeled by lm().  In the article I referenced above, it tells you how to set up coding to do repeated contrasts in regression models, including lm().",2010-09-19T18:51:29.937,485,CC BY-SA 2.5,
3576,2833,2,"@dsimcha re large datasets: depends on what you mean by ""large"". Many observations? Use good graphics (boxplot, jittered dotplots, sunflowerplots). Many independent variables? Yes, you have a point there... But if you have so many IVs that you can't plot the DV against each IV, I'd question using an ANOVA at all - it looks like it may be hard to interpret in any case. Some smart machine learning approaches may be better (Brian D. Ripley: ""To paraphrase provocatively, 'machine learning is statistics minus any checking of models and assumptions'."")",2010-09-19T19:13:43.703,1352,CC BY-SA 2.5,
3577,2834,0,"@whuber Well, I mean the collection of observed data (e.g. glucose concentration) which, ideally, should be representative of the likely range of what is being measured. Relying on correlation may be misleading because it depends on the sampled units (e.g. patients in an hospital): we can get a higher correlation just by getting one or more extreme measurement on either scale, although the relation between the two methods is still the same. Hence, the idea is that the distribution of the measure of interest should not influence our conclusion about methods comparability. (...)",2010-09-19T19:16:55.103,930,CC BY-SA 2.5,
3578,2834,0,"@whuber (...) What we want to assess is the *agreement beyond the data*, not the relationship in the data (I'm quoting Carstensen 2010 p. 8-9).",2010-09-19T19:17:16.197,930,CC BY-SA 2.5,
3579,2814,0,"what exactly is your question, then?",2010-09-19T19:55:00.483,795,CC BY-SA 2.5,
3580,854,0,@chl: I don't have (free) access to the article. I'm very interested in the arguments. What can be more exact than Exact?,2010-09-19T20:11:10.880,506,CC BY-SA 2.5,
3581,854,2,Here is the electronic copy in case you want to read it (it's very instructive and includes a lot of references): http://j.mp/b2Wbpn.,2010-09-19T20:20:25.157,930,CC BY-SA 2.5,
3582,2828,0,amount of data needed to identify a good member of the model,2010-09-19T21:38:19.577,511,CC BY-SA 2.5,
3583,2814,0,"Same as before - I am looking for an analytical solution for correlation matrix stress testing. Solutions have been suggested for stress testing utilising multiplication but not addition of stress levels (addition was my original question above). Either way, thank you everyone.",2010-09-19T22:02:14.270,1250,CC BY-SA 2.5,
3587,1164,12,"Tests relying on many assumptions are more powerful when those assumptions are satisfied. We can test for significance of deviation assuming that observations are IID Gaussian, which gives mean as statistic. A less restrictive set of assumptions tells us to use median. We can go further and assume that observations are correlated to get even more robustness. But each step reduces the power of our test, and if we make no assumptions at all, our test is useless. Robust tests implicitly make assumptions about data and are better than classical only when those assumptions match reality better",2010-09-19T23:20:07.890,511,CC BY-SA 2.5,
3588,2846,1,"@Tal .. thanks :-) But I wanted to know more and possibly expand the Wiki article, which is not too informative.",2010-09-19T23:45:37.980,1307,CC BY-SA 2.5,
3589,2814,0,"ah, I think I see: you want to add a fixed amount to every off-diagonal element? I think my other answer can be adapted to that case.",2010-09-20T04:14:54.913,795,CC BY-SA 2.5,
3590,1884,0,"See also http://en.wikipedia.org/wiki/Wald‚ÄìWolfowitz_runs_test , where the mean number of runs matches that given here.",2010-09-20T04:51:32.113,795,CC BY-SA 2.5,
3591,2221,0,"yes, it is correct. Both formulations are a little bit fishy, though. Imagine if the dart board were 30 feet in diameter. I don't think either of these models would be appropriate in that case",2010-09-20T05:27:01.637,795,CC BY-SA 2.5,
3592,2852,0,"When you say you ""want to compare the difference between the first column [and the] other columns"" are you implying that row 1 represents 6 different measurements on the same object, subject, unit (whatever), row 2 likewise, etc? So we have only 6 things being measured. Or are we looking at 36 different units being measured? And you have not explained why you want to ""compare the difference..."". What are you trying to test?",2010-09-20T06:35:54.450,521,CC BY-SA 2.5,
3593,2885,1,"I've read that paper and it seems like Stochastic Complexity fixes the problem of not being able to distinguish between models of same dimensions, but introduces a problem of sometimes not being able to distinguish between models of different dimensions.  Geometric distribution is assigned infinite complexity, surely not what we'd expect for such a simple model!",2010-09-20T06:45:50.200,511,CC BY-SA 2.5,
3594,1857,1,"That rule of thumb mainly applies to classical methods like l2 regularized maximum likelihood, L1 regularized methods can learn effectively when number of adjustable parameters is exponential in the number of observations (ie, Miroslav Dudik, 2004 COLT paper)",2010-09-20T07:03:45.523,511,CC BY-SA 2.5,
3595,2885,0,"Very good point about infinite stochastic complexity (SC). Solutions to the problem of infinite SC exist, but are not very elegant; Rissanen's renormalization works well in linear models, but is not easy to do for the Poisson/Geometric problem. The MML (or SMML) encoding of Poisson/Geometric data is fine though.",2010-09-20T07:07:59.100,530,CC BY-SA 2.5,
3596,2846,0,Great incentive :),2010-09-20T07:19:51.593,253,CC BY-SA 2.5,
3597,2853,1,+1 for the great answer that wraps everything up. How to apply the mentioned numerical procedures is nicely and applicably described in Tabachnik and Fidell's Using Multivariate Statistics (for SPSS and SAS): http://www.amazon.com/Using-Multivariate-Statistics-Barbara-Tabachnick/dp/0205459382/ref=sr_1_1?ie=UTF8&s=books&qid=1284969084&sr=8-1 (But see the Erratas on the accompanied web page),2010-09-20T07:53:29.283,442,CC BY-SA 2.5,
3598,2888,0,"Yes, the Wu et al. 2009 is a nice paper. Incidentally, I've been working on GWAS and ML during the last two years; now I'm trying to go back to clinical studies where most of the time we have to deal with imperfect measurements, missing data, and of course... a lot of interesting variables from the point of view of the physicist!",2010-09-20T08:37:51.073,930,CC BY-SA 2.5,
3599,2836,2,"Just a quick note: minimum description length is very powerful and useful, but it can take ages to obtain results, especially when using normalized maximum likelihood withslighltty larger datasets. I once took 10 days running FORTRAN code to get it for just one model",2010-09-20T09:02:18.237,447,CC BY-SA 2.5,
3601,2892,0,"Do you have in mind any particular application, that is do you seek for general advices about how many EVs we need to consider apart from any application (i.e. on a pure mathematical side) or should it apply to a specific context (e.g. factor analysis, PCA, etc.)?",2010-09-20T10:32:13.380,930,CC BY-SA 2.5,
3602,2892,0,"I'm interested more in the mathematical side, ie eigenvalues as a property of the data  underlying a correlation matrix. If it makes sense to discuss this in terms of specific context, feel free to do so too.",2010-09-20T11:00:01.263,1250,CC BY-SA 2.5,
3603,2894,0,Any reason why you marked this as CW? The question seems specific enough to allow for *one* right answer and thus should not be CW.,2010-09-20T14:24:05.950,,CC BY-SA 2.5,user28
3604,2834,0,"Thank you; that clarifies your position well.  This is essentially an exercise in *calibration* except that we do not appear to have a reference standard for comparison; we merely assume that the physical samples chosen by the experimenter cover some range of true concentrations.  Thus, as you write, correlation *per se* is not necessarily a useful measure of agreement among the two methods.  Typically though, especially for chemical analyses, the true concentration is known (because the experimenter introduced a known amount of a substance into the matrix).",2010-09-20T14:25:48.863,919,CC BY-SA 2.5,
3606,2884,1,"At this point you need to consider how the data were generated and you need to more formally specify your study objectives.  The Kruskal-Wallis test, as I recall, looks for a shift of location but assumes homogeneous variances, which obviously is not the case here (compare V4 to V6).  The data-generation process may suggest useful analyses (e.g., these data might be rescaled counts or they could be autocorrelated or the low values might represent measurement noise only).",2010-09-20T14:38:19.013,919,CC BY-SA 2.5,
3607,2834,0,"@whuber That's right. In the absence of a gold standard, we are merely interested in the extent to which the two methods yield ""comparable"" results, hence the idea of relying on so-called limits of agreement. Although the true measure may be known in advance, each measurement instrument carry its own measurement error -- at least for those I used to deal with in the biomedical (e.g. blood glucose concentration) and neuropsychological (e.g. depression level) domain.",2010-09-20T14:42:39.633,930,CC BY-SA 2.5,
3608,2894,0,"Although it doesn't answer your question, you may find this answer to be helpful in your quest http://stats.stackexchange.com/questions/1807/how-to-perform-students-t-test-having-only-sample-size-sample-average-and-popul/1836#1836",2010-09-20T14:44:25.233,1036,CC BY-SA 2.5,
3610,1929,0,"Srikant:> No. The definition has to have two important feature of the univariate median. a) Invariant to monotone transformation of the data, b) robust to contamination by outliers. None of the extentions you propose have these. The Tukey depth has these qualities.",2010-09-20T16:57:58.290,603,CC BY-SA 2.5,
3611,2894,0,"@Srikant Because I didn't understand the nature of CW posts. (I went just now and looked it up on meta -- oops.) I thought I was being helpful. Alas, it looks like there's no way to undo it.",2010-09-20T17:01:14.617,1376,CC BY-SA 2.5,
3612,2891,0,Is this compositionnal data (i.e. do the lines sum to a constant)?   http://en.wikipedia.org/wiki/Compositional_data,2010-09-20T17:01:58.563,603,CC BY-SA 2.5,
3613,2890,0,"I don't really understand ""model mimicry"", but cross-validation seems to just postpone the task of assessing complexity. If you use data to pick your parameters *and* your model as in cross-validation, the relevant question becomes how estimate the amount of data needed for this ""meta""-fitter to perform well",2010-09-20T17:09:21.123,511,CC BY-SA 2.5,
3615,1929,0,@kwak What you say makes sense.,2010-09-20T18:25:06.690,,CC BY-SA 2.5,user28
3617,2894,1,@josh that's fine. I was just curious about your choice.,2010-09-20T18:25:54.347,,CC BY-SA 2.5,user28
3618,2904,0,"are BetaH, BetaM, BetaL also parameters (i.e. things to estimate)?",2010-09-20T18:31:58.943,603,CC BY-SA 2.5,
3619,1929,0,"@Srikant:> Check the R&S paper cited by Gary Campbell above ;). Best,",2010-09-20T18:38:45.233,603,CC BY-SA 2.5,
3620,1929,0,"@kwak On thinking some more, the taxicab metric does have the features you mentioned as it basically reduces to univariate medians. no?",2010-09-20T18:40:07.397,,CC BY-SA 2.5,user28
3621,2904,0,Kwak -- yes; I'll clarify that in the question statement.,2010-09-20T18:40:28.190,53,CC BY-SA 2.5,
3622,1929,0,"@Sri:> no. For instance: if you pre-multiply $x,y$ by a non singular,symmetric $2 \times 2$ matrix $A$, the ranking of the $|(x_i,y_i)-(m_x,m_y)|$ will change. The taxicab metric is not affine invariant. The median is invariant to a even larger group of transformation (it is monotone invariant). Same for robustness, an arbitrary small contamination of your dataset by an observation $x_i$ located at $+\infty$ will cause the location of $m_x$ to shift without bounds, hence changing, again, all the rankings of the $|(x_i,y_i)-(m_x,m_y)|$. The taxicab metric has a breakdown point of $0$.",2010-09-20T18:57:42.323,603,CC BY-SA 2.5,
3623,1929,0,"The median, again, has a break down point of 50 percent (the bivariate tukey depth has a break down point of 33 percent (1/(1+p)) and is monotone invariant.",2010-09-20T18:58:37.627,603,CC BY-SA 2.5,
3624,2686,0,very good question (somehow i just saw it). +1.,2010-09-20T19:05:01.293,603,CC BY-SA 2.5,
3625,2737,0,Shabbychef:> i think Farrel specifically mentioned an argument against this approach in his question.,2010-09-20T19:06:00.403,603,CC BY-SA 2.5,
3626,578,0,In chemometrics the instrument responses are often nonlinear and heteroscedastic.  At a minimum that imposes a certain amount of caution when conducting and interpreting the regression.,2010-09-20T19:06:19.173,919,CC BY-SA 2.5,
3627,2896,3,Why would one need to invoke the CLT when sampling from a known (or assumed) Gaussian distribution?  The mean of even a sample of one will be Normally distributed!,2010-09-20T19:09:50.000,919,CC BY-SA 2.5,
3628,2894,1,"Google ""adaptive sampling"" and ""sequential sampling"".  If you're still stuck, include ""Wald"" as a keyword and then work forward historically (i.e, look at papers that reference Wald's work on sequential sampling, then look at papers that reference them, etc.).",2010-09-20T19:10:59.647,919,CC BY-SA 2.5,
3629,1929,0,@kwak I see the point. I will not delete this incorrect answer for the benefit of future readers.,2010-09-20T19:13:21.837,,CC BY-SA 2.5,user28
3631,2908,2,"Yes, I've read his remarks in (I believe) JASA, and he seems to really try to be respectful to statisticians. Very unlike the way he treats economists.",2010-09-20T19:18:21.143,666,CC BY-SA 2.5,
3632,2908,1,"I agree that his books are full of bluster, but the ideas are IMHO valid, and in many cases, scary.",2010-09-20T19:41:40.913,247,CC BY-SA 2.5,
3633,1931,0,phv:> one can ask for 'the' generalization to preserve (in higher dimensions) some of the interesting properties of the median. This severly limits the number of candidates (see the commenting after Srikant's answer below),2010-09-20T20:10:15.770,603,CC BY-SA 2.5,
3634,1931,0,@Whuber:> then notion of ordering can be generalized to R^n for unimodal distributions (see my answer below).,2010-09-20T20:11:05.130,603,CC BY-SA 2.5,
3635,1929,2,@Srikant:> there are no incorrect answer to phv's questions because there are no 'good answers' either; this area of research is still under development. I simply wanted to point out why it is still an open problem.,2010-09-20T20:13:22.633,603,CC BY-SA 2.5,
3636,2908,1,+1 Loved your blog post. Explains Taleb's ideas very well.,2010-09-20T20:27:37.163,666,CC BY-SA 2.5,
3639,2894,0,"Is there anything wrong with computing the sample mean, the sample variance and a confidence interval?",2010-09-20T20:48:04.647,352,CC BY-SA 2.5,
3640,2911,0,"Thanks for the links! The question is open to any statistical software -- I use Python and Stata from time to time, so I wonder if confirmed users may bring interesting recommendations there.",2010-09-20T20:51:34.757,930,CC BY-SA 2.5,
3641,2904,0,"I looks to me like you should only need 3 guesses, one for when each of $H'$, $M'$ and $L'$ is largest.  But, I may simply misunderstand the question.",2010-09-20T20:55:39.107,352,CC BY-SA 2.5,
3642,2911,0,Absolutely; although I would add that the recommendations in the above links could really apply to any statistical project (regardless of the language).,2010-09-20T20:58:44.827,5,CC BY-SA 2.5,
3643,2911,0,"Definitely, yes! I updated my question at the same time.",2010-09-20T21:03:00.437,930,CC BY-SA 2.5,
3644,1931,0,"@kwak: could you elaborate a little?  The usual mathematical definition of an ordering of a space is independent of any kind of probability distribution, so you must implicitly have some additional assumptions in mind.",2010-09-20T21:14:45.240,919,CC BY-SA 2.5,
3645,2901,3,But doesn't the third (central) moment do this in a more stable and practical way?,2010-09-20T21:16:58.407,919,CC BY-SA 2.5,
3646,2905,1,I'm curious: what is the trick?,2010-09-20T21:20:06.720,795,CC BY-SA 2.5,
3647,2894,1,"@Robby McKilliam: But what data do you use?  This question arises before any data have been collected.  If you collect values one at a time and compute a CI after each new one is added to the dataset, you cannot use standard formulas for the intervals due to the correlated multiple comparisons you are making.  Thus, you need a *stopping rule* that optimizes the sum of the statistical risk of your estimator and the cost of collecting each additional sample.",2010-09-20T21:22:44.220,919,CC BY-SA 2.5,
3648,1865,0,why has this question/answer pair been imported _in toto_ from the math SE?,2010-09-20T21:28:39.703,795,CC BY-SA 2.5,
3649,2913,0,"i.e. the data matrix is m-by-n, where m is (usually) greater than n. Forming the covariance matrix from that will always give you an n-by-n matrix no matter how you increase or decrease m. (That's part of why using the normal equations for least-squares seduces a lot of people...)",2010-09-20T21:38:24.010,830,CC BY-SA 2.5,
3650,2894,1,"@whuber thanks! I'm still digesting the material, but I think that this is exactly what I'm looking for. If this were an answer, I'd accept it...",2010-09-20T21:41:32.760,1376,CC BY-SA 2.5,
3651,2913,0,"@whuber: Well, computing the sample covariance matrix does depend on the number of data points as well and this computation costs something like O(N*P^2) for N data points having P features each. Besides, many of the fast iterative algorithms for PCA (e.g., the Expectation Maximization algorithm for PCA) have a time complexity dependent on N. It can really make a difference if N is large. So one can't say in general that the computational complexity will not at all depend on the number of data points.",2010-09-20T22:15:08.057,881,CC BY-SA 2.5,
3652,2913,0,"@ebony1: But O(N*P^2) is O(N), which is exactly what I was saying.  If your N really is around 1000, you can't improve computational performance (which is already going to be fast unless P is huge) by more than a factor of 2.  In most cases that's hardly worth sweating over and in this case, for reasons ably pointed out in your problem statement, it seems like subsampling may not be a good idea.",2010-09-20T22:18:15.610,919,CC BY-SA 2.5,
3653,2901,3,"@Whuber:> the third is measuring overall asymmetry, which is not the same thing as tail asymmetry. Because of the higher exponent, the value of the fifth is nearly entirely determined by the tails.",2010-09-20T22:31:26.543,603,CC BY-SA 2.5,
3654,2914,0,"Just to throw a couple of ideas on the subject, if the study discloses standard regression statistics you could focus on the t stats and p values of the coefficients.  If the RSquare of the model is high; but, one or more of the variables have a t stat < 2.0; this could be a red flag.  Also, if the sign of the coefficients on some of the variables defy logic that is probably another red flag.  If the study does not disclose a hold out period for the model, that may be another red flag.  Hopefully, you will have other and better ideas.",2010-09-20T23:12:15.183,1329,CC BY-SA 2.5,
3655,2909,0,How accurate do you need it? Could you just simulate the walk and avoid fully-functional solutions?,2010-09-20T23:20:43.030,53,CC BY-SA 2.5,
3658,604,0,"I understand your question, and I share your conundrum.  I would think that any method to estimate something would be able to replicate its exact estimates if the latter are used as the dependent variable.  The method you use has its own constraints (linear relationships, normal distribution, or whatever).  But, the estimates by definition reflect the constraints of the method you use.  So, you should replicate the estimates exactly.",2010-09-20T23:35:40.460,1329,CC BY-SA 2.5,
3659,2914,0,One way is to see how the model performs on other (but similar) data.,2010-09-20T23:36:49.820,5,CC BY-SA 2.5,
3660,2919,0,Is Doug Bate's mixed-effects model the same as a hierarchical model?,2010-09-20T23:45:52.673,1329,CC BY-SA 2.5,
3661,2919,1,yes: http://en.wikipedia.org/wiki/Multilevel_model,2010-09-20T23:56:47.153,603,CC BY-SA 2.5,
3662,2909,0,"good point. I don't need atomic-physics level accuracy. in fact, 3 sigfigs is probably just fine....",2010-09-21T00:11:39.167,795,CC BY-SA 2.5,
3663,2737,0,The question is somewhat ambiguous; my takeaway was that the OP was looking for 'buzzwords'.,2010-09-21T00:13:21.707,795,CC BY-SA 2.5,
3664,2905,0,Do you mean the _eigenvectors_ of $\tilde{C}$ in 3? not _values_?,2010-09-21T00:16:51.123,795,CC BY-SA 2.5,
3665,2905,0,no. $\lambda_1$ is a scalar.,2010-09-21T00:27:52.160,603,CC BY-SA 2.5,
3667,2833,0,"Good comment, +1.  Even though this specific question is about ANOVA, I was thinking on a more general level about the question of plots vs. tests when I wrote my response.",2010-09-21T01:28:46.920,1347,CC BY-SA 2.5,
3668,1931,1,"@Whuber:> You state: ""R1 can be ordered but R2, R3, ... cannot be"". R2,..,R3 can be ordered in many ways by mapping from Rn to R . One such way is the tukey depth. It has many important properties (robustness to some extend, non parametric, invariance,...) but these only hold for the case of unimodal distributions. Let me know if you want more details.",2010-09-21T01:47:08.540,603,CC BY-SA 2.5,
3669,2925,2,what do you mean by 'verify'? does this mean the third party controls the external event? this would contradict the pmf here.,2010-09-21T02:39:30.683,795,CC BY-SA 2.5,
3670,2905,0,This is a very odd procedure; has it been published somewhere?,2010-09-21T02:42:57.420,795,CC BY-SA 2.5,
3671,2913,0,@whuber: My question wasn't as much about a specific case of 500/500 split. It was just an example. Maybe something like 10% in-sample/90% out-of-sample would have been a better example. My question in general was about the reliability of the embeddings of the out-of-sample data points if you do PCA using a small sample of the data and use the learned projection to compute the embeddings of the out-of-sample data points.,2010-09-21T02:51:28.380,881,CC BY-SA 2.5,
3672,2925,0,"Obviously the third party should not control the external event for the question to make sense. As an example of what I mean, suppose we agree that the protocol is this: If it rains tomorrow in New York X=0 otherwise X=1. The draw is based on an external event and is verifiable in the sense that someone can check if it really rained post-facto.",2010-09-21T02:58:55.517,,CC BY-SA 2.5,user28
3674,2929,0,"So, this would mean when having two distributions to compare, I could plot two Lorenz curves and calculate two Gini coefficients for them. This should then be comparable with creating one QQ plot with the advantage that the Lorenz/Gini solution would allow me to compare the mentioned distributions numerically, whereas the QQ-plot only allows me to compare visually. Correct?",2010-09-21T03:56:29.570,608,CC BY-SA 2.5,
3675,2931,2,"Wow, thanks Andrew Moore's tutorial on cross-validation is world class.",2010-09-21T04:08:38.987,1329,CC BY-SA 2.5,
3676,2927,0,Thanks for the links to the AIC and BIC tests.  Do they add much value vs Adjusted R Square that does a similar thing by penalizing models for adding variables?,2010-09-21T04:10:47.860,1329,CC BY-SA 2.5,
3677,2929,0,"What question are you trying to answer? If you are asking, ""are these two distributions the same?"", then you should use the QQ plot for a visual and the Kolmogorov-Smirnov test for the similarity of the distributions. This test basically uses the absolute maximum distance from the 45-degree line in the QQ plot as its statistic. The KS test can also be thought of as the maximum distance between two cumulative distribution functions plotted in the same figure. A CDF is a general concept; a Lorenz curve is a special case of the CDF when the domain of the random variable is from 0 to 1.",2010-09-21T05:28:26.187,401,CC BY-SA 2.5,
3678,2937,0,Aren't these tests not restricted to hypotheses testing (testing the null hypothesis that they are the same)? I would like to have a similarity measure that quantifies their (dis)similarity in addition to simply testing IF they are (dis)similar and preferably in some comparable form (e.g. 20% dissimilar or 0.2 (i.e. a normalized value between 0 and 1 as a measure for similarity) etc.). Would you recommend to use the p value for that? Thanks in advance!,2010-09-21T05:40:39.733,608,CC BY-SA 2.5,
3679,2929,0,"I have a distribution ranging from 1 to 1024 (screen coordinates) with frequencies of user activities at these coordinates. I produce a density plot of this distribution and, since I have a number of activities I would like to compare, I have several of such distributions. So, to answer your question, my rage is naturally not [0,1]. However, if I normalize the screen between 0% and 100% I can fit it that way (just like in economic problems where the income level is scaled between 0% and 100% in 20% steps). Does that sound reasonable?",2010-09-21T05:47:51.763,608,CC BY-SA 2.5,
3680,2929,0,"I'm sorry, but I'm still not understanding the hypothesis that you want to test. Is it, ""clicks are uniformly distributed across the screen""? If so, you can use the KS test to answer whether your distribution of clicks is statistically different from a normal distribution. You wouldn't need to normalize or think of this as a Lorenz curve; just think of it as a CDF and the KS test statistic is the maximum absolute distance between your CDF (with a domain from 0 to 1024) to the CDF of the uniform ""reference"" distribution (the 45-degree line).",2010-09-21T06:01:17.367,401,CC BY-SA 2.5,
3681,2929,0,The KS test can compare one empirical distribution to a known theoretical distribution or one empirical distribution to another empirical distribution. It is a nonparametric test for equality of distributions. See the Wikipedia page: http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test,2010-09-21T06:02:40.580,401,CC BY-SA 2.5,
3682,2905,0,"@Shabbychev:> no, but i had the opportunity to work on a related problem (just not one involving time series) a while ago (same problem as this one http://stats.stackexchange.com/questions/2572/correlation-stress-testing )",2010-09-21T06:45:22.317,603,CC BY-SA 2.5,
3683,2935,1,Gaetan:> Elane seems to suggest that the slope in the summer are different from the slopes in the winter (not just the intercept).,2010-09-21T06:50:29.373,603,CC BY-SA 2.5,
3684,1735,0,"Elaine:> what do you want to do? From your question i gather that you want a relative ranking of the coefficient for winter and one for summer. Is this correct, or just an ersatz for something else you have not been able to do.",2010-09-21T06:52:10.103,603,CC BY-SA 2.5,
3685,2927,1,"@Gaeten, Adjusted R-squared will increase when an F-test of the before vs after model is significant, so they are equivalent, except normally calculating an adjusted R-squared doesn't return a p-value.",2010-09-21T07:33:42.250,521,CC BY-SA 2.5,
3686,2927,1,"@Gaeten - AIC & BIC are more general than F-tests and adjusted R-squared which are usually limited to models fit by least squares. AIC & BIC can be used for any ,model where the likelihood can be calculated and degrees of freedom can be known (or estimated).",2010-09-21T07:44:14.057,521,CC BY-SA 2.5,
3687,2929,0,"I simply want to know how similar they are. Lets say I have clicks across the screen for one distribution and eye fixations (measured with an eye tracker) for the other. Now I want to know what is the % of similarity between them using some normalized qualifier/measure. This could be accompanied by a test of course, and I take your advice on that, but in addition to such a test, I still would like to quantize the similarity somehow.",2010-09-21T07:55:52.923,608,CC BY-SA 2.5,
3688,2937,0,"Ampleforth:> yes, you can use the p-value as a continuous (not just 0-1) and normalized measure of the differences between the two distributions. It'll be more accurate (in the sense of precise,  or powerfull) than Gini's I.",2010-09-21T08:29:00.083,603,CC BY-SA 2.5,
3690,2913,0,"@Ebony:> Whuber's point is still valid, in the sense that even with 1%-99% split, your proposal does not affect the (very low) O(n) complexity of the overall procedure so from a theoretical point of view your concerns are void. Shifting to a practical point of view, you can perform PCA on 100K's of observations and 10K's features in seconds on a modern computer. It's hard to imagine a setting in which splitting the sample would be worthwhile.",2010-09-21T08:45:49.390,603,CC BY-SA 2.5,
3691,2890,0,"@Yaroslaw: I don't really understand your issue with cross-validation, but to be honest I am no expert there. However, I would really like to make a point for measuring model mimicry. Therefore, see my updated answer.",2010-09-21T08:46:20.653,442,CC BY-SA 2.5,
3693,2828,0,Yaroslav:> Shouldn't that be *Amount of data needed to identify when a model is **not** a good description of the data*?,2010-09-21T09:01:03.227,603,CC BY-SA 2.5,
3694,2896,0,Good point! Didn't RTQ properly.,2010-09-21T09:22:29.153,229,CC BY-SA 2.5,
3695,2929,0,"@Ampleforth But it seems to me that in this case you should rather focus on some kind of distance (e.g. euclidean) between eye coordinates on the screen and mouse position for each event (I assume that you have 2 (x,y) pairs -- eye + mouse -- for each mouse click). Then you can derive a measure of overall error (e.g. RMSE).",2010-09-21T09:38:18.470,930,CC BY-SA 2.5,
3696,2700,1,The tutorial was really great. Could you suggest any further tutorials as a follow-up?,2010-09-21T09:54:57.307,1250,CC BY-SA 2.5,
3698,2913,1,"@kwak: Okay, consider this -- if P >> N, then it's suggested that you do PCA using the eigen-decomposition of X'*X (which is NxN) rather of the covariance matrix X*X' (which is PxP). In this setting, the computational complexity is going to be O(N^3). If N is reasonably large (even though smaller than P) I don't think you can ignore the cubic complexity.",2010-09-21T11:21:13.590,881,CC BY-SA 2.5,
3700,2737,0,I google searched time-based boxcar and came up with very little. Can you point us to a worked example?,2010-09-21T13:03:43.460,104,CC BY-SA 2.5,
3701,2907,0,do you have a url that you can point me at. I have never done a time series analysis in my life. But it looks as if this is what I need to teach myself. Is there a worked example that I can look at that uses time series filters?,2010-09-21T13:06:25.183,104,CC BY-SA 2.5,
3704,2936,0,What would be a specific example for an external event that I could use for $Y$?,2010-09-21T13:58:31.033,,CC BY-SA 2.5,user28
3705,2913,4,"@ebony1: It is true that the situation changes when P >> N.  That's an interesting refinement of your original problem, because in effect any model is going to be a gross overfitting: more explanatory variables than data.  That does not augur well for the prospects of obtaining stable results via subsampling.  At a minimum, it points to the importance of retaining as much data as possible.",2010-09-21T14:08:24.857,919,CC BY-SA 2.5,
3706,2944,1,"@2 is clever.  It might even be better to draw correlated random subsamples: e.g., randomly partition the data into $k$ parts of approximately $n/k$ values each and run $k$ PCAs for a computational cost of only $1 / k^2$ of a full PCA.",2010-09-21T14:11:01.777,919,CC BY-SA 2.5,
3707,2937,2,"+1 for the second suggestion.  The Q-Q plot is one of the strongest and most insightful ways to compare two empirical distributions, because it graphically displays variations between the two throughout their range.  It is also a natural graphical adjunct to the KS test, whose statistic is the maximum deviation between the Q-Q plot and its reference line (Y==X).",2010-09-21T14:15:28.080,919,CC BY-SA 2.5,
3711,2907,0,"@farel:> if you click on the blue words, you will be directed to actual websites. The last document in particular contains working out examples (repeated in the edted version of my post).",2010-09-21T14:57:17.853,603,CC BY-SA 2.5,
3712,2939,0,"Note that (as I understand it), the K-S test is for testing empirical data against an a priori distribution. It's not appropriate for comparing two empirical distributions, nor is it appropriate to compare empirical data against an a priori distribution whose parameter values were estimated from the empirical data.",2010-09-21T15:07:19.677,364,CC BY-SA 2.5,
3713,2947,0,Your paper includes very nice figures :),2010-09-21T15:19:47.427,930,CC BY-SA 2.5,
3714,2936,1,"As an example I first thought of old numbers games where they used some decimal point of specific stock reports in the news paper. Of course you would worry about the true randomness of this metric though, so I imagine there are better examples.",2010-09-21T15:31:31.480,1036,CC BY-SA 2.5,
3715,2915,0,Should this be CW as it is asking for a collection of resources? What would constitute a 'correct' answer for such a question?,2010-09-21T15:37:31.407,,CC BY-SA 2.5,user28
3716,2939,4,"@Mike, you can use the K-S test to compare two empirically derived distributions, see Charlie's prior answer and comments http://stats.stackexchange.com/questions/2918/lorenz-curve-qq-plot/2929#2929",2010-09-21T15:39:48.407,1036,CC BY-SA 2.5,
3717,2944,0,"@Whuber:> thanks. However i have two questions: a) PCA on the full dataset is not the same as the mean of PCA on the parts (do you know of a way to recover the 'total-sample' pca from the 'partial-sample' ones?). b) wouldn't we (under your proposal) lose the interpretation of the 'partial-sample' PCA in terms of sampling variation ? In other words, do the distribution of the 'partial-sample' pca constructed from correlated sample have a statistical interpretation ?",2010-09-21T15:41:56.580,603,CC BY-SA 2.5,
3718,2947,0,@chi: They were all created in R using ggplot2. It's a fantastic graphics production system!,2010-09-21T15:42:11.167,364,CC BY-SA 2.5,
3719,2915,0,"I am new here, so I don't know what CW is?  Does it mean Community Wiki or something?  And, what does this mean?  Should I have posted this question somewhere else within this site?  And, where? and how?",2010-09-21T15:50:30.073,1329,CC BY-SA 2.5,
3720,2935,0,"kwak, you raise an excellent point.  I have been told that linear regression can handle that whereby you have dummy variables for the intercepts of the seasons and another one for their respective slope coefficients.  Unfortunately, I have not used such a method.  So, I can't explain it further.",2010-09-21T15:55:53.830,1329,CC BY-SA 2.5,
3721,2939,0,"@Andy, Ah, I took point 3 from http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm as having the corollary that you can't compare two empirical CDFs, but I see that my assumption wasn't appropriate. Good to know, thanks!",2010-09-21T16:02:47.997,364,CC BY-SA 2.5,
3724,2915,1,When you ask a question or edit a question there is a check box that appears at the bottom of the question box towards the right hand side that says 'Community wiki'. You need to check that box *if* you wish to make a question or answer CW. The idea behind CW is enable the community to edit the questions/answers collaboratively and to prevent users from gaining rep for certain type of questions. See this link for more details: http://meta.stats.stackexchange.com/questions/6/what-should-our-faq-contain/198#198,2010-09-21T16:18:24.963,,CC BY-SA 2.5,user28
3725,2937,0,@whuber: good point about KS statistic being the maximum deviation.,2010-09-21T16:26:45.117,795,CC BY-SA 2.5,
3726,2937,0,"You can use the K-S test statistic as a measure of dissimilarity. For a two-sided test, the K-S test statistic is the largest absolute difference between the two distribution functions. As the distribution functions lie in [0,1], the difference also lies in [0,1]. If the two distributions differ slightly the p-value will tend to 0 as the sample sizes increase (since the power of the test increases) but the K-S test statistic will tend to a limit.",2010-09-21T16:33:21.053,449,CC BY-SA 2.5,
3728,2920,0,"For MSD, if MSD = t_{alpha,2n-2}*sd*sqrt(2/n), is SE = MSD*n/(t_{alpha,n}*sqrt(2)) correct?",2010-09-21T16:35:22.887,1381,CC BY-SA 2.5,
3729,2950,2,kmeans() (you can find it by typping ?? folowed by the name of what you want as in ??kmean),2010-09-21T17:36:02.947,603,CC BY-SA 2.5,
3730,2950,0,Another useful way to find functions is the `apropos` function.,2010-09-21T17:38:11.993,5,CC BY-SA 2.5,
3731,2950,3,@kwak: why not post your comment as an answer?,2010-09-21T17:38:45.440,5,CC BY-SA 2.5,
3733,2945,1,"I would agree that Cross Validation solves the problem of measuring model complexity. Maybe I'm asking the wrong question, because a practical question is the sample complexity of the fitting procedure. Cross-validated learner would try different models and pick the one with lowest cross validation error. Now the question is -- is this learner more likely to overfit than one that fits a single model by maximum likelihood?",2010-09-21T18:30:22.880,511,CC BY-SA 2.5,
3735,2945,0,"Yaroslav Bulatov:> yes but you can use ML only to compare nested models. Insofar as you specified (in your question) mentioned models with the same number of parameters, then they cannot be nested.",2010-09-21T18:39:43.600,603,CC BY-SA 2.5,
3737,2851,1,"Hurlbert, S.H. (1984) Pseudoreplication and the design of ecological field experiments. Ecological Monographs, 54, 187‚Äì211.",2010-09-21T20:34:04.193,603,CC BY-SA 2.5,
3738,2953,4,"You can also do `RSiteSearch(""k-means"")`",2010-09-21T20:42:23.407,582,CC BY-SA 2.5,
3740,2950,0,"@all:> i propose to change the title of the post, so as to make it easier to find for someone looking for the R implementation of other procedures (beside k-means).",2010-09-21T21:11:13.487,603,CC BY-SA 2.5,
3741,2939,2,"However, point 3 does imply that you can't use K-S to test whether your data come from a normal distribution with mean and sd *estimated from the data*. This is a popular error among the psychology students I meet.",2010-09-21T21:28:00.580,1352,CC BY-SA 2.5,
3742,2945,0,Another issue is that cross-validation doesn't add to our understanding of model complexity. Measures like AIC/BIC make it clear that lots of parameters encourage overfitting. Now the question becomes -- what aspects of the model besides dimension increase capacity to overfit?,2010-09-21T21:56:49.153,511,CC BY-SA 2.5,
3743,1858,0,Did you find any paper finally?,2010-09-21T22:00:31.033,930,CC BY-SA 2.5,
3744,2950,0,"@kwak Yes, I think it's better to change the title because k-means vs. kNN seems ambiguous.",2010-09-21T22:07:11.987,930,CC BY-SA 2.5,
3745,1858,0,@chl Not yet; but thanks for reminder.,2010-09-21T22:11:37.700,,CC BY-SA 2.5,user88
3747,2945,0,"Yaroslav:> Again, very good point.",2010-09-21T22:28:07.310,603,CC BY-SA 2.5,
3748,1858,0,There's no hurry :) Didn't find anything interesting myself; maybe Pubmed isn't the right search engine for this particular case...,2010-09-21T22:34:03.017,930,CC BY-SA 2.5,
3749,1858,0,@chl That's also my problem here. It really seems n<<p has became a synonym for biomed data.,2010-09-21T22:46:27.957,,CC BY-SA 2.5,user88
3750,2888,0,"BTW, I just came across a paper that made me think of this question...it's very rare for Machine Learning papers to talk about confidence intervals, but here's a notable exception http://www.ncbi.nlm.nih.gov/pubmed/19519325",2010-09-21T22:50:39.667,511,CC BY-SA 2.5,
3752,2954,0,"I believe the k nearest-neighbors test is a multivariate analysis. Perhaps I have confused cluster analysis with classification, but I don't think either apply in this situation. I already have categorized the variables into groups, and now want to see if these groups statistically differ from one another. This test is different that an ANOVA or linear model. Any suggestions? Thanks.",2010-09-21T23:03:10.253,,CC BY-SA 2.5,Sharon
3754,2820,0,"Hello Stephan,
I still think you can still use the method=BY (for Benjamini Hochberg Yekuteli Procedure) in p.adjust in R, as pointed by Tal.

Definitely, using Bonferroni can be conservative.",2010-09-22T03:44:28.350,1307,CC BY-SA 2.5,
3755,2960,0,But the question is: how would you verify that to someone on the other side of the world?  See item b) in the question.,2010-09-22T05:16:54.667,71,CC BY-SA 2.5,
3756,2963,1,"Tangent: Dirk Eddelbuettel wrote an R package (http://cran.r-project.org/web/packages/random/index.html) to access random numbers.  Not verifiable, however.",2010-09-22T05:27:53.243,71,CC BY-SA 2.5,
3757,2962,3,"I'm not aware of such a function, but perhaps someone could look at the formulas in Olejnik and Algina  (2003) http://www.cps.nova.edu/marker/olejnik2003.pdf and write a function",2010-09-22T05:34:03.457,183,CC BY-SA 2.5,
3758,2962,3,"@Jeromy Nice reference! This one is worth looking too: Recommended effect size statistics for repeated measures designs (BRM 2005 37(3)), http://j.mp/cT9uEQ",2010-09-22T06:56:03.430,930,CC BY-SA 2.5,
3759,2954,0,"@Sharon I'm a little bit puzzled about the term 'randomization test' (the only paper I found using this keyword, http://j.mp/9OCB9I, seems rather to focus on power considerations) -- I rest on my idea that you could use cross-validation to assess the classification accuracy in your data, e.g. leave-one-out or k-fold CV for this purpose (available in `kknn` and `class` packages). Now, I don't know on which basis your classes were defined, but it is generally meaningless to use ANOVA to test if the groups differ on the variables that were used to define class membership.",2010-09-22T07:30:34.920,930,CC BY-SA 2.5,
3760,2962,2,"@chl Thanks. Apparently, ezANOVA() in the ez package in R reports generalised eta squared.",2010-09-22T07:49:19.490,183,CC BY-SA 2.5,
3761,2958,0,Can you provide more detail Justin?,2010-09-22T07:54:06.930,521,CC BY-SA 2.5,
3763,2954,0,@Sharon; can you explain your data a bit better (I'm a (sort of) ecologist and use isotopes all the time so you can be quite technical if it helps)?,2010-09-22T11:54:48.233,1390,CC BY-SA 2.5,
3765,2958,0,"What else do you know about the distribution apart from its 16th percentile (which is what i assume you mean by ""16% quartile"")?",2010-09-22T12:09:02.947,449,CC BY-SA 2.5,
3766,2963,0,This works perfectly.,2010-09-22T12:56:36.803,,CC BY-SA 2.5,user28
3768,2967,0,"Thanks for the great suggestion. I did a quick analysis using isoreg in R. It was certainly robust to the outliers (i.e., the very slow times). It was a little bit more stepped than would seem appropriate if it was updating its estimate based on every observation. I also assume that it also wouldn't deal well with the situation where the runner experiences an injury or perhaps changes their running style and thus the runner's latent performance actually declines.",2010-09-22T13:22:25.880,183,CC BY-SA 2.5,
3769,2970,0,"Thanks. I have a concern with the nonlinear regression approach (or the linear regression of transformed x and y). 1. It assumes the functional form is known; 2. least squares is influenced by outliers, which I would argue are less relevant to the quantity of interest; 3. The quantity of interest is more like median of a positively skewed running time distribution with outliers, whereas least squares would estimate the mean.",2010-09-22T13:28:48.333,183,CC BY-SA 2.5,
3770,2967,0,Jeromy:> first point/ have you tried fudging with the bandwith selector ? Second point/ Injury is a valid example (running style is not since it does not impact latent [potential] speed): see edited answer.,2010-09-22T13:30:09.703,603,CC BY-SA 2.5,
3771,2967,0,"Thanks. I'll play with the bandwidth selector. As for running style, I guess your argument would be that the runner could go back to their original running style and return to their previous speed. Sometimes I'd be interested in seeing the person's new potential in terms of perhaps a new running style that they have committed to.",2010-09-22T13:35:20.377,183,CC BY-SA 2.5,
3772,2967,0,@Jeromy:> see updated answer.,2010-09-22T13:40:04.550,603,CC BY-SA 2.5,
3774,2820,0,"suncoolsu, I think that this method only works when the correlation is positive (not negative) between the variables.  Cheers.",2010-09-22T13:57:04.110,253,CC BY-SA 2.5,
3775,2970,0,@Jeromy: I'm sure you're right. Just sharing my ignorance.,2010-09-22T14:24:23.967,1270,CC BY-SA 2.5,
3776,2884,0,"Whuber,but I foud ""The Kruskal-Wallis test  does  not  assume  population  normality  nor  homogeneity  of  variance,  as  does  the parametric ANOVA,  and  only  requires  ordinal  scaling  of  the  dependent  variable."" that you can see from http://www.jstor.org/pss/1165320 or http://oak.ucc.nau.edu/rh232/courses/EPS525/Handouts/Understanding%20the%20One-way%20ANOVA.pdf.",2010-09-22T14:52:44.943,,CC BY-SA 2.5,Chuangye
3777,2701,0,"That is great thanks, the graphpad page was perfect - Fig 6.4 gave me what I needed to know.  We havent yet got 50% dead, so technically it's not possible to work out, but i did some forecasting to get to that point.",2010-09-22T15:13:36.870,12517,CC BY-SA 2.5,
3778,2947,0,What do you mean with fitted CDF?,2010-09-22T15:44:54.097,608,CC BY-SA 2.5,
3779,2929,0,"I believe that would be too noisy. My wish to focus on distributions rather than sequences of data points is based on the fact that the data is rather noisy. I think distributions provide the general tendencies rather than individual pattern. Also, I am summarizing over many users which I can combine in that way to detect the main trends. So, I basically have two PDF (or CDFs) and would like to know how similar they are. I like the idea of the Gini coefficient that uses the relative area under a Lorenz curve but might need something slightly different.",2010-09-22T15:51:45.943,608,CC BY-SA 2.5,
3780,2929,0,"By the way, thank you all for continuing this discussion. I just realized that they became rather lengthly :)",2010-09-22T15:53:02.740,608,CC BY-SA 2.5,
3781,2947,0,"@Ampleforth, in that paper, I fit a distribution to empirical data, so by ""fitted CDF"" I meant the theoretical CDF of the fitted distribution. Sorry, I see how I could have been more clear!",2010-09-22T17:06:35.607,364,CC BY-SA 2.5,
3782,2976,1,"Would factor analysis not do the job for qn 1? Question 2 is a bit vague. 'Relationship' seems a synonym for 'correlation' or at least one form of relationship is linear relationship and correlation captures that. Perhaps, you need to clarify qn 2.",2010-09-22T17:08:32.457,,CC BY-SA 2.5,user28
3783,2951,2,"many thanks for pointing this out to me, I had completely missed the reference to the weights in the documentation.",2010-09-22T17:42:54.440,1007,CC BY-SA 2.5,
3784,2977,0,"@Srikant I know all this. CV is a means for finding ""best"". What is the definition of ""best""?",2010-09-22T18:13:26.627,1134,CC BY-SA 2.5,
3785,2977,0,"@bart 'best model' = a model that 'best' captures global patterns while avoiding local features of a data. That is the best I can do for a non-math description. Perhaps, someone else can elaborate a bit more or be more specific.",2010-09-22T18:15:38.403,,CC BY-SA 2.5,user28
3786,2977,0,"@bart: ""best"" means the the function that fits the training data the best, and that ""generalizes"" well to the validation/unseen-test set data. I think this is quite clear from Srikant's answer. There are many ways to formally define a good generalization behavior. In a non-formal sense, you can think of it as finding a function that is ""smooth"" and not much wiggly. Trying to fit solely on the training data may lead to the wiggly looking function whereas smoothness usually ensures that the function will do reasonably well on both training and validation/test data.",2010-09-22T18:27:35.370,881,CC BY-SA 2.5,
3787,2977,0,@ebony: You are missing the point. I've rephrased the question to hopefully make it clearer,2010-09-22T18:34:28.597,1134,CC BY-SA 2.5,
3788,2980,0,"I thought the meaning of 'external event' was clear. To clarify, because of the constraints imposed on the protocol, the external event should be the source of the random generation and the draw must be verifiable. Your answer addresses verifiability but not the external event part. Nevertheless, +1 for an interesting answer. Out of curiosity, doesn't the algorithm breakdown if both Bob and Alice cheat and report a value (0 or 1 as they choose) *without* flipping a coin?",2010-09-22T19:09:32.683,,CC BY-SA 2.5,user28
3789,2807,21,"@nico:  Sure it can be negative, but there's some finite limit to it because there are only so many protons and electrons in the Universe.  Of course this is irrelevant in practice, but that's my point.  Nothing is **exactly** normally distributed (the model is wrong), but there are lots of things that are close enough (the model is useful).  Basically, you already knew the model was wrong, and rejecting or not rejecting the null gives essentially no information about whether it's nonetheless useful.",2010-09-22T19:39:17.780,1347,CC BY-SA 2.5,
3790,2947,0,"Oh, please don't apologize. My lack of statistics is rather large and that is the only problem here ;) Also I did not read your paper, but only glanced through your graphs which I really liked.",2010-09-22T19:57:07.493,608,CC BY-SA 2.5,
3791,2984,1,Is $\lambda$ a parameter that is free to be chosen?,2010-09-22T21:49:03.633,352,CC BY-SA 2.5,
3792,2984,0,@Robby:> thanks. I slightly appended the text to make the distinction between parameters and hyperparameters clear.,2010-09-22T22:12:04.800,603,CC BY-SA 2.5,
3793,2979,0,> this is not the same model (i.e. Kelly's model is non linear in the parameters).,2010-09-22T22:35:14.190,603,CC BY-SA 2.5,
3794,2835,0,> yes it does local quantile regression as well (see the vignette (http://cran.r-project.org/package=quantreg).,2010-09-22T22:40:08.920,603,CC BY-SA 2.5,
3795,2984,0,"@kwak: I'm sorry to say I haven't a clue what this means. What do the symbols p, q, lambda, x, y, m and beta signify?",2010-09-22T23:05:43.507,1134,CC BY-SA 2.5,
3796,2985,0,You understand the question. I'll follow the links.,2010-09-22T23:09:13.920,1134,CC BY-SA 2.5,
3797,2984,0,"@bart:> My answer is essentially the same as Srikant's. Where he provides an intuitive explication, I wanted to add a more rigorous one for the benefits of future visitors that may have the same question as you, but are more familliar with math than non-formal language. All the symbols you mention are defined in my answer (altough, again, this is done formally).",2010-09-22T23:35:03.450,603,CC BY-SA 2.5,
3798,2985,1,You should know that these links are unlikely to take you anywhere `practical'. If you are trying to *build* something using cross validation (or some other sort of model selection) then in practice it is likely to always come down to something heuristic and a bit ad-hoc (although I agree this is unsatisfying).,2010-09-22T23:36:43.837,352,CC BY-SA 2.5,
3799,2985,0,Now we are getting somewhere. http://en.wikipedia.org/wiki/Minimum_message_length seems to be what I was thinking. Thanks!,2010-09-22T23:48:45.697,1134,CC BY-SA 2.5,
3800,2985,0,"No worries. This is just reflection, not practical.",2010-09-22T23:50:31.153,1134,CC BY-SA 2.5,
3801,2984,0,"@kwak: Where, for example, is p defined?",2010-09-22T23:54:18.337,1134,CC BY-SA 2.5,
3802,2986,0,"@user1396: many thanks! I was thinking I had to do this 'by hand'. : ) I ran `community.to.membership(ag, z$merges, steps=12)` on the output of running `fastgreedy.community()` with the graph provided as an example in the docs and the output was `$membership
 [1] 2 2 2 2 2 0 0 0 0 0 1 1 1 1 1

$csize
[1] 5 5 5`
I am confused by the repeated values in the $membership vector. How should I read them? I thought this would contain the vertice names. many thanks!",2010-09-23T00:41:25.187,1007,CC BY-SA 2.5,
3803,2984,0,"@bart:> line 1 (p,q)>= 1 means p and q are taken to be numbers larger than 1. Latter on (line 1 of last ¬ß), it says that for that particular property to hold p should be equal to either 1 or 2.",2010-09-23T00:45:51.087,603,CC BY-SA 2.5,
3804,2986,0,@user1396: I think I got it; a '2' in the i-th element in the membership vector is telling me the i-th vertex belongs to community 2. correct? :),2010-09-23T00:47:25.193,1007,CC BY-SA 2.5,
3805,2979,0,"Paul-- believe it or not, we tried this as a starting point for the  optimization (taking into account the concerns you raised), but the estimator was way off from where we'd expect it to converge to. No good explanation why. We do have a reasonable workaround, which I'll be sharing with the question shortly.",2010-09-23T01:15:09.080,53,CC BY-SA 2.5,
3806,2979,0,"@kwak:> what do you mean by ""this""? Not the same model as 2sls or not the same model as what the GHK simulator is usually applied to, ie not a multinomial probit? I know it's not a multinomial probit, that's why I suggested a slight variant of the GHK simulator. As to whether 2sls applies, doesn't the equation for W have some error term, say, e, and doesn't the full model imply E[e|Z,X] = 0 and E[(H,M,L)|Z] not 0?",2010-09-23T01:17:15.533,1229,CC BY-SA 2.5,
3807,2976,0,You have stated what you want to do. What is your question? Is it about implementation or whether your analysis approach is appropriate? or something else?,2010-09-23T01:21:15.650,183,CC BY-SA 2.5,
3808,2980,0,"Hi Srikant, yes, the algorithm requires at least one honest party. You have no internal source of randomness at all? Then you could take the SHA-256 of the PDF version of The New York Times every day, or the SHA-256 of the volume and closing price of all the stocks in the Dow Jones Industrial Average, or the really the secure hash of anything that can be mutually observed and that you can't influence. If you want one bit, take one bit of the SHA-256. If you want a normal distribution, take the whole thing (256 bits) as a uniform deviate and use the Box-Muller transform to get a normal deviate.",2010-09-23T02:47:10.723,1122,CC BY-SA 2.5,
3809,2980,0,"Yes, your modification using the hash of NYT/stock prices should also work although it has the one weakness of requiring at least 1 honest party. Interesting way to approach the issue.",2010-09-23T03:06:53.443,,CC BY-SA 2.5,user28
3810,3000,0,"Criteria will be 10% difference in probability of good vs bad outcome. Or lets say since it will be logistic regression, odds ratio = 2. alpha= 0.05, power=80%, I do not yet know what the pooled variance on the continuous variable is but let us assume that the standard deviation is 7mmHg.   Sequential analysis would be good but the final outcome is two  years after the measurement is taken.",2010-09-23T03:36:42.700,104,CC BY-SA 2.5,
3811,2988,0,Do you expect some dropouts during follow-up? Are there any other covariates to be included in your model?,2010-09-23T06:34:13.363,930,CC BY-SA 2.5,
3812,2979,0,"> you state *You could avoid the problem altogether by simply estimating* which is certainly correct. Nonetheless, the model you propose is a different one than the one Kyle wanted to fit (it is not a reformulation of the same model).",2010-09-23T07:37:05.127,603,CC BY-SA 2.5,
3813,2989,0,Thanks alot (both for the clever initial suggestion and for the folow thru).,2010-09-23T07:39:16.557,603,CC BY-SA 2.5,
3820,3009,0,"I pointed to similar books in my own response, sorry didn't see your response while I was writing... Card's book is very great!",2010-09-23T09:36:32.393,930,CC BY-SA 2.5,
3822,3005,0,"@kwak Ok, the LARS algorithm seems largely more sophisticated than simple thresholding on variable importance, but the point is that I don't see a clear relation between the penalty parameter and the # of variables that are asked to be kept in the model; it seems to me we cannot necessarily find a penalty parameter that would yield exactly a fixed # of variables.",2010-09-23T09:51:08.970,930,CC BY-SA 2.5,
3823,3010,0,Wow. That's a great set of references. I'm assuming that husbands and wives would not be interchangeable. I wonder if that matters.,2010-09-23T09:51:37.143,183,CC BY-SA 2.5,
3825,3010,0,"It depends on the questions asked in the questionnaire. I came across a clinical study dealing with erectile function and sexual life (wife + husband), in this case the role of the respondents are not symmetric.",2010-09-23T09:55:19.353,930,CC BY-SA 2.5,
3826,3009,0,"@chl Great, that you mentioned the Thompson/Walker-paper about ""The Dyad as the Unit of Analysis"".",2010-09-23T10:00:39.687,307,CC BY-SA 2.5,
3827,3005,0,"@chl:> S-PLS you mean ?(you wrote LARS which is a different thing from either algorithm you discuss). Indeed, there is a monotoneous relationship between the penalty parameter and the # of component, but it is not a linear relationsip and this relationship varies on a case per case basis (is dataset/problem dependant).",2010-09-23T10:04:11.630,603,CC BY-SA 2.5,
3830,3005,0,"@kwak L1-penalty may be achieved using LARS, unless I am misleading. Your second point is what I have in mind in fact; have you any reference about that point?",2010-09-23T10:21:59.497,930,CC BY-SA 2.5,
3832,3005,0,"@chl:>* L1-penalty may be achieved using LARS, unless I am misleading* i didn't know that (and sort of doubt it). Can you provide a reference ? Thanks. for your second question: look On the ‚Äúdegrees of freedom‚Äù of the lasso

Hui Zou, Trevor Hastie, and Robert Tibshirani
Source: Ann. Statist. Volume 35, Number 5 (2007), 2173-2192. (there are many ungated versions).",2010-09-23T10:47:08.533,603,CC BY-SA 2.5,
3833,3005,1,"@kwak Check out Tibshirani's webpage, http://www-stat.stanford.edu/~tibs/lasso.html and the `lars` R package; other methods include coordinate descent (see JSS 2010 33(1), http://bit.ly/bDNUFo), and the Python `scikit.learn` package features both approaches, http://bit.ly/bfhnZz.",2010-09-23T10:58:16.690,930,CC BY-SA 2.5,
3834,2989,0,"No problem, kwak; remember only that it does premultiplication, though modifying the algorithm to do postmultiplication shouldn't be too hard.",2010-09-23T11:45:26.313,830,CC BY-SA 2.5,
3835,3002,3,"Monotonic *and increasing*, of course.",2010-09-23T13:32:59.260,919,CC BY-SA 2.5,
3836,2733,1,"Besides that fact that the former link wasn't really related to *mathematical* statistics, the latter one is neither: ""Data Mining with STATISTICA Video Series"" - http://www.youtube.com/user/StatSoft#g/c/B804A810436AFB03",2010-09-23T14:00:53.827,653,CC BY-SA 2.5,
3838,2971,1,"Best is the model with lowest future error and cross-validation gives you that estimate. The reason for c(Complexity)+e(Error) formulas is because you could use error on training data as estimate of future error, but that's overly optimistic, so you add a term to make this estimate unbiased, which is usually some function of model complexity",2010-09-23T14:32:33.153,511,CC BY-SA 2.5,
3839,2959,0,Thanks for the pointers. Will need to hit the library to make sense of it all.,2010-09-23T14:53:10.090,1393,CC BY-SA 2.5,
3841,2852,0,"Thank you,Thylacoleo.These are DNA divergence data of six different groups.I want to know the difference between the first column band each one of others with P value.",2010-09-23T15:00:35.967,,CC BY-SA 2.5,Chuangye
3847,2917,0,"I am not sure if most CIs are really computed via t-values or rather via z-values. However, on bigger ns (> 30) this shouldn't make much of a difference.",2010-09-23T16:45:44.500,442,CC BY-SA 2.5,
3848,2891,0,"A typical ratio would be 1042:42. 1042 turn left, 42 turn right.",2010-09-23T17:13:32.613,,CC BY-SA 2.5,Pierre 303
3849,2989,0,M.:> it should be okay: i only need Q (i.e. qmult(eye(p))),2010-09-23T20:23:54.853,603,CC BY-SA 2.5,
3850,3005,0,@chl:> thanks. (i also recommend the lars paper www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf),2010-09-23T20:27:50.763,603,CC BY-SA 2.5,
3853,2975,0,"It seems to me no valid comparison is possible among these series unless there is some persistent spatial phenomenon producing the ""temperature signature.""  Otherwise they all measure different things.  If there is a spatial pattern, then you really have a problem of spatial interpolation, not of time series normalization.",2010-09-23T21:56:58.190,919,CC BY-SA 2.5,
3854,2901,1,"@Kwak: Thank you for clarifying your meaning.  Of course, the same response could be applied to any odd moment: they measure asymmetry further and further out in the tails.",2010-09-23T22:07:46.273,919,CC BY-SA 2.5,
3855,2901,0,"@Whuber:> Of course. Note that even for a fair tailed distribution like the gaussian, by the 7th moment you are already in effect comparing the max to the min.",2010-09-23T23:12:54.533,603,CC BY-SA 2.5,
3856,2945,0,"If overfitting is the tendency of a model fitting procedure to fit noise in addition to signal, then we can look at a given procedure to see where such tendencies could arise.  Perhaps due to a lack of imagination or knowledge, while considering a few different procedures, I couldn't boil this down to something that can't be restated as ""number of parameters"" (or ""effective number of parameters"").  We could flip this on its head and ask: all else equal, what happens when we introduce noise to our data?  Then we arrive at measures such as Ye's GDF.",2010-09-23T23:22:11.097,251,CC BY-SA 2.5,
3857,2945,0,"@ars:> suppose you compare two models for $y$ a) $y=\alpha_1*xz$ and $y=\alpha_1*x$. They have the same number of parameters, but the first one is sensitive to noise in both x and z.",2010-09-23T23:29:34.983,603,CC BY-SA 2.5,
3858,2945,0,"@kwak: That doesn't meet my ""all else equal"" case -- sorry if that was unclear.  I'm really trying to arrive at an answer to Yaroslav's question, perhaps restated: is there something other than dimensionality to account for tendency to fit noise in addition to signal?  And keeping the considered case very simple, I'm failing ...",2010-09-23T23:36:08.993,251,CC BY-SA 2.5,
3860,3032,0,wow. i am an embarrassed nOOb. thanks for the pointer.,2010-09-24T03:33:32.593,1410,CC BY-SA 2.5,
3861,3034,0,"This is great Dason, thanks so much. Rob's code was great but your example made it obvious to me what was happening -- thanks for taking the time to do that!",2010-09-24T03:39:19.963,1410,CC BY-SA 2.5,
3864,3038,0,"what do you mean by ""this assumes the null hypothesis to be true"" ?",2010-09-24T08:39:27.173,223,CC BY-SA 2.5,
3865,3038,0,"If you want to be able to control the probability of declaring wrongly ""there is a difference""  you need to separate the two hypothesis (did I already mentionned I love this quote: http://stats.stackexchange.com/questions/726/famous-statistician-quotes/728#728 ;) )",2010-09-24T08:40:19.950,223,CC BY-SA 2.5,
3866,2775,1,"> the problem with this method is that you can't control the ratio of smallest to largest eigenvalue (and i think that as the size of your randomly generated dataset goes to infinity, this ratio will converge to 1).",2010-09-24T09:28:03.940,603,CC BY-SA 2.5,
3867,3045,2,"> the problem with this method is that you can't control the ratio of smallest to largest eigenvalue (and i think that as the size of your randomly generated dataset goes to infinity, this ratio will converge to 1).",2010-09-24T09:28:29.650,603,CC BY-SA 2.5,
3868,2783,0,"> Not random: two matrices generated from the same Whishard will not be independant from one another. If you plan to change the Whishart at each generation, then how do you intend to generate those Whishart in the first place?",2010-09-24T09:32:47.007,603,CC BY-SA 2.5,
3869,3045,0,"> besides, the method is not very efficient (from a computationnal point of view)",2010-09-24T09:36:01.417,603,CC BY-SA 2.5,
3870,3045,1,"Your ""random matrix"" is a specially structured one called a ""diagonal plus rank-1 matrix"" (DR1 matrix), so not really a good representative random matrix.",2010-09-24T10:08:08.447,830,CC BY-SA 2.5,
3871,3038,0,@Robin the p value of a null hypothesis significance test is the probability of seeing as or more extreme data than that observed assuming the null hypothesis is true; but perhaps I could word the statement above better.,2010-09-24T10:26:31.300,183,CC BY-SA 2.5,
3872,3038,0,@Robin I modified the question to try to make my point clearer,2010-09-24T10:32:39.133,183,CC BY-SA 2.5,
3873,2783,0,@kwak: I don't understand your question: the Bartlett decomposition will give independent draws from the same Wishart distribution.,2010-09-24T11:03:38.877,495,CC BY-SA 2.5,
3875,2783,0,"> Let me rephrase this, where do you get the scale matrix of your whishart distribution from ?",2010-09-24T12:16:44.293,603,CC BY-SA 2.5,
3876,2971,0,On the other hand reasoning in light of Runge phenomenon (Physics inspirations again) drives to a conclusion that future error is something about Complexity/Train_Error.,2010-09-24T12:25:26.540,,CC BY-SA 2.5,user88
3877,2783,3,"@kwak: it's a parameter of the distribution, and so is fixed. You select it at the start, based on the desired characteristics of your distribution (such as the mean).",2010-09-24T12:40:23.173,495,CC BY-SA 2.5,
3880,2901,1,"@Kwak: Two quick follow-up questions; no need to respond if you don't want.  (1) ""Fair tailed""??  (2) What are the min and max of a Gaussian?",2010-09-24T13:29:44.777,919,CC BY-SA 2.5,
3883,3051,4,Have you seen [this](http://rss.acs.unt.edu/Rdoc/library/TTR/html/MovingAverages.html)?,2010-09-24T14:52:24.400,830,CC BY-SA 2.5,
3884,3051,0,"Can you give some background on this ""slide"" idea?",2010-09-24T15:01:01.503,5,CC BY-SA 2.5,
3885,3055,0,"Thank you Shane, but this seem to find local minima (or maxima) - i.e. a single point in a region. My data ( as any biological data) IS NOISY> I don't really care about point minima themselves but about larger regions which are low.",2010-09-24T15:53:49.667,634,CC BY-SA 2.5,
3886,3055,0,"If you have local maximum and minimum points, you can easily calculate the differences.  So you want to know instances when the differences are both large in magnitude and in ""duration""?  Is this time series data?",2010-09-24T15:59:20.717,5,CC BY-SA 2.5,
3887,3055,0,"@david Perhaps, you can iteratively use this function. Use the function to identify a minima. Drop that point and surrounding points (say x points within some tolerance level). You can choose a tolerance level (e.g., +- 10 counts) which would define a flat region for your application.  Find a new minima on the new dataset. Will that work?",2010-09-24T16:01:07.127,,CC BY-SA 2.5,user28
3888,3055,0,@shane The analogy that comes to mind is that of valleys in a mountainous region. I think the goal is to identify all the valleys and the issue is some valleys are 'deeper' and some are 'shallow' relative to the mountains.,2010-09-24T16:02:57.350,,CC BY-SA 2.5,user28
3889,3055,0,"@Shane It's not a time series, these are coordinate along the genome (chromosome).",2010-09-24T16:03:43.913,634,CC BY-SA 2.5,
3890,3055,0,"@srikant: this function gives you all the local maximum and minimum indexes.  You can easily use these to calculate the depth of the valleys.  What I'm missing in this question is whether there is some other factor to be considered (e.g. ""width"")?",2010-09-24T16:05:04.203,5,CC BY-SA 2.5,
3891,3052,0,Could you provide a small data sample?,2010-09-24T16:20:43.850,5,CC BY-SA 2.5,
3893,3052,0,@Shane see update,2010-09-24T16:43:03.593,634,CC BY-SA 2.5,
3894,3052,0,"@David Thanks.  As both the answers imply, time series analysis can be applied here since you have ordered observations.",2010-09-24T16:50:52.710,5,CC BY-SA 2.5,
3895,2901,0,"@Whuber; the explanation for (2) is quiet subtile. The basic idea is based upon 'a quantile alternative to kurtosis' (J.J.A. Moors). Just as the kurtosis is determined by the normalized sum of (E_7-E_5) and (E_3-E_1) (where E_i is the ith octile), the 8^th central moment is determined by the normalized sum of (G_15-G_14) and (G_3-G_1) (where G_i is the ith 100/16 percentile) -the top and bottom 5% of the data.",2010-09-24T17:39:26.417,603,CC BY-SA 2.5,
3896,2901,0,The same applies to the odd moments.,2010-09-24T17:45:00.873,603,CC BY-SA 2.5,
3897,3052,0,"This is kind of hard to answer without know exactly what you're looking for. Can you maybe circle the points on the plots that you're looking to capture? What do you consider a ""valley""? how low does it have to go and what are you looking to return? It's hard to formulate a solution without knowing the question, ie thresholds and such.",2010-09-24T17:48:37.710,1409,CC BY-SA 2.5,
3898,2935,0,Gaetan:> yes linear regression can handle slope dummy.,2010-09-24T17:56:04.143,603,CC BY-SA 2.5,
3899,2901,0,"@Kwak: I'm not grasping the subtlety of attributing a min and max to a distribution that doesn't have one! ;-)  I suspect you may be referring to sampling theory, whereas I read the original question as one about theoretical distributions.",2010-09-24T18:00:51.377,919,CC BY-SA 2.5,
3900,2901,0,"@whuber:> Oh, then indeed i missunderstood your comment. Yes min (max) was a poor choice of terms.",2010-09-24T18:13:22.357,603,CC BY-SA 2.5,
3901,3053,0,"This does not return what the asker wanted, but 5.33 5.00 6.33. However, it looks quite interesting. Can you explain your idea, because I don't get it.",2010-09-24T18:14:07.487,442,CC BY-SA 2.5,
3902,3057,0,"@cxr Thank for your response. I has a look at `surveillance` and `DCluster `, but could you please be a bit more specific? They are both relatively large packages and their aim seems quite specific. I'm not sure where to begin.",2010-09-24T18:37:44.347,634,CC BY-SA 2.5,
3903,3052,0,"@ Shane‚ô¶ Thank you. As I have no experience with time-series analysis also, could you leave a few pointers of where should I start?",2010-09-24T18:39:08.233,634,CC BY-SA 2.5,
3904,3061,0,Duplicate? http://stats.stackexchange.com/questions/3062/how-do-we-find-concordance-between-two-sets-of-non-linear-non-monotonic-data,2010-09-24T19:13:01.753,5,CC BY-SA 2.5,
3905,3061,0,@user1417: You should edit a question rather than posting a new one.,2010-09-24T19:13:28.303,5,CC BY-SA 2.5,
3906,3061,0,You should give a more precise reference than 'Lin's concordance correlation coefficient'.,2010-09-24T19:24:28.720,603,CC BY-SA 2.5,
3907,3052,0,@David: Why not start by looking at the drawdowns function that I posted.  Walk through the example and look at the algorithm.  It's very simple.,2010-09-24T19:41:12.803,5,CC BY-SA 2.5,
3908,3052,0,"@David: By the way, I hate to state the obvious, but just looking at the regions you selected in your graph, it looks like you chose the area in the lowest quartile.  Is that a sufficient criteria?",2010-09-24T19:44:51.190,5,CC BY-SA 2.5,
3909,3064,0,"Oh, and feel free to retag - my browser isn't working with the auto-suggest, so I'm having a hard time seeing what's out there.",2010-09-24T19:55:12.150,71,CC BY-SA 2.5,
3910,3058,0,Please don't downvote without providing a comment.  How am I supposed to know what's wrong?,2010-09-24T19:59:02.007,71,CC BY-SA 2.5,
3911,3065,0,"Obviously you can apply the same approach using something else than the moving average, this was just to give an idea.",2010-09-24T20:38:43.047,582,CC BY-SA 2.5,
3912,3065,0,"+1 Thank you very much, nico. Let me see if I got you right: at the end, this is basically like setting some global threshold and defining any point with value < threshold as a part of a valley. The sampling etc. is just used to get some meaningful measure (quantile) to set the threshold. Why can't we use a single threshold for the entire points, I mean, if we did enough simulations we would get straight (read and yellow) lines. Also, correct me if I'm mistaken, but this does not take into account the surrounding environment but examines the absolute value of each point.",2010-09-24T21:29:29.190,634,CC BY-SA 2.5,
3913,3065,0,"@David B: of course, you could use a global threshold and that would probably save you some calculation time. I guess choosing something like 1/3 of the global mean could be a start. This swapping process is probably more helpful if you use some other statistics than the moving average, it was mostly to give an idea. Anyway the moving average will take into account the surrounding, in the example it will take into account a window of 10 points.",2010-09-24T21:35:14.623,582,CC BY-SA 2.5,
3914,2945,0,"ars: for maximum likelihood estimation, curvature of the parametric manifold affects sample complexity",2010-09-24T21:58:55.737,511,CC BY-SA 2.5,
3915,2945,0,"Yaroslav Bulatov:> even more intuitively, the median is unaffected by shifts concerning <50% of the sample, yet it's still a one parameter location model (just like the mean).",2010-09-24T22:08:31.837,603,CC BY-SA 2.5,
3916,3054,0,"BTW, I once wrote about a usage of this function to implement the notion of ""quantile loess"" : http://www.r-statistics.com/2010/04/quantile-loess-combining-a-moving-quantile-window-with-loess-r-function/",2010-09-24T22:11:50.897,253,CC BY-SA 2.5,
3917,3052,0,it sounds like you want to build histograms of depths of coverage for each span from 1 to 100k (or so).  Then you probably might like to color-code your overall genomic chart where each vertical bar's intensity depends on for how many histograms it fell into the bottom percentile (or so).,2010-09-24T23:13:47.350,486,CC BY-SA 2.5,
3918,3060,0,Seems interesting. Do you know of a simple R implementation?,2010-09-25T01:54:38.520,104,CC BY-SA 2.5,
3919,3008,0,Do you have a worked case in R?,2010-09-25T02:07:25.970,104,CC BY-SA 2.5,
3920,2988,0,"Let me suck a dropout rate out of my thumb - 20%.  We will indeed collect many variables for instance, age, trauma score but I wanted to keep things as simple as possible for the power calculation. I have often found it useful to discuss a primary model and then secondary models that are loaded with more finesse and nuance.",2010-09-25T02:11:12.877,104,CC BY-SA 2.5,
3921,2766,0,"Regarding the wikipedia link that you gave which requires data to be normally distributed, however my data is not presumed to be normal or of any other distribution. Further, what is the best way to compare distributions if you think the mentioned method is not? Thanks !",2010-09-25T03:49:38.800,,CC BY-SA 2.5,user1102
3922,2766,0,"@Harpreet: true enough, however, if your distribution has a finite variance, the central limit theorem ( http://en.wikipedia.org/wiki/Central_limit_theorem ) describes the asymptotic distribution of the sample mean. Since you presumably do not know the population variance, you would have to estimate that as well. see also http://en.wikipedia.org/wiki/Student's_t-statistic#Prediction",2010-09-25T03:58:17.563,795,CC BY-SA 2.5,
3923,3070,0,"+1, good points.  On this: ""unpleasant ideas about memory management"" .. interesting, can you elaborate?",2010-09-25T05:27:26.547,251,CC BY-SA 2.5,
3924,3070,1,"my memory is going _somewhere_; my experience with Java outside of Matlab usage indicate it is the likely culprit, and running in `-nojvm` appears to help...",2010-09-25T05:42:58.403,795,CC BY-SA 2.5,
3928,2975,0,"It is complex and messy real-world data. The ""thermal anomaly"" is (I hypothesize) variable in time but consistent in location. Overlaid on this is a different large-scale regional temperature gradient, seasonal and day/night variations, noise, instrument variability, etc. The flight paths come and go in different directions (so the mean temp of two flight paths should not be equal, even if season+time-of-day are removed, because of the large-scale regional gradient, etc.)...",2010-09-25T07:14:29.190,957,CC BY-SA 2.5,
3929,2978,0,"This is helpful, thank you. And addresses the simplified problem stated above. Unfortunately, as commented above, the real world is much more complex than what I initially described. I'm still working on it...",2010-09-25T07:15:46.247,957,CC BY-SA 2.5,
3930,2988,1,"Ok, but usually the expected % dropout, the number of covariates, and whether covariates are measured with errors (see e.g., http://j.mp/9fJkhb) enter the formula (in all case, it will increase the sample size).",2010-09-25T07:37:32.087,930,CC BY-SA 2.5,
3931,3053,1,"@Henric I use this trick frequently, yet user1414's code return this roll with slide 1, not 2, as intended by OP. Check out `(c(0,0,x)+c(0,x,0)+c(x,0,0))/3` to see what I mean (and how does it work). The proper formula would be: `(c(0,0,x)+c(0,x,0)+c(x,0,0))[1:(length(x)-3)*2+1]/3` (we must cut 0-padding at the beginning and select even elements then.",2010-09-25T08:00:54.957,,CC BY-SA 2.5,user88
3932,3058,0,"It wasn't me, but this is slow (but not much slower than `rollapply`).",2010-09-25T08:06:01.900,,CC BY-SA 2.5,user88
3933,3072,0,"+1 Not tada, rv/windowsize ;-)",2010-09-25T08:24:54.523,,CC BY-SA 2.5,user88
3934,3072,1,"This marg... comment box is too narrow for this code, so I've posted a new answer.",2010-09-25T08:31:46.993,,CC BY-SA 2.5,user88
3935,3054,0,"You may add a 0 at the end of x (`x<-c(x,0)`) to get the last element of answer.",2010-09-25T08:36:22.007,,CC BY-SA 2.5,user88
3936,3070,1,"My favorite example of MATLAB strange built-in codes is shuffle, which reorders the data with the ordering returned by sorting a freshly created random vector.",2010-09-25T08:49:05.917,,CC BY-SA 2.5,user88
3937,3054,1,"@mbq; that is making a strong assumption that the observation is 0. I had been mulling this point and T-Burns is making the same assumption (an unobserved 0). I would prefer perhaps to pad with NA and pass in the `na.rm = TRUE` argument to `mean`. The answer won't be the same as what the OP requested, but it seems more useful. I'll edit my answer to include this.",2010-09-25T08:59:38.317,1390,CC BY-SA 2.5,
3938,3054,0,"@ucfagls Yet this is easy to change and as you said this assumption was made by the OP. On the other hand, I would be even more restrictive and removed the last average.",2010-09-25T09:24:45.050,,CC BY-SA 2.5,user88
3939,3058,2,"wasn't me either, but as mentioned by yourself, pre-allocation of the result object will help with the speed issue. One trick, if you don't know, or it is tedious/difficult to determine, the size of the result object you need. Allocate something reasonable, perhaps pre-filling with NA. Then fill in with your loop, but add a check that if you are approaching the limit of the preallocated object, allocate another big chunk, and continue filling.",2010-09-25T09:43:32.883,1390,CC BY-SA 2.5,
3940,3058,1,"@mbq; Speed of results, whilst important, isn't the only consideration. Instead of having to reinvent the while and handle all the indexes etc in the custom solutions, the one-linear that is `rollapply` is much easier to understand and grep the intention of. Also, `rollapply` is likely to have had many more eyeballs checking its code than something I might cook up one afternoon. Horses for courses.",2010-09-25T09:45:50.687,1390,CC BY-SA 2.5,
3941,3069,2,"This should be community wiki, IMO.",2010-09-25T12:01:46.490,5,CC BY-SA 2.5,
3943,3058,0,"ucfagls, thanks for adding some info on pre-allocating when you don't know the ultimate size of the object - that's helpful.  And while I agree that rollapply is probably almost definitely the way to go, I a) wanted to provide an example that would calculate the edge cases with less than three values, and b) just wanted to use `while()`, which I've yet to have a use for ;)",2010-09-25T14:43:16.413,71,CC BY-SA 2.5,
3944,3075,0,"Thanks for your answer.  Regarding not knowing the true status: we do have extensive questionnaires and are well aware of the BCG vaccine issue with the skin test - in fact, these blood tests are supposed to resolve that issue because they use a different set of antigens than the PPD you're used to.  That's almost a separate question, however, and one we're going to be working on a bit later - right now, my interest is in making this test 'longitduinally aware'.",2010-09-25T14:50:42.067,71,CC BY-SA 2.5,
3945,3075,0,"... especially because some individuals do flip from negative to positive, and that's often a product of their typical nil and TB results making small fluctuations - nil down a bit, TB up a bit, and suddenly they're positive.  Next test, they've gone back to being negative.  I can see that as I review individual results, but I'm not sure how to appropriately incorporate my intuition into a model.",2010-09-25T14:54:01.327,71,CC BY-SA 2.5,
3946,3075,0,"Finally, while I have tried taking the log results, that doesn't seem to be sufficient to get them even close to normality.  They are very, very skewed, and the truncation at the high end further complicates this by adding a noticeable blob of density at the ceiling.  Interestingly, however, the sample-wide nil and TB result distributions are quite similar, with the only difference being that that blob on the ceiling is much larger for the TB results.",2010-09-25T14:57:14.367,71,CC BY-SA 2.5,
3947,3075,0,Thanks for taking the time to read and answer this beast of a question!,2010-09-25T14:57:47.267,71,CC BY-SA 2.5,
3948,2915,0,I don't know what starting a bounty mean?  I have been asked to start one.  What are its implications?,2010-09-25T16:06:10.577,1329,CC BY-SA 2.5,
3949,1001,0,"Indent your code with 4 spaces to get it converted into code. Or, equivalently, select it and use the button with binary data.",2010-09-25T17:05:36.933,,CC BY-SA 2.5,user88
3950,1001,0,mdb: Not sure what you mean. I would not ask if it would all be correct and without question. I am not sure what you mean with indenting. I hope it has nothing do to with using this website 'right' ;),2010-09-25T19:22:40.460,608,CC BY-SA 2.5,
3951,3078,0,"Thank you for your input. I will read more about the Anderson-Darling test, which I have not heard of . Regarding the Chi-Square test, doesn't this test require that the distribution is Chi Square? I agree that all other assumptions are very relaxed...",2010-09-25T19:28:40.227,608,CC BY-SA 2.5,
3952,1001,0,"I think it's just to remind you that this is a Markdown enabled website, so you can benefit from easy syntax highlighting (which facilitates reading) through md markup (http://j.mp/9bQHMC) -- or the utilities provided in the on-line editor.",2010-09-25T21:38:14.443,930,CC BY-SA 2.5,
3953,3069,0,Would you care to explain why you couldn't also look at R?,2010-09-25T23:32:31.163,334,CC BY-SA 2.5,
3954,3069,0,"@DirK: I've hardly heard of R. Moreover I wanted to learn some programming language like Python, and then again too I don't think R is anywhere close to python, IMO. I hope it answers your question.",2010-09-25T23:37:38.127,,CC BY-SA 2.5,user1102
3955,3069,2,"Poke around a little here and at StackOverflow in terms of what people recommend for *statistical analysis and programming*. Many of us feel that there is no real alternative to R.  But just like  beauty, this is in the eye of the beholder, so good luck.",2010-09-25T23:41:07.090,334,CC BY-SA 2.5,
3956,2897,0,"the result you state for [sub]gaussian tails does not look right. according to the bound [$A\sqrt{p}$] you cite, the $p^{th}$ norm of a centered gaussian variable would not [in the limit] exceed 1. but the $p^{th}$ norm of a rv tends to its ess sup, which is $+\infty$ for a gaussian variable.",2010-09-26T02:00:25.923,1112,CC BY-SA 2.5,
3957,3078,0,"Ampleforth, I am not sure that is the case.  I think you can test all sorts of data with the Chi Square test.",2010-09-26T04:18:22.340,1329,CC BY-SA 2.5,
3958,3076,0,"+1 I like the second point (I use roxygen + git). The first point makes me think also of the possibility to give your code to another statistician that will be able to reproduce your results at a later stage of the project, without any help.",2010-09-26T09:52:17.847,930,CC BY-SA 2.5,
3959,3082,4,"I just can't understand why people think this swapping one is 'simpler' or 'more naive' than FY... When I was solving this problem for a first time I have just implemented FY (not knowing it has even a name), just because it seemed the simplest way to do it for me.",2010-09-26T09:56:18.700,,CC BY-SA 2.5,user88
3961,3082,1,"@mbq: personally, I find them equally easy, although I agree that FY seems more ""natural"" to me.",2010-09-26T10:29:21.267,582,CC BY-SA 2.5,
3962,3079,0,I¬¥ll look into the matching package. My first analysis does control for the same factors as implemented in the match procedure in a multivariate linear regression model. Thx for the feedback..,2010-09-26T11:50:40.637,1291,CC BY-SA 2.5,
3963,3082,3,"When I researched shuffling algorithms after writing my own (a practice I have since abandoned), I was all ""holy crap, it's been done, and **it has a name**!!""",2010-09-26T13:55:04.440,830,CC BY-SA 2.5,
3964,3081,0,"I particularly like the likelihood ratio as a means of aggregating evidence in meta-analysis; if you have sufficient data to compute them for each study, you simply compute the product across studies to represent the aggregate evidence for/against a hypothesis.",2010-09-26T14:36:21.077,364,CC BY-SA 2.5,
3965,3086,0,How many individuals are used to estimate the item intercorrelations? What software is used for computing both correlations?,2010-09-26T15:42:05.733,930,CC BY-SA 2.5,
3966,3073,0,"thanks for translating. I figured it would be an easy exercise, and I learned some R from it",2010-09-26T15:59:52.410,795,CC BY-SA 2.5,
3967,3089,1,"@shabbychef Yes, it's the MLE of correlation in a two-way table, see Hamdam (1970), http://j.mp/9SN7Lk, or Brown & Benedetti (1977), http://j.mp/aJjjzu.",2010-09-26T17:11:20.817,930,CC BY-SA 2.5,
3968,3088,0,But aren't these steps already part of meta-analysis?,2010-09-26T17:14:57.470,930,CC BY-SA 2.5,
3969,3086,0,matrix is 5000 x 300. Software is SAS. PROC CORR and PLCORR. Convergence was not reached so the number of iterations have been changed to 100 iterations.,2010-09-26T17:29:12.980,1154,CC BY-SA 2.5,
3970,3091,1,"You may be interested in this related question, http://stats.stackexchange.com/q/2628/930.",2010-09-26T18:30:37.800,930,CC BY-SA 2.5,
3971,3093,0,"Thanks for both of your responses. I will dig a little further into some of the references you suggested chi. I guess what I was getting at in my question, was aside from the issue of spatial effects, if I actually did have population-wide independent data, would classical hypothesis tests suffice - since they are based on sampling theory? Is multi-level modelling the only approach here? Forests aren't really nested in counties, rather I'd say they make up a measured proportion of each county, and those measurements are correlated in space...",2010-09-26T20:16:16.410,,CC BY-SA 2.5,kip
3972,3093,0,"@kip Would you mind explaining ""they make up a measured proportion of each county""? I initially thought there was a % of Lyme disease recorded at each site (in your case all the forests in each country).",2010-09-26T20:29:16.593,930,CC BY-SA 2.5,
3973,3093,0,"@kip About your second point, well if you consider you have a set of measurements collected on a fixed period of time on all possible statistical units (this is what you called your population), you can apply inferential procedures provided you are willing to assume there is an evolving generating model underlying the observed data (but see the related question I put in comment for a thorough discussion, e.g. by @ars, on this point).",2010-09-26T20:29:53.787,930,CC BY-SA 2.5,
3974,3089,0,"@chl thanks, I thought I that was the case, but it always seems like a too remarkable fact because the cutoffs are unknown.",2010-09-26T20:31:35.547,795,CC BY-SA 2.5,
3975,3089,1,"@shabbychef Yes, the threshold remains unknown. To my opinion, John Uebersax provides a great overview on TCs correlation, http://j.mp/bnhem7.",2010-09-26T20:34:57.947,930,CC BY-SA 2.5,
3976,3088,3,"@chl: True, but the point is that these steps get to the essence of the question.  A meta-analysis would be helpful only when there are many studies (not just two) and their merits have already been carefully evaluated.  The question before us is really asking how one goes about evaluating the quality of a study, or pair of conflicting studies, in the first place.  Cyrus has pointed to some of the many aspects of this; a reasonable treatment usually requires one or two semesters of university-level study.  In this light I think his use of the term ""heroic"" is somewhat understated!",2010-09-26T20:38:33.363,919,CC BY-SA 2.5,
3977,3095,2,Thanks for your (always) enlightened remarks! You provide a more thorough response than mine wrt. OP (I must admit I'm a little bit biased toward epidemiology). [I'll +1 ASAP],2010-09-26T20:40:18.207,930,CC BY-SA 2.5,
3978,3081,0,"I commented on the (ir)relevance of meta analysis after Cyrus's answer, but upvoted this response for everything else, especially the bullet points.",2010-09-26T20:40:59.527,919,CC BY-SA 2.5,
3979,3080,0,Maybe this should be CW?  There will not be a unique answer to this question and multiple perspectives and approaches might emerge.,2010-09-26T20:42:23.330,919,CC BY-SA 2.5,
3980,3088,1,"@whuber Yes, I agree with you and @Cyrus. Of course, assessing the quality and trustiness of previous studies is a mandatory step (and it takes time to review every studies, especially when we have to contact authors because informations are missing in the MS); I just thought this was part of the meta-analysis, and the ""statistical part"" reduces to bringing a quantitative summary of trustworthy results.",2010-09-26T20:50:34.073,930,CC BY-SA 2.5,
3982,3093,0,"@chi Sorry for the confusion, I meant a proportion for forest cover, and a binary indicator for presence or absence at each country. I think I get what you are saying here, would the assumption of not having an evolving generating model essentially be an assumption of 'stationarity'?",2010-09-26T21:41:44.703,,CC BY-SA 2.5,kip
3983,3095,0,"Thanks a lot for this explanation. It really helps. The distinction between process and population is what I was missing (among other stuff!).. that the process is stochastic, and will vary from place to place and time to time",2010-09-26T21:47:13.227,,CC BY-SA 2.5,kip
3984,3080,2,@whuber I would vote against CW because even if there are different perspectives there is likely to be one *best* approach. This is similar to how the same hypothesis can be tested using different frameworks/models but there is likely to be one best approach.,2010-09-26T22:17:32.030,,CC BY-SA 2.5,user28
3985,2761,0,"The unpaired student t test has two formulas.  The large sample formula is applied to samples with more than 30 observations.  The small sample formula is applied to samples with less than 30 observations.  The main difference in those formulas is how they calculate the pooled standard error.  The small sample formula is much more complicated and counterintuitive.  And, in reality it really makes very little difference.  I have tested that several times.  That's why I think most people have forgotten about this distinction.  And, they use most of the time the large sample formula.",2010-09-26T22:45:11.677,1329,CC BY-SA 2.5,
3986,3066,0,@suncoolsu: many thanks! I have just followed your advice and ran prcomp. I also stored the loadings matrix it produced. But how can I use this matrix to group together the pages?,2010-09-26T23:00:03.900,1007,CC BY-SA 2.5,
3987,3087,1,"""Let's be generous, too, and suppose you are actually selecting distinct pairs of indexes uniformly at random for your shuffles"". I don't understand why that assumption can be made, and how it is generous. It does seem to discard possible permutations, resulting in an even less random distribution.",2010-09-27T01:14:05.660,1421,CC BY-SA 2.5,
3988,3085,1,"+1. That demonstrates that the probability for a given card to end up in a given position approximates the expected ratio as the number of shuffles increases. However, the same would also be true of an algorithm that just rotates the array once by a random amount: All cards have an equal probability to end up in all positions, but there is still no randomness at all (the array remains sorted).",2010-09-27T01:30:05.487,1421,CC BY-SA 2.5,
3990,3081,0,"@whuber @Gaetan's question assumes that one study is closer to the truth. I try to take a step back and situate variations in results between studies within a meta-analytic framework, acknowledging the possibility that the studies may be of equal quality, but that random sampling or substantive differences may be the explanation.",2010-09-27T02:43:35.087,183,CC BY-SA 2.5,
3991,3081,0,"@whuber Even with two-studies it would be possible to form a meta-analytic estimate of the effect of interest. Of course, the confidence interval of the estimate of effect may be large. But a high degree of uncertainty is to be expected if only two studies have been conducted and they are giving conflicting results.",2010-09-27T02:44:04.557,183,CC BY-SA 2.5,
3992,3090,0,"Thanks a lot for the answer, I'll do the inquiry when I'll have more time",2010-09-27T04:56:38.863,223,CC BY-SA 2.5,
3993,3070,2,"@mbq: `shuffle` might be in a toolbox, is not stock matlab. could hardly be worse than builtin `randperm` which returns sort index of a random vector. Again, this is probably the wrong algorithm (I just learned about the Knuth-Fisher-Yates shuffle here on stats.SE)..",2010-09-27T05:14:06.270,795,CC BY-SA 2.5,
3994,3070,0,"@shabbychef True, I was trying to write about `randperm` and I've missed the name. And it is not a bad algorithm (it is correct and quite elegant), it is just blatantly inefficient because of a redundant sort.",2010-09-27T07:19:12.820,,CC BY-SA 2.5,user88
3995,3089,0,"Thanks for explanation and references. But what i would know is when values obtained from tetrachoric are slightly different of person coefficient. Because they're supposed to be close. Indeed, in many programs, for estimation, Pearson r are used as start values for algorithm which compute tetrachoric correlation.",2010-09-27T07:33:07.207,1154,CC BY-SA 2.5,
3996,3085,0,"@Thilo: Sorry I don't follow your comment. An ""algorithm  rotates by a random amount"" but there's still ""no randomness""? Could you explain further?",2010-09-27T08:21:59.907,8,CC BY-SA 2.5,
3997,3085,0,"If you ""shuffle"" an N-element array by rotating it between 0 and N-1 positions (randomly), then every card has exactly the same probability to end up in any of the N positions, but 2 is still always located between 1 and 3.",2010-09-27T08:26:39.787,1421,CC BY-SA 2.5,
3998,3085,1,"@Thio: Ah, I get your point. Well you can work out the probability (using exactly the same idea as above), for the Pr(A in position 2) and Pr(A in position 3) - dito for cards B and C. You will see that all probabilities tend to 1/3. Note: my answer only gives a particular case, whereas @whuber nice answer gives the general case.",2010-09-27T08:46:09.197,8,CC BY-SA 2.5,
4000,3093,0,"@kip Finally, I like @whuber point of view: ""you're modeling a process, not a population"", which seems pretty close to what I had in mind.",2010-09-27T09:27:46.160,930,CC BY-SA 2.5,
4001,3102,1,"Thanks. The idea of modelling the two curves together seems like a good one. However, I have theoretical reasons for wanting to quantify the degree to which the shape of the curves for the 100 and 400 metres are the same. Does the approach you mention do this?",2010-09-27T09:28:06.533,183,CC BY-SA 2.5,
4002,3102,1,"@Jeromy:> I'm afraid not. On the other hand, i would pay attention to the issue of sample size and model complexity as 200 observations ain't a lot.",2010-09-27T11:42:14.847,603,CC BY-SA 2.5,
4003,3080,0,"@Srikant: In any particular case I can imagine you could amass a strong defense to support your assertion.  In general, though--which is the present situation--the best answer will depend on the context.  As a simple (and incomplete) example, contemplate the differences between evaluating a pair of designed physical experiments (such as measuring the speed of light, where historically most of the confidence intervals have missed the truth!) and an observational study in the social sciences.",2010-09-27T15:03:40.153,919,CC BY-SA 2.5,
4004,3087,1,"@Thilo: Thank you.  Your comment deserves an extended answer, so I placed it in the response itself.  Let me point out here that being ""generous"" does not actually discard any permutations: it just eliminates steps in the algorithm that otherwise would do nothing.",2010-09-27T15:23:04.257,919,CC BY-SA 2.5,
4005,3116,5,"I agree with you and about a decade ago collected a bunch of such problems for a course (see http://www.quantdec.com/envstats/homework/class_03/paradox.htm ).  However, there is a strong pedagogical counter-argument: Probability itself can be confusing, so if you start off with counter-intuitive examples, you risk losing your audience forever (like Augustus DeMorgan, a pioneering probabilist, who later in life completely gave up on probability as hopelessly difficult!).  So caution is in order here, especially if you want to *motivate* people in an *introductory* setting.",2010-09-27T15:32:34.597,919,CC BY-SA 2.5,
4006,3080,0,"@whuber Perhaps, we should continue this conversation on meta. I admit that I am still fuzzy about when to use CW and when not to but to take up your point: the very best answer to this question would then be that the answer is context dependent and explain why via a few examples. In any case, I somehow feel that this question should not be CW but I am unable to articulate any more reasons beyond the ones I have outlined above.",2010-09-27T15:55:20.627,,CC BY-SA 2.5,user28
4007,3115,1,Have a look at my question here: http://stats.stackexchange.com/questions/1881/analysis-of-cross-correlation-between-point-processes,2010-09-27T16:35:52.950,582,CC BY-SA 2.5,
4008,3070,1,"@mbq: the other good part about `randperm` is that it is affected by the seeding of `randn`, whereas a mex'ed version of Knuth-Fisher-Yates perhaps cannot access the randn seed 'internally', and a pure .m version of shuffle would probably be too slow.",2010-09-27T16:38:31.493,795,CC BY-SA 2.5,
4009,3100,0,"When are two curves ""consistent,"" Jeromy?  (Without a clear definition, many different answers are possible.)  Your second initial thought is suggestive, but not dispositive.  For example, unless you choose just the right way to express the results--should they be times, speeds (miles per hour), or inverse speeds (hours per mile), for instance?--then you might fail to identify and quantify a ""consistency"" that is really present.  In particular, I would **expect** $\theta_2$ to be smaller for the shorter race.",2010-09-27T16:45:07.457,919,CC BY-SA 2.5,
4010,2897,0,Thanks for catching that.  I forgot the exponent on the RHS; it's corrected now.,2010-09-27T16:52:17.833,89,CC BY-SA 2.5,
4011,3119,0,"Please elaborate : do you want to scale x2 so it lies between 0 and 0.5 like x1, or do you want to be able to predict x1 from x2?",2010-09-27T17:00:17.347,1124,CC BY-SA 2.5,
4012,3116,1,"I think it causes polarization. The students who are not interested in mathematics/probability will become confused, and the inquisitive/interested students will be inspired to learn more. Like you said, it might be best to exercise caution. Nothing could be worse than a confusing teacher presenting a confusing example!",2010-09-27T17:12:43.163,1118,CC BY-SA 2.5,
4013,3087,2,"This problem can be analyzed fully as a Markov chain on the Cayley graph of the permutation group.  Numerical calculations for k = 1 through 7 (a 5040 by 5040 matrix!) confirm that the largest eigenvalues in size (after 1 and -1) are exactly $(k-3)/(k-1) = 1 - 2/(k-1)$.  This implies that once you have coped with the problem of alternating the sign of the permutation (corresponding to the eigenvalue of -1), the errors in *all* probabilities decay at the rate $(1 - 2/(k-1))^n$ or faster.  I suspect this continues to hold for all larger $k$.",2010-09-27T17:48:33.853,919,CC BY-SA 2.5,
4014,3119,0,x1 and x2 are predictions from 2 different classifiers. I want to scale x2 so it lies between 0 and 0.5 like x1.,2010-09-27T17:57:12.317,,CC BY-SA 2.5,Bob
4015,3121,1,"I'm not sure what this means; do you have the numbers 1-100 in a bag, and are going to pick them out one at a time without replacement?",2010-09-27T18:05:22.537,795,CC BY-SA 2.5,
4016,3119,0,"try squaring your x2. it will look more like x1, then.",2010-09-27T18:07:25.700,795,CC BY-SA 2.5,
4017,3121,1,"@shabbychef: The formula in that case is n = 0, because the average and median are both 50.5! :-)",2010-09-27T18:13:22.507,919,CC BY-SA 2.5,
4018,3121,2,"@whuber: yes, this is why I'm not sure what is being asked. although the question could be: how many numbers must I pick until I pick one of 49,50,51 or 52, with 95% probability...",2010-09-27T18:32:22.263,795,CC BY-SA 2.5,
4019,3108,1,Is there an explanation for the appearance of 7?,2010-09-27T18:52:29.477,,CC BY-SA 2.5,user28
4020,3119,0,Let me explain the problem in another way. After adjusting one solution to the scale of the other I want to fit linear regression so that intercept=0 and slope=1.,2010-09-27T19:16:39.193,,CC BY-SA 2.5,Bob
4021,3119,0,"@Bob run a linear regression to get $x_1 = m x_2 + b$, then transform   as $\hat{x_2} \leftarrow m x_2 + b$. Then a linear regression of $\hat{x_2}$ vs $x_1$ will have intercept 0 and slope 1.",2010-09-27T19:28:36.470,795,CC BY-SA 2.5,
4022,3119,0,"@shabbchef: You're right, but the relationship between $x_1$ and $x_2$ will still be curvilinear.  One needs to find *nonlinear* re-expressions of the variables before removing any ""tilt"" via linear regression.",2010-09-27T19:49:00.890,919,CC BY-SA 2.5,
4023,3121,2,"@shabbychef: I completely agree with you; my comment was just an amusing way to emphasize the vagueness.  The *answers* are obvious--sequential methods or standard experimental design methods, with a reference to the literature on the relative power of robust methods to handle the case of the median--but exactly *which* answer is appropriate (or best) will depend on how the numbers are being obtained and assumptions about their possible distribution.  E.g., if the numbers are in the range $[1,100]$ *and that's all you know,* I can guarantee 2% relative error with 95% confidence in 9228 draws.",2010-09-27T20:23:30.093,919,CC BY-SA 2.5,
4024,3121,1,"(continued)...But if you use a sequential method and assume normality and it turns out all the values are around 98 - 100, then you can stop after about a half dozen draws!",2010-09-27T20:26:05.950,919,CC BY-SA 2.5,
4025,3125,2,"You assumed a uniform distribution, so why do a simulation?  An exact answer is straightforward to obtain.  (Indeed, my facetious comment still applies: by assuming a uniform, you know *a priori* that the mean is 50.5, so you don't need to pick *any* numbers at all!)  But if *all* you know is that the numbers are in the range [1,100], you have to plan for the worst case, which will require up to 9000+ draws (when half the values are 1 and half are 100).  First we need to hear from the OP concerning what assumptions are valid to make here.",2010-09-27T20:30:14.263,919,CC BY-SA 2.5,
4026,3119,0,(Sorry about misspelling your handle: it's the cleverest one around and so deserves to be typed correctly!),2010-09-27T20:31:50.283,919,CC BY-SA 2.5,
4027,3125,0,"@whuber, I assumed the numbers where NOT put back in the ""pool"" (that is why with 100 draws you have 100% of the trials giving the good result). Also, the simulation works for ANY set of 100 numbers, I just used 1:100 because that's what the question asked (well, I guess at least), but it can be any set of 100 numbers :)",2010-09-27T20:49:32.763,582,CC BY-SA 2.5,
4028,3121,1,"@whuber: I would think this was a homework question, but it was so ambiguously worded.",2010-09-27T20:50:35.070,795,CC BY-SA 2.5,
4029,3126,0,Thanks for the link on the R-help mailing list. There is also this one on s-news: http://j.mp/8Zol8W.,2010-09-27T20:52:31.620,930,CC BY-SA 2.5,
4030,3125,0,"Anyway, you can easily modify the script to allow for resampling the same number multiple times (add `replace=TRUE` in the `sample` call). The result turns out to be around 5000 in that case",2010-09-27T21:06:16.690,582,CC BY-SA 2.5,
4032,3119,0,@whuber the OP states he wants to 'scale x2' to get a linear fit with given slope and intercept. I live to serve.,2010-09-27T21:37:00.250,795,CC BY-SA 2.5,
4033,3119,2,"@shabbychef: Ditto.  I think I got lucky this time in guessing the intent.  We have to recognize that non-statisticians often are unaware of the technical distinctions we make.  After all, a great deal of statistical consulting amounts to figuring out what someone might *really* be asking you!  Here the best clue is that the OP himself suggested a nonlinear re-expression (""link"").  The scatterplot he posted helps immensely in understanding the situation, too.  The combination of those useful clues seemed sufficient to venture an answer rather than to keep probing for clarifications.",2010-09-27T21:43:39.537,919,CC BY-SA 2.5,
4034,3121,2,"@shabbychef: Yep, the ambiguity is certainly there.  The followup about the median could make for some difficult homework, though: you have to make a lot of assumptions and the analysis is not found in elementary courses.  (Under some distributional assumptions the median is a more efficient estimator of central tendency than the mean, but most of the time in practice it's less efficient.)",2010-09-27T21:47:05.547,919,CC BY-SA 2.5,
4035,3108,1,"My general hand-waving explanation is this: people avoid {1, 5, 10} because they are too obvious and therefore ""not random"". Numbers less than 5 - well who wants a small RN! People then tend to go for the middle number between 5 and 10. I've tried this example six times now (in classes of size ~100) and it's worked each time.",2010-09-27T21:49:28.800,8,CC BY-SA 2.5,
4037,3125,1,@nico: Now you're getting to the heart of the matter: the trick is to find a method that works for the full range of possible pool contents consistent with one's assumptions.  All a simulation can do is indicate what can be achieved by sampling when one particular assumption is true.  Your modification already reveals what's at stake: the difference between a sample of 98 and one of 5000 is enormous.  This suggests looking a Bayesian and/or sequential sampling methods instead of designing a fixed sample size based on a frequentist assumption.,2010-09-27T22:09:29.823,919,CC BY-SA 2.5,
4038,3108,2,"And of course, 17 is the least random number.  http://www.catb.org/~esr/jargon/html/R/random-numbers.html  but my favorite random number is 37: http://jtauber.com/blog/2004/07/09/37_is_a_psychologically_random_number/  (though, also see http://scienceblogs.com/cognitivedaily/2007/02/is_17_the_most_random_number.php )",2010-09-27T23:17:01.850,251,CC BY-SA 2.5,
4039,3111,2,"This is my favorite example too (HIV test), but unsure if conditional probability is too ""advanced"" given the introductory nature (plenty of studies showing that it's not too intuitive).  If you do teach this, I recommend perusing Gigerenzer and the frequency method: http://library.mpib-berlin.mpg.de/ft/gg/GG_How_1995.pdf",2010-09-27T23:21:00.407,251,CC BY-SA 2.5,
4040,3111,0,"@ars:> maybe first you state them all the relevant informations in table form, then the problem ""what do you think is  p(AIDS|test=1)?"", then the counter intuitive punchline, only then  you show them the problem re-casted as a 'tree' (where the final 4 nodes are all possible cases) and the branches show the respective probability. In my experience, the last leg need not be understood by everybody, but it has to convey the importance of having a principled way of thinking about these issues.",2010-09-28T00:08:12.337,603,CC BY-SA 2.5,
4041,3100,0,"@whuber Thanks for the point about $\theta_2$. The analysis is motivated by a theoretical and qualitative interest in consistency. Thus, my question is asking ""what is a good way to quantify consistency"". I.e., what is a good definition of consistency?",2010-09-28T00:51:03.830,183,CC BY-SA 2.5,
4043,3066,0,"Hello Laramichaels,
please find my answer below.",2010-09-28T04:05:03.050,1307,CC BY-SA 2.5,
4044,3101,0,Can you explain why this is a pyramind and not a triangle?  Or a triangle and not a ladder?  I'm not being snarky -- I'm wondering if I'm missing something significant that this visualization is trying to convey.,2010-09-28T05:03:31.033,251,CC BY-SA 2.5,
4045,3125,0,"@whuber: OK, I understand what you're saying now. Actually, to clarify on my previous comment: the code works for any 100 number set, but it does not give the same result. If we only use numbers between 90 and 100, for instance, 2 or 3 draws are enough to estimate the mean. Still, an interesting problem",2010-09-28T07:03:40.353,582,CC BY-SA 2.5,
4046,3089,0,"@shabbychef Sorry, I didn't read your updated answer before posting mine. The bias would increase when the cutoff depart from the mean, but it will be even worse when the departure is assymmetrical wrt. joint density.",2010-09-28T07:43:22.707,930,CC BY-SA 2.5,
4048,3136,5,"@Skarab Maybe I'm totally off, but wouldn't you expect that the frequency of any word will be inversely proportional to its rank in the frequency table of words, according to Zipf's law (http://j.mp/9er2lv)? In this case, check out the `zipfR` package.",2010-09-28T09:29:57.893,930,CC BY-SA 2.5,
4049,3136,1,I agree with @chl - it would be minor miracle if your data was normally distributed. Perhaps another question about what you want to do with the data would be worthwhile. Don't reinvent the wheel!,2010-09-28T09:32:54.513,8,CC BY-SA 2.5,
4052,3129,5,"I agree with you, although most of the sample size calculation that are done when devising RCTs are based on parametric models. I like the bootstrap approach, but it appears that very few studies rely on it. I just found those papers that might be interesting: http://bit.ly/djzzeS, http://bit.ly/atCWz3, and this one goes in the opposite direction http://bit.ly/cwjTHe for health measurement scales.",2010-09-28T11:22:16.953,930,CC BY-SA 2.5,
4059,3140,0,"Can you please make your question a bit more understandable? It seems as you want to use Cross-Validation to make classification itself, but this is nonsense.",2010-09-28T14:50:24.703,,CC BY-SA 2.5,user88
4060,411,0,Thank you for this great overview :) Are there books you know that especially discuss such distance measures for distributions in detail? I am particularly interested in non-parametric measures that operate with minimal assumptions.,2010-09-28T15:03:02.660,608,CC BY-SA 2.5,
4061,3136,3,How could your data be distributed according to a model that gives non zero probability to negative occurrence?,2010-09-28T15:05:49.790,603,CC BY-SA 2.5,
4062,3100,1,"That's not a statistical question, LOL!  But (to stave off possible objections) I agree that its answer can be usefully informed by statistical thinking.  ""Consistency"" depends on what you are studying and what use you will make of a decision that two response curves are ""consistent"" or not.  In some applications it would be enough that they are both increasing or decreasing; in others it would amount to a test of equality of all parameters.  Without more information, one can only guess where along this spectrum your needs fall.",2010-09-28T15:06:45.227,919,CC BY-SA 2.5,
4064,3130,1,It's *binomial* estimation.  (There is no marking or recapturing at all. which leads to hypergeometric estimation.),2010-09-28T15:11:05.110,919,CC BY-SA 2.5,
4065,3136,1,What is the reason for doing this test?,2010-09-28T15:12:36.353,919,CC BY-SA 2.5,
4066,3134,2,"The answer of 25000, which is obviously ridiculous no matter what the true state of affairs may be, indicates that a power calculation is not appropriate here.  The answer of 9228 I obtained guarantees that no matter what the distribution may be (provided only that its values all lie between 1 and 100), 95% of all simple random samples (without replacement) with n=9228 will lie within 2% of the mean.  Power is not relevant!",2010-09-28T15:31:33.217,919,CC BY-SA 2.5,
4067,3123,1,"The asymptotic distribution of the median is Normal with variance inversely proportional to the square of the pdf at the median.  Thus, experimental designs for median estimation have to make some specific assumptions about the nature of the distribution near its middle.  (See http://mathworld.wolfram.com/StatisticalMedian.html for example.)  Usually people study a range of parametric alternatives to compare the efficiency of the median to that of the mean; relative efficiencies translate via the usual square-root law into relative sample sizes.",2010-09-28T15:35:51.293,919,CC BY-SA 2.5,
4069,3089,1,"@chl: not a problem. I like that you posted R code (I am slowly learning R); my version is in homebrew Matlab, which I cannot share. Having code that the OP can test is definitely the way to go.",2010-09-28T16:40:17.440,795,CC BY-SA 2.5,
4070,3145,2,"Huh, factors are well handled using the `factor` command, so no need to rely on dummy coding (otherwise, it's accessible through `model.matrix(lm(x~a*b))` for my toy example).",2010-09-28T18:01:10.887,930,CC BY-SA 2.5,
4071,3145,2,"@chl thanks. I know very little R, but suspected this simple algebraic trick could be applied in any case, or at least illustrate what is going on 'under the hood'",2010-09-28T18:05:48.227,795,CC BY-SA 2.5,
4072,3145,0,"Yes, definitely! this was just a precision. (and you have my +1)",2010-09-28T18:14:52.873,930,CC BY-SA 2.5,
4073,3144,0,"Thanks! It did work, question: it did not force the condition that coefficients sum up to 0, but minimized the sums for both factors. Is there a way to force the ==0 condition, or it simply does not do that since it would affect the error ?",2010-09-28T19:26:09.390,1439,CC BY-SA 2.5,
4074,3148,0,"I've performed a Chi-Squared test, but that was based on the subjective results that I received from user surveys. They sound quite similar though?",2010-09-28T19:37:35.640,1441,CC BY-SA 2.5,
4075,3148,0,"Yes, you could do a chi-squared test as well if that is familiar to you. Both the test I recommended and the chi-squared test will give you the same answer.",2010-09-28T19:43:23.337,,CC BY-SA 2.5,user28
4076,3066,0,"@suncoolsu: I am dealing with a similar problem, but I want to ""cluster"" the individuals that have the same ""dynamics"" (actually I have a huge number of timeseries per regions and I want to model them). I was thinking to use pam with the correlation distance (1-rho). Is this a recommended way? Could you please suggest some paths to explore?",2010-09-28T19:59:12.917,1443,CC BY-SA 2.5,
4077,3125,0,I think this is what I was looking for nico. The numbers arent put back into the pool.,2010-09-28T20:05:38.950,,CC BY-SA 2.5,Greg
4078,3148,0,Well I'd rather use a different method because I have already implemented the Chi-Squared test on other results..I'm just not sure if this test is too similar to the Chi-Squared one or not..?,2010-09-28T20:34:14.790,1441,CC BY-SA 2.5,
4079,3125,0,"by the way, how can I copy the formula and change the numbers?",2010-09-28T20:44:53.360,,CC BY-SA 2.5,Greg
4080,3143,3,Please **do not crosspost** simultaneously here and on SO.,2010-09-28T20:47:17.103,334,CC BY-SA 2.5,
4082,3101,0,"Hi ars, it could well be the triangle of evidence. The reason for the word pyramid is probably historical, I am simply using it as it was taught to me.  BTW, it shouldn't be a leader, since the width of the triangle also reflects the abundance of such an evidence in practice (Or at least, that's my guess :) )",2010-09-28T21:09:48.540,253,CC BY-SA 2.5,
4083,3125,0,"@Greg, I am not completely sure what you are asking... if you want to change the numbers just change the `numbers` variable. For instance `numbers <- c(1, 5, 18, 32, 36, 42, 95, 97)` or whatever you want to use :) The rest of the script will run for any set of numbers.",2010-09-28T21:10:06.900,582,CC BY-SA 2.5,
4084,3144,1,@Vytautas: I think you're forgetting the reference level for your factor which is not shown (since it's the baseline for the others).,2010-09-28T21:14:14.897,251,CC BY-SA 2.5,
4085,3125,0,"@nico, Im asking how I can run the script for myself. Like is there a website I can cut and paste it to? I tried on here but it didnt display a graph.",2010-09-28T21:17:02.977,,CC BY-SA 2.5,Greg
4086,3101,1,"I hadn't come across this before, so just curious for any insight into what it was saying.  Your point about abundance of evidence makes sense.  Thanks.  :)",2010-09-28T21:32:23.253,251,CC BY-SA 2.5,
4087,3144,1,"@Vytautas or take a look at *Interpreting model matrix columns when using contr.sum*, http://j.mp/9pNFQe.",2010-09-28T21:43:49.100,930,CC BY-SA 2.5,
4093,1931,2,"@Kwak: A continuous mapping from R^n to R cannot induce a total order.  Technically, I was wrong, because any bijection from R^n to R will ""order"" R^n via the order on R, but this order will not be compatible with any of the metric structure on R^n (which is an essential part of the concept of a ""median""), and that was the spirit of my comment.",2010-09-28T22:31:51.093,919,CC BY-SA 2.5,
4097,3156,0,"What's p?  what's h?  Start at the beginning and just say what you're measuring, why the distributions are different, and what you want to accomplish.",2010-09-28T23:28:48.340,601,CC BY-SA 2.5,
4098,3156,0,Perhaps if I guess some things it would help??  Are the p's the probability of the mean showing up in each distribution?  You want to say what one is farthest fromthe mean?... closest to it?,2010-09-28T23:46:38.250,601,CC BY-SA 2.5,
4100,3156,0,@John: This is my question (http://stats.stackexchange.com/questions/2639/how-to-compare-different-distributions-with-reference-truth-value-in-matlab) where I have defined my problem about what I want to accomplish. Thanks.,2010-09-28T23:55:29.797,,CC BY-SA 2.5,user1102
4101,3066,0,"@Musa .. Can you be bit clearer. I don't think I understand the ""dynamics"" mentioned by you. Definitely _pam_ is OK for clustering. But you can also try the R packages pvclust and hopach as mentioned by me. Also, SOM (self organizing maps) are a different way of looking at clustering. Please see Ripley and Venable (2002) book - MASS for further details. The book offers a thorough treatment of clustering.",2010-09-29T00:26:35.203,1307,CC BY-SA 2.5,
4103,3148,0,@Mark The two tests give you the same information in terms of what you would conclude regarding the effectiveness of method 1 vs method 2 but the way they approach the issue is different. Any particular reason to avoid the chi-square? Feel free to choose either one.,2010-09-29T01:28:14.663,,CC BY-SA 2.5,user28
4104,3134,0,"Conceded whuber. I'll modify the answer to reflect this, although the question appears to have changed in the interim. BTW it's ""with replacement"".",2010-09-29T02:31:20.320,521,CC BY-SA 2.5,
4105,3100,1,"@whuber Thanks. I see the translation of a theoretical question into a statistical question as one of the most important skills that a data analyst can acquire. In most areas of statistics, there are multiple ways of making the translation, with a body of knowledge existing on when and why you would apply one approach over another. Thus, I'm interested in knowing whether any standard methods exist for quantifying consistency of two fitted curves and when and why you would apply one approach over another. I'll have a think about how I can edit the question to make my specific aims clearer.",2010-09-29T04:14:33.540,183,CC BY-SA 2.5,
4106,1013,0,you mean the standard deviation...,2010-09-29T04:16:36.990,795,CC BY-SA 2.5,
4108,3066,0,"@suncoolsu: Sorry for the poor formulation! I have 200 timeseries that I want to model (i.e. to simulate). I think that I can cluster ""similar"" (i.e. having the same behavior over time: the straight forward approach is to use the correlation) timeseries and simulate only the cluster timeseries...",2010-09-29T06:57:13.590,1443,CC BY-SA 2.5,
4110,3159,0,@grautur I'd like to add this one too: http://mkweb.bcgsc.ca/circos/tableviewer/,2010-09-29T07:26:35.843,930,CC BY-SA 2.5,
4112,3136,0,I want to estimate if the huge result of the Information Extraction is  correct. I want to check if the distribution of the entities found in the text follows my expectations (I know the domain and the text corpus).,2010-09-29T09:06:59.363,1389,CC BY-SA 2.5,
4113,3066,0,"@Musa as you say that your data is a time series, the first step should be removing all the trends from the data .. as you don't mention the size and type of your data (ie size -- moderate, large etc and type -- count data, continous data?), I will assume your data is moderate sized and continuous. There are better methods than clustering if you just want to model the time series. Please check out the (one of the many) time series package in R named zoo. There are great books: http://bit.ly/dmsWtl and http://amzn.to/9DmG6b to accomplish this task. Clustering may not be v.informative in ur case",2010-09-29T09:37:01.803,1307,CC BY-SA 2.5,
4116,3125,0,"@Greg: Ah, OK! Well, you need to install R, you can download it from http://cran.r-project.org",2010-09-29T11:24:11.803,582,CC BY-SA 2.5,
4130,3165,0,"In your Q2, do you mean `cex.lab` instead of `cex.axis`?",2010-09-29T12:43:17.143,1390,CC BY-SA 2.5,
4131,3166,0,"It won't fit text labels to the maximum available space in the margins (which is what the OP ask for, to my understanding).",2010-09-29T12:54:22.453,930,CC BY-SA 2.5,
4132,3166,2,"@chl; my opening line says I can't think of an answer to Q1, but Q2 *can* be done via my example, substituting in the maximum values for cex.lab in place of where I have `2` and `3` now. That was just an example of how to label the plot separately and never claimed to answer Q1. Maybe I misread/misinterpretted what Tal wanted with his Q2 though? Your reading of Q2's meaning seems obvious if Q1 is TRUE, hence I thought Tal was aksing the Q2 I answered...?",2010-09-29T13:11:01.580,1390,CC BY-SA 2.5,
4134,3168,0,Ouaouh! You're just summarizing in a few words a lot of R good practices!,2010-09-29T13:49:11.140,930,CC BY-SA 2.5,
4136,3165,0,"Why would you want to do this? Surely when you include your graph within another document, the text sizes will now look too big.",2010-09-29T14:18:30.887,8,CC BY-SA 2.5,
4137,3170,0,That's not working for me (you probably need to edit out the `cex.lab = mycex` part of the call to `plot()` to get that example to run) as the y-axis label is half outside the plot window and the x-axis label extends across the axis and into the plot region.,2010-09-29T14:21:44.340,1390,CC BY-SA 2.5,
4138,3134,0,Thanks about the BTW.  I mis-typed and intended to say *with* replacement.,2010-09-29T14:33:06.620,919,CC BY-SA 2.5,
4139,1013,0,@shabbychef: You are right. Fixed.,2010-09-29T14:35:05.990,56,CC BY-SA 2.5,
4140,411,0,"@Ampleforth: I'm not familiar with any one source that discusses many of these distances in general, and what I have seen is very much from a mathematician's and not a statistician's perspective (hence my question).  I just happened to see a reference to a book *Probability for Statisticians* by Shorack which apparently discusses many such distances.",2010-09-29T14:46:53.530,89,CC BY-SA 2.5,
4141,3169,0,"Yes, I am aware of TeXexample.net. Besides showing all fancy features, I'm a bit disappointed in the number of 'normal' line plots they have (showing different styles)",2010-09-29T15:02:22.787,190,CC BY-SA 2.5,
4142,3168,0,"This is not completely what I was looking for but thanks! The command in point 2 is very useful, only why use a bitmap format (png) when the result is pdf? Isn't it way better to have svg, pdf or pgf output?",2010-09-29T15:03:59.653,190,CC BY-SA 2.5,
4144,3168,0,@Peter See my updated response.,2010-09-29T15:24:17.380,5,CC BY-SA 2.5,
4146,3157,2,"@Harpreet When constructing confidence intervals, say a 95% CI for a difference of means, you are explicitely assuming a sampling distribution (t or z, i.e. that of your test statistic under $H_0$).",2010-09-29T16:28:10.380,930,CC BY-SA 2.5,
4152,3165,2,"This is really an R-code question, not a stats question.",2010-09-29T19:18:32.653,601,CC BY-SA 2.5,
4153,3064,0,"Is your dependent variable continuous or discrete? Or, perhaps, the underlying test result is continuous and it is converted into a discrete answer (i.e., 'positive', 'negative') depending on some cut-off? Could you also clarify why an individual would flip from negative to positive despite not being exposed to TB? A specific example (with some numbers thrown in) of such a flip may help.",2010-09-29T19:21:19.027,,CC BY-SA 2.5,user28
4154,2611,0,"A very interesting question. I look forward to hearing from your results, if you want to share them...",2010-09-29T19:25:24.387,930,CC BY-SA 2.5,
4157,3171,0,"I am not sure I see what the issue is. Don't you associate the responses of each respondent over time with some sort of dummy id (to preserve anonymity)? If so, you just have a repeated measures design with missing data. The missing data would be for those respondents who responded to a survey in the past but do not do respond to a later survey. Whether a respondent's answer changes or does not change over repeated measurements is not a data or sampling artifact but an indication of the stability or unstability of response for that individual.",2010-09-29T19:57:21.750,,CC BY-SA 2.5,user28
4158,3171,0,"Thanks Srikant. I would agree in theory, but in practice the physical well-being of the subjects depends on their anonymity and we can't risk anything that might identify them from their answers. I expect there probably exists a robust system to solve this, but we're not security experts and neither can we afford one.",2010-09-29T20:17:01.260,1343,CC BY-SA 2.5,
4160,3169,0,"Nice that you cited TeXexample.net! Maybe too much for that purpose, anyway `tikz` is very powerful for illustrations and diagram.",2010-09-29T20:38:03.697,930,CC BY-SA 2.5,
4162,3176,0,why would you want to do that ?,2010-09-29T22:36:49.400,603,CC BY-SA 2.5,
4163,3171,0,"So, as I understand it, you have no way to tell which response belongs to which respondent as you do not record any id consistently across respondents. Is that correct? I would imagine that you do know the sample size of each poll though, right?",2010-09-29T22:48:06.247,,CC BY-SA 2.5,user28
4164,3176,2,"General remark -- try to give questions reasonable, informative titles and make them clear and understandable for people not working in your field. Also good question has a bigger chance for a satisfying answer.",2010-09-29T23:35:58.257,,CC BY-SA 2.5,user88
4165,3064,1,"The examples are really helpful to visualize the data. Another question regarding your caveat: ""the values clump at the floor and the ceiling and that the data are not normal."" Can you tell me if (a) the data on the lower end of the scale look normal and (b) the data on the upper end of the scale look normal?",2010-09-30T00:23:22.700,,CC BY-SA 2.5,user28
4166,3178,6,"Thank you for providing this comment!  But unless you can quantify the amount of skewness, that fact by itself is not very useful.  Plenty of distributional families are skewed but have practical normal approximations, such as the Chi-square (Gamma) and Poisson, and plenty more can be strongly skewed but rendered close to (or exactly) Normal through a simple re-expression of the variable, such as the Lognormal.  Could you perhaps amplify your answer to explain how the knowledge of the skewness could be used to estimate p-values from reported ORs?",2010-09-30T03:57:26.437,919,CC BY-SA 2.5,
4167,3179,0,"""clear, polished, and most importantly, succinct manner"" Sounds like ggplot2 to me.",2010-09-30T04:49:08.917,776,CC BY-SA 2.5,
4168,3179,1,"I'm looking not just for R libraries, but also any specific types of graphs. My knowledge of graphs is limited to scatter, box, qq, histograms, violins, kernel density estimations, etc. Any slightly more obscure graphs that can reveal more about the data than those would be fantastic.",2010-09-30T04:52:27.967,1118,CC BY-SA 2.5,
4170,3159,0,"@chl:  very handy, I hadn't seen it.  Thanks for pointing it out!",2010-09-30T06:08:01.957,251,CC BY-SA 2.5,
4171,3180,0,Does this study involve only one disorder (with low prevalence as I understand) or are there multiple diagnoses assessed by multiple indicators?,2010-09-30T06:18:42.450,930,CC BY-SA 2.5,
4172,3182,1,"@ars Let's add Hadley's R tools to enhance GGobi experience, e.g. `DescribeDisplay` and `clusterfly`.",2010-09-30T06:28:38.703,930,CC BY-SA 2.5,
4173,3180,0,@chl Just one disorder,2010-09-30T06:43:50.650,183,CC BY-SA 2.5,
4174,3180,0,Do the scales overlap to some extent (i.e. shared constructs across the questionnaires)?,2010-09-30T06:45:31.350,930,CC BY-SA 2.5,
4175,3180,0,@chl they correlate but they are conceptually distinct; I've updated the question a little bit to reflect your two queries.,2010-09-30T06:49:26.497,183,CC BY-SA 2.5,
4176,3182,0,"Hi ars, as I wrote on my answer - my experience with ggobi is that it doesn't handle large datasets well.  Do you have another experience with that ?",2010-09-30T08:09:00.373,253,CC BY-SA 2.5,
4177,3179,2,parallel coordinates sounds like another one to mention.  Also methods of dimensionality reduction might be helpful.,2010-09-30T08:09:48.437,253,CC BY-SA 2.5,
4179,3170,0,Nice answer John.  Why do you think this is not a good idea ?,2010-09-30T08:27:20.857,253,CC BY-SA 2.5,
4180,3165,0,"John - you are right. But when I get to these, it is always borderline for me if to put them here or stackoverflow.  Thanks for the answer either way.",2010-09-30T08:28:16.347,253,CC BY-SA 2.5,
4181,3182,0,"@Tal The problem comes from not relying on glyph for screen display/rendering, which is common to R base graphics. This was discussed at the latest DSC conference (http://j.mp/bpOhBH). Actually, there is an ongoing project with Qt as a backend, and a new port of GGobi, to enhance interactive display of large data sets.",2010-09-30T08:37:56.540,930,CC BY-SA 2.5,
4182,3188,0,Thanks for the links about `rflowcyt` and Acinonyx.,2010-09-30T08:43:11.107,930,CC BY-SA 2.5,
4183,3188,0,"BTW, `rflowcyt` has been deprecated with recent releases of Bioconductor, it is now recommended to use `flowViz`. Anyway, both rely on `lattice`.",2010-09-30T08:53:19.970,930,CC BY-SA 2.5,
4184,3066,0,"@suncoolsu another formulation: what I have is some panel data with very short time series (9 points for each of them). I want to model them, but not all of them (I will do a Monte Carlo simulation and do not want to simulate 200 time series). I was thinking that by clustering I can reduce the number of time series to simulate to manageable order...",2010-09-30T08:58:11.463,1443,CC BY-SA 2.5,
4186,3170,0,the sizes on each side could vary enough that that would be distracting.  I think a good idea could be made of it by simply combining the min() of each separate axis to the min of all of the variables.  But that isn't what you asked for.  It could then be wrapped into a nice function you use frequently called getMaxLabelCEX(),2010-09-30T10:28:55.807,601,CC BY-SA 2.5,
4187,3156,0,None of this answers why you're comparing these distributions to a mean and why simple analysis of the confidence interval could possibly help.  You don't compare a distribution to a mean just because it has some numeric property.  That's why you can't get an answer for your question.,2010-09-30T10:43:28.977,601,CC BY-SA 2.5,
4188,3066,0,"@musa .. is there a specific reason that you don't want to simulate 200 time series? Using hierarchical modeling (using WinBUGS, OpenBUGS or JAGS), you can easily model such data sets. It will be definitely better than clustering and then proceeding to inference (and/or simulation).",2010-09-30T11:17:44.233,1307,CC BY-SA 2.5,
4189,3066,0,"@suncoolsu I have some time constraints (I am working) and want to have a quick solution. At a later stage I am also thinking to do a complete inference/simulation. My goal is to be able to forecast from the model. I have 2 questions:1)is it reasonable to proceed, for now, with clustering+inference or should I concentrate on some other models e.g. random effects model? 2)assuming that I want to model the whole time series what is the best way of doing it, could please give me a reference that I can quickly access?",2010-09-30T12:15:19.520,1443,CC BY-SA 2.5,
4190,3066,0,"I am looking, as a first try, for a solution where the simulation is not needed or it should run very quickly (less than 5 minutes)...",2010-09-30T12:41:46.947,1443,CC BY-SA 2.5,
4192,3182,0,"chl, it's great to know.  After seeing the latest release was on 2008: http://www.ggobi.org/downloads/ I thought this project was dormant, but looking at their blog I see they are having some activity - http://ggobi.blogspot.com/  I hope they'll grow even more.  Best, Tal",2010-09-30T13:53:58.280,253,CC BY-SA 2.5,
4193,3198,0,"All the answers are similar, but slightly different. I don't think it really matters.",2010-09-30T13:58:47.103,8,CC BY-SA 2.5,
4194,3195,0,Point taken about CW! I made it that way because stats is not my field so I assumed my question would require some correction or extension. Will consider this more carefully next time. Thanks!,2010-09-30T14:08:42.453,1343,CC BY-SA 2.5,
4195,3199,0,2x2 matrix's upper left corner is only one element... Can you reformulate the question?,2010-09-30T14:11:44.297,,CC BY-SA 2.5,user88
4196,3199,0,"@mbq: I tried to reformulate, let me know if it's still unclear (high values and low values refer to block matrices inside the big matrix)",2010-09-30T14:17:40.323,900,CC BY-SA 2.5,
4197,3171,0,"@Srikant, more-or-less correct; individuals that conduct the polling *may* remember who said what, but there is no physical record between response and respondent. Polling is more continuous than punctual: roughly 6 interviews are conducted per week though this can vary wildly as external factors that disrupt access to the group prohibit a more consistent sampling. We have 3 years of data. Since I arrived I've been enforcing putting dates on the results, though many old records are un-dated.",2010-09-30T14:22:05.467,1343,CC BY-SA 2.5,
4198,3199,1,"Much better now, thanks. Still, the more details you'll put here (Are those clusters sharp or smooth? Is it all noisy? How large should be those clusters?) the more useful answer you get.",2010-09-30T14:27:10.220,,CC BY-SA 2.5,user88
4199,3191,0,"Great job chl!  Would it be o.k. by you if I where to publish this on my blog? (I mean, this text is cc, so I could, but I wanted you permission any way :) )
Cheers,
Tal",2010-09-30T14:49:02.897,253,CC BY-SA 2.5,
4200,3191,0,"@Tal No problem. It's far from being an exhaustive list, but maybe you can aggregate other useful links at a later time. Also, feel free to adapt or reorganize in a better way.",2010-09-30T15:07:36.027,930,CC BY-SA 2.5,
4201,3183,0,"Thanks for the two novel libraries. My main conflict with these two is that I am submitting my report via paper copy, so interactive graphics might not be fully used. The graphics of Mondrian look pretty complex. I'll give it a look-see.",2010-09-30T15:20:13.930,1118,CC BY-SA 2.5,
4202,3183,0,"@Christopher For Mondrian, you have the ""equivalent"" R version through `iplots` cited by @Tal. About Paraview, you have the option to save a screenshot of your viz. `DescribeDisplay` is the way to go for exporting dynamic visualization from GGobi, http://cran.r-project.org/web/packages/DescribeDisplay/index.html.",2010-09-30T15:27:23.300,930,CC BY-SA 2.5,
4203,3191,0,"+1 This is a nice list.  You might consider ""accepting this"" so that it's always on top; given that it's CW, anyone can keep it updated.",2010-09-30T15:34:48.407,5,CC BY-SA 2.5,
4204,3201,0,It can't be done unless you come to some objective criterion; probably most of possible ratings can be constructed with some combination of your parameters.,2010-09-30T15:39:21.887,,CC BY-SA 2.5,user88
4205,3191,0,"@Shane Well, I am indebted to you for providing a first answer with so useful links. Feel free to add/modify the way you want.",2010-09-30T15:45:02.120,930,CC BY-SA 2.5,
4206,3205,0,"(+1) The 1st display really conveys a lot of information, and it will allow to see clusters (wrt. time or individual), if any.",2010-09-30T15:51:42.530,930,CC BY-SA 2.5,
4207,3205,0,"@chl Thanks.  But I think it may illustrate some of the problems to which the OP is reacting: it does not clearly show the extent to which units may be active for less than one day.  It could be improved in this regard (e.g., by coloring all streaks less than a day long), but it still does not lend itself well to gauging the extent of the problem.  What it does provide is the ability to distinguish one unit from another.  Displaying the two together on a common time scale suggests that if one single graphic won't do, maybe a collection of related graphics will.",2010-09-30T15:57:10.147,919,CC BY-SA 2.5,
4208,3191,0,I republished it here.  Great list!  http://www.r-statistics.com/2010/09/managing-a-statistical-analysis-project-guidelines-and-best-practices/,2010-09-30T16:03:47.190,253,CC BY-SA 2.5,
4209,3066,0,"@ musa .. in genomics there is a specific modeling strategy for ""time course experiments"". You can use it to get to see if the modeling strategy fits your data. Genes correspond to pages and individuals correspond to patients. I also listed 2 references in my previous comment.",2010-09-30T16:18:15.263,1307,CC BY-SA 2.5,
4210,3209,1,"This looks nice, but won't it just pick highest-variance attributes and biggest clusters of cross-correlated ones?",2010-09-30T16:21:38.610,,CC BY-SA 2.5,user88
4211,3196,5,"+1 for the reference: it's a lengthy treatise on practical die testing.  Halfway down the author suggests using a KS test and then goes into ways to identify specific forms of deviation from fairness.  He's also well aware that chi-square is an approximation for small numbers of rolls per face (e.g., for 100 rolls of a 20-sided die), that power varies, etc., etc.  In short, anything the OP might like to know is clearly laid out.",2010-09-30T16:25:31.363,919,CC BY-SA 2.5,
4212,3188,0,"Very thorough answer, Tal! Plot generation time should not be a huge issue. I've been doing most of my graphs with the base package, and the issue of having the graphs look nicer was for when I decide to use a graph for the paper. I had considered using a scatterplot matrix for the numerical variables, but since many of them are of different units (some are in dollars, others in sqft), the only valuable information I'd get would be general trends, but with ~8 numeric variables, an 8x8 SPM is a bit cluttered.",2010-09-30T16:49:58.013,1118,CC BY-SA 2.5,
4213,3211,0,"@Gaetan Well, for PCA you have to find a suitable numerical coding for variable such as ""textual content""...",2010-09-30T17:02:17.817,930,CC BY-SA 2.5,
4214,3148,0,"I want to avoid it because I have already used a chi-squared test for another part of the study. However, I don't know if I'm being stupid by saying that. I guess it wouldn't really matter how many times I used a test within a paper.",2010-09-30T17:08:41.457,1441,CC BY-SA 2.5,
4215,3209,1,"Alternatively, one can perform *multiple correspondence analysis* or *multiple factor analysis* for mixed data (if numerical recoding happens to be not realistic for some variables), and the rest of your idea (computing factor scores and looking at variable loadings on the 1st dimension) applies as well.",2010-09-30T17:16:08.697,930,CC BY-SA 2.5,
4216,3182,1,"@Tal: My experience is that it's quite slow when refreshing/repainting the views, e.g. when adding a variable or dragging to rearrange displays in the PCP.  Still, it is usable though not as interactive as advertised with large data.  @chl: That's really good to know, thanks!",2010-09-30T17:16:45.277,251,CC BY-SA 2.5,
4217,3182,1,@ars @Tal Here are the links on Qt interface for R (http://j.mp/d1AJp7) and GGobi (http://j.mp/cUOvfp). See also Hadley's Github repository!,2010-09-30T17:37:19.883,930,CC BY-SA 2.5,
4219,3212,2,str(variable) is your best friend.,2010-09-30T18:21:15.193,776,CC BY-SA 2.5,
4221,3211,0,"That's not the issue I am raising.  PCA can handle dummy variables as you suggest.  PCA is incredibly powerful and flexible that way.  But, it is the interpretation of the principal components that gets really challenging.  Let's say the first principal component starts like this: 0.02 years of experience - 0.4 textual content of reviews + 0.01 associations...  Maybe you can explain it.  An expert performance is proportional to years of experience, but inversely proportional to textual content of reviews?  It seems absurd.  But, PCA often does generate counter-intuitive results.",2010-09-30T18:42:19.873,1329,CC BY-SA 2.5,
4222,3214,0,Can you provide an example of the outcomes you consider in your experiments? What is your sample size (in each group)?,2010-09-30T18:43:03.467,930,CC BY-SA 2.5,
4223,3211,0,"@Gaetan Still, I reiterate my opinion that the problem lies in how you choose to represent your variables (or how you find a useful metric). I agree with you about the difficulty of interpreting a linear combination of variables when dealing with non-continuous measurements or a mix of data types. This is why I suggested in another comment to look for alternative factorial methods. Anyway, developing scoring rules based on user preferences or expert reviewing (as is done in clinical assessment) also calls for some kind of statistical validation (at least to ensure scores reliability).",2010-09-30T19:01:03.413,930,CC BY-SA 2.5,
4225,3206,0,"Thanks! If you do not have access to the original data but only a table of regression coefficients, is the Bonferroni adjustment your only choice?",2010-09-30T19:08:18.570,1458,CC BY-SA 2.5,
4226,3206,4,"Presumably you also have the p-values :-).  But with only those and the coefficients, it's hard to imagine what else you might do besides a Bonferroni adjustment.  (I always make such an adjustment whenever reading any paper with multiple tests: it's a quick way to winnow out the results that are likely to be junk.)  Most people also provide summary statistics for the variables: you can use ranges or sds along with the coefficients to estimate how much effect each explanatory variable might have on the predictand.",2010-09-30T19:17:13.900,919,CC BY-SA 2.5,
4227,3206,0,"Thanks for your explanation, esp. on cross-validation. I appreciate your last argument, i.e. that we also have to look for theoretical relevance (beyond p-values).",2010-09-30T19:20:13.547,930,CC BY-SA 2.5,
4228,3215,2,"Could you confirm that (a) d1 and d2 are the side lengths (and not angles); (b) that you are assuming the angle between them is a right angle (for otherwise the atan formula is suspect); and (c) that you are interested in the distribution of one of the other angles of this right triangle?  Also, presumably, the SD of each length distribution is much smaller than its expectation because the triangle shouldn't have any appreciable probability of a negative side length :-).",2010-09-30T19:25:25.873,919,CC BY-SA 2.5,
4229,3215,0,"Exact. I've rephrased the problem to make it a bit clearer. And yes, the SD will be small relative to the dimensions.",2010-09-30T19:47:32.383,77,CC BY-SA 2.5,
4230,3215,0,"Using formulas for multiplication and addition, you can try Taylor expansion.",2010-09-30T20:04:10.813,,CC BY-SA 2.5,user88
4231,3211,0,"@Gaetan, Yes some of your comments make a lot of sense, and you're right in saying that it is not merely a statistical exercise but involves elements that are more subjective. The reason being that the intent from a user/customers standpoint might differ. Assuming he's doing a search for an expert, then i just add filters to allow him to select experts >X number of years of experience and so on But let's say he's narrowed down to 2 experts, and wants an independent comparison. So i'm just looking for a generic method to compare any two experts.",2010-09-30T20:19:52.127,1459,CC BY-SA 2.5,
4232,3211,0,"@Gaetan, So yes i'm assuming i can somehow quantify all of the attributes, which again might be subjective. But anyhow i see some interesting points in this discussion and will think some more on that before commenting further.",2010-09-30T20:26:10.080,1459,CC BY-SA 2.5,
4233,3211,0,"@chl numerical coding for ""textual content"", could be as simple as some sort of sentiment analysis and giving them a binary value of good/bad?",2010-09-30T20:30:33.727,1459,CC BY-SA 2.5,
4234,3211,0,"@Sidmitra, your comments make sense.  Maybe you will derive good use out of PCA after all.  I would suggest you may run PCA several times by deriving the simplest and most explainable models that still explains most of the variance between experts.  By doing so, hopefully you may end up with a PCA model with principal components that are readily explainable to others and to yourself.",2010-09-30T20:43:29.537,1329,CC BY-SA 2.5,
4235,3217,0,"Great idea! I have latitudes and longitudes of all datapoints already, so such a task would be relatively elementary. I was thinking the maps library would be a good way to go, unless there's something better.",2010-09-30T20:43:39.287,1118,CC BY-SA 2.5,
4236,3211,2,"+1 for pointing out this is not a statistical exercise.  At best, PCA can describe relationships within a particular data set and, conceivably, simplify the data by identifying near-collinearities.  It is not apparent how it can inform us about how to *rank* the experts.",2010-09-30T21:03:53.277,919,CC BY-SA 2.5,
4237,3209,3,"It seems to me the first component will merely point out a strong direction of commonality among the experts.  How could it possibly tell us who is better and who is worse, though?  That requires additional information concerning the relationships between these variables and the quality of being a ""good"" or ""bad"" expert.  If we believe all the variables are monotonically associated with goodness or badness, then perhaps PCA can help us explore the frontier of extreme (or maybe just outlying!) experts.  Watch out though--even the monotonicity assumption is suspect.",2010-09-30T21:09:03.570,919,CC BY-SA 2.5,
4238,3217,2,"@Christopher You can also do this with `ggplot2` (esp. if you don't need to draw country boundaries), http://had.co.nz/ggplot2/coord_map.html. Otherwise, `maps`, `gmaps` are better. There's also `GeoXp` and an R interface to GRASS. BTW, Mondrian has a plugin for geographical data :)",2010-09-30T21:12:38.490,930,CC BY-SA 2.5,
4239,3209,1,"@whuber I see the point, thanks. Maybe you could add this in your own response (which is very welcomed)?",2010-09-30T21:41:02.990,930,CC BY-SA 2.5,
4240,3216,1,"There was never any chance of it being normally distributed.  It is an angle!  It only takes values on $[-\pi, \pi)$.",2010-10-01T00:06:22.883,352,CC BY-SA 2.5,
4242,3224,0,"Oh neat, I didn't know plot could do that.",2010-10-01T00:31:50.840,251,CC BY-SA 2.5,
4243,3224,1,"Thanks! In examples like plot(sin, -pi, 2*pi), is there a default spacing for sampling the range in domain?",2010-10-01T01:16:11.087,1005,CC BY-SA 2.5,
4244,3224,1,The default is 101 points. See help(plot.function).,2010-10-01T01:21:02.630,159,CC BY-SA 2.5,
4245,3216,1,P(Y/X $\le$ q) = P(Y $\le$ qX) is not correct if X is a normal r.v. - X *can* be negative too.,2010-10-01T02:38:42.110,1112,CC BY-SA 2.5,
4246,3216,1,"@ronaf: actually, since $X$ and $Y$ are the side lengths of a physical triangle, we should *not* have negative $X$!",2010-10-01T04:26:34.060,795,CC BY-SA 2.5,
4247,3216,0,"d'accordo shabbychef! - but you can't eat your cake and have it. if X and Y are normal r.v.s. they will be negative occasionally. perhaps one solution is to think of the triangle as pointing down into the third quadrant if X > 0 but Y < 0 [etc.]  in any case, one can still consider the behavior of arctan(X/Y) - which will then range in ($-\frac{\pi}{2}, \frac{\pi}{2}$).",2010-10-01T04:49:12.223,1112,CC BY-SA 2.5,
4250,3170,0,"Thanks John, that makes sense - I'll wrap that later and possibly republish it (with credit).  Thanks again :)    Tal",2010-10-01T08:27:29.527,253,CC BY-SA 2.5,
4252,3202,0,"I second chl's recommendation to use some kind of penalized regression (e.g., the Lasso).",2010-10-01T09:28:34.730,1352,CC BY-SA 2.5,
4253,3202,11,"@chl: I'm unhappy with recommending stepwise predictor selection. Usually, this is based on p-values (""exclude a predictor with p>.15, include it if p<.05"") and leads to biased estimates and bad predictive performance (Whittingham et al., 2006, Why do we still use stepwise modelling in ecology and behaviour? J Anim Ecol, 75, 1182-1189). However, AIC-based stepwise approaches have the same weakness - Frank Harrell discussed this in a post to R-help on Mon, 09 Aug 2010 16:34:19 -0500 (CDT) in the thread ""Logistic Regression in R (SAS -like output)"".",2010-10-01T09:40:13.540,1352,CC BY-SA 2.5,
4255,3202,1,"@Stephan +1 Thanks for this. I know Frank Harrell's point of view. His book is a ""salvation"" for biostatistics, as well as the more recent one by EW Steyerberg. This is why I referred to his work at the end of my response, assuming the interested reader would look for a more thorough explanation. My initial thought was just to point to different ways of approaching the problem. As whuber nicely pointed it out, there's room for improvement in my response, esp. with cross-validation and conceptual issues around predictive modeling.",2010-10-01T09:49:30.420,930,CC BY-SA 2.5,
4256,3202,1,"@Stephan Now the problem is that often we want to keep some variables in our model (whatever their p-values), as @whuber said; this is why I suggested to look at the `penalized` package from J Goeman, because it allows to penalize only a subset of the covariates. From my experience, I found results from Lasso/Ridge regression a little bit difficult to explain to an external audience, esp. when data don't really called for shrinkage/regularization (e.g. ""ideal"" case with 500 subjects and 10 variables, no collinearity or mediation issues). So when we can keep things simple without overfitting...",2010-10-01T10:07:33.010,930,CC BY-SA 2.5,
4257,3230,0,"I was thinking of confidence intervals, but the question is not clear to me because there seems to be different outcomes each time (but may I don't understand the question) which would prevent from pooling anything at all.",2010-10-01T10:56:55.520,930,CC BY-SA 2.5,
4258,2818,0,"@Thylacoleo Thanks for the links (esp. the genetic related one). So, what Stata command would you recommend?",2010-10-01T11:07:58.510,930,CC BY-SA 2.5,
4259,2818,0,The question concerns Cox regression. I'll post an expmple (above) provided with Stata 11.,2010-10-01T12:37:53.337,521,CC BY-SA 2.5,
4260,2818,0,"@Thylacoleo Ok, this was just to be sure that the `stcox` command was the correct approach.",2010-10-01T12:39:56.313,930,CC BY-SA 2.5,
4261,2749,1,Also as a note to the poster you can not estimate the shared frailty in SPSS as in Thylacoleo's example. You can only include covariates in the Cox regression (either time varying or static).,2010-10-01T14:54:45.633,1036,CC BY-SA 2.5,
4262,2818,0,"@Thylacoleo, do you know if you included fixed effects for all the matched groups (ie dummy variables) would you get the same covariate coefficients when you define shared frailty?",2010-10-01T14:57:50.777,1036,CC BY-SA 2.5,
4263,3239,0,I had come accross cointegration a few years back - but it did seem terribly complicated to me (I didn't understand it!). I was hoping there would be a less theoretical (i.e. more practical) solution ...,2010-10-01T15:08:16.437,1216,CC BY-SA 2.5,
4264,3239,3,"The Engle-Granger method is not especially complicated: you just take the residuals of a regression between the two series and determine if it has a unit root.  This is certainly practical: it's used regularly for a broad spectrum of problems.  That said, I imagine that any answer to your question will require some statistical knowledge (for instance, you should understand things like stationarity, independence, etc.)...",2010-10-01T15:19:35.990,5,CC BY-SA 2.5,
4265,3214,0,I'm a bit confused what the goal is as well. Is there a reason examining a plot of each of the 5 hazard functions (and/or their confidence intervals) is insufficient? Do you need a test statistic to state where the curves intersect? As chl suggested in a comment to Thylacoleo's answer pooling seems inappropriate with different outcomes.,2010-10-01T15:29:21.947,1036,CC BY-SA 2.5,
4266,3216,2,"@ronaf: That's the right idea.  If one uses signed side lengths and also considers the angle as a real value (rather than its value modulo $2\pi$), there is no inconsistency with normality in either case.  Your point about the inequality possibly being wrong is excellent.  All I can do in response is to claim that the equation is an excellent approximation under the assumptions made because the chance of X or Y being negative is negligible.",2010-10-01T16:01:38.573,919,CC BY-SA 2.5,
4267,3239,0,is there a better way to do this than to test all pair-wise series for co-integration (with the same ideal in mind to cluster series together?) Also wouldn't this suggestion be dependent on the fact that the series themselves are integrated at the onset?,2010-10-01T16:34:55.210,1036,CC BY-SA 2.5,
4268,3239,0,"@Andy: I'm sure that there is a better way, and I look forward to hearing about it.  This is a pretty basic approach.",2010-10-01T16:38:29.650,5,CC BY-SA 2.5,
4269,3238,1,"You may also be interested in the responses to this question, http://stats.stackexchange.com/q/2777/1036",2010-10-01T16:39:38.900,1036,CC BY-SA 2.5,
4270,3217,0,"Assigning a best answer can be difficult when there's several great suggestions, but I feel this is the right direction, keeping ""succinct"" in mind. I will give ggplot2 a try, and take a look at maps, GeoXp, and Mondrian. Thanks for the idea of graphing spatially!",2010-10-01T17:34:52.817,1118,CC BY-SA 2.5,
4271,3244,0,Could you link to the spreadsheet Mike Lawrence provided?,2010-10-01T17:43:53.083,1036,CC BY-SA 2.5,
4272,3215,0,"Thanks for both your excellent answers, which (as far as I can tell with my limited stats expertise) are both intuitive and sound.",2010-10-01T18:03:21.920,77,CC BY-SA 2.5,
4274,3246,1,"I second this. There are many examples of something that looked like a power law, but when examined a little more rigorously turned out not to be....and no, the high R^2 on the chart is not enough.",2010-10-01T18:32:38.170,247,CC BY-SA 2.5,
4275,2888,0,"Thanks for the additional link. Still for me the problem is with the small $n$ and the heterogenous predictors. It seems to me that the $n\ll p$ case is now increasingly well-studied in genetics, neuroimaging studies, or when we can assume an exponential relationship between $n$ and $p$, but at the moment I never found any evidence of the relevance or predictive power of boosting in the particular study I presented. I am currently running MC simulations to see how RFs and sparse regression perform in this case. I'll let you know all of any progress in this direction.",2010-10-01T19:08:27.593,930,CC BY-SA 2.5,
4276,3244,0,Here is the URL https://spreadsheets.google.com/ccc?key=0Ap2N_aeyRMGHdHJxUnVNeEl5VGtvY1RVLVc5UjU4Vmc&hl=en#gid=0 It was related to his question http://stats.stackexchange.com/questions/2956/have-i-computed-these-likelihood-ratios-correctly,2010-10-01T19:13:11.320,1329,CC BY-SA 2.5,
4277,2525,3,"Great recommendation, thank you -- I got a copy recently based on this and it really is quite good.",2010-10-01T19:14:28.050,251,CC BY-SA 2.5,
4278,2525,2,I'm glad to hear someone else appreciates this book!,2010-10-01T20:14:08.543,919,CC BY-SA 2.5,
4279,3198,0,Thanks for the answer.  I accepted this because it included all the newbie stuff about p values and rejecting.,2010-10-01T20:22:05.867,1456,CC BY-SA 2.5,
4280,3236,2,"Don't you mean `mean(replicate(N, system.time(f(...))[3]), trim = 0.05)` ?",2010-10-01T20:29:55.107,46,CC BY-SA 2.5,
4282,3248,0,thanks for your feedback.  This is the exact type of comments I was interested in.  You have really leveraged the sharing and importing component of Google docs.  Good for you.  I'll read your material to learn more about it.,2010-10-01T20:43:16.037,1329,CC BY-SA 2.5,
4283,3246,0,"""So you think..."" is an excellent reference.  Points 1-6 (out of 7) directly address the question posed here.",2010-10-01T20:45:26.157,919,CC BY-SA 2.5,
4285,3241,1,"Chris, regular clustering won't cut it. You either have to acknowledge that a series is highly correlated with it's own past by putting each $y_{1,t}$ as a dimension of it's own (i.e. resulting in N*T dimensions) or you trow all the dimensions together, but then (given the high correlation inside a series) you will always end up with a single cluster. Also most clustering methods are ill-suited/advised for highly correlated variables (there is a more or less bending assumption of spherical clusters).",2010-10-01T21:01:15.290,603,CC BY-SA 2.5,
4286,3239,1,"> i can't suggest anything else, but cointegration is both very fragile ('parametric assumptions' gone wild series) in practice and ill suited for the task at hands: at each step, it amounts to doing hierarchical clustering, at most merging two series unto one (the co-integrated mean).",2010-10-01T21:09:58.127,603,CC BY-SA 2.5,
4287,3246,0,"But a power-law *distribution* isn't the same thing as fitting a power law relationship between two separate variables. I'd assumed the question was about the latter, though i'm not certain.",2010-10-01T21:39:59.267,449,CC BY-SA 2.5,
4288,3251,2,"Actually, the arima function in R will not fit your model (1). arima() does regression with ARIMA errors and your equation (1) is an ARMAX model.",2010-10-01T23:56:20.467,159,CC BY-SA 2.5,
4289,3255,0,"Hi Kwak and Rob.  Thanks for looking at this.  I wanted to use exponential smoothing because this is what I'm more familiar with.
I'm thinking that I need to learn on how to use the ARIMA framework.  Could you recommend a good book that would help me to learn enough about the ARIMA framework to apply such a dummy variable approach?

I have Bowerman's ""Forecasting, Time Series, and Regression"" and Levenbach ""Forecasting: Practice and Process for Demand Management"", which I used to learn about exponential smoothing.   I don't know if these go in sufficient detail for what I'd need.
Thanks!",2010-10-02T00:25:27.677,1479,CC BY-SA 2.5,
4291,3246,0,"Non-expert's question: apart from ""robustness"", are there other reasons why one should check goodness-of-fit with Kolmogorov-Smirnov instead of $\chi^2$ in this case?",2010-10-02T02:02:56.740,830,CC BY-SA 2.5,
4293,3246,2,"@J.M.: not really, chi-square is sensitive to binning and tail fluctuations complicate that.  I think even with the KS, they reweigh the statistic for extremal points, and there's some discussion of other tests.  @onestop: I assumed the other way, and on re-reading, you could be right.  I'm not really sure ..",2010-10-02T05:51:33.583,251,CC BY-SA 2.5,
4294,3251,0,"Rob:> i've edited equation one. Can you point to a source where the difference(s) between armax and regression with arima errors are explain (or alternatively provide an intuitive explanation). Also, would you know of a R package that implements ARMAX models ? Thanks in advance.",2010-10-02T07:46:42.523,603,CC BY-SA 2.5,
4296,2818,0,"@Andy W. No as frailty is a the equivalent of a random effect in survival analysis. So the cathetar model is equivalent to a mixed model with both fixed and random coefficients. To show this I'll add the dummy variable example above. Perhaps it could be interpreted as an ""order effect"" - second infection versus first.",2010-10-02T08:11:37.257,521,CC BY-SA 2.5,
4297,3248,0,"Dear Gaetan, I am delighted by your response - thank you for the kind words.  Best,  Tal.",2010-10-02T08:17:52.557,253,CC BY-SA 2.5,
4300,3228,3,"You mean, notepad++ with the use of npptor :)",2010-10-02T08:33:16.710,253,CC BY-SA 2.5,
4302,2818,0,@Thylacoleo (+1) Thanks for all of this. It looks definitively better with your annotated Stata example.,2010-10-02T08:37:50.653,930,CC BY-SA 2.5,
4303,3240,1,mod-tip: post the second part as a comment to the Dirk's answer.,2010-10-02T09:28:27.997,,CC BY-SA 2.5,user88
4304,3247,0,"@Gaetan Aside from my response, I gave my +1 to the question because I think it is very relevant for debating about statistical practice and project management.",2010-10-02T09:35:43.670,930,CC BY-SA 2.5,
4307,3260,0,zenna:> do you have a set of pre-identified break points ? is the dates of points were 'interesting changes across different datasets' did occur ?,2010-10-02T13:06:49.490,603,CC BY-SA 2.5,
4309,3260,0,"no, I have to find them also.  They are (normally) local minima within each dataset, the problem is there are lots of local minima which are not 'interesting points' which is why I am trying to combine multiple datasets to find the meaningful local minima.",2010-10-02T13:23:29.660,809,CC BY-SA 2.5,
4310,3236,2,"If the f() call is long then it's fine.  However, if the f() call is short then any timing call overhead will likely be increasing error measurement.  With a single call for system.time() over many repetitions of f() one gets to divide out the error the call until it's some infinitesimal value (and it returns faster).",2010-10-02T16:07:29.607,601,CC BY-SA 2.5,
4311,2400,1,Here's something interesting I found about divergences recently -- each step of Belief Propagation can be viewed as a Bregman Projection http://www.ece.drexel.edu/walsh/Walsh_TIT_10.pdf,2010-10-02T17:05:35.383,511,CC BY-SA 2.5,
4312,3257,1,"This looks like a good, tractable starting point. thanks for the links.",2010-10-02T17:17:38.017,1216,CC BY-SA 2.5,
4313,3235,3,"Just for the record, since I'm clearly far too late to change the course of this question: this is the kind of issue that I think is best suited for StackOverflow.",2010-10-02T17:22:02.910,71,CC BY-SA 2.5,
4315,3264,0,"I agree that permutation tests and other explicit manifestations of randomness can be quite educational.  This suggests showing dynamic simulations to the class, so they can watch the permutations being done and see the effects on the statistics.  Just to tweak you a little bit (apropos a different thread): one of the best tools available for that is...Excel!  (It helps that the students will have access to this and be familiar with it, unlike a better platform like Mathematica.)",2010-10-02T18:13:52.550,919,CC BY-SA 2.5,
4316,3264,1,"@whuber Thanks. Even before using any software, I like discussing Phillip Goud example (updated in my answer) and let them do the calculation by hand. Then, I think any software will do the job, provided they feel involved and do it themselves.",2010-10-02T18:27:20.373,930,CC BY-SA 2.5,
4317,3260,0,"Statistics works by training. The idea is that you need to first identify (by yourself) a set of points satisfying what you are looking for, then try to train a statistical procedure to do this job for you.",2010-10-02T18:51:32.927,603,CC BY-SA 2.5,
4318,3266,0,"Thanks for the link. What about imputation when data are not missing at random? It seems to me the problem lies in the fact that there is a complete block of measurements that is missing, and we cannot assume MAR or MCAR, nor are these data missing by design (in which case ML estimation yields unbiased parameters estimates).",2010-10-02T19:10:27.867,930,CC BY-SA 2.5,
4319,3008,1,"@Farrel - here's a very short script, which assumes [0,1]-uniformly distributed covariates, an OR of 2 between the first and third quartile of the covariate and standard normal noise, leading to power .34 for n=100. I'd play around with this to see how sensitive everything is to my assumptions: runs <- 1000; nn <- 100; set.seed(2010);
detections <- replicate(n=runs,expr={covariate <- runif(nn);
outcome <- runif(nn)<1/(1+exp(-2*log(2)*covariate+rnorm(nn)));
summary(glm(outcome~covariate,family=""binomial""))$coefficients[""covariate"",""Pr(>|z|)""] < .05})
cat(""Power:"",sum(detections)/runs,""\n"")",2010-10-02T20:04:27.407,1352,CC BY-SA 2.5,
4320,3008,1,"You can attach your code as a pastie (http://pastebin.com/) or a Gist (http://gist.github.com/) if you feel it's more convenient, and link back to it in your comment.",2010-10-02T20:20:43.807,930,CC BY-SA 2.5,
4321,3266,1,"@chi MAR is exactly the sort of assumption that Manski's approach is meant to avoid. Perhaps a simple example will make it clearer. This is straight out of the paper I linked above. Suppose you want to estimate the average of some function of a variable y, E[g(y)]. Let z=1 for people with y observed, and z=0 otherwise. Also, suppose g is bounded between g0 and g1. Then you know that E[g(y)|z=1]P(z=1) + g0 P(z=0) < E[g(y)] < E[g(y)|z=1]P(z=1) + g1 P(z=0). E[g(y)|z=1] and P(z) can be estimated as the sample mean of the observed y, and the portion of the sample with y observed.",2010-10-02T20:23:52.823,1229,CC BY-SA 2.5,
4322,3266,0,"Thanks for the explanation. I have no access to Manski's paper, but I will try to get it at work. (I have not vote left, so I'll +1 your response ASAP)",2010-10-02T20:31:52.957,930,CC BY-SA 2.5,
4323,3008,0,"@chl: +1, thanks a lot! Here's the gist: http://gist.github.com/607968",2010-10-02T20:41:40.577,1352,CC BY-SA 2.5,
4324,3255,0,Bowerman O'Connell and Koehler is quite good for introducing ARIMA models but I don't think it includes ARIMA with covariates. You could try my 1998 textbook which covers ARIMA modelling and regression with ARIMA errors at an introductory level. See http://robjhyndman.com/forecasting/ for details.,2010-10-02T21:41:39.857,159,CC BY-SA 2.5,
4325,3251,0,"A first order ARMAX model with one covariate is
y_t = a + bx_t + cy_{t-1} + e_t
where e_t is iid zero mean.
The corresponding regression with ARIMA error is
y_t = a + bx_t + n_t where n_t = phi*n_{t-1}+z_t
and z_t is iid zero mean.",2010-10-02T21:44:55.933,159,CC BY-SA 2.5,
4326,3266,0,"@chi This ungated paper by Kline and Santos might also be useful. It focuses on quantile regression, but has references to similar papers about other models. http://www.econ.berkeley.edu/~pkline/papers/missing4.pdf",2010-10-02T22:25:22.967,1229,CC BY-SA 2.5,
4328,3251,0,"Thanks Rob, but i'm still a bit confused with the n_t in 'y_t = a + bx_t + n_t where n_t = phi*n_{t-1}+z_t and z_t is iid zero mean'. It seems that 'n_t' follows a moving average process (i.e. not autoregressive). Can you confirm that a regression with ARMA errors is actually a 'MAX' model (moving average + eXogeneous) ? Also, does it mean that regressions with ARIMA errors cannot be fitted by OLS ?",2010-10-02T23:02:18.317,603,CC BY-SA 2.5,
4330,3251,0,"Also, I get exactly the same estimates when fitting 'arima(y,order= c(1, 0, 0),xreg=x)' from package stats and 'arimax(y,order= c(1, 0, 0),xreg=x)' (using the arimax function in package TSA). Does that mean that arimax() does not fit ARIMAX models ?",2010-10-02T23:26:31.530,603,CC BY-SA 2.5,
4332,3251,1,"@kwak. First, n_t = phi*n_{t-1} + z_t is AR(1). A moving average process of order 1 would be n_t = theta*z_{t-1} + z_t. Second, an regression with MA errors is equivalent to a MAX model. But once you add AR terms in the error process, there is no equivalence between the two classes. Third, the arimax() function in TSA fits transfer function models, a special case of which is a regression with ARIMA errors. It does not fit ARIMAX models. I might write a blog post about this as it is hard to find the various model classes compared and discussed anywhere.",2010-10-03T03:38:12.633,159,CC BY-SA 2.5,
4333,3274,0,"one such $f$ is based on EM, as described in Little & Rubin (I was somewhat put off by their treatment of regression, but perhaps I should revisit).",2010-10-03T04:19:38.183,795,CC BY-SA 2.5,
4334,3276,0,Similar question at http://stats.stackexchange.com/questions/2957/ols-is-blue-but-what-if-i-dont-care-about-unbiasedness-and-linearity/2961#2961. A nice review paper on the topic is http://webee.technion.ac.il/Sites/People/YoninaEldar/Download/67j-04490210.pdf,2010-10-03T07:30:40.507,352,CC BY-SA 2.5,
4335,3261,0,What is $\epsilon$?,2010-10-03T07:49:16.740,352,CC BY-SA 2.5,
4338,3143,0,"Certainly, Sir :)",2010-10-03T11:04:56.197,1439,CC BY-SA 2.5,
4340,3247,0,A comment for the downvote would be greatly appreciated.,2010-10-03T12:25:48.637,930,CC BY-SA 2.5,
4341,3278,0,What a wonderful synopsis!,2010-10-03T12:57:04.367,919,CC BY-SA 2.5,
4342,3282,0,Nice book!,2010-10-03T13:20:11.830,930,CC BY-SA 2.5,
4343,2749,0,"@Andy Thanks for the tips. In fact, I don't really use SPSS (only from time to time for Factor Analysis, and to exchange with colleagues), but it might be of interest for SPSS users. (my +1)",2010-10-03T13:23:25.520,930,CC BY-SA 2.5,
4345,3278,0,Thanks @chl this is a great description (merci bien pour le link egalement. Le premier link as une bonne explication des lignes et de collones). I have one more question. What would you say about the relationship between people with red hair and people with green eyes?,2010-10-03T18:13:26.300,776,CC BY-SA 2.5,
4346,3278,1,"@Brandon The 1st axis is an axis of ""dominance"" (light -> dark) for both modalities, but we can also see that the 1st axis opposes blue and green eyes to brown and hazel eyes (their coordinates are of opposite signs), and red hair/green eye combination--which is quite uncommon--contribute mostly to the 2nd factor axis. As this axis only explains 9.5% of the total inertia, it is rather difficult to draw firm conclusions (esp. wrt. genetic hypotheses).",2010-10-03T18:35:01.597,930,CC BY-SA 2.5,
4347,3278,1,"@Brandon Two further references (in english this time): the PBIL course (http://j.mp/cHZT7X) and Michael Friendly's resources (http://j.mp/cYHyVn + `vcd` and `vcdExtra` R packages, the latter including a nice vignette).",2010-10-03T18:37:19.003,930,CC BY-SA 2.5,
4349,3278,0,"When you refer to modalities, are you referring to ""hair colour"" and ""eye colour""? Also, using the ""ca"" package, I see clearly how inertia for the dimensions is stated but I'm not clear on which numbers you used to identify that red hair/green eyes contributed to the 2nd factor. Are you looking at the ""cor"" column (high/low) where k=2?",2010-10-03T19:29:22.967,776,CC BY-SA 2.5,
4350,3278,2,"@Brandon Yes, one modality = one category for your variable. For your 2nd question, `cor` is the squared correlation with the axis, and `ctr` is the contribution (it has to be divided by 10 to be read as a %). So ""red hair"" contributes 55.1% of the inertia of the 2nd axis. In a certain sense I found the FactoMineR output more ""intuitive"" (`CA(tab, graph=FALSE)$row$contrib` gives you directly the %).",2010-10-03T19:52:17.793,930,CC BY-SA 2.5,
4351,3283,0,"Thanks for the help!  I do have different probabilities for each game, which makes this method inefficient for calculating outcome probabilities.  However, I have come up with a way to enumerate all outcomes without having to check all 3^10 possibilities by using dynamic programming!",2010-10-03T19:57:04.227,,CC BY-SA 2.5,Kenny
4352,1470,0,This post has also useful references related to RCTs: http://j.mp/bAgr1B.,2010-10-03T20:06:52.653,930,CC BY-SA 2.5,
4353,3278,0,Here's another example for follow-up. http://stats.stackexchange.com/questions/3287/interpreting-2d-correspondence-analysis-plots-part-ii,2010-10-03T21:24:49.827,776,CC BY-SA 2.5,
4354,3286,4,"please, read the highest graded answer (i.e. mbq's) here: http://stats.stackexchange.com/questions/1001/spearmans-correlation-coefficient-to-compare-distributions",2010-10-03T21:33:34.677,603,CC BY-SA 2.5,
4355,2247,0,could you provide a link to what you feel is the best tutorial that you've seen using R and survival?,2010-10-03T21:45:56.817,776,CC BY-SA 2.5,
4356,3261,0,$\epsilon$ is a zero-mean error term.,2010-10-04T01:49:59.700,795,CC BY-SA 2.5,
4357,3274,0,It seems like imputation would not work for covariance estimation.,2010-10-04T01:51:07.453,795,CC BY-SA 2.5,
4358,3283,0,If you're enumerating all outcomes you're looking at all 3^10 possibilities.  There's no free lunch here ;-).,2010-10-04T02:05:43.500,919,CC BY-SA 2.5,
4359,618,11,"A colleague of mine looked at the data for this in the post-2000 period, and found that the relationship held fairly well 'out-of-sample', which is even more disturbing...",2010-10-04T04:12:47.273,795,CC BY-SA 2.5,
4360,3283,0,"You're right, I'm not actually enumerating all individual game outcomes, just overall outcomes AFTER 10 games.  I'm looking at the possible outcomes for only one category, and then adding the new possible outcomes with each new additional category (one at a time).  With this method, you're able to find the probability of all (W,L,T) outcomes after X games in polynomial time: O(n^3) :-)",2010-10-04T05:09:30.730,,CC BY-SA 2.5,Kenny
4361,3236,0,"@John: Thanks but I don't quite get what you said. I am still wondering which is better, repeating f() inside or outside system.time()?",2010-10-04T05:59:47.657,1005,CC BY-SA 2.5,
4362,3284,1,There's also an example of using Mfuzz in my [tutorial paper](http://www.mas.ncl.ac.uk/~ncsg3/microarray/),2010-10-04T08:51:19.263,8,CC BY-SA 2.5,
4365,3236,0,"Every call to the system.time() command has some variable time it takes to call that causes some amount of measurement error. This is a small amount.  But what if f() is a very brief call?  Then this error can be conflated with the time taken to call f().  So, when you call f() 1e5 times inside a single system.time() call the error gets divided down into 1e5 chunks.  When you call system.time() for every f() it's impact could be meaningful if time for f() is small.  Of course, if all you need is relative timing it doesn't much matter.",2010-10-04T10:13:52.917,601,CC BY-SA 2.5,
4366,3236,0,"Oh, and the second part is that it would be faster to just call system.call() once.",2010-10-04T10:14:36.733,601,CC BY-SA 2.5,
4367,3288,5,"To make it clearer, here you will rather need Kolomogorov-Smirnov distance than KS-test.",2010-10-04T10:22:05.020,,CC BY-SA 2.5,user88
4368,3286,1,Wikipedia has some sort of overview of distribution distances here: http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_distance#Other_probability-distance_measures,2010-10-04T10:22:48.173,,CC BY-SA 2.5,user88
4370,3251,2,I've tried to summarise the various models at http://robjhyndman.com/researchtips/arimax/,2010-10-04T12:41:45.107,159,CC BY-SA 2.5,
4371,3267,0,+1.  Absolutely agree with stressing the real-world relevance of stats and focussing on fundamental concepts.,2010-10-04T12:53:34.967,266,CC BY-SA 2.5,
4373,3247,0,"@chl: although I didn't downvote this answer, I think I understand why one would downvote it. The information you have provided is correct, very very important and think-provoking. HOWEVER, most of it (except for the last two paragraphs) do not answer the question. Ideally, one would write this large disclaimer elsewhere and give a link to it.",2010-10-04T13:23:51.490,1496,CC BY-SA 2.5,
4374,3247,0,"@chl: despite what I said in my comment, I love your answer and up-vote it",2010-10-04T13:24:35.593,1496,CC BY-SA 2.5,
4375,3283,0,Are you sure it's not O(3^n)?,2010-10-04T13:34:50.743,919,CC BY-SA 2.5,
4376,3289,0,What is the statistical motivation for this question?  (It seems like it would be more appropriate to pose it on a math forum.),2010-10-04T13:37:06.733,919,CC BY-SA 2.5,
4378,3247,0,"@bgbg Thanks for your comment. Maybe I didn't answer the CW question. However, I never intended to give a purely provocative answer. The OP asked about potential ""bugs and flaws"" in GDocs: I provide illustrations about what I know from Excel, acknowledging the fact I don't know how it would translate to GDocs. I also understand part of the question as ""what are the benefits of using GDocs for data analysis"", and I just gave some arguments against the use of spreadsheet for large scale projects, or analysis at the bleeding edge (still, I acknowledged at the beginning that this would be biased).",2010-10-04T14:21:58.300,930,CC BY-SA 2.5,
4379,3293,1,'Elements of...' largely covers the same topics as 'Modern Multivariate...'.,2010-10-04T14:30:50.113,603,CC BY-SA 2.5,
4380,3293,0,"@kwak Well, some savings for @shabbychef! I found Izenman's book more an applied textbook featuring some methods not covered in ESL (e.g. correspondence analysis), but you're right they are pretty written in the same spirit one each other. (I'll update my answer)",2010-10-04T15:11:03.217,930,CC BY-SA 2.5,
4381,3303,1,(+1) Thx for the link. I like the PGP framework too.,2010-10-04T15:53:07.730,930,CC BY-SA 2.5,
4382,3296,0,Could you indicate what classification method(s) is/are used?,2010-10-04T16:56:11.790,930,CC BY-SA 2.5,
4383,3288,1,Bonus marks for explaining how to find out stuff for myself.,2010-10-04T17:03:03.273,5141,CC BY-SA 2.5,
4384,3295,0,". Yes, but one of the difficulties I had was in extracting the clusters from the heatmap.2 object. Is there an easy way of extracting the clusters? I am aware of the `cutree` command which can be used to extract clusters from the heatmap.2 object.",2010-10-04T17:33:26.677,1307,CC BY-SA 2.5,
4385,3311,0,"This is just a recap' of what I read so far. Obviously, *I will not accept my own answer*. Any other thoughts would be much appreciated.",2010-10-04T17:58:01.817,930,CC BY-SA 2.5,
4386,3296,0,@chl: see update to the question,2010-10-04T18:52:28.690,1496,CC BY-SA 2.5,
4387,3310,1,"Thank you ""Extreme value theory"" was what I was looking for!",2010-10-04T18:59:45.897,253,CC BY-SA 2.5,
4388,3308,0,"when I say they are misclassified, I mean that when I test different alternative models on the training set, all those samples are misclassified by all or most of those models",2010-10-04T19:00:36.237,1496,CC BY-SA 2.5,
4389,3293,0,"> i agree with you btw (i own both books). On the other hand, if budget is a binding constraints, you might want to diversify.",2010-10-04T19:27:55.740,603,CC BY-SA 2.5,
4391,3293,0,@kwak For sure :) Add your must-have references! (thx for the undelete),2010-10-04T19:58:53.140,930,CC BY-SA 2.5,
4393,3159,0,another funny website: http://www.visualizing.org/,2010-10-04T20:15:24.103,930,CC BY-SA 2.5,
4394,3314,2,"Thanks for the reference.  It's always fun to see early work (even when it was published in one's own lifetime, LOL!). The results are simple because they combine two simple results: (1) we know the pdf of the Normal distribution (which allows us to give a name to its cdf, the Gaussian integral,which has no closed form evaluation).  (2) the standard expressions for distributions of the order statistics of any pdf.  Thus you do obtain ""exact"" expressions for the order statistics, but in they end they must be polynomials in the Gaussian integral. (Computing them in 1961 wasn't easy.)",2010-10-04T20:18:24.217,919,CC BY-SA 2.5,
4395,3317,0,"Cont'd:                                                          2. Cook R.J. and farewell V.T. Multiplicity considerations in the design and analysis of clinical trials. Journal of the Royal Statistical Society, Series A 1996; Vol. 159, No. 1 : 93-110",2010-10-04T20:41:43.223,1501,CC BY-SA 2.5,
4396,3317,0,"Thank you for your comments, Brenden, especially the last one about prediction vs. causal explanation.  And welcome to the site!  I hope to see many more of your contributions in the future.",2010-10-04T20:44:50.773,919,CC BY-SA 2.5,
4397,3317,0,"Cont'd:                                                           3. Rothman K.J. No adjustments are needed for multiple comparisons. Epidemiology 1990; Vol. 1, No. 1 : 43-46            4. Marshall J.R. Data dredging and noteworthiness. Epidemiology 1990; Vol. 1, No. 1 : 5-7                                        5. Greenland S. and Robins J.M. Empirical-Bayes adjustments for multiple comparisons are sometimes useful. Epidemiology 1991;
Vol. 2, No. 4 : 244-251",2010-10-04T20:45:48.947,1501,CC BY-SA 2.5,
4398,3314,0,"@whuber:> Yes. the link will probably be less useful than the Gumbel approximation (except perhaps for the <strike> min and max <\Strike> *largest* and *smallest* draw) but i cited the paper nonetheless because, as you said, it has this 'head in the cloud' cachet about it :)",2010-10-04T20:59:04.617,603,CC BY-SA 2.5,
4399,3295,1,@suncoolsu: I've updated my answer. Does that help?,2010-10-04T21:13:41.803,8,CC BY-SA 2.5,
4400,3317,0,(+1) You may be interested in the following thread: http://stats.stackexchange.com/questions/3252/how-to-cope-with-exploratory-data-analysis-and-data-dredging-in-small-sample-stud. It seems we share a lot of links in common :-),2010-10-04T21:14:57.903,930,CC BY-SA 2.5,
4406,3289,0,"@whuber: i was hoping that some clever chap, such as you, would have some insights about how to approach this. btw - there is a longish list of related problems on the RHS of the screen, to many of which your question could be applied.",2010-10-04T23:50:36.020,1112,CC BY-SA 2.5,
4407,3278,1,"@chl: wow, for someone who knows nothing about CCA or the ""French way"", this was a great read!  Many thanks.  I also found this with some googling that might be of interest: http://www-stat.stanford.edu/~susan/papers/dfc.pdf",2010-10-05T00:53:11.253,251,CC BY-SA 2.5,
4408,3324,0,"From your question you may be reading the code slightly wrong. The syntax is using the differences as the ""instruments"" to estimate the lag of the dependent variable.",2010-10-05T01:39:55.210,1036,CC BY-SA 2.5,
4410,3326,0,"Thank you for answering, many voted, but not so many answered =) I have been looking at Multi Hypothesis Tracking for a while now. I will check out your suggestions, have not heard of two of them!",2010-10-05T07:47:02.787,1411,CC BY-SA 2.5,
4411,3331,3,you might get a few ideas from an earlier question on clustering individual longitudinal data trajectories http://stats.stackexchange.com/questions/2777/modelling-longitudinal-data-where-the-effect-of-time-varies-in-functional-form-be,2010-10-05T07:52:45.383,183,CC BY-SA 2.5,
4412,3285,1,"The `timecourse` package isn't really for determining clusters, rather it's for calculating which genes are differentially expressed.",2010-10-05T09:41:49.263,8,CC BY-SA 2.5,
4413,3285,0,"@csgillespie (+1) Thanks. I thought it might be used to isolate genes with varying temporal profiles across biological conditions, or as a first step before using a clustering procedure (in fact, I was thinking of `kml` but I'm not really an expert in that domain).",2010-10-05T09:53:59.010,930,CC BY-SA 2.5,
4414,3285,0,"You are correct in that you would tend use to isolate interesting genes before any clustering - basically thin down your list of genes. I suppose it does perform clustering of a sort, i.e.differentially expressed vs non-differentially expressed.",2010-10-05T10:09:48.373,8,CC BY-SA 2.5,
4415,3278,1,"@ars (+1) Thanks for the link (didn't know about this monograph, it looks interesting). My best recommendations for recent developments are actually ALL papers from Jan de Leeuw and these two books: *Multiple Correspondence Analysis And Related Methods* from Greenacre, and *Geometric Data Analysis: From Correspondence Analysis to Structured Data Analysis* from Le Roux & Rouanet (the french way).",2010-10-05T10:46:46.753,930,CC BY-SA 2.5,
4416,2635,1,... and maybe a better topic?,2010-10-05T10:52:27.663,,CC BY-SA 2.5,user88
4417,3331,1,@Jeromy Anglin Thanks for the link.  Did you have any luck with `kml`?,2010-10-05T12:24:07.557,179,CC BY-SA 2.5,
4418,3334,0,Sometimes I just want to thwack myself upside the head.  Excellent suggestion.,2010-10-05T12:28:18.390,1499,CC BY-SA 2.5,
4420,3342,0,see http://stackoverflow.com/ for R question,2010-10-05T13:04:33.243,1154,CC BY-SA 2.5,
4421,3342,7,@pbneau R questions are acceptable on here.  See http://meta.stats.stackexchange.com/questions/252/should-we-allow-more-computing-questions and the FAQ.  See also the recent discussion: http://meta.stats.stackexchange.com/questions/474/printhello-world-n,2010-10-05T13:10:23.157,5,CC BY-SA 2.5,
4422,3341,0,"interesting ! there are testing images to try it, exercices to learn how to use it...",2010-10-05T13:16:52.360,223,CC BY-SA 2.5,
4423,3343,4,I did not know about this. Now I can manipulate the `h$count` vector to my nefarious purposes. Marvelous. Thanks,2010-10-05T13:21:57.397,5141,CC BY-SA 2.5,
4424,3339,1,"@csgillepsie: there is catch here. The answers about comparing distributions assume the draws from the distributions are iid. From the tone of the questions (..""mapped to line chart"") this may not be the case in Tristan's application. That would invalidate all the solutions (k-s, chi-2,...) proposed.",2010-10-05T13:26:26.503,603,CC BY-SA 2.5,
4425,3339,0,"@kwak: Very good spot and something that I missed. Just to clarify it invalidates the *k-s* and *chi-2* solutions, not all **all** the answers given, i.e. my answer and the answer by Joris still stand.",2010-10-05T13:31:59.310,8,CC BY-SA 2.5,
4426,3339,0,@kwak @csgillepsie: yeah just to clarify the data i'm dealing with is not iid at all.,2010-10-05T13:40:58.620,1505,CC BY-SA 2.5,
4427,3334,1,I also just thought of using the `kmeans` function in R.  I really shouldn't ask questions between midnight and 4am.,2010-10-05T13:44:33.200,1499,CC BY-SA 2.5,
4431,3347,0,"Unfortunately, the Valencia meetings are no more. The last one was this year! Instead, we're just going to have general ISBA meetings.",2010-10-05T16:23:44.217,8,CC BY-SA 2.5,
4432,3345,0,Thanks Kwak. I have ordered the first book from Amazon. I could still do with a few lines to get me started though ...,2010-10-05T16:36:42.660,1216,CC BY-SA 2.5,
4434,3347,0,Oh!  I didn't know.  Crap.,2010-10-05T17:08:35.347,1499,CC BY-SA 2.5,
4435,3347,1,yeah Dr. Bernardo has apparently told others that he is too old to take care of the valencia conference. He wants young guns to take care of it now.,2010-10-05T18:08:48.767,1307,CC BY-SA 2.5,
4436,3353,0,Thanks for the note. I updated my question to include the priors on beta_0 and tau; I had omitted these for simplicity,2010-10-05T21:07:04.240,1381,CC BY-SA 2.5,
4438,3355,0,"Sorry, M. Tibbits updated the answer later, but my comments are still valid for David's question.",2010-10-05T21:59:18.247,1307,CC BY-SA 2.5,
4439,3327,0,"If I have two possible models to fit to my data, the sine wave as described in my original question and a LMS straight line fit, could I simply compare the average squared deviation from the true data values of the sine wave with the residuals of the LMS fit line and then choose the model with the lower overall value on the grounds that this model exhibits a more accurate fit to the data? If so, would it also be valid to perhaps split the data into halves and do the same with each half separately, using the same sine wave/LMS fits to see how each model may be improving/getting worse with time?",2010-10-05T22:24:15.647,226,CC BY-SA 2.5,
4440,3283,0,Yes I'm sure :-),2010-10-06T02:11:04.877,,CC BY-SA 2.5,Kenny
4441,3283,0,(see answer below for clarification),2010-10-06T02:24:10.200,,CC BY-SA 2.5,Kenny
4442,3357,0,"Thank you for the clarification.  The point is that at each stage you only have to consider the probability distribution over (W,L,T) and not how it was derived.",2010-10-06T02:44:23.820,919,CC BY-SA 2.5,
4443,3331,0,"I've had a quick look, but for the moment I'm using a customised cluster analysis based on selected features of the individual time series (e.g., mean, initial, final, variability, presence of abrupt changes, etc.).",2010-10-06T02:59:14.890,183,CC BY-SA 2.5,
4444,3327,0,"I'm not sure.  My suggestion was to use a Least Squares metric, but I wasn't saying to run linear regression.  You might check out [Periodic Regression](http://www.google.com/search?client=ubuntu&channel=fs&q=Periodic+Regression&ie=utf-8&oe=utf-8).",2010-10-06T03:41:21.377,1499,CC BY-SA 2.5,
4445,3327,0,"As to your other question, could you cut the data in half, I would be very cautious in doing so -- because that would double the minimum frequency you could consider.  I think you may end up needing to look at Fourier coefficients (take an [FFT](http://en.wikipedia.org/wiki/Fast_Fourier_transform) or a [DCT](http://en.wikipedia.org/wiki/Discrete_cosine_transform) and regress on them?!? -- __Not sure__).  Or perhaps periodic regression as mentioned above.",2010-10-06T03:47:50.440,1499,CC BY-SA 2.5,
4446,2611,0,"@chl: I can send you the tables with the results when I'm done, but I won't be inventing anything new really. So far I'm just planning to compare MI under a two-level imputation model (R package pan) to MI under a simple normal model (ignoring the two-level structure, R package norm) and listwise deletion. Under different sample sizes, values of the variance component etc. This should be enough for the seminar (I'm a PhD student), but not exactly groundbreaking. If you have any ideas on how to ""jazz up"" the simulation study, i'd love to hear.",2010-10-06T07:37:47.373,1266,CC BY-SA 2.5,
4447,2611,1,"One other thing: i'm not sure that a proper analytical solution to this problem even exists. I've looked at some additional literature, but this problem is elegantly looked over everywhere. I've also noticed that yucel&demirtas (in the article i mentioned, page 798) write:‚ÄúThese multiply imputed datasets were used to estimate the model [‚Ä¶] using the R package lme4 leading to 10 sets of (beta, se(beta)), (sigma_b, se(sigma_b)) which were then combined using the MI combining rules defined by Rubin.‚Äù",2010-10-06T07:40:59.840,1266,CC BY-SA 2.5,
4448,2611,0,"It seems they used some kind of shortcut to estimate the SE of the variance component (which is, of course, inappropriate, since the CI is asymmetrical) and then applied the classic formula.",2010-10-06T07:47:05.907,1266,CC BY-SA 2.5,
4449,3311,0,"Thanks for accepting my answer chi, though your own reference list is much better and more recent. I really should have thought of a couple of them myself as I've got them on my hard drive, and may have even read parts of them...",2010-10-06T08:18:51.513,449,CC BY-SA 2.5,
4450,2611,0,"Ok, thx for that. Can you put your comments into an answer so that it can be voted?",2010-10-06T09:25:40.647,930,CC BY-SA 2.5,
4452,3360,4,"For record keeping: The DOI of the ""The distribution of product of independent beta random variables with application to multivariate analysis"" is 10.1007/BF02480942.",2010-10-06T13:59:29.430,1512,CC BY-SA 2.5,
4453,3360,3,The other one is 10.1137/0118065 :),2010-10-06T14:00:17.970,1512,CC BY-SA 2.5,
4454,3358,0,"I appreciate you came back to share your experience with this problem. Unfortunately, I have no real solution but maybe other suggestions will come up.",2010-10-06T14:30:17.857,930,CC BY-SA 2.5,
4455,3072,1,"Thanks, but MATLAB isn't free!!",2010-10-06T15:33:25.953,1024,CC BY-SA 2.5,
4456,3051,0,@J.M - I hadn't! Thank you! I'm about to see how it works.,2010-10-06T15:36:57.103,1024,CC BY-SA 2.5,
4457,3051,0,@Shane - Yes! I'm sorry that wasn't clear. The slide is the number of positions/indices you move to start computing the next window of averages. So rather than the next window starting after the end of the last there is some overlap when the slide is smaller than your window size. The idea is to smooth out the data points a bit.,2010-10-06T15:39:42.370,1024,CC BY-SA 2.5,
4458,3362,0,"I find the question interesting. However, it would help if you could briefly summarize the idea in the paper so that users need not download and read the paper to answer your question. I think it would help a lot if your question is self-contained to get quick and quality answers.",2010-10-06T15:46:05.883,,CC BY-SA 2.5,user28
4459,3054,0,"Thanks! Especially for noting the last value as zero assumption, I hadn't considered that. I definitely care about that last window!!",2010-10-06T15:59:54.633,1024,CC BY-SA 2.5,
4460,203,6,"You may be interested in this recent article published in PARE, *Five-Point Likert Items: t test versus Mann-Whitney-Wilcoxon*, http://j.mp/biLWrA.",2010-10-06T17:05:16.600,930,CC BY-SA 2.5,
4461,3072,0,"@T-Burns: octave is free, however; also R is close enough to Matlab that this code can easily be translated. In fact, @mbq did that..",2010-10-06T17:24:20.507,795,CC BY-SA 2.5,
4462,3361,0,The last page alone is worth $1.20! maybe we can wedge that into @Carlos Accioly's knapsack by buying one of the books used..,2010-10-06T17:26:47.897,795,CC BY-SA 2.5,
4463,3361,1,The last page comes from this article: http://www.math.wm.edu/~leemis/2008amstat.pdf.  I've posted a condensed version with links for footnotes here: http://www.johndcook.com/distribution_chart.html,2010-10-06T17:28:55.197,319,CC BY-SA 2.5,
4466,3348,0,"This looks very useful! Just to be clear (since I'm not fluent in R): this calculates spatial correlation, but weights more highly correlations between things that are closer together. Is that correct?",2010-10-06T19:44:02.333,900,CC BY-SA 2.5,
4467,3348,1,"Precisely. It uses the inverse distance between the pixels (as measured in pixels) to weight the measure of spatial autocorrelation. Note that I generated a grayscale image, but you could similarly apply this to a color image either treating the colors separately or taking some combined score.",2010-10-06T19:46:05.587,1499,CC BY-SA 2.5,
4468,3368,0,Please consider adding some references to this question (e.g. for the Nyquist frequency).,2010-10-06T19:57:33.723,5,CC BY-SA 2.5,
4469,3362,0,"Good suggestion.  I'm not sure I understand the method well enough yet to summarize how it works accurately... in the mean time, I've excerpted the brief summary from the R implementation.",2010-10-06T20:46:30.953,644,CC BY-SA 2.5,
4470,3356,0,"Hi Andy. I don't know stata code. That is why i do not mention the code snipped in my answer, which has to be understood as a response to the part of the question that is formulated in english.",2010-10-06T21:17:38.833,603,CC BY-SA 2.5,
4471,3324,0,lara: could you edit your question to explain in plain terms the meaning of the stata code snipped ?,2010-10-06T21:19:16.257,603,CC BY-SA 2.5,
4472,3372,0,Kaelin:> do you mean whether there exists 'on the fly' method for computing summary stats such as median and quartiles ? If this is what you want i could give you links to papers detailing them. You could also give more details about the platforms you are working on as efficient GNU implementation of these methods likely exists in R.,2010-10-06T21:24:31.530,603,CC BY-SA 2.5,
4473,3349,0,"Thanks, Srikant! Sorry, I somehow missed your comment earlier. The upper cluster is really just a spike right at the ceiling - there's no variability there except for the long stretch of uniformity that links it to the lower distribution, which is basically as you describe.  It'll take me some time to parse out your answer (especially since I'm stuck in IE and can't see the LaTeX properly right now), but I really appreciate your dedication to this odd little question.",2010-10-06T23:09:06.687,71,CC BY-SA 2.5,
4474,3064,0,"Note: I apparently missed the deadline to actually award the bounty, so I'm setting up another so that I can properly reward Srikant for his help. More answers are always welcome, but the bounty is for him.",2010-10-06T23:13:29.863,71,CC BY-SA 2.5,
4475,3372,0,"@kwak: Yes, that sounds like what I am looking for. I would greatly appreciate those links. :-) I am working on Mac OS X‚Ä¶ I can use R for post-processing data, but can't link GPL code into my company's product for the usual reasons.",2010-10-06T23:20:49.910,1515,CC BY-SA 2.5,
4477,3331,0,Is this a duplicate? http://stats.stackexchange.com/questions/3238/time-series-clustering-in-r,2010-10-07T01:32:55.087,159,CC BY-SA 2.5,
4478,3238,1,And this one: http://stats.stackexchange.com/questions/3331/is-it-possible-to-do-time-series-clustering-based-on-curve-shape,2010-10-07T01:33:23.713,159,CC BY-SA 2.5,
4479,3368,0,What do you mean by `I have 3 time series'? Do you have 1 time series that has been divided it into 3 pieces? Or do you have 3 devices all independently measuring the same `signal' at the `same' time?,2010-10-07T01:55:12.883,352,CC BY-SA 2.5,
4480,3356,0,"@kwak - I was not criticizing your post, I agree with everything you said. I was simply wondering if there was some logic as to why someone would use the differences as instruments that I was unaware of. I can't imagine any situation in which the differences would meet any of the requirements for such a procedure.",2010-10-07T02:04:20.827,1036,CC BY-SA 2.5,
4482,3356,0,"Hi Andy:> i didn't take you're comment as a critic. Your post is highlighting a key aspect of the question that neither Rob nor I (admittedly) understood. If anything, it illustrates the importance of collaboration.",2010-10-07T02:49:50.533,603,CC BY-SA 2.5,
4484,3378,0,"Wow, really interesting approach. I didn't know about that, and I will keep it in mind. Unfortunately, in this case it will not work, since I have really restrictive requirements from the point of view of memory usage, so M should be really small, and I guess there would be too much precision loss.",2010-10-07T05:26:10.993,667,CC BY-SA 2.5,
4485,3331,1,"@Rob This question doesn't seem to assume irregular time intervals, but indeed they are close one each other (I didn't remind of the other question at the time of my writings).",2010-10-07T06:11:08.007,930,CC BY-SA 2.5,
4486,3365,0,"@Chi Thanks! This works, but means I have to plot each item individually (and combine them with par). What I was hoping for was to simply obtain the same result by using par(mfrow=c(3,3)) and simply plot(fit1, 'some edits to 'main='"") which gives me all the plots (9 in my case).",2010-10-07T06:14:54.317,913,CC BY-SA 2.5,
4488,3365,0,"@Tormod I'm afraid you can't. Look at the R code for that function (just type `plot.grm` at the R prompt or `edit(plot.grm)`) near line 52 and 91: Dimitri R. automatically prefixes the title if missing and append ""Item"".",2010-10-07T06:25:55.320,930,CC BY-SA 2.5,
4489,3365,0,"@Chi Thanks again! Still, your solution works fine so I am still able to achieve what I want, albeit with a few more lines of code.",2010-10-07T06:31:47.643,913,CC BY-SA 2.5,
4490,3374,5,"Note that you will get the same results with `principal(attitude, 2, rotate=""none"")` from the `psych` package and that Kayser's rule (ev > 1) is not the most recommended way to test for dimensionality (it overestimates the number of factors).",2010-10-07T06:36:59.833,930,CC BY-SA 2.5,
4491,3365,0,@Tormod The same applies for the `eRm` package. It's difficult to provide users with both a friendly and generic interface :(,2010-10-07T06:40:03.617,930,CC BY-SA 2.5,
4493,3363,1,"Thanks for your very detailed answer chi.  I've got `kml` running on my data, but as you suggested it is clustering mostly based on magnitude rather than curve shape, so I'm trying a few pre-processing steps to see if I can improve matters.  The work by Sangalli et al. looks very promising for what I want to do - I cannot find an implementation of their approach however.  I probably do not have time to create my own implementation of their work for this project, hwoever.  Are you aware of any FOSS implementations?",2010-10-07T06:45:18.550,179,CC BY-SA 2.5,
4494,3363,0,"@fmark No OSS implementation to my knowledge (the work is quite recent, though); they use k-means and k-medoids which are both available in R. To my opinion, the most critical parts are to generate template curves and implement the warping function. For that, you could find additional infos by looking at morphometry/procruste analysis, or lookup the code of the Matlab PACE toolbox (but this should be full of EM or things like that). My best recommendation would be: Ask the author for any free of charge implementation of their algorithm.",2010-10-07T06:53:49.593,930,CC BY-SA 2.5,
4495,3363,2,I'll report back if I get an affirmative :)  Their paper [k-mean alignment for curve clustering](http://mox.polimi.it/it/progetti/pubblicazioni/quaderni/13-2008.pdf) has some more implementation details that also might be useful to someone wanting to do this themselves.,2010-10-07T07:10:19.817,179,CC BY-SA 2.5,
4497,3247,0,"@chi: I can see why someone would downvote this, but I think your answer is a valid response.  A list of circumstances when using spreadsheets is not a good idea is a handy addition to a discussion of the advantages of using them.",2010-10-07T08:27:38.400,266,CC BY-SA 2.5,
4498,3376,1,+1 Right; I was still in dark ages of making approximation from histogram.,2010-10-07T08:50:21.420,,CC BY-SA 2.5,user88
4499,3368,0,"Shane, by references do you mean tagging? If so, it seems I'm not allowed to add new tags as I'm a newbie.",2010-10-07T10:08:32.523,1513,CC BY-SA 2.5,
4500,3368,0,"No, the three time series are not divided from one. Although independently sampled, they are strongly correlated with each other.",2010-10-07T10:15:30.717,1513,CC BY-SA 2.5,
4501,3373,1,"Really appreciate your help! Using your cell tower as an example, my scenario is more like the following: three cell phones at three equally-spaced locations receive signal from the same cell tower. Would this be considered ""in phase""? Thanks again!",2010-10-07T10:33:02.617,1513,CC BY-SA 2.5,
4503,3373,1,"I'm not sure.  If you look at your data, does the first sample collected in each of the three time series correspond to the same point in time in the underlying process?  If so, then they're ""in phase"".  Or is there a lag between when one series is collected and the other two are collected?  If there is a lag, then they would be out of phase. There's a cute picture on [this Wikipedia](http://en.wikipedia.org/wiki/Phase_%28waves%29) page which might provide some insight.",2010-10-07T11:28:20.737,1499,CC BY-SA 2.5,
4504,3381,7,What's wrong with conventional kernel density estimation?,2010-10-07T12:08:17.477,449,CC BY-SA 2.5,
4505,3375,1,"Thanks Srikant, yes I think that summarizes the method well.  The examples I've seen have applied the method to an aggregate dependent variables like GDP or cigarette sales per capita (estimated from tax revenues).  Is there anything special I'd need to do when using a survey-based dependent variable instead?  I guess add confidence bands to my time-series plots??  Then do some power estimates based on what divergence between treated and control groups I wouldn't want to miss by chance, then extrapolate/guesstimate how many control-groups responses I need to form useful synthetic control??",2010-10-07T12:53:56.537,644,CC BY-SA 2.5,
4506,3375,0,Are there other things I'd need to do to take into account the fact that my dependent variable is an estimate?,2010-10-07T12:55:49.853,644,CC BY-SA 2.5,
4507,3383,0,Can you distinguish what pre-treatment weight is assigned to each different treatment group? (i.e. I know these 16 observations are pre-treatment weights for treatment A). Also were treatments randomly assigned?,2010-10-07T13:03:32.487,1036,CC BY-SA 2.5,
4508,3383,0,"The 16 observations are 16 fishes in a tank. The treatment is given to the tank. So the 16 fishes in the tank were weighed before and after treatment. But as the fishes weren't marked, it's impossible to link the weights to a specific individual.",2010-10-07T13:08:06.660,1124,CC BY-SA 2.5,
4509,3385,0,"I've been thinking about that as well, but the initial mean weight differs significantly between tanks, and using all t=0 observations as one control makes for a very unbalanced design. On top of that, the hypothesis is formulated in difference between treatments, which cannot be formally tested using a control as reference group. The multi-level framework doesn't help me either for the same reason.",2010-10-07T13:55:38.403,1124,CC BY-SA 2.5,
4510,3385,0,"I don't see why the unbalanced design is a problem, but the mean weight difference is obviously a problem with my suggestion. I would still think the multi-level framework would allow you to assess differences between treatments somehow (I do not know how offhand though). The only other thing I can think of would be to simply graph each control/comparison like this response did (minus the connecting lines) http://stats.stackexchange.com/questions/2067/follow-up-in-a-mixed-within-between-anova-plot-estimated-ses-or-actual-ses/2138#2138 , although I imagine this is not entirely satisfactory.",2010-10-07T14:10:36.467,1036,CC BY-SA 2.5,
4511,3374,5,"Yes, I know psych principal wraps this up.  My purpose was to show what SPSS ""factor analysis"" was doing when using the principal components extraction method.  I agree that the eigenvalue rule is a poor way to select the number of factors.  But, that is exactly what SPSS does by default and this was what I was demonstrating.",2010-10-07T14:21:18.797,485,CC BY-SA 2.5,
4512,3383,1,"Your issues with independence of observations go beyond repeated measurements.  Your 16 fish within a tank are not independent and the treatment was applied to the tank.  You do not have 16 independent pre and post observations.  In fact, you have 6 observations assigned to one of 6 treatments.  Any analysis that uses the fish as the analytical unit will grossly overestimate your degrees of freedom.  At very least, you need to account for the nested structure and the intra-tank correlations.  Needless to say, you need to increase your N, which is 6 right now (see pseudo-replication).",2010-10-07T14:41:28.690,485,CC BY-SA 2.5,
4513,3382,0,well to console you i have just started out and was just looking around some. i have no interest in getting in your way (: its just something that has been bugging me for a while. there should be some kind of normalization possible with sequences,2010-10-07T15:08:36.047,1516,CC BY-SA 2.5,
4514,3383,1,"@Brett : I am aware of the fact that my observations aren't independent. But I don't agree with your statement that N=6. That would mean that a clinical study with 4 hospitals involved would have N=4, which obviously doesn't make sense. It's impossible to give fish in the same tank different treatments, as the treatment is in the water. The 16 fish are independent, but the measurements at t=0 and t=1 aren't. I correct for this by calculating the df using Satterthwaite and then divide them by 2, which gives me a df of appx. 14 for the t-tests.",2010-10-07T15:09:33.383,1124,CC BY-SA 2.5,
4515,3375,0,"@heather Why do you call your DV as an estimate? If you call it as an estimate because you are taking the mean/sum of your likert scales then I do not see how that matters conceptually. As far as power is concerned, that would be dependent on the model/estimation specifics and I am unable to offer any suggestions on that issue. Perhaps, you can just email the authors for some suggestions or do a simulation to see how much sample size you need for desired power?",2010-10-07T15:11:03.453,,CC BY-SA 2.5,user28
4516,3383,0,"In your hospital example, it depends on how the patients are randomized.  If the hospitals are assigned to the treatments, then your N is the number of hospitals.  If patients are randomly assigned within hospitals, then your N is the number of patients.  Again, I refer you to the literature on pseudoreplication which you'll find easily.  Once you say ""the treatment is given to the tank"" you are done.  Level of treatment implies appropriate level of analysis.  Here is the original article http://www.masterenbiodiversidad.org/docs/asig3/Hurlbert_1984_Pseudoreplication.pdf There's plenty more.",2010-10-07T15:31:17.050,485,CC BY-SA 2.5,
4517,3388,1,"@gaetan I do not understand the remark about a single column vs multiple columns. Are you suggesting that categorical variables should be coded as 1, 2, 3 etc in a single column instead of using dummy variables? I am not sure that makes sense to me as you are then imposing an implicit constraint that the difference in the effect on dv between leve1s 1 and 2 is the same as the difference in the effect on dv between levels 2 and 3. Perhaps, I am missing something.",2010-10-07T16:07:12.280,,CC BY-SA 2.5,user28
4518,3375,0,"Great, Srikant, thanks.  The fact that you don't think it matters conceptually answers it for me!  I wanted confirmation of that I guess.",2010-10-07T16:13:46.760,644,CC BY-SA 2.5,
4519,3378,0,"@gianluca: it sounds like you have 1. a lot of data, 2. limited memory resources, 3. high precision requirements. I can see why this problem is freaking you out! Perhaps, as mentioned by @kwak, you can compute some other measure of spread: MAD, IQR, standard deviation. All of those have approaches which might work for your problem.",2010-10-07T16:22:27.510,795,CC BY-SA 2.5,
4520,3383,0,"@Brett Magill - I completely disagree with the statement ""Level of treatment implies appropriate level of analysis"". Your units of analysis will be determined both by the question at hand and restrictions on your data. Now I agree that this design causes nesting complications, but it doesn't make it unreasonable to make assumptions about the independence of units within treatment groups and make adjustments to degrees of freedom accordingly. Your statement is essentially refuting the methodology of multi-level models and that entire body of work (as well as any non-experimental study.)",2010-10-07T16:45:27.370,1036,CC BY-SA 2.5,
4521,3383,0,"Notice, I said implies rather than dictates or controls.  Also notice the mention that you need to at least account for intra-tank correlation--a nod to a multi-level approach.  In fact, in this case, this is exactly what multi-level models will do--penalize the d.f. according to the degree of intra-class correlation, effectively adjusting the N downward to account for the structure.  By the way, I've got slides from George Casella's Experimental Design session at ASA that talk about this specific problem--randomizing tanks rather than fish--as an example of pseudo-replication.",2010-10-07T17:00:37.870,485,CC BY-SA 2.5,
4523,3383,0,"If you don't want to read the original article and the ensuing literature that I cited previously, take a look at this 1.5 page writeup from the statistical consulting unit at Cornell. It's pretty easy to follow. http://cscu.cornell.edu/news/statnews/stnews75.pdf",2010-10-07T17:24:08.353,485,CC BY-SA 2.5,
4525,3388,1,"@Gaetan I am not sure I follow you. How exactly does XLStat transform the 'text' values of cold, mild or hot into numerical values for the purpose of estimation? If there is a method that will let you estimate the effects of categorical variables without using dummy variables surely that should be independent of the software you use as there should be some underlying conceptual/model based logic.",2010-10-07T17:35:04.523,,CC BY-SA 2.5,user28
4527,3378,0,"gianluca:> Give us more quantitative idea about the size of memory, arrays and accuracy you want. It may well be that your question will be best answered @ stackoverflow though.",2010-10-07T18:28:03.013,603,CC BY-SA 2.5,
4528,3388,0,"@Gaetan I don't follow your point unless you consider that your ordinal variable is treated as a continuous one (this might make sense sometimes, although we clearly assume that the variable can inherit the property of an interval scale as pointed by @Skrikant). Usually, a variable with $k$ levels is represented in the design matrix as $k-1$ columns, and I think this is quite independent of the software used (surely, XLStat takes care of constructing the correct design matrix as R, SPSS or Stata does).",2010-10-07T18:42:08.173,930,CC BY-SA 2.5,
4530,3388,0,"@Srikant.  XLStat in its demo used an example a model where the dependent variable was probability of renewing a subscription.  And, one categorical variable within a single column had 6 different age ranges of subscribers.  Using a maximum likelihood algorithm, it can interpret each specific age range as a separate data set equivalent to a separate dummy variable in its own column.  When you choose that specific column, you just have to state it is a ""qualitative"" variable (instead of a ""quantitative"" one).  From everyone comments, I gather this is not something you can code in SPSS.",2010-10-07T19:01:26.587,1329,CC BY-SA 2.5,
4531,3382,0,"@tarrash Nah, this fourth point is rather too fresh to be tested enough. And as I wrote, normalization is either in aligning or in getting into spectra.",2010-10-07T19:10:21.303,,CC BY-SA 2.5,user88
4532,3388,1,"@Gatean Ok, in this case, the same can be done in SPSS (you have the choice between numerical/ordinal/nominal for each variable) -- then, the design matrix is constructed accordingly.",2010-10-07T19:21:50.903,930,CC BY-SA 2.5,
4533,3388,2,"@Gaetan @chl To summarize my understanding: The features of SPSS and XLStat whereby you can specify the measurement scale (nominal, ordinal etc) decreases the data file size. However, in both instances, the software uses the correct coding scheme (e.g., expand a nominal variable with J categories into J-1 dummy variables) as part of the estimation process in the background. Would that be a fair assessment of the situation?",2010-10-07T19:32:54.027,,CC BY-SA 2.5,user28
4534,3388,0,@Skrikant It seems you are summarizing the situation much better than me!,2010-10-07T19:49:18.240,930,CC BY-SA 2.5,
4535,3397,2,(+1) You are nicely echoing the discussion on Medstats about the need to keep a record of data edit and analysis (http://j.mp/dAyGGY)! Thx.,2010-10-07T20:01:03.863,930,CC BY-SA 2.5,
4536,3383,0,"@Brett : I did read the article and I understand what you're getting at, but this is not an ecological experiment. Following Hurlbert, each fish should have its own tank, and the water in these tanks should not come from the same source. But the experiment done here is not equivalent to the ecological studies he describes. Plus, this approach is impossible if you can't collect water in 192 different lakes... Furthermore, his paper is not a statistical result, but a logical -and in a number of cases valid- argument that not everybody agrees upon.",2010-10-07T20:11:23.440,1124,CC BY-SA 2.5,
4537,3383,0,"Ok, how about Casella in hist Statistical Design of Experiments Book.  Here's a link the relevant page in Google books.  http://books.google.com/books?id=sqnbSUtryVAC&pg=PA5&lpg=PA5&dq=casella+pseudoreplication&source=bl&ots=634YgjM_h7&sig=hf8C_vWd03hELis6bC-nxyltTWA&hl=en&ei=3ymuTOeDJdT-nAfis4DuBQ&sa=X&oi=book_result&ct=result&resnum=2&ved=0CBYQ6AEwAQ#v=onepage&q&f=false",2010-10-07T20:14:30.780,485,CC BY-SA 2.5,
4538,3396,0,Thanks for your help!!! Do you know if anything of the sort exists for Matlab by any chance? I am not that familiar with R..,2010-10-07T20:15:00.437,,CC BY-SA 2.5,BobJones
4539,3397,0,"Saving your ""work and blind alleys"" isn't any harder to do with Excel than with R.  It's just a matter of actually doing it.  The main problem with Excel is related to its strength: it's all too easy to change something inadvertently.  But for EDA--the focus of the OP--we rarely if ever save everything we do.  EDA, after all, is supposed to be *interactive.*",2010-10-07T20:18:51.347,919,CC BY-SA 2.5,
4540,3383,0,"@Brett : As I said, it is a point of view. One can easily argument that every fish has its own physiology. Biologically spoken, adding the weights of all fish in 1 tank doesn't even make sense, as you have males and females, different age groups and the likes. If I would have done the experiment, I would have marked the fish so it would be a truly repeated design. But having 4 seperate tanks filled with exactly the same water and given exactly the same treatment, does not mean for me that I suddenly have more degrees of freedom. It merely means I have split up 1 tank in 4.",2010-10-07T20:19:45.760,1124,CC BY-SA 2.5,
4541,3396,0,"@BobJones: I am unfamiliar with MatLab but quite familiar with a lot of the geostatistical software out there; I haven't run across anything specifically mentioning MatLab.  It's not hard to create an empirical variogram, though: it involves binning the squared differences of values associated with all distinct pairs of (X,Y) points, summarizing the values by bin, and graphing those summaries.  It's so easy, though, to learn to import a simple dataset (your X, Y, Z values) and call an R routine that you could be up and running quickly if you want.",2010-10-07T20:25:34.460,919,CC BY-SA 2.5,
4542,3383,0,"@Brett : the main idea behind Hurlbert, is that you can't distinguish between the effect of ""tank"" and the effect of ""treatment"" theoretically spoken. Practically, it's proven in the lab that the used procedures do not cause a ""tank"" effect beyond natural variation between fish. Hence... edit : for the record, I do appreciate your input, it makes me think about the setup and it gives me something to tell the lab people not to repeat this setup any more in their life :-)",2010-10-07T20:31:03.667,1124,CC BY-SA 2.5,
4543,3398,7,@whuber A nice and handy overview of pros and cons!,2010-10-07T20:32:22.593,930,CC BY-SA 2.5,
4544,3388,0,"@Skrikant, you are summarizing the situation correctly.  I may have confused everyone with an earlier comment whereby I used ""nominal"" incorrectly.  I thought this adjective related to real numbers so to speak.  I realize that nominal means just a label which is what it should be when dealing with qualitative categorical variables.",2010-10-07T20:36:32.897,1329,CC BY-SA 2.5,
4545,3394,0,"I like the nuance about ""different levels of discussion.""",2010-10-07T20:39:50.200,919,CC BY-SA 2.5,
4546,3398,5,"+1 nice and balanced.  I especially like the point about ""immediacy of interacting directly"" which I think is Excel's (or really, the spreadsheet's) biggest selling point.  Declarative programming for the masses -- which explains why some people think that 80% of the world's business logic is written in Excel (worth pointing out to programmers and statisticians who argue about R v SAS or Java v C++, etc).",2010-10-07T20:42:44.387,251,CC BY-SA 2.5,
4547,3386,3,"This is just a partial answer: even when you create the dummies explicitly (rather than using the software's implicit capabilities), keep them together in all analyses.  In particular, they should all enter together and all leave together in a stepwise regression, with the p-value computed appropriately for the total number of variables involved.  (This is Hosmer & Lemeshow's recommendation, anyway, and it makes a lot of sense.)",2010-10-07T20:53:03.170,919,CC BY-SA 2.5,
4549,3398,2,I heard that Microsoft hired some numerical analysts several years ago to fix the broken functions in Excel.  Do you know whether the problems with Excel are still there in the 2007 or 2010 versions?,2010-10-08T00:44:30.780,319,CC BY-SA 2.5,
4550,3398,0,"John, the McCullough and Heiser paper that Carlos references addresses Excel 2007.  I will not adopt Excel 2010 for various reasons, so I haven't had the opportunity to test it.",2010-10-08T02:29:18.043,919,CC BY-SA 2.5,
4551,3399,2,Thanks for the additional points and for sharing your perspective.,2010-10-08T02:31:07.517,919,CC BY-SA 2.5,
4552,3403,4,"@ars: Everything is correct and nicely stated.  But one thing seems to be missing: the standard deviation of the ""best approximation"" j/n depends on the *true* proportion of redheads, not the estimated one.  The problem, of course, is that we don't know the true proportion.  But the fact remains that the standard error does not actually equal the standard deviation of the approximation except when the estimate happens to be exactly correct.  I know you don't need reminding of this subtlety, nor will most readers, but it's rather relevant to the original question.",2010-10-08T02:39:29.737,919,CC BY-SA 2.5,
4553,3405,8,"This is far and away the aspect of Excel that infuriates me the most.  Data storage needs explicit data types, not formatting.",2010-10-08T02:45:13.517,71,CC BY-SA 2.5,
4554,3404,0,I fully agree with you.  An observational study may be good to uncover some associations that in turn one can test using a much more rigorous framework (randomized trial as you suggest).,2010-10-08T03:42:40.613,1329,CC BY-SA 2.5,
4555,3405,5,"Actually, this is something about MS software in general that annoys me: it changes your input into what it believes you actually meant, and you usually don't even see it happening.",2010-10-08T04:17:49.740,666,CC BY-SA 2.5,
4556,3405,0,@csgillespie (+1) Good to hear of Excel from this perspective!,2010-10-08T06:15:51.263,930,CC BY-SA 2.5,
4558,3397,1,"it is possible to keep a reproducable record of your methods if you do it in VB, but the GUI focus of Excel doesn't encourage that behaviour.",2010-10-08T10:06:19.590,229,CC BY-SA 2.5,
4559,3381,0,"Correct me, conventional KDE uses *all* the input data points for *each* estimate(x) -- quite unpractical for many points. I'm looking for ways to combine a) get some nearby points (a general problem), b) weight the values from this small sample",2010-10-08T10:54:06.927,557,CC BY-SA 2.5,
4560,3396,0,Ok cool. I have played around with R a bit so maybe I'll try to combine both somehow. Thanks again for your help.,2010-10-08T12:17:07.883,,CC BY-SA 2.5,BobJones
4561,3412,0,"Your models are not nested, what would be the rationale for using an LRT between the two?",2010-10-08T12:52:42.560,930,CC BY-SA 2.5,
4563,3405,6,"My favorite error occurred when Excel used to quietly truncate fields during export to other formats.  In a file of pesticide concentrations in soil, it converted a value of 1,000,050 (extraordinarily toxic) to 50 (almost inconsequential) by clipping off the initial digit!",2010-10-08T13:04:25.073,919,CC BY-SA 2.5,
4566,3414,0,Thank you! Do you know any Matlab / Mathematica based packages?,2010-10-08T14:12:08.797,1250,CC BY-SA 2.5,
4567,3414,1,"No, I'm afraid not.  But R is fully open source so all the algorithms above will be very transparent if you want to re-implement.",2010-10-08T14:20:03.580,5,CC BY-SA 2.5,
4568,3416,1,I was thinking that this wouldn't be appropriate because the two would actually be highly correlated.  As the center is far away so will the near tend to be.,2010-10-08T14:34:55.313,601,CC BY-SA 2.5,
4569,3416,0,@John Good point.,2010-10-08T14:41:46.183,,CC BY-SA 2.5,user28
4570,3414,2,and if you google for matlab hurst exponent you'll find several.,2010-10-08T14:42:09.200,247,CC BY-SA 2.5,
4571,3414,0,First result from google: [Hurst Exponential](http://www.mathworks.com/matlabcentral/fileexchange/9842),2010-10-08T14:59:39.040,1499,CC BY-SA 2.5,
4572,3416,0,I think your point is good as well... I'm actually not sure it matters.  I know it's high but under 0.8... still analyzable.,2010-10-08T15:03:32.863,601,CC BY-SA 2.5,
4573,3417,1,"I would probably add, that the difference between regions in terms of occaisonal smokers is largely restricted to the second axis of variation and as we can see in the stats this is such as small component as to not really be worth interpreting. The structure in the data is really the contrast between BC and the other regions in terms of daily smokers and former smokers.",2010-10-08T16:25:56.173,1390,CC BY-SA 2.5,
4574,3403,0,"@whuber: This clarification left me a bit confused. Given a $j$ and an $n$, what would be the standard error, described by $j$ and $n$? (In contrast to being dependent on the *true* proportion of redheads, which we can't know.)",2010-10-08T16:42:05.280,5793,CC BY-SA 2.5,
4575,3403,2,"@cool-RR: ars is correct about the standard error.  The point is that the standard error itself is an estimate of how accurate the statistic j/n estimates the true proportion.  For example, suppose 10% of all people are redheads.  Then in many cases it can happen that j=0 when n=10.  You would obtain an SE of Sqrt(0(1-0)/10) = 0.  This obviously underestimates the actual precision of your statistic p = j/n = 0/10.  The true precision is Sqrt(0.10(1-0.90)/n), even though you don't know that!",2010-10-08T17:19:14.910,919,CC BY-SA 2.5,
4577,3396,1,@BobJones: You can see http://www.mathworks.com/matlabcentral/fileexchange/20355 for variograms or http://mgstat.sourceforge.net/ for a full-out matlab geostatistics toolkit,2010-10-08T17:29:22.580,900,CC BY-SA 2.5,
4578,3395,0,I asked a similar question that you might find useful: http://stats.stackexchange.com/questions/3199/clustering-of-a-matrix-homogeneity-measurement,2010-10-08T17:32:34.273,900,CC BY-SA 2.5,
4581,153,8,"John Tukey wrote otherwise (back in 1960) in a monograph ""Data Analysis and Behavioral Science"" (published in Collected Works v. III).  One result he obtained is that if you're getting better than about 10% test-retest agreement, your scale isn't narrow enough!",2010-10-08T20:17:35.517,919,CC BY-SA 2.5,
4582,663,2,"Michael Lavine is a clear lecturer and thinker, so I have little doubt his book is worth a look.",2010-10-08T20:26:34.293,919,CC BY-SA 2.5,
4583,3403,0,"Again: I am interested in what I *can* know, not in what I *can't* know. Let's take your example where $j=0$ and $n=10$. The most likely proportion of redheads is 0%, but there's good chance it's 2% or 5% or 10%. So my question is: Given that $j=0$ and $n=10$, what is the probability distribution function of the proportion of redheads, *from the information that I know, not the information that I don't know?*",2010-10-08T20:50:22.417,5793,CC BY-SA 2.5,
4586,3403,1,"@cool-RR: for small samples, use the Agresti-Coull interval specified in the Wikipedia link on confidence intervals.  Based on your observations, you will obtain a 95% interval for estimate.  Then, what you will know, based on what you observed, is inherent in the definition of a 95% CI.",2010-10-08T21:26:40.960,251,CC BY-SA 2.5,
4587,3424,0,"Yeah, I thought about using a log scale. Unfortunately, I don't think our selected charting package (highcharts) supports it. Also, the data is such that even with a log scale it's still pretty extreme. I guess I was hoping there'd be some obscure viz type that I haven't heard of. :)",2010-10-08T21:31:18.143,1531,CC BY-SA 2.5,
4588,3368,0,@user1513 By references I mean links to more information.,2010-10-08T21:31:34.307,5,CC BY-SA 2.5,
4589,3424,1,"@sprugman I wouldn't accept this just yet.  There are plenty of other possible answers.  But I do think that scaling the data so that it's more linear between small and large values would be a good approach (you don't have to use a log scale, it can be something else...).  And scaling can be done on the data before it is plotted, so you can do that with any package.",2010-10-08T21:33:22.120,5,CC BY-SA 2.5,
4590,3424,0,"ok, shane, you talked me out of accepting your answer. Thanks. :)",2010-10-08T21:43:10.357,1531,CC BY-SA 2.5,
4591,3296,0,It is not clear to me: are the independent (i.e. right hand side) variable continuous or discrete ?,2010-10-08T22:02:54.020,603,CC BY-SA 2.5,
4592,3427,8,"+1. Also, in case it's helpful, the term to google when comparing strings is ""edit distance"": http://en.wikipedia.org/wiki/Edit_distance",2010-10-08T22:05:39.320,251,CC BY-SA 2.5,
4593,3427,0,"@ars:> thanks, that's a handy list to feed unto a R search engine and see what comes out!",2010-10-08T22:09:46.757,603,CC BY-SA 2.5,
4594,3424,3,"+1, log scale sounds good to me -- @sprugman: couldn't you just preprocess your data and log-transform it before dispatching it to your charting routine?",2010-10-08T22:22:45.080,251,CC BY-SA 2.5,
4596,663,3,"I'm teaching out that book this semester.  It may be a good *statistics* book, but I'm having second thoughts regarding its light probability content.",2010-10-09T01:21:48.787,319,CC BY-SA 2.5,
4598,3422,0,"I didn't specify that I had been fitting with REML = FALSE.  I'm still in a bit of a quandry though... The AIC gives me a measurement of the whole likelihood including the random effects.  That's a large component. And of course, the AIC's are extremely unlikely to be exactly the same.  Therefore, it seems unwise to just select the larger value with out some analytic way of telling how much larger it is.",2010-10-09T07:49:55.163,601,CC BY-SA 2.5,
4599,3422,0,"@John This lecture highlights interesting point about REML vs. ML and AIC (and points to what you said, John), http://j.mp/bhUVNt. Bolker's review of GLMM is also worth to take a look at: http://j.mp/cAepqA.",2010-10-09T08:29:22.517,930,CC BY-SA 2.5,
4600,3436,2,"Agree with last para. The Gaussian kernel uses all the data points but other commonly-used kernels (Epanechnikov, biweight, cosine, Parzen, triangular, ...) are finite-width, i.e. they are defined to be zero outside a finite interval. R's density() function defaults to Gaussian but Stata's -kdensity- command defaults to Epanechnikov.",2010-10-09T09:38:31.357,449,CC BY-SA 2.5,
4603,3433,0,Hi Brandon - I added a sample of the data.  Thanks!,2010-10-09T13:17:59.283,253,CC BY-SA 2.5,
4604,3440,0,(I have version 7.) I have no problem loading the Statistics package. But what's the function in there called? Because I get the impression that this `Quantile` line will do the calculation manually instead of using a formula.,2010-10-09T14:13:25.680,5793,CC BY-SA 2.5,
4605,346,2,@Srikant:> it's a pretty active area of research in statistics :) The solution closest to the lower theoretical bounds in terms of storage involve some pretty clever probability constructs as well. All in all i was surprised when i first looked unto it a couple of month ago; there is more stats here than meets the eye.,2010-10-09T14:21:03.117,603,CC BY-SA 2.5,
4606,3439,1,"Quantile *vs.* percentile (it's merely a matter of terminology), http://j.mp/dsYz9z.",2010-10-09T14:22:20.277,930,CC BY-SA 2.5,
4607,3440,0,"Evaluate it with symbolic parameters (i.e. don't assign values to `mu`, `sigma`, and `q`); you should get an expression involving the inverse error function.",2010-10-09T14:24:37.310,830,CC BY-SA 2.5,
4608,3425,2,"Hi Tal:> Given that these seems to be typo-free scientific names, i would try the Levenshtein metric first (in the context of a 92-by-55 distance matrix) and see how it comes out.",2010-10-09T14:31:27.313,603,CC BY-SA 2.5,
4609,3439,1,"While we are in, in R Wald-adjusted CIs (e.g. Agresti-Coull) are available in the `PropCIs` package. Wilson's method is the default in `Hmisc::binconf` (as suggested by Agresti and Coull).",2010-10-09T14:36:02.540,930,CC BY-SA 2.5,
4610,3424,1,"I agree with @ars -- log the data first, then plot and adjust the axis labels.",2010-10-09T15:41:05.537,,CC BY-SA 2.5,user88
4612,2371,2,"Looks interesting, thanks.  I have similar problem to the OP trying penalized + coxph on a big data set, wonder if this helps.",2010-10-09T16:48:11.853,251,CC BY-SA 2.5,
4614,3445,0,"@onestop This is a software for population genetics, http://j.mp/a75Z4Y",2010-10-09T18:07:00.527,930,CC BY-SA 2.5,
4618,3450,0,Good that you cite the whole book from Pearl.,2010-10-09T19:26:33.330,930,CC BY-SA 2.5,
4619,1957,0,"I like this ""original"" approach (wrt. other entries). Still, I always find difficult to explain why bootstrap works in practice...",2010-10-09T19:38:21.807,930,CC BY-SA 2.5,
4620,3412,0,restated things as per your comment,2010-10-09T19:56:16.933,601,CC BY-SA 2.5,
4621,3427,2,Levenshtein edit distance is implemented as part of the base package via agrep(),2010-10-09T20:05:57.667,776,CC BY-SA 2.5,
4623,3453,0,"@ccgillespie:> i think my question may have been poorly worded. In the package i see (GPUtools, magma) double precision seems to be used as standard (with the loss of performance you describe). I was wondering why single precision is not offered as an option.",2010-10-09T20:46:34.330,603,CC BY-SA 2.5,
4624,3453,0,@kwak: The double precision values must be converted to single precision by the wrapper. The wrapper was just trying to be helpful.,2010-10-09T20:51:10.617,8,CC BY-SA 2.5,
4625,3453,0,"@ccgillespie:> yes, but it seems the wrapper comes with  performance costs exceeding the factor 2 you cite (again, correct me if i'm wrong on this) and in some cases no tangible benefits (i can think of many application in stat were SP FP arithmetics would be okay). I was wondering whether it makes sense to ask for an option to switch off said wrapper.",2010-10-09T20:54:58.997,603,CC BY-SA 2.5,
4626,3453,2,"@kwak: Glancing at the GPUtools help file, it seems that `useSingle=TRUE` seems to be the default in the functions.  Am I missing something here?",2010-10-09T20:56:27.733,251,CC BY-SA 2.5,
4627,3453,0,"@csgillespie: Remember, until relatively recently most nvidia cards simply **couldn't** do double precision computation. The factor of 2 hit is what I observed using raw C/CUDA code. Having a python/R wrapper may make this worst.",2010-10-09T21:08:08.437,8,CC BY-SA 2.5,
4628,3453,0,"@ars:> argh. I didn't have a NVdia cart at home, so i didn't bother to read the manual. The white papers (and the GPU+R site) are all about double precision arithmetic. Would you be so kind to post you comment as a response so i can close the question ?",2010-10-09T22:36:36.610,603,CC BY-SA 2.5,
4629,3381,7,"If you use a kernel with a finite support (e.g., the quadratic Epanechnikov kernel), then only nearby points contribute anything to a kde. Even with a Gaussian kernel, the contribution of points more than 3 bandwidths from x is negligible. I would definitely use kde rather than your proposed approach.",2010-10-09T22:39:23.083,159,CC BY-SA 2.5,
4630,3453,0,"@kwak: ah, got it.  I posted a separate answer.",2010-10-09T22:57:15.440,251,CC BY-SA 2.5,
4632,3453,0,"As noted above, the Fermi cards support double precision.  The GTX 2xx series also support double precision -- but the Fermi cards GTX 4xx have approximated twice the double precision performance of the 2xx series.  __However__, the Tesla 20xx product is four times faster still.  Here is a [discussion](http://www.vizworld.com/2010/04/geforce-gtx-480-18supthsup-double-precision-performance/) about the crippled double perf. of GTX 480.  [Specs for Tesla 20xx](http://www.nvidia.com/object/product_tesla_C2050_C2070_us.html) (515 GFlops DP Perf)",2010-10-10T00:12:41.527,1499,CC BY-SA 2.5,
4633,3453,0,"[Specs for GTX 480](http://www.nvidia.com/object/product_geforce_gtx_480_us.html) -- 168 GFlops Peak DP Perf  [Specs for GTX 285](http://www.nvidia.com/object/product_geforce_gtx_285_us.html) -- 88.5 GFlops Peak DP Perf.  The performance numbers are not listed on the nvidia website for the GTX products, I found them [here.](http://techreport.com/articles.x/18682)",2010-10-10T00:18:10.857,1499,CC BY-SA 2.5,
4635,3454,0,"@kwak: I find the answer above helpful, but it really doesn't answer the question posed - ""is single precision so bad?"" Perhaps you should reword your question?",2010-10-10T07:57:45.337,8,CC BY-SA 2.5,
4636,3433,0,Hi Brandon - your solution worked great - thank you.,2010-10-10T08:23:07.410,253,CC BY-SA 2.5,
4637,3435,0,Thank you Andy - I will have a look at it in the future.,2010-10-10T08:23:27.813,253,CC BY-SA 2.5,
4639,3455,0,"Can you elaborate a bit? It seems some iterative algorithm (matrix invert, QR decomposition) seem to work well. I'm also curious as to whether the inaccuracy of SP becomes more of a problem for operations involving larger arrays.",2010-10-10T08:23:43.977,603,CC BY-SA 2.5,
4640,3427,0,Great answer Kwak - I will have a look at it in the future!,2010-10-10T08:24:00.807,253,CC BY-SA 2.5,
4641,3427,0,"Personally, I feel that this is a more complete answer to Tal's question. +1 for pointing our RecordLinkage - I'll definitely have to try that out.",2010-10-10T08:40:15.807,776,CC BY-SA 2.5,
4642,3433,0,+1 for the link to the previous question on the subject in S.E. (thaks for the pointer to agrep()).,2010-10-10T08:50:58.123,603,CC BY-SA 2.5,
4643,3296,0,@kwak the variables are continuous (fixed the question),2010-10-10T09:43:57.340,1496,CC BY-SA 2.5,
4645,3463,3,Are the time series both stationnary ? www.econ.ohio-state.edu/dejong/note1.pdf,2010-10-10T11:54:40.560,603,CC BY-SA 2.5,
4647,3458,5,"Don't bother with neural networks, this is an obsolete technology.",2010-10-10T13:22:11.813,,CC BY-SA 2.5,user88
4648,3466,1,"When you say ""condition"", do you mean group assignment?",2010-10-10T13:49:45.367,561,CC BY-SA 2.5,
4649,3464,6,"I would have given almost the same answer but here it is and well stated, too.  Let me add just two things based on experience.  First, I have found it's almost always worthwhile re-running any analysis I possibly can: it serves to check my understanding and, more often than one might expect, it exposes errors in the paper itself.  Second, it's essential to locate the key references and to find references of your own by searching the Web for phrases in the paper.  A substantial number of contributions recently are (auto-)plagiarisms or bald attempts to get another paper out of old work.",2010-10-10T15:28:07.633,919,CC BY-SA 2.5,
4651,3446,2,"I confess to being mystified by this, despite struggling several times to make sense of it: is there a question here?  ""So bad"" is vague and has no referent.  What exactly are you seeking to understand or find out?",2010-10-10T16:19:55.700,919,CC BY-SA 2.5,
4652,3462,0,(+1) Nice concrete recommendation! I was about to suggest the use of the `mvoutlier` package (which relies on mahalanobis distance and displays outlying obs. in an interactive display).,2010-10-10T16:27:41.813,930,CC BY-SA 2.5,
4655,3465,2,"kernlab also uses libsvm for the optimization, so there isn't a big difference in that sense (although it is much more flexible).",2010-10-10T16:44:26.677,5,CC BY-SA 2.5,
4662,3461,0,"@Tal Here is a fair (or I think so) review of SVM *vs.* RFs: A comprehensive comparison of random forests and support vector machines for microarray-based cancer classification, http://j.mp/ab7U8V. I also prefer `kernlab` to `e1071`.",2010-10-10T19:25:28.780,930,CC BY-SA 2.5,
4663,3307,2,"Can you elaborate a little bit about the purpose/context of this particular analysis, or why do you seek an alternative measure of association?",2010-10-10T19:33:06.003,930,CC BY-SA 2.5,
4666,3459,2,+1 Great answer.  I also agree with the caret recommendation.,2010-10-10T21:00:48.300,5,CC BY-SA 2.5,
4670,3455,0,"There are two parts to it: 1) What does the data represent? 2) How do you process the data?  If you're looking at thousands of points of data from a medical study, single precision would likely be plenty for quantifying patient wellness, and I doubt you would ever need double.  Geometry, on the other hand, could require either single or double precision depending on your scale & zoom.  Calculating the trajectory of a probe to Saturn would always require doubles, as even small errors could drastically effect the result.  You need to look at the data and decide what your tolerances are.",2010-10-10T22:55:50.203,1539,CC BY-SA 2.5,
4672,3466,1,@propofol: yes. apologies if my language is not clear.,2010-10-11T00:47:50.343,183,CC BY-SA 2.5,
4677,3464,0,"I've added an additional question. If it's not too much hassle, would you update your answer?",2010-10-11T08:29:16.833,8,CC BY-SA 2.5,
4678,3478,0,"I've added an additional question. If it's not too much hassle, would you update your answer?",2010-10-11T08:29:52.717,8,CC BY-SA 2.5,
4679,3455,1,It will depend on the numerical stability of the algorithm you are using and how well-conditioned the problem is. Remember that double precision gives you access to smaller numbers as well as larger ones.,2010-10-11T11:18:17.260,229,CC BY-SA 2.5,
4680,3462,0,"@chl:> actually, i wonder whether your solution would not ultimately be better. I think you should post it, either for the benefit of bgbg or that a future readers (though it is mentioned in the paper i cited). The main point, in my opinion, is to outline the fact that in this setting (binary dependent, continuous independent) it is quiet satisfactory to look for the outliers on the design space.",2010-10-11T11:43:49.863,603,CC BY-SA 2.5,
4681,3462,0,I would be equally happy with this second option added to your own response.,2010-10-11T12:01:08.807,930,CC BY-SA 2.5,
4682,3448,0,"Thanks for your detailed answer!  Actually, I do not know how to convert my binary data into genotype data? Do you have any idea about that?",2010-10-11T12:05:59.327,,CC BY-SA 2.5,jacki
4683,3448,0,"@jacki Well, actually it is not very clear to me what's the repeated lines of binary value mean: Does it stand for an haplotype block (1=mutant, 0=ancestral allele) on different chromosomes, simulated for different individuals (each new line starting with //), because in this case the haplotype are of varying size... Is it possible to compute haplotype frequencies easily from your software? Maybe I need to reread some papers and come back to you, but if you have any further information (how do we read the output in terms of haplotype, allele marker, etc.), pls update your original question.",2010-10-11T12:28:49.873,930,CC BY-SA 2.5,
4685,3454,0,"@csgellespie: you are totally correct. I will reword this question so as it can be used by future readers. Indeed, the wording was particularly poor.",2010-10-11T14:01:49.117,603,CC BY-SA 2.5,
4686,3446,0,"@Whuber:> My question was poorly worded. It probably was due to it being borne out of ignorance: i had read some white papers on use of GPU , (although, unfortunately it turns out, not the R command reference of GPUtools) and could not understand why all the tests were carried out in DP. I will re-phrase the question (and the title).",2010-10-11T14:02:30.793,603,CC BY-SA 2.5,
4689,3474,1,"@kwak I'm not sure why changing the title was necessary...the ""homework"" tag should be sufficient.",2010-10-11T14:10:03.100,5,CC BY-SA 2.5,
4690,3474,0,@Shane:> ok changed back to the original.,2010-10-11T15:18:48.490,603,CC BY-SA 2.5,
4691,3464,1,"@csgillespie: I guess I'm too early in my career to answer that, as I probably do not get asked to review as many papers as someone with more experience then me. I think @whuber answer makes a lot of sense though.",2010-10-11T16:03:25.597,582,CC BY-SA 2.5,
4693,3381,0,"@Rob, thanks; I'll try to clarify the question, and split off ""adaptive histograms"" as a separate question.",2010-10-11T16:40:54.847,557,CC BY-SA 2.5,
4694,3448,0,"@chl:  I am trying to do programming in R for haplotype frequencies, for number of segregating sites, etc. 
          SNPs
Ind1   0 0 0 0 1
Ind2   1 0 1 0 0
Ind3   0 1 0 1 0
Ind4   1 1 1 0 0

In above data, each column-wise is SNP and row-wise is individual(Ind1, Ind2,...). As I said before it was simulated on whole genome. The position is normalize random variable from uniform distribution [0,1]. 
Thanks for your help!",2010-10-11T17:28:14.557,,CC BY-SA 2.5,jacki
4695,3448,0,"@jacki If I read it correctly, 00001 means 5 markers/alleles with one variant, and you have $n$ such vectors with varying locations for SNP (it was not clear `ms` manual)?",2010-10-11T17:36:18.367,930,CC BY-SA 2.5,
4696,3487,0,"I've heard about World Programming System (WPS), http://j.mp/9GjesM. Don't know how comparable it is, though. There was an old GNU project, called Dap (http://j.mp/955DqP), but it never reaches a mature state. In biomedical research, SAS is still considered as THE reference software, although FDA is on her way to (progressively) accept R-based analyses for clinical trials.",2010-10-11T17:57:20.487,930,CC BY-SA 2.5,
4697,3487,0,Interesting.  I've never heard of WPS.  I'll check it out - it mentions the ability to read SAS datasets...,2010-10-11T18:01:22.710,1499,CC BY-SA 2.5,
4698,3491,0,"Thanks for your answer. Yes, the distribution does represent the scores for the population that should be rejection by my biometric system but are instead authenticated. However, I'm not sure how to choose my threshold in this case, so I need to determine that before I can proceed.",2010-10-11T18:04:16.200,1224,CC BY-SA 2.5,
4700,3491,2,"I'd bet that the threshold would be a design parameter set by the client.  Basically it will come down to which is more important for a specific client Type I or Type II error (probably Type II - Falsely accepting someone who should've been denied access).  With a knowledge of what the client wants, you could then state that under a specific threshold, the probability of falsely admitting someone who should be denied access is 1 / 10,000,000 or something.",2010-10-11T18:09:49.417,1499,CC BY-SA 2.5,
4701,3490,0,"Is getting certified actually gold?  I've heard lots of mixed reports about the impact of technical certifications in general, but I don't know how that applies to SAS.  Can you expand on that a little bit?",2010-10-11T18:09:59.077,71,CC BY-SA 2.5,
4702,3478,1,"Interestingly, most researchers in genetics studies are encouraged or pleased (it depends on the review) to make data available. I also remind of @csgillepsie nice answer about *reproducible research*, http://stats.stackexchange.com/questions/1980/complete-substantive-examples-of-reproducible-research-using-r/1993#1993",2010-10-11T18:10:25.740,930,CC BY-SA 2.5,
4703,3491,0,"I could however envision a client who must employ a certain biometric authentication device due to government law or otherwise -- and said individuals might hate when their scanner goes wonky and won't give anyone access -- hence, they might care more about Type I error - not admitting someone who __should be allowed__ (because they probably have six other security measures and this one is only a deterrent for *wandering eyes...*",2010-10-11T18:12:16.453,1499,CC BY-SA 2.5,
4704,3490,1,"I've also heard mixed reports -- I taught the SAS class at PSU for 4 semesters, had a bunch of student report that companies were happy that they'd simply had exposure to SAS.  Several students mentioned being drilled on *which* Procs they knew during interviews, but mostly the feeling I got was that employer want a familiarity with SAS and accept that you'll need to learn specific procedures or methods after you've been hired.  Within the programming community, employer will sometimes view Certs as a red flag -- a waste of time, or a book knowledge over practical/experience-oriented info.",2010-10-11T18:20:29.867,1499,CC BY-SA 2.5,
4705,3490,1,Here's a link to a certification discussion over on [Not Programming Related](http://programmers.stackexchange.com/questions/44/are-certifications-worth-it) -- also a Stack Exchange website.,2010-10-11T18:21:03.340,1499,CC BY-SA 2.5,
4706,3491,0,"So, does the threshold need to be the minimum score that needs to be obtained in order to be authenticated?",2010-10-11T18:53:02.207,1224,CC BY-SA 2.5,
4707,3491,0,"I think so, but this is highly dependent on your implementation.",2010-10-11T18:56:44.357,1499,CC BY-SA 2.5,
4709,3483,1,@chl of all the people here you should be the least concerned about voting enough!,2010-10-11T20:06:08.107,919,CC BY-SA 2.5,
4710,1151,3,I upvoted a long ago but re-reading your answer it remind me that I always liked *Plane Answers to Complex Questions* from Christensen (http://j.mp/atRp9w).,2010-10-11T21:14:48.740,930,CC BY-SA 2.5,
4711,3491,1,"@rohanbk Did you look at Signal Detection Theory or ROC curve analysis (http://j.mp/b49wDl in French, http://j.mp/aTjobH in English)? Determining the cut-off is exactly finding the best compromise between sensitivity and 1-specificity, as M. Tibbits said.",2010-10-11T21:23:54.363,930,CC BY-SA 2.5,
4712,3455,1,"Not necessarily smaller or larger numbers; remember, we're dealing with floating point.  Rather, it lets you use larger and smaller numbers in relation to each other, while preserving the significant digits.",2010-10-11T21:54:47.987,1539,CC BY-SA 2.5,
4713,3491,0,Excellent link @chl.  My brain balked on that one.,2010-10-11T21:57:56.630,1499,CC BY-SA 2.5,
4714,3481,0,"Ahh, it's sort of coming back to me from multivariable calculus. I remember doing problems like that. How do I find the radius as a function of the remaining variable? It still seems like I'm going to have some sort of monster integral left over.",2010-10-11T22:13:13.907,1545,CC BY-SA 2.5,
4716,3481,5,"Let the remaining variable be $y$.  Then $x^2 \le 1-y^2$ describes the region over which you have to integrate.  Evidently the radius equals $\sqrt{1 - y^2}$, whence the cross-sectional area equals $\pi (1 - y^2)/2.$  That's a pretty simple formula :-).  (Remember, the theme here is geometry, not calculus...)",2010-10-11T22:29:45.320,919,CC BY-SA 2.5,
4717,3481,0,"Oh, right. That crossed my mind, but it seemed too simple. I guess I was determined for it to be complicated. Thanks!",2010-10-11T22:40:12.100,1545,CC BY-SA 2.5,
4718,3481,0,I forgot to ask: how does c figure into this?,2010-10-11T22:49:48.457,1545,CC BY-SA 2.5,
4719,3448,0,@chl: Now it is clear to you? How we can covert this binary data into genotype data?,2010-10-12T00:42:20.273,,CC BY-SA 2.5,jacki
4720,1151,0,"@chl: cool, definitely going to check it out then.  :)",2010-10-12T02:30:51.990,251,CC BY-SA 2.5,
4721,3483,1,@chl: you set a high bar in every way! :) Maybe our first polystats project should be to set up some scripts to maintain and update a set of charts like these: http://meta.stats.stackexchange.com/questions/314/vote-early-vote-often/317#317,2010-10-12T02:44:02.130,251,CC BY-SA 2.5,
4722,3490,0,"I suppose my comment was stated with little backing evidence, and in retrospect, cannot be backed fully. I have seen a few jobs in the San Francisco Bay Area that, in the qualifications section, ask that the individual be familiar with using a data analytics package, and add that SAS Certification is recommended or highly recommended. It's very likely they are instead just looking for some way for you to show you are comfortable with SAS and not just putting it on your resume because it is ""one of the most common requirement is experience of SAS"".",2010-10-12T05:52:42.927,1118,CC BY-SA 2.5,
4723,3506,0,"I am sorry chl, I think we were almost answering this question together at the same time. Basically, you and me are pointing to the same fact.",2010-10-12T07:52:46.110,1307,CC BY-SA 2.5,
4724,3505,0,"Hi @suncoolsu, there are n=6 replicates in each of the 12 treatments. It was set up as an RCBD with three blocks with n=2 reps/treatment/block because there were three trays with for 24 samples each, but there was no block effect so I dropped it from my analysis. Sorry for the omission",2010-10-12T08:21:33.420,1381,CC BY-SA 2.5,
4725,3506,0,"@chi The effect of A was expected, while the effects of B and A*B were both main hypotheses. And I am going to show the effect sizes in a figure. Thank you for reminding me not to make the error of accepting Ho.",2010-10-12T08:32:55.220,1381,CC BY-SA 2.5,
4726,3505,0,"Hello @David, I will be a little uncomfortable with saying - ""there was no block effect"". Please see my updated reply for your answer.",2010-10-12T08:43:29.517,1307,CC BY-SA 2.5,
4727,3481,2,"In my opinion, Whuber's answer deserves to be upvoted for two reasons. First it answers the question asked, second as a model for how we could in the future handle (explicitly stated) homework questions: this type of answers actually contributes to the learning process and could be a better policy with respect to homework question than that adopted at MO/SO.",2010-10-12T08:50:32.957,603,CC BY-SA 2.5,
4728,3478,0,"@chl: yes, making data available very much depends on the discipline, and I would love to see more of this in ""mainstream"" psychology - I just can't recall having seen a single instance of a psych paper that actually did give out the data.",2010-10-12T09:42:04.303,1352,CC BY-SA 2.5,
4729,3504,4,"In papers, when interaction effects aren't significant I still mention in passing that I looked for them to stop reviewers asking about interaction effects. This is obviously different from ""accepting H_0"" and making a big thing about no effect.",2010-10-12T10:56:00.077,8,CC BY-SA 2.5,
4730,3513,0,"@wok Of note, the scikit.learn package also offers efficient implementation in Python for this kind of stuff.",2010-10-12T12:03:50.520,930,CC BY-SA 2.5,
4731,3461,2,"@chl I don't like this paper while it made from the SVM learning perspective -- making one repetition of a stochastic algorithm (RF) is just a junk; also appendix 2 shows how bad it may be to apply SVM workflow to RF. Yet I agree that almost always SVM  can be tuned to outperform RF because of the kernel trick (which plain RF does not have, while it doesn't mean it can't have it in general), but with exponentially growing optimization effort.",2010-10-12T12:13:13.917,,CC BY-SA 2.5,user88
4733,3515,0,"Ok, let me correct my question. How do you know if a binary variable follows the binomial distribution?",2010-10-12T12:42:15.150,,CC BY-SA 2.5,Roger
4734,3481,0,"@Jarrod: you can use c to determine the normalization constant for the marginal density.  Alternatively, forget c, derive the functional form of the marginal (that is, up to a constant factor), then integrate the marginal to find its normalizing constant.",2010-10-12T12:45:48.243,919,CC BY-SA 2.5,
4735,3515,1,"See http://en.wikipedia.org/wiki/Binomial_test . Talking as a moderator now: so can you please make a new, good question? You'll get a better answer and the wiki spirit of this site will be held.",2010-10-12T12:49:40.767,,CC BY-SA 2.5,user88
4736,3461,0,"@mbq Indeed, this a good point.",2010-10-12T13:03:05.217,930,CC BY-SA 2.5,
4737,3481,0,@Kwak: Thanks for pointing out there is a policy concerning homework questions!  I continued this discussion on meta at http://meta.stats.stackexchange.com/q/12/919 .,2010-10-12T13:12:55.210,919,CC BY-SA 2.5,
4738,3515,0,Sure. Let me do that.,2010-10-12T13:14:50.837,,CC BY-SA 2.5,Roger
4740,3517,1,(+1) The second point is very interesting.,2010-10-12T13:38:58.233,930,CC BY-SA 2.5,
4741,2739,0,"@Henrik Yes, as a base of rule-based classifiers.",2010-10-12T13:41:15.413,,CC BY-SA 2.5,user88
4742,3514,0,vote to close.  (but I'm below 500r),2010-10-12T13:47:57.743,1499,CC BY-SA 2.5,
4744,3496,0,"I suspect that the answer is domain specific (i.e., specific to spam filters). If you can calculate the components P(A|B) etc then you should be able to calculate the simpler P(A) as you stated. Or, perhaps the answer is related to pedagogy so that readers understand the relationship between P(A) and its decomposition in terms of P(A|B), P(B) etc.",2010-10-12T14:06:35.957,,CC BY-SA 2.5,user28
4745,3519,2,R is based on Scheme but is pretty good in pretending to the beginners that it is rather C based.,2010-10-12T14:31:24.267,,CC BY-SA 2.5,user88
4746,3519,2,"> I will not post this as an answer because there are many people more knowledgeable in these things than me. However, i  think that concerning speed you have to distinguish between R base and the mix good programmer/great packages. A good programmer can leverage some the tools in cran such as multicore, GPUtools&magma (soon but not yet usefull for MCMC), Rcpp,... to make a pretty fast code. I don't think matlab central has anything comparable to offer.",2010-10-12T14:56:49.033,603,CC BY-SA 2.5,
4747,3519,0,@kwak -- I probably should've posted my answer as a comment too.  Sorry about that.,2010-10-12T15:02:41.683,1499,CC BY-SA 2.5,
4749,3505,0,"Hi @suncoolsu, Thank you again for your response. re: 1. This is an interesting point, I will look at Venable's reference. re: 2. The blocking and replication within blocks was based on the fact that the trays held 24 samples, so I blocked them in case there was any effect of tray positon or time of measurement. re: 3. Thanks for again for reminding me not to incorrectly interpret my findings, e.g. by saying that there was no block effect. It is a bad habit. re: 5. I should have clarified that by 12 treatments, I meant the 12 permutations of three levels of A and four of B.",2010-10-12T15:28:00.363,1381,CC BY-SA 2.5,
4750,3519,2,"MT, that's crazy - your answer is great.  If anything, kwak should make his a proper answer, too.  Let those more knowledgeable vote or answer as they see fit.",2010-10-12T15:33:50.570,71,CC BY-SA 2.5,
4752,3523,13,(+1) The exchangeability assumption is also at the heart of permutation tests.,2010-10-12T15:53:46.153,930,CC BY-SA 2.5,
4754,3517,1,"@chl: Yes, it occurred to me as I was writing about the CLT that there could be problems with the variances converging to zero, for then the Lindeberg condition could be violated.  It's easy to see why the resulting limit distribution might be non-normal, for if (for example) the chance of observing a 1 approaches zero sufficiently rapidly, then the distribution of the mean might remain strongly skewed and never get close to normal.",2010-10-12T16:00:11.377,919,CC BY-SA 2.5,
4755,3521,0,"+1 for C++; while it is quite easy to embed C/C++ to R I often wrap my codes and run them inside R -- then it is nicer to pass parameters, do live visualisation and obviously analyse results without thinking of output file format.",2010-10-12T16:42:55.483,,CC BY-SA 2.5,user88
4756,3521,0,"well put; MC will eventually require one to move to C/C++. I do not have enough experience in R to comment, but have had a number of headaches using C/C++ with Matlab due to different versions of shared object libraries (under Linux) being pulled in by the Matlab executable than what I want to link with my code.",2010-10-12T16:54:29.177,795,CC BY-SA 2.5,
4757,3525,4,"I completely agree @Henrik.  If you're concerned about profiling, Matlab has an **excellent** profiling tool (even back in version 7.1!!).  Rprof on the otherhand leaves a lot to be desired.  I end up profiling by executing each command several times in a for loop and comparing the `system.time` difference amongst different versions.  [Here is an interesting case study](http://jeromyanglim.blogspot.com/2010/02/case-study-in-optimising-code-in-r.html)",2010-10-12T16:57:24.667,1499,CC BY-SA 2.5,
4758,3463,0,"@kwak: No, the series are both NOT stationary.",2010-10-12T17:49:19.967,1216,CC BY-SA 2.5,
4760,3503,0,"Hello Gaten, Thank you for looking into this. I am still not sure that I understand the logic. The full sample beta.hat is and estimate of the true beta. My sample.mean.beta.hat is an estimate of beta.hat is it not? Is the CLT argument that both beta.hat and sample.mean.beta.hat converge towards each other?",2010-10-12T18:24:59.633,,CC BY-SA 2.5,Joseph Rickert
4761,3507,0,"Hello suncoolsu. Yes, I think that the exchangeability assumption is crucial. Thank you for pointing that out. Do you know of any results on rates of convergence?",2010-10-12T18:29:47.580,,CC BY-SA 2.5,Joseph Rickert
4762,304,5,"A test of normality is usually too severe.  Often it suffices to obtain symmetrically distributed residuals.  (In practice, residuals tend to have strongly peaked distributions, partly as an artifact of estimation I suspect, and therefore will test out as ""significantly"" non-normal no matter how one re-expresses the data.)",2010-10-12T19:02:17.600,919,CC BY-SA 2.5,
4763,301,3,"@cgillespie: Concentrations, yes; but age?  That is strange.",2010-10-12T19:02:52.670,919,CC BY-SA 2.5,
4764,3526,1,Perhaps [this discussion](http://groups.google.co.bw/group/sci.stat.edu/browse_thread/thread/c247a85a83c5ae00) is relevant as -- it isn't often used because it is very conservative (much like [Scheffe's Method](http://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method))?,2010-10-12T19:06:18.953,1499,CC BY-SA 2.5,
4766,3495,0,"This does not give the exact answer, for the following reason: $\sqrt{\sum_i x_i} \ne \sum_i \sqrt{x_i}$. You are computing the former, while the OP wants the latter.",2010-10-12T19:45:10.130,795,CC BY-SA 2.5,
4767,3512,0,"Thanks. I am reading this and thinking about it. However, this would be a huge modification of the current algorithm.",2010-10-12T19:53:06.370,1351,CC BY-SA 2.5,
4768,3513,0,The coordinate descent algorithm is interesting. Thanks. Still thinking about it.,2010-10-12T19:53:46.397,1351,CC BY-SA 2.5,
4769,301,1,"@whuber: I suppose it's very data dependent, but the data sets I used, you would see a big difference between a 10 and 18 yr old, but a small difference between a 20 and 28 yr old. Even for young children the difference between a 0-1 yr old isn't the same as the difference between a 1-2.",2010-10-12T20:00:38.490,8,CC BY-SA 2.5,
4770,3507,1,"Hello Joseph, as with most (_not in general_) results in classical statistics: parametric bootstrap converges around rates $n^{\frac{1}{2}}$, where$n$ is the sample size. In your case $n$ corresponds to $M$, as you are taking average of $M$ bootstrap estimates. This is equivalent to the asymptotic normality (or _CLT_). The assumptions and the details of the result can be found in: Hall, P. 1988. Rate of Convergence in Bootstrap Approximations. Annals of Probability.",2010-10-12T20:02:07.850,1307,CC BY-SA 2.5,
4771,3505,0,"Hello @David, I understand your point of blocking on trays to remove the variation because of them. This is perfectly justified and you should block if you can. But what I am trying to point out is: as you said, there are 2 replicates for a particular block and treatment level (say: tray 1 had two replicates of first level for trt A) IMHO is _not correct_. If you really want to block for positions as well, probably a more efficient design like **latin square** or **split plot** designs may be the way to go. The choice of specific design depends on your resources, your parameter of intrst. etc.",2010-10-12T20:14:35.550,1307,CC BY-SA 2.5,
4772,3481,0,"In case anyone was dying to know if there was a way to solve this with calculus, you can instead recognize that $x = \sqrt{1-y^2}$ is of the form $u = \sqrt{a-b^2}$, which has the special integral $\int udx = \frac{1}{2}(xu + a^2 arcsin \frac{x}{a})$, which gives you the same result as wheber's method when integrated from $-\sqrt{1-y^2}$ to $\sqrt{1-y^2}$. My professor discussed this in class today, but I prefer wheber's method for obvious reasons.",2010-10-12T20:19:28.303,1545,CC BY-SA 2.5,
4773,3507,0,"Addendum: when I say $n^{1/2}$, I mean error goes to zero with that rate $O(n^{-1/2})$.",2010-10-12T20:32:46.480,1307,CC BY-SA 2.5,
4775,3523,13,"Given the question of the when & why of exchangeability, chl's pointer to permutation tests may merit a few additional words. Permutation tests are a nonparametric technique used when normality and similar assumptions are untenable - instead one uses the much weaker ""null assumption"" of exchangeability, approximates the distribution of a test statistic under this null assumption (by permuting) and looks whether the actually observed test statistic is extreme compared to this null distribution. There is an accessible book by P. Good, ""Permutation, Parametric, and Bootstrap Tests of Hypotheses"".",2010-10-12T20:43:34.077,1352,CC BY-SA 2.5,
4776,3527,4,"I like the part about ""My current working practice is to use R to prototype and use C when I need an extra boost of speed."" - it sounds like the job description of my poor unworthy self and the C++ developers in the next office... and I think it really captures basically *any* situation involving R, C/C++ and an issue in statistical computation.",2010-10-12T20:48:37.787,1352,CC BY-SA 2.5,
4777,3523,0,"@Stephan I like this book! Still, exchangeability is weaker than independence...",2010-10-12T21:01:42.570,930,CC BY-SA 2.5,
4778,3481,0,"@Jarrod: Thank you for the follow-up.  If the purpose of your course is to learn applications of calculus, then your professor's solution has pedagogical merit.  If it's to learn about probability, then seeing multiple solutions--including some relatively simple ones--can help you identify the key *probability* ideas and learn them better.  (The calculus stuff might then just be a distraction...)",2010-10-12T21:36:44.923,919,CC BY-SA 2.5,
4779,301,0,"Yes it will be data dependent: your ability to conduct an insightful and effective analysis is the ultimate arbiter of this issue, not my preconceptions.  I was just trying to envision situations where age as an *independent* variable would merit such a strong transformation.  Some strange things will happen with newborns, too ;-).",2010-10-12T21:40:22.480,919,CC BY-SA 2.5,
4780,3507,0,Hello suncoolsu. Thank you for the reference. I very much appreciate it. I'll do my homework.,2010-10-12T21:47:05.337,,CC BY-SA 2.5,Joseph Rickert
4782,3526,3,"Surely you mean ""after rejecting the null"" not ""after failing to reject the null""? And it seems there's only one L in 'Marascuilo' (NIST's error, not yours):

Leonard A. Marascuilo. Large-sample multiple comparisons. Psychological Bulletin, 1966; 65(5): 280-290. http://dx.doi.org/10.1037/h0023189.",2010-10-12T21:58:37.800,449,CC BY-SA 2.5,
4784,3536,0,Thanks for the pointers to R packages/functions. I will take a look at them.,2010-10-12T22:40:13.260,,CC BY-SA 2.5,user28
4785,3495,0,"I agree that the method is inexact.  However, I disagree with your diagnosis of the inexactness.  Welford's method for calculating variance, which does not even contain a sqrt, has a similar error.  However, as `n` gets large, the `error/n` gets vanishingly small, suprisingly quickly.",2010-10-12T23:06:18.687,179,CC BY-SA 2.5,
4786,3495,0,"Welford's method has no sqrt because it is computing the variance, not the standard deviation. By taking the sqrt, it seems like you are estimating the standard deviation, not the mean absolute deviation. am I missing something?",2010-10-12T23:33:53.820,795,CC BY-SA 2.5,
4787,3529,0,"+1. Josh, that's a pretty complete reference sample -- thanks!",2010-10-13T00:06:21.587,251,CC BY-SA 2.5,
4790,3542,0,+1 Great question; will be interested to see the answers.,2010-10-13T02:00:42.150,5,CC BY-SA 2.5,
4791,3542,2,"@Jeromy, I don't know if Andrew Gelman will be happy with the question, but definitely for small tables this question needs to be addressed. +1",2010-10-13T02:04:15.647,1307,CC BY-SA 2.5,
4793,3495,0,"@shabbychef Each iteration of Welfords is calculating the contribution of the new datapoint to the absolute deviation, squared.  So I take the square root of each contribution squared to get back to the absolute deviance.  You might note, for example, that I take the square root of the delta before I add it to the deviance sum, rather than afterward as in the case of the standard deviation.",2010-10-13T05:07:59.337,179,CC BY-SA 2.5,
4794,3542,2,@suncoolsu I suppose any good resource on table design should talk about the pros and cons of tables versus graphics.,2010-10-13T05:25:13.613,183,CC BY-SA 2.5,
4795,3542,1,"@Jeromy Just to point to the `apsrtable` R package which offers an alternative display of Tables, compared to `xtable`, and `reporttools` described in the JSS, http://j.mp/97GXWV",2010-10-13T06:17:35.793,930,CC BY-SA 2.5,
4796,3537,0,"I think it would be excellent if you could give more detail (see suncoolsu's answer). Furthermore, with design, do you mean how to analyze the data?",2010-10-13T08:59:28.513,442,CC BY-SA 2.5,
4797,3521,0,tibbits: How do you generate RN's when using openMP?,2010-10-13T09:05:34.927,8,CC BY-SA 2.5,
4798,3463,0,"Here: http://stats.stackexchange.com/questions/1881/analysis-of-cross-correlation-between-point-processes I was proposing a Monte Carlo approach to determine confidence limits. The idea was to do this for two point processes, but I guess it could be easily adapted for your situation.",2010-10-13T10:53:44.703,582,CC BY-SA 2.5,
4800,3538,0,"Hi all,
Thanks for you answers!

1) I want check the effect of treatment(control, stress) with time (week1, week2,...,week12) and with varieties (var1,var2,...,var12).

2) difference b/w replicates is because of source available
           3)Some description of data is given below:

All Citrus plants were grown in pots with same soil and all condition same. Then only' Stress' labelled plants were treated with chemical 'NO3' and other plants 'Control' were untreated.             
 Chlorophyll content (Data given) was recorded at day 1 and then each week for exactly the same plants.",2010-10-13T11:27:00.330,,CC BY-SA 2.5,jacki
4802,3521,0,"Right now, I'm not.  The most expensive parts of my MCMC algorithms are computing several likelihoods so I try to bunch them together best as possible and compute them in parallel.  But all of the setup, RN generation (for proposals), are done on a single cpu core.  For parallel RNGs, I'd start with [DC](http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/DC/dc.html) for the Mersenne Twister -- but I've never personally used it beyond a trivial translation to CUDA for GPUs (more as an exercise really).",2010-10-13T11:35:31.243,1499,CC BY-SA 2.5,
4803,3546,3,"Three additional references:  1.  [Beyond Kappa: A review of interrater agreement measures](http://onlinelibrary.wiley.com/doi/10.2307/3315487/abstract) by Mousumi Banerjee, Michelle Capozzoli, Laura McSweeney, & Debajyoti Sinha  2.  [Interrater reliability and agreement of performance ratings: A methodological comparison](http://www.springerlink.com/content/76887531n77808n4/) by John W. Fleenor, Julie B. Fleenor & William F. Grossnickle",2010-10-13T12:28:39.690,1499,CC BY-SA 2.5,
4804,3546,0,3. [Statistical methods for assessing measurement error (reliability) in variables relevant to sports medicine.](http://www.ncbi.nlm.nih.gov/pubmed/9820922) by Atkinson G & Nevill AM.  The first reference is specific to ordinal data and discusses other measures beyond kappa for ordinal data.  The second and third are specific to interval data.,2010-10-13T12:30:37.377,1499,CC BY-SA 2.5,
4806,3546,0,"(+1) Much Thanks M. Tibbits! I generally provide a lot of references and examples during my lectures in psychometrics, including the first you cited, but I didn't know the two others.",2010-10-13T13:22:58.753,930,CC BY-SA 2.5,
4809,2467,0,"Could you confirm that ""classification"" is not meant as ""cluster analysis"" (which is called *classification* in French), that is do you really seek to apply a supervised method following your FA?",2010-10-13T13:59:10.147,930,CC BY-SA 2.5,
4810,3544,1,Would you mind explaining why such procedure should be preferred?,2010-10-13T14:20:44.713,930,CC BY-SA 2.5,
4811,3538,0,"@Jacki, Your experimental design looks like a special case of Split plot design, but to decide the special case, I need extra details. Can you please represent your design inform of a picture (see my update) ? Currently, I don't see how you use the varieties? Is the difference between varieties of interest or not?",2010-10-13T15:11:30.000,1307,CC BY-SA 2.5,
4812,3558,3,"This one is *much* older than xkcd, LOL!  Did you read J.M.'s contribution (August 9)?  Same joke, different animal.",2010-10-13T15:21:18.877,919,CC BY-SA 2.5,
4815,3495,3,"I see the problem; Welfords obscures the problem with this method: the online estimate of the mean is being used instead of the final estimate of the mean. While Welford's method is exact (up to roundoff) for variance, this method is not. The problem is *not* due to the `sqrt` imprecision. It is because it uses the running mean estimate. To see when this will break, try `xs <- sort(rnorm(n.testitems))`  When I try this with your code (after fixing it to return `a.dev / n`), I get relative errors on the order of 9%-16%. So this method is not permutation invariant, which could cause havoc...",2010-10-13T17:05:59.583,795,CC BY-SA 2.5,
4818,3558,0,http://stats.stackexchange.com/questions/1337/statistics-jokes/1436#1436,2010-10-13T17:34:34.270,,CC BY-SA 2.5,user88
4821,1388,10,I do not have enough reputation to downvote this!,2010-10-13T17:41:41.090,1077,CC BY-SA 2.5,
4822,3560,9,"A ""perfect fit"" is so far from being attainable in any realistic logistic regression that it seems unfair to use it as a reference or a standard.",2010-10-13T17:47:46.273,919,CC BY-SA 2.5,
4823,3565,8,Linear transformations (like these) never change correlation coefficients.  The point to standardization is to improve the conditioning of the normal matrix.,2010-10-13T17:57:00.530,919,CC BY-SA 2.5,
4824,3563,0,"Nice idea.  (It doesn't generalize well to categorical predictors, though.)  I suspect that many other strategies could be interpreted from this point of view, too.  For example, selecting a subset of $k$ of the predictors could be interpreted as approximating a basis for the span of the $k$ largest eigenvectors in a PCA.",2010-10-13T17:59:45.683,919,CC BY-SA 2.5,
4827,3563,0,"In an explanatory approach, then you have to interpret how your linear combination(s) of the $p$ variables relate to the outcome, and this might sometimes be tricky.",2010-10-13T18:05:59.427,930,CC BY-SA 2.5,
4828,3565,1,"Standardizing the variables will not affect the correlations among the independent variables and will not ""reduce the effect of correlation"" in any way that I can think of with respect to this problem.",2010-10-13T18:15:24.217,485,CC BY-SA 2.5,
4829,3567,2,"(+1) Now, the problem is that the OP didn't indicate how many variables enter the model, because in case they are numerous it might be better to do both shrinkage and variable selection, through e.g. the *elasticnet* criterion (which is combination of Lasso and Ridge penalties).",2010-10-13T18:22:26.763,930,CC BY-SA 2.5,
4830,3565,2,"@Brett, a typical example where standardization helps is _Polynomial Regression_. It is always recommended to standardize the regressors. Standardizing doesn't change the correlation matrix, but makes the var cov matrix (which is now the correl matrix) well behaved (called conditioning by @whuber pointing to the condition number of the matrix, IMHO).",2010-10-13T18:23:23.650,1307,CC BY-SA 2.5,
4831,3565,0,"Agreed.  Centering is useful when entering higher order terms, like polynomial or interaction terms.  That doesn't seem to be the case here and will not otherwise help with the problem of correlated predictors.",2010-10-13T18:28:56.520,485,CC BY-SA 2.5,
4832,3562,1,"(+1) I was initially thinking of expanding my response (that came just after yours), but definitely your answer is self-sufficient.",2010-10-13T18:31:14.580,930,CC BY-SA 2.5,
4833,3560,1,"@whuber True but you could use the standard to compare the relative performance of two competing models. Your points of low R^2 in your answer and its implications are good points but if you *have* (e.g., reviewers demand it etc) to use some form of R^2 then Nagelkerke is preferable.",2010-10-13T18:32:23.877,,CC BY-SA 2.5,user28
4834,3560,1,"@Skridant Yes, still the problem of reviewers that want to see $R^2$ and Bonferroni correction everywhere...",2010-10-13T19:32:18.657,930,CC BY-SA 2.5,
4835,3560,0,"@Srikant, @chl: A cynical reading of this thread would suggest just picking the largest R^2 among all those the software reports ;-).",2010-10-13T19:44:32.320,919,CC BY-SA 2.5,
4837,3560,2,@chl Offering push-back to reviewers/clients is of course necessary but sometimes we have to be pragmatic as well. If readers do not mis-interpret low R^2 as lack of adequate model performance then the issues raised by @whuber will be mitigated to some extent.,2010-10-13T19:46:09.110,,CC BY-SA 2.5,user28
4838,3561,1,"I'm sorry see @Suncoolsu's answer was deleted.  It and the comments that followed clarified a difference between multicollinearity and ill conditioning.  Also, in a comment Suncoolsu pointed out how preliminary standardization can help with polynomial regression.  If it happened to reappear I would vote it up ;-).",2010-10-13T19:49:11.747,919,CC BY-SA 2.5,
4840,3573,0,I second this answer... and this really *is* my area (or at least I tell myself it is).,2010-10-13T20:03:36.193,1352,CC BY-SA 2.5,
4841,3564,0,"Could you tell us a little more about your problem? Why exactly do you want to use KDE? I happen to be rather active in the area of forecasting & replenishment, and I have never seen anyone use KDE. As onestop below notes, at first glance one would much rather use some kind of time series analysis method.",2010-10-13T20:05:24.857,1352,CC BY-SA 2.5,
4842,3573,0,"I've retagged the Q to add 'time-series'. Turns out there's a max of 5 tags, so i took out 'pdf'.",2010-10-13T20:10:29.473,449,CC BY-SA 2.5,
4843,1377,22,Was his name Ronald Coase? http://en.wikipedia.org/wiki/Ronald_Coase#Quotes,2010-10-13T20:26:15.290,449,CC BY-SA 2.5,
4845,3563,0,"@chl Good point.  But since the principal components are linear combinations, it's straightforward (although sometimes a bit of a pain) to compose the fitted regression model (=one linear transformation) with the projection onto the components (=another linear transformation) to obtain an interpretable *linear* model involving all the original variables.  This is somewhat akin to orthogonalization techniques.  Note, too, that Srikant's latest proposals (sum or average the regressors) essentially approximate the principal eigenvector yet induce similar explanatory difficulties.",2010-10-13T20:37:38.437,919,CC BY-SA 2.5,
4846,3569,4,"Completely agreed, +1.  But the characterization of PCA as a ""mathematical trick"" unfairly disparages it, IMHO.  If you agree (I'm not sure you do) that summing or averaging groups of regressors, as Srikant suggests, would be acceptable, then PCA should be just as acceptable and it usually improves the fit.  Moreover, the principal components can provide insight into which groups of predictors are correlated and how they correlate: that's an excellent tool for the thinking you are advocating.",2010-10-13T20:40:44.353,919,CC BY-SA 2.5,
4847,3570,2,Thanks for pointing out that paper; somehow I missed it (and it appeared when I was in the middle of a big logistic regression project!).,2010-10-13T20:44:17.940,919,CC BY-SA 2.5,
4848,3561,0,"@ŒóŒªŒØŒ±œÇ : The product is likely to be unstable in many applications.  It can be plagued by many zeros if the individual regressors have some zeros; its absolute value is likely to have strong positive skew, giving rise to some high-leverage points; it might amplify outlying data, especially simultaneous outliers, further adding to their leverage.  It might be rather difficult to interpret, too, especially if the regressors already are re-expressions of the original variables (like logs or roots).",2010-10-13T20:47:57.727,919,CC BY-SA 2.5,
4851,3569,3,"@whuber, I see and agree with your point, and I don't want to disparage PCA, so definitely +1. I just wanted to point out that blindly using PCA without looking at and thinking about the underlying problem (which no one here is advocating) would leave me with a bad feeling...",2010-10-13T21:22:18.010,1352,CC BY-SA 2.5,
4852,3571,1,"This makes sense if the regressors are all measured on the same scale. In psychology, various subscales are often measured on different scales (and still correlated), so a weighted sum or average (which is really the same here) would be appropriate. And of course, one could view PCA as providing just this kind of weighting by calculating axes of maximum variance.",2010-10-13T21:24:28.820,1352,CC BY-SA 2.5,
4853,3564,0,Another question: how does a Gaussian *distribution* enter into KDE? Are you sure you are not looking at a Gaussian *kernel*?,2010-10-13T21:27:12.503,1352,CC BY-SA 2.5,
4854,3573,0,"I suspect you are right. Maybe I should recast my question ""How do I know I have a problem where KDE can help""?",2010-10-13T21:33:43.313,1574,CC BY-SA 2.5,
4855,3564,0,"Mostly because I read KDE generalizes histograms, and our current program essentially buckets sales and calculates an average, which seemed like it was related to me.  

I probably meant Gaussian kernel ... apologies, I'm not up on the terminology.",2010-10-13T21:35:06.613,1574,CC BY-SA 2.5,
4857,3572,0,My initial idea was to add take into account the pairwise  interaction of the regressors,2010-10-13T22:17:12.480,1077,CC BY-SA 2.5,
4858,3538,0,"Thanks for you detailed answer and help!
The design you made is fine. Yes I am also interested to see the differences between varieties.
Do you know how to do in SPSS?",2010-10-13T22:40:26.643,,CC BY-SA 2.5,jacki
4862,3565,0,I deleted it because I didn't want to confuse people with wrong answer. Probably the moderators brought it up again.,2010-10-14T00:14:45.607,1307,CC BY-SA 2.5,
4863,3538,0,"@Jacki, you would do this in SPSS using the mixed model options. I am sorry, but this is the end of my knowledge in SPSS (not a regular user :-( ). I can point out some good reference for R or SAS, in case you are interested.",2010-10-14T00:17:12.640,1307,CC BY-SA 2.5,
4864,3538,0,"Also, please modify your question above so that we can see the full detail. The comment space is too small for such discussions.",2010-10-14T00:39:47.077,1307,CC BY-SA 2.5,
4865,3556,0,"Can you give alittle more context as to what you mean by ""what works"" or the particular goals of your project at hand. I've used them for visualizing spatial point processes but I doubt that is what you had in mind when asking this question.",2010-10-14T01:11:10.957,1036,CC BY-SA 2.5,
4867,3582,3,Loess is a variable kernel REGRESSION method. The question asked about variable kernel DENSITY estimation.,2010-10-14T02:04:15.347,159,CC BY-SA 2.5,
4868,2103,0,@Srikant: Done.  Thank you for the excellent guidance.,2010-10-14T02:31:45.483,919,CC BY-SA 2.5,
4869,3503,0,"@Joseph.  I am not sure I understand your comment.  We just use a slightly different syntax.  I don't know what beta.hat means.  My point was that a greater sample N will give you greater statistical significance (lower standard error, higher t stat, lower p value) on all regression coefficients within a single run.  Meanwhile, the greater number of iterations M will give you greater statistical significance for the Mean of each specific coefficients across all iterations.  They are two different things.",2010-10-14T03:34:22.003,1329,CC BY-SA 2.5,
4870,3495,0,"Ah, well spotted :)  I guess the utility of this algorithm will depend on the dataset then...",2010-10-14T04:07:56.833,179,CC BY-SA 2.5,
4871,3585,5,"This model assumes the response is an additive function of traveling to each place, which is highly unlikely.  It can still be made to work by including interaction terms.  A full set of all possible interactions might be needed (beyond just the two-way interactions).  (That would be mathematically identical to providing a separate dummy for each possible combination of destinations.)",2010-10-14T04:59:39.657,919,CC BY-SA 2.5,
4872,3563,0,"@whuber Yes, I agree with both of your points. I extensively used PLS regression and CCA, so in this case we have to deal with linear combinations on both side (st. a max. covariance or correlation criteria); with a large number of predictors, interpreting the canonical vectors is painful, so we merely look at the most contributing variables. Now, I can imagine that there is not so much predictors so that all of your arguments (@Stephan, @Mike) make sense.",2010-10-14T05:56:18.487,930,CC BY-SA 2.5,
4873,3574,3,"The original HL $\chi^2$ GoF test is not very powerful for it depends on categorizing the continuous predictor scale into an arbitrary number of groups; H & L proposed to consider decile, but obviously it depends on the sample size, and under some circumstances (e.g. IRT models) you often have very few people at one or both end of the scale such that cutoffs are unevenly spaced. See A comparison of goodness-of-fit tests for the logistic regression model, Stat. Med. 1997 16(9):965, http://j.mp/aV2W6I",2010-10-14T06:36:24.097,930,CC BY-SA 2.5,
4874,3582,0,"Oops, you're right. Misread the question.",2010-10-14T06:56:25.680,1569,CC BY-SA 2.5,
4875,3585,4,Better have a *lot* of data if you use all interactions (15 parameters) rather than just the main effects (4 parameters)...,2010-10-14T06:56:40.920,1352,CC BY-SA 2.5,
4876,3587,0,"I'm using the riskratio function from the EpiTool package, manual attached here:
http://bm2.genes.nig.ac.jp/RGM2/R_current/library/epitools/man/riskratio.html

I'm still digesting your answer, thanks again!",2010-10-14T07:01:20.257,588,CC BY-SA 2.5,
4877,3589,2,"This does not sound like you have observations (or ""cases"") on which you observe both an X and a Y realization. How do you find out which X is associated to which Y?",2010-10-14T07:01:47.680,1352,CC BY-SA 2.5,
4879,3589,1,I suppose I forgot to mention this. X and Y are stock prices. Company X has been public for a much shorter time period than Y. I wanted to tell how correlated the prices of X and Y are. I could definitely get a correlation for the period of time that X and Y both exist. I wanted to know if knowing the stock prices for several extra years of Y that X did not exist yielded me any additional information.,2010-10-14T07:10:00.317,1118,CC BY-SA 2.5,
4880,3574,0,"Thanks chi, that's a useful ref, though your j.mp link took me to a BiblioInserm login prompt. Here's a doi-based link:
http://dx.doi.org/10.1002/(SICI)1097-0258(19970515)16:9<965::AID-SIM509>3.0.CO;2-O",2010-10-14T07:20:21.123,449,CC BY-SA 2.5,
4881,3558,0,I just noted that I got it via xkcd forum. I didn't mean this is its origin. Thank you for pointing out the deer hunting skit. Also funny. :),2010-10-14T07:25:31.703,144,CC BY-SA 2.5,
4882,3586,0,"I think the condition of 5 is for the actual cell count and I see that one of the cell has cell count = 3. Further, the p-value calculated by the ChiSq test is based on pchisq( .. , lower.tail=F), if I understand the code correctly. Therefore, the p-value is based on one sided test, where as the CI is two sided, hence the discrepancy.",2010-10-14T07:25:56.070,1307,CC BY-SA 2.5,
4883,3574,0,Sorry for the incorrect link... I seem to remember Frank Harrell's `Design` package features the alternative H&L 1 df test.,2010-10-14T07:31:47.527,930,CC BY-SA 2.5,
4884,3586,0,"I'll investigate on the ""one-sided/two-sided"" issue, but for condition of 5, I think this should be referred to the EXPECTED FREQUENCY as from http://en.wikipedia.org/wiki/Pearson%27s_chi-square_test: ""Expected Cell Count ‚Äì Adequate expected cell counts. Some require 5 or more, and others require 10 or more. A common rule is 5 or more in all cells of a 2-by-2 table, and 5 or more in 80% of cells in larger tables, but no cells with zero expected count. When this assumption is not met, Yates' correction is applied.""",2010-10-14T07:53:58.303,588,CC BY-SA 2.5,
4885,3591,0,"actually I have used the small-sample adjustment, and it then CI and Chi-Square fits perfectly, but how can I justify that it is a ""small sample""?",2010-10-14T07:55:09.813,588,CC BY-SA 2.5,
4888,3593,0,(+1) Both responses provide a handy summary of possible discrepancies between all three statistics.,2010-10-14T08:11:47.547,930,CC BY-SA 2.5,
4889,3589,2,"@Christopher I'd recommend that you update your question to reflect your above comment. Also, for correlation to be meaningful, more than just equal dimensions are required; the actual measurements have to come from the same cases, which in your case is presumably the same time points.",2010-10-14T08:13:20.753,183,CC BY-SA 2.5,
4890,3591,2,"The asymptotics (and power) depends much more on the total number of positives than the overall total (strictly, the total number of the whichever outcome is rarer). Your total number of positives is 14. That's pretty small. 'Expected value 5 or more in all cells' is only a rule of thumb. 

I'm not sure what the small sample adjustment used by epitools is doing exactly (can't find any explanation in the documentation), but it's certainly something more sophisticated than Yates' correction.",2010-10-14T08:22:41.480,449,CC BY-SA 2.5,
4892,2697,4,What about the eigenvectors & eigenvalues?,2010-10-14T08:29:12.460,1077,CC BY-SA 2.5,
4893,3492,1,"I think it's wrong to say that R would not be helpfull in most corporate settings.  Knowing R won't really help you learn SAS if that's what you're stuck on, but that's like saying you need a hammer when faced with a box of screws.....",2010-10-14T08:34:30.327,114,CC BY-SA 2.5,
4894,3589,2,I second Jeromy's comment on updating the question...,2010-10-14T08:53:30.187,1352,CC BY-SA 2.5,
4896,3589,0,Another question: you mention that X and Y have the same number of columns. Would that be one each? Or do you have multiple series for both X and Y (prices at different stock exchanges or some such)?,2010-10-14T09:00:46.130,1352,CC BY-SA 2.5,
4897,3565,0,"@suncoolsu: yes, standardization helps in polynomial regression - but I would go the whole way and transform the data into appropriate polynomial basis functions. Numerical people have been thinking a lot about stuff like this.",2010-10-14T09:04:30.863,1352,CC BY-SA 2.5,
4900,3591,0,"@lokheart In fact, the ""expected counts greater than 5"" has been shown to be somewhat too stringent, at least in the case of the Pearson $\chi^2$ test, see Chi-squared and Fisher-Irwin tests of two-by-two tables with small sample recommendations, Stat. Med. 2007 26:3661, and Campbell's webpage, http://bit.ly/dbPfOO.",2010-10-14T09:18:44.200,930,CC BY-SA 2.5,
4901,2467,1,Yes i think to cluster analysis rather than classifcation.,2010-10-14T09:55:37.543,1154,CC BY-SA 2.5,
4902,3591,0,"Thanks for the ref chl. Wald CIs have worse finite-sample properties than the Pearson chi-square test though, so it may well not be stringent enough when it comes to appropriateness of Wald CIs.",2010-10-14T10:12:59.693,449,CC BY-SA 2.5,
4903,3595,6,"Can you pls be a bit more specific? What do you actually want to data mine and how do you plan to do it? I have used R to analyze similar size records as yours, and it wasn't a bad experience at all.",2010-10-14T10:31:53.547,1307,CC BY-SA 2.5,
4906,3585,0,"@whuber and @Stephen, Thanks for the responses, and I agree completely with each of you. I personally would be ok with the main effects dummy variable approach if multiple responses weren't all that common, which may not be a tenable assumption given the original posters concerns. I would maybe propose other designs if the original poster was interested in the risk of travelling to A vs B (such as some type of matching procedure). And I agree additive risk does not make sense except if some selection bias is occurring.",2010-10-14T12:31:35.670,1036,CC BY-SA 2.5,
4908,3492,0,"Let me rephrase: I don't think R will help anyone get a SAS programming position in most corporate settings, primarily because I doubt most people in corporate SAS shops even know what R is.",2010-10-14T13:20:59.863,71,CC BY-SA 2.5,
4909,3511,0,Still could not get a better convergence by adapting IRLS. :'(,2010-10-14T13:29:45.227,1351,CC BY-SA 2.5,
4910,3594,0,"For basic financial time series references, you can see my answer here: http://stats.stackexchange.com/questions/328/resources-for-learning-about-the-statistical-analysis-of-financial-data/329#329.  The Tsay text is one of the most popular.",2010-10-14T13:34:08.087,5,CC BY-SA 2.5,
4912,3605,0,Clarification: I didn't mention GARCH to deal with the missing data problem (which of course would not make sense) - but to improve on a simple calculation of correlation between the time series at times where both exist.,2010-10-14T14:12:55.287,1352,CC BY-SA 2.5,
4913,2095,0,"And what if x=12 and y=1/2? Then your answer will have  F(12, 1/2)=12+1/2-1=23/2.",2010-10-14T14:15:44.917,247,CC BY-SA 2.5,
4914,3599,0,"> *in that values of B above a threshold will change C* Do you know that threeshold in advance or does it need to be estimated as well ? Also, when you write B*t do you actually mean $B_t$ ?",2010-10-14T14:20:12.987,603,CC BY-SA 2.5,
4915,3603,0,"How did you deduce that B and C are correlated?  They might be, but this does not seem to be necessarily the case.  In particular, if B rarely exceeds that threshold, B and C might be approximately independent.",2010-10-14T14:23:22.533,919,CC BY-SA 2.5,
4916,3605,0,@Stephan: OK.  I mentioned it mainly to show I wasn't ignoring you!,2010-10-14T14:24:38.210,919,CC BY-SA 2.5,
4917,3603,0,"@whuber There is no issue at all if B rarely exceeds the threshold. We may as well assume that they are independent and use standard logistic models. I was implicitly assuming that there is some correlation between B and C given that there are threshold effects, the fact that changes in C impact B and that B and C are continuous variables.",2010-10-14T14:30:54.077,,CC BY-SA 2.5,user28
4918,3588,0,"Thanks Kwak. ""... for Gaussian distributed random variables""; would you know of newer work for ""clumpy"" distributions ?",2010-10-14T14:38:07.343,557,CC BY-SA 2.5,
4919,2095,0,@PeterR good point! Fixed the error.,2010-10-14T14:44:31.593,,CC BY-SA 2.5,user28
4920,3505,0,"Hi @suncoolsu. Your points are good but, as the experiment is complete, the design is fixed.",2010-10-14T14:46:21.043,1381,CC BY-SA 2.5,
4921,3603,0,"@Srikant You're right.  The concern lies with intermediate situations.  In those cases the correlation is probably not great enough to warrant special measures to deal with correlated regressors.  Moreover, although we speak of ""correlation"" the relationship likely is not linear, so a lot of care is needed in dealing with it.",2010-10-14T14:48:29.277,919,CC BY-SA 2.5,
4922,3505,0,"Re the first point (1) - I read Venables' article that you referenced about main effects being an artifact of experimental design (the levels chosen of each factor) when interactions are present (p. 13), but I believe my study is one of the 'rare and special' exceptions, because the main effect of A on Y is very large, i.e., Y is an order of magnitude different at each level of A, while any effect of B would have been on the order of a 10-50% change or so; of course the point is moot since the interaction effect was not significant.",2010-10-14T14:54:22.817,1381,CC BY-SA 2.5,
4923,3505,0,"""Three solid gold (significance) stars on the main effects will do very nicely, thank you, and if there are a few little stars here and there on the interactions, so much the better!"" -Venables 2000 [N.B. said in jest, but this attitude is so pervasive in research, peer review, and graduate training is so great I thought it would be edifying to include it here.",2010-10-14T15:05:54.693,1381,CC BY-SA 2.5,
4924,3597,0,@chl: Have you yet found an effective parallel computing solution for 64-bit R?  When I last looked (late this summer) the only non-commercial ones appeared to work only in 32-bit R.,2010-10-14T15:07:42.383,919,CC BY-SA 2.5,
4925,3582,0,"@Rob, excuse my naive questions: if varying kernel width is (sometimes) good for local regression / Kernel smoothing, why is it bad for density estimation ? Isn't density estimation a case of f() estimation for f() == density() ?",2010-10-14T15:13:04.727,557,CC BY-SA 2.5,
4926,3582,0,"@Hong Ooi, how many points in what Ndim have you used ? Thanks",2010-10-14T15:15:06.263,557,CC BY-SA 2.5,
4927,3589,0,"Stephan: Thanks for editing my question to reflect the new information. I actually have a few columns, because I have date, opening price, highest price of the day, lowest price, and closing value.",2010-10-14T15:28:23.673,1118,CC BY-SA 2.5,
4928,3605,1,"Thank you, whuber. This is in line with what I was looking for. I don't think the backcasting will be of much use (or feasibility) to add a couple extra weeks of X when the mutual time frame between X and Y is about 16 years already.",2010-10-14T15:36:47.693,1118,CC BY-SA 2.5,
4929,3503,0,"@Joseph, using your language.  I am not sure that the CLT argument suggests that both beta.hat and sample.mean.beta.hat will converge towards each other.  But, that their respective distributions of outcome (defined by their standard error around the mean) will be normally distributed.  I think the two beta.hat(s) will converge towards each other simply because they will each become more firmed up or statistically significant as you use greater N and greater M.",2010-10-14T15:49:41.200,1329,CC BY-SA 2.5,
4930,730,14,"And this is an actual quote, as opposed to something ""attributed to"" Box. It appears, e.g., in Box & Draper (1987), *Empirical model-building and response surfaces*, Wiley, on page 424. Yes, I did go and look it up before using it in a paper.",2010-10-14T15:53:20.163,1352,CC BY-SA 2.5,
4933,3605,2,"@Christopher: !! With 16 years (of daily closings?) you have enough data not only to find a correlation, but also to explore how it has been changing over time.  (This I believe is the spirit of @Stephan Kolassa's reply.)",2010-10-14T16:08:41.903,919,CC BY-SA 2.5,
4934,3495,0,"@fmark nevertheless, your method seems to work reasonably well when the data are i.i.d. I suspect, however, that using Welford's trick somewhat obscures the approach, and that it is unnecessary for this approximation. The simplified version would then have `a.dev <- a.dev + abs(x - mean)` as the update.",2010-10-14T16:41:04.653,795,CC BY-SA 2.5,
4935,3597,1,"@whuber Nope. I had to switch to 64 bits last year to manage large genetic data sets, but the statistical models we used do not call for parallelization (as far as I know). I thought there was an OpenMP binding for R but did not investigate this further. I know Revolution Analytics have made effort in this sense (http://j.mp/d7dFb5), but still in 32 bits (this is probably what you referred to). I found R/parallel (http://www.rparallel.org/) in the meantime, but I don't know how reliable/mature it is.",2010-10-14T16:47:50.797,930,CC BY-SA 2.5,
4936,555,31,"ANOVA can be seen as ""syntactic sugar"" for a special subgroup of linear regression models. ANOVA is regularly used by researchers who are not statisticians by training. They are now ""institutionalized"" and its hard to convert them back to using the more general representation ;-)",2010-10-14T16:52:51.193,1307,CC BY-SA 2.5,
4937,3560,0,"@Skridant There are alternative measures of prediction performance that makes more sense or are more intuitive (e.g. ROC area, Somers D) and that can be reported together with pseudo $R^2$.",2010-10-14T16:55:48.357,930,CC BY-SA 2.5,
4938,3597,0,@chl I tried them all but couldn't get any of them to work.,2010-10-14T17:53:29.933,919,CC BY-SA 2.5,
4939,825,0,"@chl: Thanks for bumping this up.  In fact, I checked out all the non-commercial references from this thread shortly after it appeared but couldn't find anything that works on Win 7 x64.",2010-10-14T18:07:11.927,919,CC BY-SA 2.5,
4940,3588,0,@Denis:> 'Clumpy'=?concentrated=?with narrower tails than the gaussian?,2010-10-14T18:28:07.767,603,CC BY-SA 2.5,
4942,3607,0,That is quite analogous to what I¬¥m looking into. I¬¥ll look into their work. Thx for the input.,2010-10-14T19:05:22.870,1291,CC BY-SA 2.5,
4943,3599,0,Unfortunately we do not know the threshold and the threshold itself might not be constant over time.,2010-10-14T19:09:03.470,1291,CC BY-SA 2.5,
4944,3612,0,@chi: Thanks a lot. I'll look the papers. Would you please comment the first question? Is hazard variable always time?,2010-10-14T19:12:54.947,1586,CC BY-SA 2.5,
4945,3615,0,"@Srikant Excellent answer, but does that function resolve to something arithmetic (ie: not recursive)?",2010-10-14T20:06:46.517,1456,CC BY-SA 2.5,
4946,3615,0,"@C. Ross Unfortunately I do not think so. But, I suspect that the recursion should not be that hard as long as are dealing with reasonably small n and small s. You could just build-up a lookup table and use that repeatedly as needed.",2010-10-14T20:10:30.160,,CC BY-SA 2.5,user28
4947,3616,0,"What do you mean by ""different""?",2010-10-14T20:21:50.570,5,CC BY-SA 2.5,
4948,3616,0,"What do you mean with ""5-10 data points *per data set*""?",2010-10-14T20:25:09.053,1352,CC BY-SA 2.5,
4949,3612,0,"@yuk Not necessarily, as suggested by @whuber. I have in mind another application of Cox regression dealing with the treatment of systematic pattern of missing responses in educational testing, as it arises when a student has not enough time to complete the test (missing responses might then be considered as right-censored) -- in this case, this is item ordering that is considered as the time scale. I'll look at the original paper (although I think this was also the subject of a PhD).",2010-10-14T20:43:01.703,930,CC BY-SA 2.5,
4950,3582,0,@Denis. Great question. Can you please add it as a proper question on the site and we'll see what answers people can come up with.,2010-10-14T20:58:14.657,159,CC BY-SA 2.5,
4951,3605,0,I agree. Using techniques to figure out what values X would've taken prior to its IPO seems prone to error. I might also question the relevance of data that's 16 years old to predict modern trends.,2010-10-14T22:20:43.623,1118,CC BY-SA 2.5,
4952,3618,0,Will that mathematica code work with wolfram alpha?,2010-10-14T23:56:42.000,,CC BY-SA 2.5,user28
4953,3616,0,"I think he has a collection of several time series, each one with 5-10 observations.",2010-10-15T01:07:37.033,159,CC BY-SA 2.5,
4954,3613,0,"+1, thanks for pointing out the NADA package.  I noticed it makes it easier to handle left-censored data through the survival package -- is left-censored a common scenario with environmental data?",2010-10-15T02:11:07.610,251,CC BY-SA 2.5,
4955,3612,0,"+1.  There are other papers, but I'm not sure they're necessarily better; I think Chalise does a pretty good job summing up the situation.",2010-10-15T02:13:50.007,251,CC BY-SA 2.5,
4958,3613,0,"@whuber: Thank you for the comment, NADA package looks very interesting.",2010-10-15T05:23:24.287,1586,CC BY-SA 2.5,
4959,3613,0,@Andy: Thanks for the links. I think its worth to be an answer. I'd upvote.,2010-10-15T05:25:07.700,1586,CC BY-SA 2.5,
4960,3619,0,"Do you have success with it? If yes, for what kind of application?",2010-10-15T05:50:47.757,930,CC BY-SA 2.5,
4961,3588,0,"I'm no expert, but like ""data set clumpiness"" in the paper Lang et al., ""Insights on fast Kernel Density Estimation algorithms"", 2004, 8p",2010-10-15T10:15:12.183,557,CC BY-SA 2.5,
4962,3582,0,"@Rob, which part, can you help me formulate ? along the lines ""Is adaptive kernel width good for both kernel smoothing and KDE ?""",2010-10-15T10:15:36.397,557,CC BY-SA 2.5,
4965,3613,0,"@Yuk, per your request I made my comment into an answer, and @whuber thanks for your example.",2010-10-15T12:03:07.990,1036,CC BY-SA 2.5,
4967,3592,0,"Just as a note from what I can gather time varying in this context does not make sense, unless the criteria used to classify individuals in either before or after is time varying itself (in which case I would say you want those characteristics themselves as the covariates not the classification).",2010-10-15T12:50:18.157,1036,CC BY-SA 2.5,
4969,3613,0,"@ars: Yes, left censoring is characteristic of environmental data (and is a key concern of chemometrics in general).  It's a tricky and interesting problem.  Among the reasons are (1) the censoring limits are themselves determined by statistical estimates (through a calibration process), (2) censoring can occur in multiple ways--as limits of detection, limits of quantification, or ""reporting limits"", (3) the thresholds often vary in response to covariates (""matrix interferences"") that can be strongly correlated with the original censored values, (4) data often are lognormally distributed.",2010-10-15T14:11:52.090,919,CC BY-SA 2.5,
4970,3618,0,"@Srikant I didn't even try it because usually alpha does not process Mathematica code correctly except for simple expressions.  However, see what happens when you type ""Expand[(x + x^2 + x^3 + x^4 + x^5 + x^6)^3]"" !  ( http://www.wolframalpha.com/ )",2010-10-15T14:15:17.037,919,CC BY-SA 2.5,
4971,3632,0,"Did you really mean to include ""logistic"" in ""use logistic regression to estimate x""?",2010-10-15T14:18:30.330,919,CC BY-SA 2.5,
4972,3626,1,"+1 for noting the nonlinearity.  But your use of the phase ""expected proportion"" seems to assume the data are a random sample of the distribution over which one is averaging and that's often not the case.",2010-10-15T14:22:41.203,919,CC BY-SA 2.5,
4973,3623,0,"Over what probability distribution do you intend to form an average, David?",2010-10-15T14:23:52.330,919,CC BY-SA 2.5,
4974,3618,1,That works. I tried your earlier version but could not make any sense of the output.,2010-10-15T14:24:16.927,,CC BY-SA 2.5,user28
4975,3632,0,"I think a bit more context (e.g., other variables, what kind of data you have, dependent and independent variables etc) would help someone to give a useful answer.",2010-10-15T14:28:43.313,,CC BY-SA 2.5,user28
4976,3619,0,"Yes, RHIPE is great. Some of my friends use it to analyze internet traffic data. One of their aims is to model break-in attempts. Data is huge in such cases, petabytes is common!",2010-10-15T14:37:59.833,1307,CC BY-SA 2.5,
4977,3592,0,"@ Andy W: yes, you're right. Maybe this was why I was uncomfortable with the idea of time varying covariates.",2010-10-15T14:42:27.793,1573,CC BY-SA 2.5,
4978,3581,1,Thanks for the links. I actually requested your book from my local university. :),2010-10-15T14:43:19.077,1574,CC BY-SA 2.5,
4979,3604,0,"onestop, that's a good idea! Definitively valid with a clear conclusion to be drawn from the results. I don't have the final data until now, but hopefully I will have enough fluctuation between classes. If not, I will have a very small sample size in groups (c) and (d) and inference will be difficult. But that's the best strategy so far... :)",2010-10-15T14:47:09.627,1573,CC BY-SA 2.5,
4980,3616,0,"I still think that this question is nearly impossible to answer without understanding what ""different"" means...",2010-10-15T15:30:42.697,5,CC BY-SA 2.5,
4981,3383,0,What sort of variance correction were you thinking about?  Any error in estimate as a consequence of using the sample mean will result in increased variance in the difference scores.  So isn't the variance already adjusted for?  If anything it is going to be high.,2010-10-15T16:05:43.637,196,CC BY-SA 2.5,
4982,3636,1,"What are trying to do with the data, Robert?  How do you intend to interpret the standard deviation?",2010-10-15T16:12:47.757,919,CC BY-SA 2.5,
4984,3641,0,What's wrong with the moving average?,2010-10-15T16:55:19.890,,CC BY-SA 2.5,user28
4985,3641,0,"What's a ""period""?  You can always add variables into your model (e.g. to prevent it going to unrealistic values).",2010-10-15T16:56:25.117,5,CC BY-SA 2.5,
4986,3641,0,@Shane - each period is 10 years. Age will be the only variable i have for those years,2010-10-15T16:57:40.873,59,CC BY-SA 2.5,
4987,3641,0,"I understand that age is the only variable that you will have in your data, but you can add additional factors based on priori information (e.g. that population can never be negative).  This is especially true if you model this using a Bayesian approach.",2010-10-15T16:59:47.783,5,CC BY-SA 2.5,
4988,3641,0,"@Shane - I'm not that advanced yet as to do Bayesian analysis on the data set, but I'm a fast learner :)",2010-10-15T17:01:48.343,59,CC BY-SA 2.5,
4989,3641,0,@dassouki: Suppose that an average adult (say between 25 - 40 yrs) gives birth to 2 kids (something you can get from census). Then you can extrapolate using some assumptions: (a) Percentage of adults who have kids (say 60%) etc. Then you can perform a rolling lagged forecast using the number of kids born in 1930s who would be adults in 1960s etc Does that make sense?,2010-10-15T17:12:13.187,,CC BY-SA 2.5,user28
4990,3641,0,@Sikrant Vadali - Changed image,2010-10-15T17:25:44.500,59,CC BY-SA 2.5,
4991,3641,0,@Shane - changed Image,2010-10-15T17:26:20.913,59,CC BY-SA 2.5,
4992,3643,0,a. I need to estimate the variance and the variance of the Cauchy distribution is not defined.,2010-10-15T17:26:44.203,1381,CC BY-SA 2.5,
4993,3643,0,"b. If I understand your second point, yes, I could assume that y-1 ~ N(mu, sigma), but I still need to calculate mu and sigma from the summary statistics given for y; also, I've chosen not to consider distributions with values < 0 for variables only defined > 0 (even though in many of the cases p(X<0 | X~N(mu,s)) -> 0 )",2010-10-15T17:32:32.323,1381,CC BY-SA 2.5,
4994,3640,0,should I post the Update question about calculating the variance on random draws from the Cauchy as a separate question?,2010-10-15T17:39:48.120,1381,CC BY-SA 2.5,
4995,3644,0,"Thanks for the reference, that is where I found the Haaya 1975 reference and the equations in my question, although I'd appreciate reassurance that the equations are appropriate for my problem.",2010-10-15T17:43:55.880,1381,CC BY-SA 2.5,
4996,3636,0,Is the data normally distributed and i.i.d.?,2010-10-15T17:52:45.883,5,CC BY-SA 2.5,
4997,3643,0,Doesn't the Cauchy apply for zero mean normals?,2010-10-15T17:57:04.253,251,CC BY-SA 2.5,
4998,3644,0,"Taking a quick look at Haaya, it seems that they're concerned with obtaining a Normal approximation for the ratio and use simulations to determine when that applies (using the coefficient of variation, cv).  Does the cv in your case meet the criteria?  If so, the approximations apply.",2010-10-15T17:58:56.107,251,CC BY-SA 2.5,
4999,3613,0,"@whuber: Thanks, I usually encounter right censoring or left truncation and rarely anything else, so it's interesting to hear about other domains.",2010-10-15T18:02:18.800,251,CC BY-SA 2.5,
5000,3643,0,@ars You are correct. The cauchy then may be of limited use.,2010-10-15T18:15:34.753,,CC BY-SA 2.5,user28
5001,3643,0,"Ars: Yes, I believe the Cauchy result requires zero means. But that still means that at least in that special case, the variance that David is trying to estimate DOES NOT EXIST.",2010-10-15T18:15:35.330,319,CC BY-SA 2.5,
5002,3643,0,@David: Simply invert y and compute the sample mean and sample standard deviation and use those as estimates of mu and sigma. A normal may approximate y^-1 well if sigma is relatively small.,2010-10-15T18:17:24.907,,CC BY-SA 2.5,user28
5003,3643,0,"@John: true, good point; I missed David's first comment.",2010-10-15T18:25:26.437,251,CC BY-SA 2.5,
5005,3634,1,What is the reason for a non-parametric approach?,2010-10-15T19:01:32.327,930,CC BY-SA 2.5,
5006,3356,0,+1.  Didn't see all this earlier - Thanks for noting the issue as well as the crash/mini course on stata notation.  I took your first comment to imply the interpretation was mistaken and answered in a very general sense.  I'm glad you were more persistent and that kwak figured it out.,2010-10-15T19:14:58.587,251,CC BY-SA 2.5,
5008,3644,1,@David: use Marsaglia 1965 instead as updated in the answer.,2010-10-15T19:18:47.407,251,CC BY-SA 2.5,
5009,3638,0,"I seem to remember that Median Polish can be used for a two-way layout; how does it extend to 5 factors, including nesting?",2010-10-15T19:19:14.240,930,CC BY-SA 2.5,
5010,3643,0,"@Srikant I can't compute the sample standard deviation since I don't have the raw data... although I could do the calculation on simulated data sets, and take the average of these simulations.",2010-10-15T19:27:39.217,1381,CC BY-SA 2.5,
5011,3616,0,My applogies for the poorly worded question.  By different I mean whether over the course of the time series (rather than at individual points) there is a difference between two treatment groups.  There would be inter-subject variation (which i guess would need to be accounted for) as well as inter-group variation (which is what I am interested in).,2010-10-15T19:32:41.607,1327,CC BY-SA 2.5,
5012,3646,2,"It's unclear how ""regression"" could be invoked in a *univariate* dataset.  Also, exactly what distinction are you suggesting between the two bulleted methods?",2010-10-15T19:35:50.620,919,CC BY-SA 2.5,
5013,3638,1,@chl Median polish can work with as many factors as you might care to handle.  Tukey does some three-way examples (by hand!).,2010-10-15T19:37:15.550,919,CC BY-SA 2.5,
5014,3646,0,"I accept that the question might make very little sense - I admit I'm confused! In terms of regression, I imagine that you would input the formula for the normal distribution into some application that can handle non-linear regression, with the mean and variance being parameters of that formula. You would then let the application attempt to derive the values for those parameters that maximise the fit of your data to the formula. So I see that as a completely different approach than just using the sample mean/variance as estimates.",2010-10-15T19:42:59.653,1598,CC BY-SA 2.5,
5015,3641,1,@Srikant: more detailed data of birth rates (by women grouped into five year age ranges) are readily available for exactly the purpose you propose.  It's best to obtain these data for the particular state in question rather than using nationwide averages.,2010-10-15T19:44:52.697,919,CC BY-SA 2.5,
5016,3647,0,"This is not a US state forecast. I can get the gender, race, income, birth/death data, for the last 10 years only, would that be enough?",2010-10-15T19:48:50.513,59,CC BY-SA 2.5,
5017,3646,0,"if you have no predictor variables, there would be no way to fit a non-linear model, or any model other than 'data has a mean and variance'",2010-10-15T19:53:16.933,1381,CC BY-SA 2.5,
5018,3647,0,"@dassouki: sorry, I saw the ""statewide"" on the graphic's title and presumed (incorrectly) that it referred to a US state.  The birth/death data for the last 10 years would be excellent, because it will be a reasonably good predictor of birth/death rates into the near future.",2010-10-15T19:58:14.483,919,CC BY-SA 2.5,
5019,3648,0,"Thanks David, I suppose I was thinking of a more elaborate non-linear model, e.g. Y = (whatever it would be if the data is in normally distributed, including things like pi and the two parameters of interest, the mean and variance) + epsilon ? Sorry again if this is meaningless!",2010-10-15T20:01:04.107,1598,CC BY-SA 2.5,
5020,3648,1,"Nice example.  It might be helpful to clarify what that regression model is.  You have an intercept only regression--so you have no predictor variables.  This is precisely equivalent to a one-sample t-test for difference from 0. More simply in R: x<-rnorm(100); t.test(x); summary(lm(x~1)).  P values, SEs, estimates, intervals, etc. will all be the same.",2010-10-15T20:01:18.460,485,CC BY-SA 2.5,
5021,3646,1,"@Bio.X2Y: one of the points your respondents are making is that one ""fits...a normal model"" to the data by estimating the mean and standard deviation.  These become the mean and sd of the fitted normal distribution.  The other point they are making is that ""regression"" in your case means performing a least-squares fit of a constant to the data (""no covariates"") and that's exactly the same as using the usual mean and sample standard deviation estimates.  So the short answer is, both approaches are reasonable and they're the same.",2010-10-15T20:02:51.163,919,CC BY-SA 2.5,
5022,3648,1,I think there's a typo and you invert `se` and `sd`; BTW your `mse` (I assume it is mean square error) is actually the SS.,2010-10-15T20:03:24.210,930,CC BY-SA 2.5,
5023,3646,0,"OK, am I right in thinking that the model you're suggesting is a linear one with no predictors, where the error is normally distributed? Isn't that different from a model where you use two predictors for mean and variance, regardless of the error?",2010-10-15T20:09:13.353,1598,CC BY-SA 2.5,
5024,3646,0,"@Bio.X2Y I think you make a confusion between (a) estimating the mean and variance from an observed sample (which is strictly an univariate problem as @whuber said), while being happy with an underlying gaussian assumption, and (b) check how well your distribution fit a theoretical distribution, with unknown mean and variance. Neither of these cases call for a modeling approach. Maybe @David can update his response for (b), otherwise the first one was already suggested to you: just use the arithmetic mean (which is an unbiased estimator) and SD (I let others discuss the denominator issue).",2010-10-15T20:11:50.320,930,CC BY-SA 2.5,
5025,3646,0,"thanks chl. Yes, I think I understand the (a) option (and the n-1 vs n in the denominator), and my confusion lies with the (b) option. I suppose I don't understand why (b) *doesn't* call for a modeling approach - surely since the theoretical distribution has an unknown mean and variance, these have to be 'estimated' before a fit can be established. So the output to (b) would be something like ""mean =1, sd=2, r^2=0.94""? Thanks again",2010-10-15T20:19:53.190,1598,CC BY-SA 2.5,
5026,3648,0,"@ chi, Thanks for pointing out my error - I think its fixed",2010-10-15T20:21:03.637,1381,CC BY-SA 2.5,
5028,3646,0,"@Bio.X2Y No, checking how an empirical distribution departs from an hypothetical/theoretical distribution is not the same than modeling a relationship between an outcome and a potential explanatory variable, or simply fit a regression with only an intercept. In the latest case, it is even better to simply use a Quantile-Quantile plot (observed vs. gaussian) to check if your data follows an expected normal distribution (as is the assumed distribution of the residuals in your LM).",2010-10-15T20:27:11.033,930,CC BY-SA 2.5,
5029,3646,0,"ok, thanks again chl, looks like I have a lot more background reading to do!",2010-10-15T20:28:50.703,1598,CC BY-SA 2.5,
5030,3651,0,I could concede distributional assumptions for all of the variables. I will update my question.,2010-10-15T20:34:14.497,1036,CC BY-SA 2.5,
5031,3651,0,"@Skrikant For the 1st part: Wouldn't it be simply an hypergeometric distribution (and at the limitng case, a binomial)?",2010-10-15T20:39:29.477,930,CC BY-SA 2.5,
5032,3651,0,@chl True. As the total size of the urn is fixed. Answering a question too fast is not without its perils. :-),2010-10-15T20:44:05.487,,CC BY-SA 2.5,user28
5034,3655,2,"This approach would work if we assume that our CI are of the form 
$\hat{\beta}\pm Z_{\alpha}SE$. Unfortunately, sometimes asymmetric CI may be more sensible, for example the CI for a binomial proportion when it's close to 0. In that case pooling the SE like this may not help.",2010-10-15T20:52:17.863,1600,CC BY-SA 2.5,
5035,1642,0,@Thomas Andrew Gelman discussed type I and II errors before introducing S and M errors. I think this response is a valid and interesting one (wtr. other well-founded answers) since it allows to go beyond the traditional decision theory framework. I've upvoted this response.,2010-10-15T20:56:49.637,930,CC BY-SA 2.5,
5038,3655,0,@user1600 Good point.,2010-10-15T22:14:37.820,,CC BY-SA 2.5,user28
5039,3651,0,"@Srikant: ""Any"" deviation?",2010-10-15T22:58:50.270,919,CC BY-SA 2.5,
5040,3651,0,@whuber I meant some sort of chi-squared analysis implicitly. If you see the revisions you will see that I started off the answer with that idea. Would it help if I mention that explicitly?,2010-10-15T23:02:18.057,,CC BY-SA 2.5,user28
5041,3651,0,"@Srikant: I think it would help, given the elementary nature of the question.  If you like, add something about alternative tests, too.  For example, I suspect a one-sided KS test might be more powerful for the alternative suggested by the OP (""higher frequencies of purple balls are disproportionately drawn"").",2010-10-15T23:21:14.460,919,CC BY-SA 2.5,
5042,3651,1,@whuber Reg chi-square tests: Done. I feel that any mention of KS test and alternatives to chi-square should be added to your answer instead of mine as the idea is yours.,2010-10-16T00:15:09.130,,CC BY-SA 2.5,user28
5044,3643,0,"The variance is infinite for any normal distribution in the denominator, not just those with zero means. Similarly, the mean is undefined for any normal distribution in the denominator.",2010-10-16T03:53:38.443,159,CC BY-SA 2.5,
5045,3582,0,"@Denis. I meant the question you already asked, viz., ""If variable kernel widths are often good for kernel regression, why are they generally not good for kernel density estimation?"" I would post it myself, but then you would miss out on the rep points.",2010-10-16T03:56:40.620,159,CC BY-SA 2.5,
5046,3632,1,"re: ""use logistic regression to estimate x"".  Logistic regression is not applicable to estimating income (x).  It is for binary dependent variables only.",2010-10-16T04:58:01.913,919,CC BY-SA 2.5,
5047,825,1,"whuber, the solution I present works with win 7 and is non commercial (read the post I linked to for details).  It is bundled with a commercial environment but it can be separated from it (as my post shows how).  And the code itself is GPL...",2010-10-16T08:44:26.107,253,CC BY-SA 2.5,
5050,3660,0,Great... I wasn't aware of that!,2010-10-16T08:47:16.910,930,CC BY-SA 2.5,
5051,3272,0,"Wonderful, Wonderful(!), answer!  Thank you Drury, I'll go through it a few more times to see what I can introduce to my teachings.",2010-10-16T08:55:07.327,253,CC BY-SA 2.5,
5052,3664,1,"(+1) Thanks for mentioning the BA plot. You might be interested in an earlier response I made on a similar topic, http://stats.stackexchange.com/questions/527/what-ways-are-there-to-show-two-analytical-methods-are-equivalent/2834.",2010-10-16T08:55:35.453,930,CC BY-SA 2.5,
5054,3597,0,"@Whuber: are you on windows or a *nix box (mac, linux,...)",2010-10-16T09:07:21.790,603,CC BY-SA 2.5,
5055,3508,1,"The enclosed pictures were my first attempt at using Asymptote (http://j.mp/c8XUGq) instead of Metapost :-) Very sad idea, but I can share the code if you like.",2010-10-16T09:10:05.467,930,CC BY-SA 2.5,
5056,3508,0,That's impressive; please share.  :),2010-10-16T10:09:15.813,251,CC BY-SA 2.5,
5057,3508,3,"@ars Here it is (as Gist): http://gist.github.com/629642, http://gist.github.com/629644, http://gist.github.com/629645.",2010-10-16T10:20:15.527,930,CC BY-SA 2.5,
5061,3666,0,"(+1) I can imagine that with a 1400+ pages book the authors offer several chapters to AN(C)OVA :) BTW, there are SAS and Stata code for most of the chapters on UCLA, http://www.ats.ucla.edu/stat/sas/examples/alsm/",2010-10-16T16:18:15.870,930,CC BY-SA 2.5,
5063,3657,0,"Hello Sarah, you should close this question if you think it is answered.",2010-10-16T21:14:50.140,1307,CC BY-SA 2.5,
5064,3664,0,"onestop and chi - thanks so much for the detailed explanations an dthe refs. I will try to go through the analyses as you described, and if I have more questions, I might pester you again - thanks again for the fast reply!",2010-10-16T21:56:14.883,1603,CC BY-SA 2.5,
5065,3666,0,"Indeed, there are several chapters. I want to say that about half the book is dedicated to AN(C)OVA, while the first half is regression, so that's about 700 pages of analysis of variance. There are parts of the text (block designs, nested designs) that I felt were incredibly boring, and could've used some more work, but the regression sections were great.",2010-10-17T00:08:13.107,1118,CC BY-SA 2.5,
5066,3655,0,"This answer could be applied to any two distributions, it is just that the product of normals is a normal, giving a nice solution. MCMC simulation could be used with pairs of distributions without a closed form solution, using a Bayesian approach with one sample being the prior and the other the likelihood.",2010-10-17T04:38:32.450,1381,CC BY-SA 2.5,
5067,3671,2,"The use of circular graphics for tabular data with Circos has been evoked here too, http://stats.stackexchange.com/questions/3158/what-is-this-type-of-circular-link-visualization-called/3159. My opinion is that this is a good way to reduce large symmetric tables, but it is less useful for summary tables with p-values and the like.",2010-10-17T07:46:22.733,930,CC BY-SA 2.5,
5068,422,8,"I found Statistics in a Nutshell to be seriously flawed in terms of wrong/missing figures, mistakes in formulas, bad explanations and the book doesn't even have tables for critical values. This is especially bad at places where the authors write ""and since the critical value for this is foo, this is significant"", leaving the reader totally unclear about where this foo value comes from. The book does have a good intro section but should be edited eventually to make it good. Just look at the errata page for the book and be stunned at all the errors.",2010-10-17T09:07:29.810,1048,CC BY-SA 2.5,
5069,3597,0,"@Whuber:> there is a problem with winbox: it's not POSIX compliant, as a consequence there is no high prec. timing. This makes it difficult to do things like parallel generation of pseudo random numbers or organisation of tasks. *nix boxes are POSIX compliant. In *nix boxes all cores (as well as the GPU if it's an nvdia) can be easily accessed from within R thru packages such as multicore, gputools, magma,... (which you install as regular packages).",2010-10-17T10:40:55.403,603,CC BY-SA 2.5,
5070,3597,2,I would advise you to install ubuntu (google 'download ubuntu') and to run your windows-only apps from within ubuntu via virtualbox (http://www.youtube.com/watch?v=KXgKnd-u2R4). R and latex editors run on ubuntu like a charm.,2010-10-17T10:41:39.630,603,CC BY-SA 2.5,
5071,3665,0,"Thanks. ""You should always aim to solve the problem at hand directly, rather than use a more general method and post-process the result"" - I was wondering if you have any intuitive/mathematical justification of the statement? Will be especially helpful with the current context.",2010-10-17T11:14:29.023,994,CC BY-SA 2.5,
5072,3672,1,"A bit more details could be helpful; how they are obtained, are branches weighted?",2010-10-17T11:47:41.723,,CC BY-SA 2.5,user88
5073,3672,0,Let me give you an example. I wrote an algo to cluster variables. Clustering can be represented by a dendrogram. This dendrogram changes over time (based on time series). The structure of the dendrogram slowly evolves and I am look for a measure to describe it (sort of descriptive statistics for dendrograms).,2010-10-17T11:59:04.740,1250,CC BY-SA 2.5,
5075,3597,0,@kwak: Many thanks for the suggestions and the information.,2010-10-17T14:44:25.973,919,CC BY-SA 2.5,
5076,3674,2,"In R there is the `profdpm` package, and an overview of available indices (incl. Fowlkes & Mallows's $B_k$) to compare HCs is available here, http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.6189&rep=rep1&type=pdf.",2010-10-17T16:09:08.590,930,CC BY-SA 2.5,
5078,3683,1,Is there an approachable textbook you could recommend that discusses how and why Manhattan Distances are useful in statistics?,2010-10-17T20:44:18.923,1515,CC BY-SA 2.5,
5079,3684,1,"Of course, for discrete distributions, including distributions of finite support (like those in question here), the cf is just the probability generating function evaluated at x = exp(i t), making it a more complicated way of encoding the same information.",2010-10-17T21:24:03.630,919,CC BY-SA 2.5,
5080,3626,0,"Argh, yes, I realised this after some guys at work raised some objections about the categorical variables (interestingly, they didn't complain about the continuous variables!). The problem is even more complicated than I outlined, so I need to think about things a bit more.",2010-10-17T21:51:02.387,1144,CC BY-SA 2.5,
5081,3683,1,"@Kaelin: Unfortunately I can't think of a text which discusses this in particular.  I can tell you that the L1 distance is preferred since it's less sensitive to outliers.  It's also related to distances between empirical distributions in probability theory ( L1 is twice the ""total variation distance"": http://en.wikipedia.org/wiki/Total_variation_distance ).",2010-10-17T22:00:39.113,251,CC BY-SA 2.5,
5082,3687,0,"+1. Though I'm not sure an analysis book, even if it is Rudin, is ""approachable"".  ;-)",2010-10-18T00:35:59.303,251,CC BY-SA 2.5,
5086,3640,0,"david - since your variables are all positive, why do you want to fuss with $\mu = 0$?  btw - in your simulation, you seem to be generating variables per.c and per.n that are independent. is that correct - and if so, is that what you want?",2010-10-18T02:25:53.617,1112,CC BY-SA 2.5,
5087,3684,2,"@whuber: As you say, the cf, mgf, and pgf are more-or-less the same and easily transformable into one another, however Mathematica has a cf builtin that works with all the probability distributions it knows about, whereas it doesn't have a pgf builtin. This makes the Mathematica code for working with sums (and differences) of dice using cfs particularly elegant to construct, regardless of the complexity of dice expression as I hope I demonstrated above. Plus, it doesn't hurt to know how cfs, FTs, convolutions, and cross-correlations can help solve problems like this.",2010-10-18T02:38:02.587,,CC BY-SA 2.5,A. N. Other
5088,3643,0,"@rob - how do you define the variance of Y = 1/X when X is normal, as EY is undefined?",2010-10-18T03:26:29.073,1112,CC BY-SA 2.5,
5089,3684,1,"@Elisha: Good points, all of them.  I guess what I wonder about the most is whether your ten or so lines of Mathematica code are really more ""elegant"" or efficient than the single line I proposed earlier (or the even shorter line Srikant fed to Wolfram Alpha).  I suspect the internal manipulations with characteristic functions are more arduous than the simple convolutions needed to multiply polynomials.  Certainly the latter are easier to implement in most other software environments, as Glen_b's answer indicates.  The advantage of your approach is its greater generality.",2010-10-18T03:35:44.683,919,CC BY-SA 2.5,
5090,3687,0,"@ars: Yes, but I don't know of any one that really is.  That's why I pointed out the two Wikipedia articles.",2010-10-18T03:37:59.607,919,CC BY-SA 2.5,
5091,3643,0,"@ronaf. Good point. E[Y^2] is infinite whenever X is normal. The same goes for E[Y^m] for any even m. If m is odd, the result is undefined. Is that better?",2010-10-18T03:57:29.553,159,CC BY-SA 2.5,
5092,3685,0,"I have also wondered about this problem, but (unfortunately) haven't found any convincing answers yet. I think there is no solution. There are R/BioC packages like `hopack` (and others) which can estimate the number of clusters, but that doesn't answer your question.",2010-10-18T04:02:09.440,1307,CC BY-SA 2.5,
5093,3687,0,"I know, I liked it -- it's the right recommendation to make in case the OP wants to dig deeper.",2010-10-18T04:12:20.807,251,CC BY-SA 2.5,
5094,3618,2,"@Srikant: Expand[Sum[x^i,{i,1,6}]^3] also works in WolframAlpha",2010-10-18T07:04:52.270,,CC BY-SA 2.5,A. N. Other
5098,3698,0,"Hmm, that's what I get too: |O-E|/sqrt(E) = |17-16.5|/16.5 = 0.123. Can you add a link to the paper if it's available on the web?",2010-10-18T09:57:08.440,449,CC BY-SA 2.5,
5100,3698,0,"Unfortunately it isn't. I have collected comparable data and wish to compare my findings with the publication. I have found that the R function prop.test gives results that approximately agree with the paper, but I wanted to understand how the author arrives at his exact numbers.",2010-10-18T10:11:34.200,1614,CC BY-SA 2.5,
5101,3698,0,I've contacted the author. Perhaps he can shed light on the problem.,2010-10-18T11:27:15.950,1614,CC BY-SA 2.5,
5102,3636,0,@whuber - the std dev would give me a number that tells me how much the temperature is varying.,2010-10-18T12:17:21.790,1595,CC BY-SA 2.5,
5103,3636,0,"@Shane - It's a temperature reading.  How would I know if it is normally distributed?  It should be i.i.d., but I'm just a lowly engineer trying to implement something management wants.",2010-10-18T12:19:56.933,1595,CC BY-SA 2.5,
5104,3698,0,Are these paired proportions or two independent samples?,2010-10-18T12:32:14.880,930,CC BY-SA 2.5,
5106,3701,0,"That was my understanding and like I say, the prop.test function of R gives the same direction of results. I just wondered if there was something that I was doing wrong to not get the z=.08.",2010-10-18T12:55:33.780,1614,CC BY-SA 2.5,
5109,3690,0,"Thanks for the answer! However, I'm interested in modeling individuals within groups where group-level effects are difficult to quantify or anomalous and can't easily be included in future predictions. To alter your example a little, suppose we're predicting minute usage for individual customers over the next month using the last three month's of data. Suppose that while customer age is a significant predictor of usage, a local concert last month made young users unusually likely to make phone calls. The 'age' coefficient may be unnaturally large and skew predictions. Does this make sense?",2010-10-18T13:20:46.280,1611,CC BY-SA 2.5,
5110,3702,2,"Could you try formating your data so that we can get a clearer idea of what you want. For example, I don't see an 8x11 matrix in your example.",2010-10-18T13:52:17.997,8,CC BY-SA 2.5,
5112,3702,0,"Thanks for quick reply, I am trying format matrix in this windows but quit difficult. I describe my matrix in word: the 8x11 matrix comprise of 8 rows corresponding to ages (1, 1.25,1.5,1.75,2,3,4,5) and 11 columns (first column name ""age"", then boy01...boy05, girl01...girl05) the values of matrix is the height of 5 boys and 5 girls.",2010-10-18T14:07:04.090,1615,CC BY-SA 2.5,
5113,3669,0,Thanks. I will pose another question as I am not a stats expert and am not sure my question was clear.,2010-10-18T14:11:12.973,834,CC BY-SA 2.5,
5114,3702,0,"I do not know how to present the height of group boy as ""$hgtm"", group girl as ""hgtf"" and age column as ""$age"" as display above.",2010-10-18T14:15:34.960,1615,CC BY-SA 2.5,
5115,3690,0,@danpelota That is an example of 'interactions'. I will edit the answer to clarify.,2010-10-18T14:16:28.513,,CC BY-SA 2.5,user28
5117,3702,0,"At this point, I'm a little confused about the data structure too.  Maybe describe what you're trying to do?  You have a set of boys and girls, along with their respective heights and ages?  Are the heights measured at different ages?",2010-10-18T14:29:14.443,5,CC BY-SA 2.5,
5118,3673,2,"Thanks chl. I think the problem is that many of the answers were too good - e.g. a search for ""patient cluster analysis"" in Google scholar returns 650,000 results. So rather than saying ""here are some good directions to go in,"" I wanted a more discrete bibliography.",2010-10-18T14:39:03.467,900,CC BY-SA 2.5,
5119,3702,2,"As it hasn't been mentioned before, there are *introductory manuals* that come with R. The *Introduction* will teach you the basics of data structure use (and more); the *Data Import/Export* is helpful for data transfer questions.",2010-10-18T14:44:16.050,334,CC BY-SA 2.5,
5120,3704,1,"Please explain why you're not satisfied with suncoolsu's answer to your previous version of the same question http://stats.stackexchange.com/q/3653/449.
Also please indicate what software the code in your question is for, and add an appropriate tag. That way, someone with experience in that software package might be able to suggest explicit code.",2010-10-18T14:52:05.733,449,CC BY-SA 2.5,
5121,3657,0,"Hi - Thanks again for your answer. I forgot to mention that I am using Stata. When I add two coefficients together (using the output from Stata), can I also just add the standard errors? If so, then I should be able to obtain the standard errors by dividing the sum of the coefficients by the sum of the standard errors. Do you agree? Thanks again.",2010-10-18T15:13:34.937,834,CC BY-SA 2.5,
5122,3704,0,Sorry for posting a second question. For some reason I can't figure out how to edit my question. I'm new to the site so please bear with me. And thanks again to those who have provided answers so far!,2010-10-18T15:15:52.340,834,CC BY-SA 2.5,
5123,3708,0,What are the geographic units and do you expect them to have any geographic dependency (i.e. spatial autocorrelation?) Although we would always like to have more data you have a reasonable amount of observations to project estimates.,2010-10-18T15:28:59.090,1036,CC BY-SA 2.5,
5125,3707,0,Since their is some confusion over what we are seeing can you state how you standardized the distributions and why you standardized the distributions? Also some greater context as to your motivation might be nice although not necessary.,2010-10-18T15:33:28.003,1036,CC BY-SA 2.5,
5126,3708,0,"The 100 regions belong to the same country, so I expect that they are correlated in time and in space (in fact I began with a correlation clustering exercise that shows that they are correlated)...",2010-10-18T15:35:34.960,1443,CC BY-SA 2.5,
5127,3657,0,"Sarah, In Stata, use the 'lincom' function. Suppose you have variables var1 and var2 and want to add 3 times the coefficient on var1 and 2 times the coefficient on var2. Type 'lincom 3 * var1 + 2 * var2'. This gives the standard error and confidence interval for this estimate.",2010-10-18T15:42:56.263,401,CC BY-SA 2.5,
5128,3708,0,"What is the frequency (i.e. is it weekly,monthly, quarterly, annual data) ?",2010-10-18T15:44:28.393,603,CC BY-SA 2.5,
5129,3711,0,"I'm not sure this is right. U^n corresponds to multiplying the *same* random value by itself n times. dassouki's procedure will multiply n *different* random values. As an example, the product distribution of two standard normal r.v.'s is *not* chi-squared on 2 d.f : http://dx.doi.org/10.1137%2F0118065",2010-10-18T15:45:34.117,449,CC BY-SA 2.5,
5130,3708,0,"Annual data, so I have only 9 points per region...",2010-10-18T15:45:52.540,1443,CC BY-SA 2.5,
5131,3706,1,I did not downvote but you should see the other thread as to why this is not ok.,2010-10-18T15:47:05.240,,CC BY-SA 2.5,user28
5132,3704,1,You can edit the question by clicking 'edit' which appears below the tags for your question. If some aspect of suncoolsu's answer is not clear you should ask for clarification via commenting to the answer. There is a 'add comment' link below every answer.,2010-10-18T15:49:16.903,,CC BY-SA 2.5,user28
5133,3711,0,@onestop hmm your are right but I think the reasoning carries over to the general case. I will think about it and fix the answer.,2010-10-18T15:50:32.890,,CC BY-SA 2.5,user28
5134,3712,0,"Thanks so much Charlie. This is very useful. I didn't know about the lincom command so this will save me lots of time. However, if I want to get the impact of treatment on the outcome for quintile 2 should i not add treat + treatXquin2 (rather than adding quintile2 + treatXquin2)?",2010-10-18T15:53:39.783,834,CC BY-SA 2.5,
5136,3711,0,"@Srikant: The conclusion about convergence to a delta function isn't terribly useful or insightful.  (Analogously, if one weren't to adopt the right standardization of sums of random variables, the CLT would vanish into thin air: you would often just conclude that the sums ""converge"" to an improper uniform distribution over all the reals!)  A better way to investigate such phenomena is to seek some standardization that assures convergence to something non-trivial if that is at all possible.",2010-10-18T16:03:29.143,919,CC BY-SA 2.5,
5140,3711,2,@Srikant: The distribution of the product of n uniforms is the exponential of a (negated) Gamma(n) variate.,2010-10-18T16:08:16.203,919,CC BY-SA 2.5,
5142,3711,0,@whuber You are correct about standardization but I did not interpret the question that way (or at least it was not clear from the question context that the OP wanted to know the behavior of the product of different standardized uniform random variates. Thanks for the pointer on the product of n uniforms. I will update the answer to reflect that info.,2010-10-18T16:14:29.067,,CC BY-SA 2.5,user28
5144,3712,0,Charlie - I'm not sure I understand why you multiply treatXquin2 by 1 for yes. If that is the case then how do I obtain the estimate for the untreated (i.e. treat==0)? It doesn't seem to make sense that I would multiply treatXquin2 by 0. Any help would be much appreciated!,2010-10-18T16:17:59.827,834,CC BY-SA 2.5,
5145,3647,0,I just discovered that the data I have covers 2 periods in the last 10 years: 2 counts at -5 and -10 years,2010-10-18T16:42:34.627,59,CC BY-SA 2.5,
5146,3640,0,"no, I don't want to fuss with $\mu$ = 0; these variables are generally treated as independent, and covariance data is rarely available. Since C is fairly constant, independence is a reasonable assumption.",2010-10-18T16:47:19.380,1381,CC BY-SA 2.5,
5148,3705,2,Another point in favor of the second layout is that it is consistent with good database practices. The data normalization advantages of databases and a knowledge of SQL is very useful if used appropriately.,2010-10-18T17:10:43.347,,CC BY-SA 2.5,user28
5150,3658,1,"Thanks for the response. Don't feel obligated to update anything (as it is not your job to teach me mathematics), but I do not have any clue how you ""integrate over the prior distributions"". Does it involve just calculating all possible combinations since my distributions are uniform?",2010-10-18T18:10:49.120,1036,CC BY-SA 2.5,
5151,3705,0,"@Srikant: Thanks, I've updated my post to include.",2010-10-18T18:20:47.620,485,CC BY-SA 2.5,
5152,3711,0,"@Srikant: Yes, there's always opportunity for multiple interpretations.  I took the OP's careful investigation, including the standardization and graphing, as an indication of a deeper and more sophisticated perception of the phenomenon than just being something ""getting to zero"" or converging to a delta distribution.  The ultimate arbiter of the question's intent is its proposer, of course.",2010-10-18T18:25:21.273,919,CC BY-SA 2.5,
5155,3715,1,As I understand the [Agresti-Coull](http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Agresti-Coull_Interval) procedure is used to generate approximate confidence intervals for the binomial proportion confidence interval. I am not sure I see the relationship to Bayes theorem. Could you provide some context and your goals within the context to understand how the bayes arises in the context of the Agresti-Coull method?,2010-10-18T18:55:43.290,,CC BY-SA 2.5,user28
5157,3717,2,"@Andy Perhaps, spatial dependencies exist because employers tend to co-locate in certain geographical areas which has spill-over effects to neighboring areas? For example, during the IT bust of 2000 I am sure several regions around the Silicon valley would have had high unemployment rates but perhaps not in Detroit (which is dominated by the auto industry).",2010-10-18T18:58:43.033,,CC BY-SA 2.5,user28
5159,3717,0,"@Srikant you are right. I was thinking about it some more and spatial externalities are much more likely to occur in economic data than in crime data I am used to working with. Although I would still be skeptical with a lag of a year, it deserves more attention than my answer would suggest.",2010-10-18T19:02:11.980,1036,CC BY-SA 2.5,
5161,3717,0,"The effects that Srikant has highlighted cannot be ignored to my mind, these need to be taken into account...",2010-10-18T19:10:58.563,1443,CC BY-SA 2.5,
5163,3717,0,"@Srilant:> what you see is correct, and J. Hamilton has done some work on economic contagion between US states. But these sorts of model need more than 9 observations per cross section to be estimated. For this types of situation, i recommend the A/B estimator (see my answer) since it has been designed, *precisely* for these types of situation (small **T** large **N**).",2010-10-18T19:21:28.273,603,CC BY-SA 2.5,
5164,3715,0,@cool-RR: How exactly are you estimating those three probabilities?,2010-10-18T19:50:26.933,919,CC BY-SA 2.5,
5166,2214,0,"My understanding is that you need a finite mean to for a population L-*moment* to be defined, but not for the estimands corresponding to all other L-estimators. For example, the sample median is an L-estimator, though it isn't an L-moment.",2010-10-18T20:59:37.130,449,CC BY-SA 2.5,
5167,3715,0,"@whuber: For $P(A)$ I check how many of the messages in my finite sample have $A$, and divide that by the size of the sample. For $P(B|A)$ I check how many of the messages that have $A$ also have $B$, and divide that by the number of messages that have $A$. etc.",2010-10-18T21:01:36.793,5793,CC BY-SA 2.5,
5168,3715,2,"@cool-RR: why don't you just estimate P(A|B) directly, then, as the fraction of B samples that are also A?",2010-10-18T21:03:13.997,919,CC BY-SA 2.5,
5170,3715,0,"@Srikant: Agresti-Coull lets you draw conclusion about the true proportion from a small finite sample, and gives you a confidence interval with an answer. I have a similar situation: I have a small finite sample and I want to get the value of $P(A|B)$ along with a confidence interval. To give more context: I'm developing something like a Bayesian spam filter, and this is the probability that a message is spam given that it has a certain word.",2010-10-18T21:05:01.823,5793,CC BY-SA 2.5,
5171,3725,10,"VAR is unbiased for any distribution with a finite variance, not just for the normal distribution.",2010-10-18T21:08:53.843,159,CC BY-SA 2.5,
5173,3717,4,I wouldn't use a univariate time series method with 9 observations.,2010-10-18T21:12:58.030,159,CC BY-SA 2.5,
5174,3673,0,"(+1) Thanks for the precision. It's easier to edit your original question and ask for dedicated references, otherwise some of us may be confused; without any upvote we cannot know whether responses were useful or not, nor improve them. Good luck anyway!",2010-10-18T21:19:10.800,930,CC BY-SA 2.5,
5176,3720,0,"Given that the unemployment rate must be strictly positive, I guess, for simulation purposes, I could model the log returns for example. Is this reasonable?",2010-10-18T21:49:28.387,1443,CC BY-SA 2.5,
5177,3724,0,"@onestop The ""statistical-bias"" tag is somewhat confusing here.",2010-10-18T21:57:23.987,930,CC BY-SA 2.5,
5179,3715,0,"Wow @whuber, you blew my mind. I have no idea why I've been doing this. I got the impression this is what people do on Bayesian spam filters, so that's what I've been doing. ($P(A|B)$ is the probability a message is spam given that it contains a certain word, for example 'cheese'.) Now I tried to simply calculate $P(A|B)$ directly, and it seems to work, so I don't even need the answer to this question. Unless I'm missing something. Maybe you have an idea why people use Bayes' Theorem in Bayesian spam filtering when filtering by a single word like this?",2010-10-18T22:04:44.817,5793,CC BY-SA 2.5,
5180,3712,0,"Sorry, I misread your question. Add treat + treatXquin2 for the treatment impact for those in quintile 2. I have updated my answer.",2010-10-18T22:05:24.367,401,CC BY-SA 2.5,
5181,3724,0,"Perhaps you're right. The guidelines suggest updating tags in light of the the answers as well as the original question, so I was wondering if there was a tag for 'unbiased estimator' or 'unbiasedness', but there wasn't, so i used 'statistical bias' on the grounds that separate tags for flip sides of the same coin seem a bit unecessary, and the guidelines favor reuse of existing tags over creation of new ones. I'm about to fall sleep but I'll take another look at this in the morning.",2010-10-18T22:13:36.427,449,CC BY-SA 2.5,
5182,3706,0,"Sorry, I downvoted this answer to indicate I disagreed, but that was an inapproriate and lazy way of doing so, for which I apologise. I'm afraid it's now too late to undo my vote, however - the software won't allow it, for reasons I don't entirely understand. I've just upvoted your question in an attempt to balance things out, though I realise two wrongs don't make a right.",2010-10-18T22:21:23.707,449,CC BY-SA 2.5,
5183,3725,0,"Right, whatever variance mean when the distribution is not close to normal ;-)",2010-10-18T22:26:47.270,,CC BY-SA 2.5,user88
5184,3724,0,I changed statistical-bias to unbiased-estimator. Does that work?,2010-10-19T00:14:47.540,,CC BY-SA 2.5,user28
5185,3731,3,No. Pearson's correlation does NOT assume normality. It is an estimate of the correlation between any two continuous random variables and is a consistent estimator under relatively general conditions. Even tests based on Pearson's correlation do not require normality if the samples are large enough because of the CLT.,2010-10-19T01:46:20.777,159,CC BY-SA 2.5,
5186,3731,2,"I am under the impression that Pearson is defined as long as the underlying distributions have finite variances and covariances. So, normality is *not* required. If the underlying distributions are not normal then the test-statistic may have a different distribution but that is a secondary issue and not relevant to the question at hand. Is that not so?",2010-10-19T01:47:13.190,,CC BY-SA 2.5,user28
5187,3731,0,"@Rob, @Srikant: True, I was thinking of significance testing.",2010-10-19T01:59:28.437,251,CC BY-SA 2.5,
5189,3734,12,"Good question, although my immediate reaction, backed up by my limited experience of teaching this, is that the CLT isn't initially at all intuitive to most people. If anything, it's counter-intuitive!",2010-10-19T02:39:52.323,449,CC BY-SA 2.5,
5190,3008,0,Great code but there is a problem. I am not as smart as you are. I need it broken down stepwise. I take it is runs the number of simulations? What is nn? Is it the number of subjects in the study? Then I see you created a distribution of covariates and made them determine a yes or a no depending on a threshold.,2010-10-19T03:03:51.900,104,CC BY-SA 2.5,
5191,3643,0,"@rob + @david  if the variables in the ratio are *asymptotically* normal [with a non-zero mean in the denominator] and also consistent, the ratio is also asymptotically normal - as the usual delta method shows. perhaps that will render moot the discussion about non-existence of moments.",2010-10-19T03:15:10.907,1112,CC BY-SA 2.5,
5192,3643,0,"@ronaf. Actually, that doesn't help. It is possible to have asymptotic normality as well as having non-existent moments. Asymptotics with probability distributions can have weird properties.",2010-10-19T03:16:36.970,159,CC BY-SA 2.5,
5193,3734,4,@onestop AMEN! staring at the binomial distribution with *p* = 1/2 as n increases does show the CLT is lurking - but the intuition for it has always escaped me.,2010-10-19T03:18:40.243,1112,CC BY-SA 2.5,
5194,3725,2,"@mbq to some, the variance is a moment of inertia. there is also chebyshev's inequality, that applies to any distribution. it also has an interpretation similar to that for the normal for any location/scale family with a finite second moment - like the logistic, for example. [the pdfs need not be symmetric, tho.]",2010-10-19T03:34:14.603,1112,CC BY-SA 2.5,
5195,3643,0,"@rob - i have in mind the ratio of two sample means. even tho the actual moments may be infinite or undefined, there is an asymptotic mean and asymptotic variance that go with the limiting normal distribution [again assuming the denominator is not consistently estimating zero]. it is often the parameters of the asymptotic distribution that are relevant for analyses of the data [judging by the way in which many statistical analyses are carried out these days]. in that case, the non-existence of actual moments is a side issue [or a quibble?].",2010-10-19T03:44:15.610,1112,CC BY-SA 2.5,
5196,3725,0,"altho, as whuber points out, there are two excel functions for variance, if my recollection is correct, there is only one for covariance - and it divides by n, not n-1. [i haven't checked the latest version of excel, tho. can someone tell us if that is still the case?]",2010-10-19T03:56:00.090,1112,CC BY-SA 2.5,
5197,3643,0,"@ronaf. I don't really follow your last comment. The asymptotic normal distribution will have a well-defined mean and variance. But they are not the same as the true asymptotic mean and variance which are undefined whenever the denominator has a non-zero density at 0. In practice, this may not matter. Imagine, for example, if the denominator is N(100,1). Then the sample values of the ratio will behave nicely with very high probability.",2010-10-19T04:26:18.347,159,CC BY-SA 2.5,
5198,3720,0,"@kwak, could you provide a link to the Hamilton work you talked about in a comment to my answer. Also do you know of any work that used the A/B estimator and included spatial effects in the models?",2010-10-19T04:40:45.767,1036,CC BY-SA 2.5,
5201,3731,0,"@Srikant: I'm not sure it's a ""secondary issue"". You can compute anything after all -- it's the inference that matters. @Rob: your ""if"" qualifier is key here -- it seems to me that's central to this question. We can justify a whole lot with asymptotic hand waving; exceptions matter.",2010-10-19T05:54:39.277,251,CC BY-SA 2.5,
5202,3718,2,"Excel's RAND() is the worst pseudo RNG I have ever measured.  (I applied the DieHard tests to it about ten years ago.)  It's not even equidistributed!  Nevertheless, it's OK for this kind of investigation.",2010-10-19T05:55:05.900,919,CC BY-SA 2.5,
5204,3739,0,"the distribution is given by a few parameter already with your definition ? what is the difference for the application you havde in mind, between sampling things directly and using a transfomation after sampling ?",2010-10-19T06:20:05.567,223,CC BY-SA 2.5,
5205,3713,1,Can you try to give more specific description of what you want to cluster? or is it just a state of the art in clustering that you need?,2010-10-19T06:23:19.697,223,CC BY-SA 2.5,
5206,3702,0,"I don't see the question here. It seems to be ""how to load that type of data"" as suggested by Dirk, but the data seems already here. I vote for close.",2010-10-19T06:28:35.707,223,CC BY-SA 2.5,
5207,3734,2,Similar question with some nice ideas: http://stats.stackexchange.com/questions/643/how-do-you-convey-the-beauty-of-the-central-limit-theorem-to-a-non-statistician,2010-10-19T06:42:56.980,,CC BY-SA 2.5,user88
5209,832,0,"Looks like it could be very nice, but I just tried installing and running it in Stata 11.1 (i.e. the latest version) and it keeps giving me an r(3000) error upon clicking ""Done"" in the dialog, even if i type -version 6: clt-.",2010-10-19T06:49:38.010,449,CC BY-SA 2.5,
5211,3720,0,@Andy W:see edited answer above.,2010-10-19T07:02:19.660,603,CC BY-SA 2.5,
5212,3720,0,"@Teucer: certainly. There are some facilities in the plm package to do these transformations (i.e. log, difference, dlog,...) in a single command-line: look for the **dynformula()** function.",2010-10-19T07:04:29.857,603,CC BY-SA 2.5,
5213,3720,0,"@kwak Actually what you propose is very close to what I have in mind: I was using the package lme4 to model the log returns. However, if I well understood, the difference lies in the estimator: lme4 uses REML whereas with plm you can use GMM. Is that correct? If so, why GMM is in this case superior to REML? Btw do you think that one can use the package heavy which is fitting with heavy tailed distributions (e.g. Student t)?",2010-10-19T07:08:34.377,1443,CC BY-SA 2.5,
5214,3745,0,"Unless I'm misleading, the second moment are not exactly identical between the two.",2010-10-19T07:18:07.400,930,CC BY-SA 2.5,
5216,3720,0,"@Teucer. The difference between the two approaches is discussed in the vignette to the plm package. I would think that gmm estimation would be more efficient in your case, because one can assume that the innovations to the unemployment rate have a well behaved Gaussian distribution (i.e. as aggregation of a large number of small shocks). This assumption also explain why i don't understand the need to use the **heavy** package.",2010-10-19T07:20:34.490,603,CC BY-SA 2.5,
5217,3724,0,"Fine with me. I've just added this new tag to a few other old posts - what do you think? Also does the ""statistical bias"" tag still have a place? If so, when would you use it? Is this what the ""tag wiki"" system is meant to be for? I'm afraid I haven't worked out if or how i can edit that. Maybe we should take this to meta...",2010-10-19T07:24:00.550,449,CC BY-SA 2.5,
5220,3745,0,"@chi - I meant the usual parameterisation of the logit-normal is in terms of the mean and SD of the related normal dist, not that the mean and SD of the logit-normal are the same as that of the related normal. Sorry if that was, or still is, unclear - i'm editing in a hurry as i should really have left for work by now!",2010-10-19T07:29:31.357,449,CC BY-SA 2.5,
5222,3733,18,"Pearson's $\rho$ does not assume normality, but is only an exhaustive measure of association if the joint distribution is multivariate normal. Given the confusion this distinction elicits, you might want to add it to your answer.",2010-10-19T07:42:26.000,603,CC BY-SA 2.5,
5223,3744,7,"I'm also a big fan of Kendall's tau. Pearson is far too sensitive to influential points/outliers for my taste, and while Spearman doesn't suffer from this problem, I personally find Kendall easier to understand, interpret and explain than Spearman. Of course, your mileage may vary.",2010-10-19T07:44:51.847,1352,CC BY-SA 2.5,
5224,3733,0,@kwak. Good point. I'll update the answer.,2010-10-19T07:45:38.827,159,CC BY-SA 2.5,
5226,3717,1,"Yes, simple methods usually work better than complex ones (sometimes to a surprising extent). However, in this case univariate methods would throw away 99% of the data - whenever we forecast for one geography, we disregard all the other geographies. And I would definitely expect some kind of panel data model to be better than a univariate one. Not so much because employers co-locate, but because regulatory, tax, central bank and other external factors will be common and largely similar drivers of unemployment for *all* geographies.",2010-10-19T07:49:56.843,1352,CC BY-SA 2.5,
5227,3740,0,"Try the plots with a N(0,10^2) (i.e., 10 is standard deviation). It will not appear to be normal anymore.",2010-10-19T08:21:24.067,,CC BY-SA 2.5,user28
5228,3747,0,"Thanks @Stephan Kolassa, this will already help towards finding a solution.",2010-10-19T08:39:35.277,1623,CC BY-SA 2.5,
5229,3720,0,"@kwak On using heavy: I believe that the estimates are  more robust with heavy tailed innovations (my measurements are imperfect), but I might be wrong. Another reason was that I did a correlation clustering and looked at the distribution of log returns per year for each cluster, a student t distribution fits not too bad. But maybe it is irrelevant here. I have some questions: now assume I have more points, let's say about 25 points per region, would you still use GMM or REML for estimation? Where is the threshold? For a small sample is the asymptotic efficiency relevant?",2010-10-19T09:12:09.160,1443,CC BY-SA 2.5,
5230,3740,0,"@Srikant I agree, this is an answer to second point of the question.",2010-10-19T09:14:24.320,,CC BY-SA 2.5,user88
5231,3720,0,"@Teucer:> *Another reason was that I did a correlation clustering and looked at the distribution of log returns per year for each cluster, a student t distribution fits not too bad.* this does not in itself justifies using a $t$ distribution: mixes of Gaussian distribution with varying variances, for instance, will also converge to a fat tailed distribution.",2010-10-19T09:32:10.207,603,CC BY-SA 2.5,
5232,3720,0,"@Teucer:> * For a small sample is the asymptotic efficiency relevant?* I think there is a slight misunderstanding here. If your assumptions on the distribution of the residuals are correct, then asymptotic efficiency is a measure of mean accuracy of your estimates (the 'asymptotic' here refers to the relative average precision over a large number of estimation instances).",2010-10-19T09:35:33.387,603,CC BY-SA 2.5,
5233,3720,0,"@Teucer:> *now assume I have more points, let's say about 25 points per region, would you still use GMM or REML for estimation? Where is the threshold?* At some point, the balance indeed tilts. And for larger sample sizes the gains in efficiency do not outweighs the costs in (statistical **AND** computational) complexity of the GMM approach over the REML one. The exact point depends also to the extend to which your data conforms to the working assumptions underlying each model (REML does not,imho, have less requirements, just different ones)....",2010-10-19T09:39:28.890,603,CC BY-SA 2.5,
5234,3697,3,See http://stackoverflow.com/questions/2492947/boxplot-in-r-showing-the-mean for solutions using R,2010-10-19T09:40:03.723,229,CC BY-SA 2.5,
5235,3720,0,"@Teucer:> It's certainly a good sign if the two approaches do not lead to wildly different results (in the hypothetical that this would not be the case, one should explain why)",2010-10-19T09:40:57.370,603,CC BY-SA 2.5,
5237,3582,0,"@Rob:> given that three days have elapsed, i think for the benefit of the wider community, you should consider posting this as a separate question.",2010-10-19T10:55:46.730,603,CC BY-SA 2.5,
5238,3724,0,"@Skrikant In fact, I didn't think another tag was needed at all, but yours looks better.",2010-10-19T11:22:42.143,930,CC BY-SA 2.5,
5240,3582,0,@kwak. OK. I've posted something at http://stats.stackexchange.com/questions/3752/,2010-10-19T11:35:49.087,159,CC BY-SA 2.5,
5241,3732,0,"(+1) Thanks for the link! You may be interested in this related question, http://stats.stackexchange.com/questions/3200/is-adjusting-p-values-in-a-multiple-regression-for-multiple-comparisons-a-good-id. Feel free to contribute.",2010-10-19T11:37:43.700,930,CC BY-SA 2.5,
5242,3720,2,"@kwak, the spatial effect you suggest is what is referred to as local Geary's C, Global formula, http://en.wikipedia.org/wiki/Geary's_C, (or here is a link for the local version  http://www.passagesoftware.net/webhelp/Introduction.htm#Local_Geary_s_c.htm ) you could also consider local Moran's I, http://en.wikipedia.org/wiki/Indicators_of_spatial_association",2010-10-19T11:41:37.173,1036,CC BY-SA 2.5,
5243,3731,0,"@ars,@Srikant. Even with small samples, you can still do inference on correlations, but not using the asymptotic normality result.",2010-10-19T11:41:55.763,159,CC BY-SA 2.5,
5244,3746,1,"This answer is similar enough to help you with the averaging of the points, http://stats.stackexchange.com/questions/2493/managing-error-with-gps-routes-theoretical-framework/2497#2497, it is simple to incorporate weights in that framework. I would think you would be able to use some simple heuristics to identify outliers, but that doesn't preclude you from taking a more empirical approach like Stephan suggested.",2010-10-19T11:52:04.783,1036,CC BY-SA 2.5,
5245,3752,2,"You wrote ""On the other hand, variable kernels are usually thought to lead to poor estimators in kernel density estimation"", what is the part of the paper you mention that makes you believe that ? I have plenty of references that go in the other derection, see for example the references mentioned in this paper: http://arxiv.org/PS_cache/arxiv/pdf/1009/1009.1016v1.pdf",2010-10-19T12:37:01.253,223,CC BY-SA 2.5,
5246,3752,1,"The abstract of Terrell and Scott summarises it nicely: ""Nearest neighbor estimators in all versions perform poorly in one and two dimensions"". They only seem to find much advantage in multivariate density estimation.",2010-10-19T12:50:45.813,159,CC BY-SA 2.5,
5247,3720,0,"also to note how you define k is only limited to your imagination. There is currently no consensus on what is proper or improper, although many people suggest you try to optimize it like you suggested.",2010-10-19T13:05:20.750,1036,CC BY-SA 2.5,
5248,3755,0,"I'm running Windows 7 64-bit on Quad CPU 2.67 GHz, 4GB RAM",2010-10-19T13:07:04.933,315,CC BY-SA 2.5,
5249,3732,0,"@chl, I don't think I can add anything to the already excellent answers for that question. I actual think Brendan's response is very poignant because I suspect the original poster is really interested in causal inference not solely prediction based on the context of the question.",2010-10-19T13:12:58.473,1036,CC BY-SA 2.5,
5250,3720,0,"@kwak @Andy thx for all the explanations. Just another question: now let's assume that I can further aggregate my regions on some broader regions (synthetic, using correlation clustering, or economic) would it make sens to compute the average  $\bar{u}_{‚àíit}$ on these regions?",2010-10-19T13:13:00.600,1443,CC BY-SA 2.5,
5251,3752,3,"""Nearest neighbor"" is not the only variable kernel. The papers I mention use other tool such as Lepskii's algorithm. I'll read the AOS paper but as the performences of nearest neighbor should decrease with the dimension, I found it strange that increasing the dimension gives advantages to a ""very non-parametric"" estimator (If we admit constant bandwidth is less non parametric than varying bandwith). In this type of situation, the evaluation case that is used often determine the results ...",2010-10-19T13:13:58.207,223,CC BY-SA 2.5,
5252,3720,0,"@teucer, your asking if it would make sense to use that spatial neighborhood average as a predictor right? That is close to what local Moran's I does, it makes no difference if you choose neighbors based on theoretical reasons or empirical ones. I can probably guess why kwak initially suggested using the spatial differences as opposed to the average, but I will let kwak clarify.",2010-10-19T13:22:16.067,1036,CC BY-SA 2.5,
5255,500,2,"You may want to read this answer to a seperate question and see why adjusting p-values in such a manner may not be the best solution, http://stats.stackexchange.com/questions/3200/is-adjusting-p-values-in-a-multiple-regression-for-multiple-comparisons-a-good-id/3317#3317",2010-10-19T14:02:05.563,1036,CC BY-SA 2.5,
5256,3731,0,"@Rob: Sure, but it seems this is where one should advocate Spearman's method over Pearson's.  For example suppose small samples where X is normal but Y isn't -- you can compare the two on even terms with ranking methods such as Spearman's.  Using Pearson's requires more work, for example, finding an appropriate transformation.",2010-10-19T14:18:59.750,251,CC BY-SA 2.5,
5257,3759,1,"Hey @Jonathan; machine learning is on-topic here (in fact, the machine learning proposal was merged with this one): http://meta.stats.stackexchange.com/questions/492/are-the-machine-learning-questions-on-topic",2010-10-19T14:32:58.203,5,CC BY-SA 2.5,
5259,3720,0,"@kwak, I also think the random component to the error term is a very good idea. There are many logical situations in which you wouldn't expect Beta(s) to be the same in different regions. I remember an example that New York City probably influences its neighbors, but the neighbors of NYC are less likely to influence it.",2010-10-19T14:40:56.860,1036,CC BY-SA 2.5,
5260,3720,0,"@Teucer:> As Andy W said, any distance measure from which you can obtain a matrix of pairwise distances (geographic, economic as well as those coming from a clustering algorithm and there variations) are certainly to be tried. @Andy W: i don't advise Teucer to directly use the local average as a predictor because of the risk of correlation between the component of the residuals accounting for heterogeneity in the $\beta_s$'s and the local average. Is this what you had in mind ?",2010-10-19T14:57:52.290,603,CC BY-SA 2.5,
5261,3720,0,@Andy W:> that (NYC) is a very good intuitive example. Worth using as an illustration.,2010-10-19T14:58:55.777,603,CC BY-SA 2.5,
5262,3713,2,I don't have an immediate application in mind.  I'm just interested in a general approach to choosing a clustering method and measure of similarity.,2010-10-19T15:02:42.813,485,CC BY-SA 2.5,
5264,3720,1,"@kwak, that was actually not the reason I imagined (I was thinking more along the lines of stationary estimates of Beta(s)). Local Moran's I does scale the average (ie it Z scores the average of the neighbors based on global mean and variance), but I think your concern is still legitimate. Is that a big deal though if your only interested in prediction?",2010-10-19T15:16:44.573,1036,CC BY-SA 2.5,
5265,3732,0,"Yes, I was thinking of his answer. I have initiated a reflexion on data dredging issue (not exactly about model/variable selection issues or causal inference), but so far receive few responses. If you like to add your own ideas, it would be interesting: http://stats.stackexchange.com/questions/3252/how-to-cope-with-exploratory-data-analysis-and-data-dredging-in-small-sample-stud",2010-10-19T15:38:37.703,930,CC BY-SA 2.5,
5266,3760,1,+1 for linking to the full essay rather than one of the all-too-common oversimplifications - it's not long and is still relevant today and well worth reading in full. Hill's criteria for causation are about much more than just pyramid / piling up of evidence though.,2010-10-19T15:39:25.823,449,CC BY-SA 2.5,
5267,3762,2,"@Shadi How many discrete values do you have? Are they really to be considered to be ordinal, if they lie between 0 and 1?",2010-10-19T15:42:12.453,930,CC BY-SA 2.5,
5269,3762,0,"They are continuous, every float value between 0 and 1. Is there any incoherence between ordinal data and [0 1] interval? Thanks.",2010-10-19T15:49:16.650,1564,CC BY-SA 2.5,
5270,3758,0,Can you add the name of a function/package you use?,2010-10-19T15:51:59.797,,CC BY-SA 2.5,user88
5271,3762,1,For me this would rather be considered an interval scale: http://en.wikipedia.org/wiki/Interval_scale#Interval_scale,2010-10-19T15:54:27.337,442,CC BY-SA 2.5,
5272,65,2,"As far as I remember, I was taught the 'sample' calculation in GCSE maths and science (age 14-16) and the distinction between populations and samples and their associated variance measures was covered (though not in depth) at A-level (age 16-18).  So I'm not sure this is a simple UK/US difference.",2010-10-19T15:55:26.023,266,CC BY-SA 2.5,
5273,3720,0,"@Andy W:> *(ie it Z scores the average of the neighbors based on global mean and variance)* As you said, i don't think this alleviate my concern. The main issue for the use of RE is that we need $E(x_{it})\approx E(x_{jt})$ for any $i \neq j$. this most likely holds true if x_{it} is $u_{it}-\bar{u}_{-it}$ (or as Teucer suggested, $\log(u_{it})-\log(u_{-it})$) but it most certainly does not hold when x_{it} is $\bar{u}_{it}$ because it could be that $\bar{u}_{jt}\neq\bar{u}_{it}$. At any rate, this hypothesis has to be tested (Haussman test).",2010-10-19T15:57:22.300,603,CC BY-SA 2.5,
5274,3762,0,"Yeah, I believe you are right. So you mean I cannot use ordinal measures for interval data, right? I corrected my question. I am looking forward to your guidances. Thanks.",2010-10-19T15:59:20.330,1564,CC BY-SA 2.5,
5275,3758,0,"Thanks @mbq! I have tried to figure out which one provides F-test, but no success...",2010-10-19T16:08:22.737,930,CC BY-SA 2.5,
5276,3762,0,"When you stay between 0 and 1, does that include the endpoints 0 and 1 themselves or exclude them?",2010-10-19T16:10:34.000,449,CC BY-SA 2.5,
5277,3720,0,"@Andy W:> *Is that a big deal though if your only interested in prediction?* Can you post this as a separate question on the main board? It is a very important point, one which should interest many future readers.",2010-10-19T16:11:11.910,603,CC BY-SA 2.5,
5278,3762,0,the interval includes 0 and 1.,2010-10-19T16:14:19.157,1564,CC BY-SA 2.5,
5279,3752,0,"@Robin Girard:> * found it strange that increasing the dimension gives advantages to a ""very non-parametric"" estimator (If we admit constant bandwidth is more non parametric than varying bandwith)* is there a typo in this sentence ? Otherwise you would seem to agree with the authors, at least on an intuitive level. Thanks to confirm/correct.",2010-10-19T16:18:05.960,603,CC BY-SA 2.5,
5280,3740,0,You will run into trouble when you use any normal distribution with substantially nonzero mean.  The logistic transformation is almost linear for values near zero but becomes strongly nonlinear for more extreme values.  The resulting distribution will be strongly skewed; a normal approximation will be poor.  This explains why your answer is effective for the particular distribution proposed by the OP (so I upvoted it and hope others do too) but also puts constraints on its generalization to similar looking problems.,2010-10-19T16:21:24.807,919,CC BY-SA 2.5,
5281,3739,0,"To sample y, just sample x from a Normal(0, 0.2) distribution and compute y = e^x/(e^x+1).  You can use (0, 0.2) as the parameters: by means of this formula they completely determine the distribution.",2010-10-19T16:25:44.677,919,CC BY-SA 2.5,
5282,3762,0,"@Shadi So, I suggested you change your title accordingly.",2010-10-19T16:26:15.380,930,CC BY-SA 2.5,
5283,3762,0,"You can use any ordinal measures on interval data. But, you should not do so. You should use measures for interval data as they use more information are more powerful, etc as measures for ordinal data. Measurements are itself ordinally scaled (from low to high): nominal, ordinal, interval (and ratio). It is generally a good idea to use measures of that level of measurement that one has as these are the most powerful/informative ones.",2010-10-19T16:29:20.030,442,CC BY-SA 2.5,
5284,3767,2,"No you can't use correlation to assess the reliability of the measurements or the inter-rater agreement, even for two series of measurement. The correlation computed from two raters will remain the same even if you add some arbitrary value to the 2nd rater's assessments, while the agreement ICC will decrease and correctly reflects that there is a rater-effect.",2010-10-19T16:39:48.670,930,CC BY-SA 2.5,
5285,3740,0,"@whuber I agree, but I won't run into trouble, rather see that in such case it is not normal. This answer is just to promote trying as a technique, not to imply that resulting distribution will be always normal.",2010-10-19T16:40:21.500,,CC BY-SA 2.5,user88
5286,3767,0,"No, it is for more than two measures.",2010-10-19T16:40:42.120,1564,CC BY-SA 2.5,
5288,3005,0,"@kwak Thanks for the fruitful exchange, I think I'm going to work throughout the LARS algorithm to try to connect the two approaches.",2010-10-19T16:42:48.327,930,CC BY-SA 2.5,
5289,3767,1,"@Shadi Yes, I understand your design; I just take as an illustration @Henrik's POV with 2 series; the same line of reasoning applies with $k$ series of measurement.",2010-10-19T16:44:22.613,930,CC BY-SA 2.5,
5290,3582,0,Thanks @Rob @Kwak -- I got hung up noodling KDE variants. (Is 3 days a statisticians' limit :),2010-10-19T16:47:15.913,557,CC BY-SA 2.5,
5291,3769,0,"@ars (+1) Many thanks! I missed this package... Always better to look at the code directly, I appreciate.",2010-10-19T16:48:22.863,930,CC BY-SA 2.5,
5292,3740,1,"Thank you for sharing your philosophy.  I agree: it's good to help people learn to answer their own questions.  Empirical investigation (""trying""), though, is usually best when informed by theoretical considerations to indicate the limits of its applicability.  In this case you're safe with the Q-Q plot, because it is so sensitive to deviations from normality and you used a largish sample size of about a thousand, but the histogram alone can be deceiving.  (Almost *any* bell-shaped histogram ""looks"" normal!)",2010-10-19T16:50:36.193,919,CC BY-SA 2.5,
5293,3766,0,"If I want to explain the data in each set, it can be: 0.98, 0.01, 0.5, ... which shows 'sound1' and 'sound2' are very similar (0.98), 'sound1' and 'sound3' are much different (0.01) and so on. In this case, do you believe I can use ICC or Pearson is better?",2010-10-19T16:59:40.067,1564,CC BY-SA 2.5,
5294,3766,0,"@Shadi Did you read the thread I pointed to? You cannot use correlation-based criteria. So, I would better advice you to rely on the ICC unless someone has a better idea to cope with bounded values. For me it's not a problem as you're likely to end-up with similar results than with any other most complicated method. Still I agree with others than it makes sense not to use ANOVA or mixed-effects models with inappropriate link function in certain cases.",2010-10-19T17:00:42.773,930,CC BY-SA 2.5,
5295,3770,0,That seems to be it. I will go away and think about it so more. Many thanks all.,2010-10-19T17:51:08.960,1614,CC BY-SA 2.5,
5297,3720,0,@Teucer:> I'm sorry i do not understand your last message. But you can certainly edit your question.,2010-10-19T17:56:29.920,603,CC BY-SA 2.5,
5298,3766,0,"Yes, I have studied them. Thank you. Actually I am completely new to this concept. I did not understand almost 80% of the thread. I wanted to use ICC in Matlab, but at first I should know the meaning of 'type', 'alpha' and 'r0' to know what value is the best for my purpose. Do you know any quick way to get some information? Thanks for your guidance.",2010-10-19T18:17:20.853,1564,CC BY-SA 2.5,
5299,3720,0,"@kwak I think it will help me a lot if you can edit your answer with the model formulas and the R code: I have read the vignette of plm several times, but I do not understand the model specification! Thanks in advance for the effort...",2010-10-19T18:20:25.880,1443,CC BY-SA 2.5,
5300,3773,2,"Fisher's exact test and Pearson's chi-squares test the null hypothesis that all 5 methods are equally effective against the alternative that at least 1 is better than the others. The p-values tell me that the null is rejected. So, if I want to find out which methods is actually better than the others won't I have to do 10 pairwise comparisons?",2010-10-19T18:26:53.513,1558,CC BY-SA 2.5,
5301,3720,0,"I would have used the following specification with lme4: fm <- lmer(lu~lu1+ls+(1|R),data=df) where $lu=log(u)$, $lu1=lag(log(u),1)$, $ls=log(\bar{u})$ and R is the region factor. Is this the model you have in mind? How can I specify with pgmm the following model lu~lu1+R+(lu1|R) for example?",2010-10-19T18:29:07.223,1443,CC BY-SA 2.5,
5302,3720,0,"@Teucer:> i will do this, but not tonight ;<",2010-10-19T18:43:19.127,603,CC BY-SA 2.5,
5303,3775,1,"That's a much better answer than mine! I failed to read the question properly I'm afraid (Step 3 in particular). I thought of deleting my answer, but I stand by the greater interpretability of a Bayesian approach is it's really the ranking that's of interest.",2010-10-19T19:09:55.940,449,CC BY-SA 2.5,
5305,3775,0,Just to make sure I understand correctly- The indicator that tracks the relative difference between method 4 and 5 will be updated whenever we see a difference that is greater than 0.21.,2010-10-19T19:21:08.197,1558,CC BY-SA 2.5,
5307,3769,0,"@chl: I was looking at the code and about to look up the formula, when I saw your answer come in. Looks like we started from different ends and met in the middle -- making a nice complement of answers. :)  (Already upvoted you.)",2010-10-19T19:23:28.847,251,CC BY-SA 2.5,
5308,3769,0,@ars I have to turn back to Zar's textbook because I didn't find the function though I've heard of the alternative statistic :),2010-10-19T19:25:16.003,930,CC BY-SA 2.5,
5309,3775,0,"@sxv Yes, that's right.  (Well, I actually used greater than or equal.  Ties do happen.  I think including equality among the significant results is the correct thing to do, because we're evaluating the probability that differences *this large or larger* can occur by chance.)",2010-10-19T19:26:17.230,919,CC BY-SA 2.5,
5310,3752,0,@kwak thanks to notice that! this is a typo: I wanted to say constant bandwidth is less NP ... I can't modify my comment :( sorry about that.,2010-10-19T19:27:55.387,223,CC BY-SA 2.5,
5311,3766,0,"@Shadi Oups, sorry, I didn't think of language issue. I've updated my response. HTH",2010-10-19T19:42:50.790,930,CC BY-SA 2.5,
5313,3721,0,"I need just one number for all of the judges, not a matrix that shows the pairwise correlations of judges. for example: pearson(a,b,c)=0.53     Is it possible?   Thanks.",2010-10-19T21:39:52.147,1564,CC BY-SA 2.5,
5315,3680,1,"Is that the ""Riemann manifold Langevin"" paper? Do integrate Fisher information at some point?",2010-10-19T21:51:45.250,511,CC BY-SA 2.5,
5316,3721,0,"@shadi Correlation is a bivariate relationship in the sense that it tells you the relationship between two sets of variables. Thus, calculating pearson(a,b,c) where I presume a,b,c are different judges is not possible.",2010-10-19T21:52:18.690,,CC BY-SA 2.5,user28
5319,3731,0,"@ars. You can just use Monte Carlo methods or a bootstrap. Not much work in that, just computation.",2010-10-19T22:24:18.203,159,CC BY-SA 2.5,
5320,3752,0,@robin. I've edited your comment,2010-10-19T22:25:56.670,159,CC BY-SA 2.5,
5321,3766,0,"Dear chl, Thanks for your great guides. I'm not sure if I explained my data well or not. when you asked : ""how many discrete data do you have"", did you mean something like ""partner1"" and ""partner2"" in (uvm.edu/~dhowell/StatPages/More_Stuff/icc/icc.html)? If so, I should say just 1. I try to explain my data better. I have, for example, 100 different sounds. I want to know how similar each two sounds are. I used a program to do that for me. It generated some (4851) numbers. I also used 5 other programs to do the same thing. Now, I want to know how close the results of the 6 programs are",2010-10-19T22:43:28.310,1564,CC BY-SA 2.5,
5322,3731,2,"@Rob: Yes, we can always come up with workarounds to make things work out roughly the same.  Simply to avoid Spearman's method -- which most non-statisticians can handle with a standard command.  I guess my advice remains to use Spearman's method for small samples where normality is questionable.  Not sure if that's in dispute here or not.",2010-10-19T23:43:16.250,251,CC BY-SA 2.5,
5323,3731,3,"@ars. I would use Spearman's if I was interested in monotonic rather than linear association, or if there were outliers or high levels of skewness. I would use Pearson's for linear relationships provided there are no outliers. I don't think the sample size is relevant in making the choice.",2010-10-20T00:32:52.033,159,CC BY-SA 2.5,
5324,3779,0,Should it not be 'pick k tiles *without* replacement'? Very interesting question.,2010-10-20T00:39:34.920,,CC BY-SA 2.5,user28
5325,3769,0,"sorry for the late post, but yes, i am using agricolae package.  friedman.test doesn't work on my data as it is returning an error message ""your data is not an unreplicated design"" to that effect.       but why are there 2 p-value f's?  am i right that the first is for the the treatment and the second for the block?",2010-10-20T00:59:09.607,1627,CC BY-SA 2.5,
5326,3731,5,"@Rob: OK, thanks for the discussion.  I agree with the first part, but doubt the last, and would include that size only plays a role because normal asymptotics don't apply.  For example, Kowalski 1972 has a pretty good survey of the history around this, and concludes that the Pearson's correlation is not as robust as thought.  See: http://www.jstor.org/pss/2346598",2010-10-20T01:00:48.653,251,CC BY-SA 2.5,
5327,3769,0,"@kathy: see chl's answer for an explanation of what the two mean -- the first value you see is $F_r$ and the ""value f"" is $F_{obs}$ in his answer.  Also, it's possible you're passing the arguments in the wrong order to the friedman.test method; it should be (data, group, block).",2010-10-20T01:08:07.203,251,CC BY-SA 2.5,
5328,3769,0,"aw, thanks ars, now i get it, its the adjusted value...mmmm that's why i'm wondering why its so close to the chisqr value",2010-10-20T01:16:45.097,1627,CC BY-SA 2.5,
5329,3779,0,oops. indeed it should.,2010-10-20T01:58:11.113,795,CC BY-SA 2.5,
5330,1332,0,"I think it's a great one (I think it goes well with - ""no models are true, some  are useful"")",2010-10-20T02:27:35.627,253,CC BY-SA 2.5,
5332,3780,0,"The quick and dirty approach may not be so quick! The dictionary may contain 100,000 words, and the search for a match of the given tiles could be a coding disaster.",2010-10-20T04:26:51.890,795,CC BY-SA 2.5,
5333,189,6,"This would be the approach when the CDF is only approximated empirically. It gives lousy estimates of the PDF, though.",2010-10-20T05:13:19.947,795,CC BY-SA 2.5,
5334,3766,0,"@Shadi So you have 6 ""raters"" or ""methods"" that are assessing 100 objects similarity. How do you explain that there are 4851 measurements: Are there replicate measurements for each object (sound)? The coding (ordered or discrete, continuous) is for the measurements. In your case, I consider it as reflecting a continuous scale of similarity on [0,1].",2010-10-20T06:30:14.833,930,CC BY-SA 2.5,
5335,3780,0,@shabbychef This is something well done to suit spell checkers.  See for instance http://www.n3labs.com/pdf/lexicon-squeeze.pdf,2010-10-20T07:06:49.607,,CC BY-SA 2.5,user88
5336,3781,0,*How* have you simulation the datasets?,2010-10-20T07:07:33.130,449,CC BY-SA 2.5,
5337,3767,0,@chl You are right! I will keep this post to remind everyone that this is wrong.,2010-10-20T08:21:23.680,442,CC BY-SA 2.5,
5338,3788,0,"IMHO lag auto-correlation makes sense only if you have an ""time-seris like"" ordering otherwise it doesn't make a lot of sense. Also, independence => correlation = 0, but not the other way.",2010-10-20T08:46:25.650,1307,CC BY-SA 2.5,
5339,3790,0,"I am sorry, I am just  a first year math student. Could you please provide/recommend a link/book/paper that describes how the relationship was derived?",2010-10-20T08:52:00.580,1636,CC BY-SA 2.5,
5340,3788,0,"Why do you want to know if the data are independent? If you are fitting a model the model might assume the **errors** are IID, not the data. Can you provide more context - either as a comment or (better) by editing your Question (see the edit link beneath the tags)?",2010-10-20T08:57:34.153,1390,CC BY-SA 2.5,
5341,3780,0,"@shabbychef Reg monte-carlo- if the dictionary is sorted a match should be fairly quick no? In any case, the direct approach that I outlined earlier was flawed. I fixed it. The problem in my earlier solution was that the same word can be formed multiple ways (e.g., 'bat', 'b*t' etc).",2010-10-20T09:14:00.123,,CC BY-SA 2.5,user28
5342,3792,0,Your first point remind me of the *run test* (at least for simple design).,2010-10-20T09:31:13.887,930,CC BY-SA 2.5,
5343,3790,3,"@Sara I think it dates back to Karl Pearson, which uses this empirical relationship for his ""Pearson mode skewness"". Aside from this, you may find interesting this online article, http://j.mp/aWymCv.",2010-10-20T09:42:06.293,930,CC BY-SA 2.5,
5344,3780,1,"@shabbychef On further reflection, I agree with you that the monte carlo approach will not work. One issue is that you need to figure out which words you can actually form with the k tiles and the second one is that you can form multiple words with the k tiles. Calculating these combinations from k tiles is probably not that easy.",2010-10-20T09:49:17.853,,CC BY-SA 2.5,user28
5346,3795,0,"Out of interest, what did you find was the problem with GraphViz? Not flexible enough? In what way(s)?",2010-10-20T10:50:18.420,449,CC BY-SA 2.5,
5347,3792,0,"G. Jay Kerns:> Thanks, i've edited my post to adress your comment.",2010-10-20T11:02:06.410,603,CC BY-SA 2.5,
5348,3790,0,Thank you chl and kwak for the link and answer you have provided. I will study them.,2010-10-20T11:13:39.383,1636,CC BY-SA 2.5,
5350,3794,0,"So, in excel, if I wanted to look at a time frame of 365 days...then I'd add 365 columns along side the ID table?",2010-10-20T11:35:00.903,1641,CC BY-SA 2.5,
5351,3792,0,"OK, I've deleted my earlier comment.",2010-10-20T11:42:30.720,,CC BY-SA 2.5,user1108
5356,3799,0,"Given my limited experience, I cannot give an exact answer. However, I believe that you can use a panel data (because you consider in your example variations within individuals and between individuals) approach with logit. Maybe others can elaborate on this...",2010-10-20T12:14:46.110,1443,CC BY-SA 2.5,
5357,3799,0,"Your small example is very useful, but I assume your real dataset is larger. How much larger, i.e. (roughly) how big are your real *N* and *k*?",2010-10-20T12:25:25.513,449,CC BY-SA 2.5,
5358,3794,0,@Alex Yes. I suppose you'd need one of the more recent versions of Excel that overcomes the old column limit. And the algorithm would be more elegant in something like R.,2010-10-20T12:29:56.867,183,CC BY-SA 2.5,
5359,3588,0,"@Denis:> i would say it makes the problem worst (i.e. NN kernel should work better on less clumpy data). I have an intuitive explanation but it won't fit here, plus you may want to asks this out on the main board as a separate question (linking to this one) to have additional opinions.",2010-10-20T13:37:36.417,603,CC BY-SA 2.5,
5360,3712,0,Thanks Charlie - Is it possible to get the estimates separately for treated and untreated? The lincom command only gives me the difference between treated and untreated.,2010-10-20T13:52:24.473,834,CC BY-SA 2.5,
5361,3798,0,"@chl: thanks!  I remember plspm being announced on the semnet list -- for some reason PLS isn't as big on this side of the Atlantic, not sure why.  plotSEMM looks really interesting, can't wait to play with it.",2010-10-20T13:57:35.317,251,CC BY-SA 2.5,
5362,3802,2,(+1) Thanks for linking with OpenMx! Really great package that has replaced Mx on my Mac now.,2010-10-20T14:09:25.497,930,CC BY-SA 2.5,
5363,3690,0,"Thanks Srikant. Since the group-level effects to which I'm referring often unique, complex, and erratic, I'm not sure I'll be able to quantify them with interaction terms. However, your answer has led me to approach my problem from a different angle and refine my question. I'll restate in more formal terms once I've more clearly defined the problem.",2010-10-20T14:30:45.113,1611,CC BY-SA 2.5,
5364,3766,0,"yes, as you said I have 100 objects and if I want to know the similarity between each two sound, it will be (99*98)/2. (s1,s2)(s1,s3)(s1,s4)...(s1,s100)(s2,s3)(s2,s4)...(s2,s100)...(s99,s100).  and I know that (s2,s1)=(s1,s2) so I do not calculate that. so, I will have 4851 measurements for each method(each method calculates the similarity of all the sounds pairwise).and totally I have 6 methods. So, I have 6*4851 data in general. It is a continuous scale of similarity on [0 1]. Thanks.",2010-10-20T14:46:21.733,1564,CC BY-SA 2.5,
5365,1251,3,"That's a good answer. In addition, I'd suggest horizontally jittering the points, so they don't overlap, especially if you have more points per group than this. In ggplot2, the geom_jitter() will do that.",2010-10-20T15:00:21.693,6,CC BY-SA 2.5,
5366,3805,0,How many percentiles are included in your real data? I hope it's more than in your example!,2010-10-20T15:04:43.220,449,CC BY-SA 2.5,
5367,3793,2,What's the reason for the vote to close?  How does this problem not relate to statistical analysis?,2010-10-20T15:31:37.067,919,CC BY-SA 2.5,
5368,3794,0,@Jeromy Your idea is a good one but the implementation can be greatly improved :-).  I posted an explanation.,2010-10-20T15:33:30.733,919,CC BY-SA 2.5,
5369,3803,0,"@whuber The tree is a neat idea (upvote for that idea) but would it not require lot of memory? I guess it depends on how diverse the dictionary is but I am guessing a reasonably diverse dictionary would require many trees For example, the 'b' tree would start with the letter 'b' instead of 'a' for all those words which do not have 'a' in them. Similarly, the 'c' tree would start with the letter 'c' for those words which do not have 'a' and 'b' but have 'c'. My proposed direct approach seems simpler as it requires a one-time traversal of all the words in the dictionary, no?",2010-10-20T15:49:09.823,,CC BY-SA 2.5,user28
5370,3803,1,"@Srikant: The tree would likely require far less RAM than caching the entire dictionary to begin with.  Are you really concerned about a few megabytes of RAM, anyway?  BTW, there's only one tree, not many: they are all rooted at the empty word.  Your approach, as I have understood it, requires multiple searches of the dictionary (up to 7! of them) on *every iteration*, making it impracticable as @shabbychef fears.  It would help if you could elaborate on the algorithm you have in mind where you write ""see if you can form a word"": that hides a lot of important details!",2010-10-20T16:09:27.643,919,CC BY-SA 2.5,
5371,3803,0,@whuber: I realized the fact that there is only one tree after I posted my comment. Reg my approach- I agree that my monte carlo proposal is fuzzy and your answer fleshes out how one can actually implement monte carlo in this setting. I actually meant that the *direct approach* (see my answer) may actually be simpler as that approach requires a one-time operation on the dictionary unlike a monte carlo which requires several thousands of iterations on the tree. Just wondering on the relative merits of the approaches.,2010-10-20T16:15:11.737,,CC BY-SA 2.5,user28
5372,3803,0,"@Srikant I refrained from commenting on your direct approach because it I suspect it gets the wrong answers.  It does not appear to account for the dictionary structure: that is, the subset relationships among words.  For instance, would your formula get the correct answer of zero for all dictionaries that contain all the possible one-letter words?",2010-10-20T16:23:00.457,919,CC BY-SA 2.5,
5373,3780,0,"@Srikant I'm confused.  What exactly do you mean by ""the sth word from the alphabet""?  Alphabets contain only letters, not words.",2010-10-20T16:23:50.700,919,CC BY-SA 2.5,
5374,3780,0,@whuber Sorry. I meant that m_a counts the number of 'a's needed by the s^th word. I corrected the text.,2010-10-20T16:26:40.023,,CC BY-SA 2.5,user28
5375,3803,0,"@whuber hmmm good point. Perhaps, I am answering the wrong question!",2010-10-20T16:30:03.380,,CC BY-SA 2.5,user28
5376,3803,0,"@whuber re your bet that I could conduct this study in seconds: you overestimate my programming skills! My intuition about MC methods is they should be easy to implement: ""let the computer do the work."" If MC is going to take a lot of work, maybe the exact approach is better. In fact, it seems like once the dictionary is in tree form (+1, BTW), you are a long way towards counting the subsets of letters from the bag that form legitimate words...",2010-10-20T16:34:59.353,795,CC BY-SA 2.5,
5377,3780,1,"@Srikant Thanks.  Your formula seems to assume you have to use all k letters to form the word, but I don't think that's what the OP is asking.  (That's not how Scrabble is played, anyway.)  With that implicit assumption, you're on the right track but you need to modify the algorithm: you mustn't repeat the calculation for words in the dictionary that are permutations of each other.  For example, you mustn't subtract both t_{stop} and t_{post} in your formula.  (This is an easy modification to implement.)",2010-10-20T16:35:04.107,919,CC BY-SA 2.5,
5379,3803,0,"@shabbychef That's an excellent point.  I suspect the computation is not easy.  The leaves of my tree have the property that (a) each can be permuted into at least one word but (b) no proper subset can be permuted into a word.  We can readily count the number of k-letter sets that contain a given leaf, but we're going to double- (and triple- and quadruple-...) count things that way.  E.g., in English ""a"" is a leaf and so is ""ept"", but ""aaeptxz"" contains *both*.  We could apply PIE (principle of inclusion-exclusion) but it would ferociously complicated to do, I think.",2010-10-20T16:45:10.487,919,CC BY-SA 2.5,
5380,3805,0,"It depends on the factor I'm using.  For body weight, I'm actually using more percentiles.  But for something like dust ingestion rates, which has less observed measurements, I am using as few as four percentiles.",2010-10-20T16:45:33.467,1645,CC BY-SA 2.5,
5381,3810,0,What do you mean by *power*? If you wish to bring them to a common scale then you can use a linear transformation to convert all scales to a common scale (say 5 point). Likert scales are interval scales and hence linear transformations do not 'destroy' the properties of the scale. Is this what you want/meant?,2010-10-20T16:46:47.490,,CC BY-SA 2.5,user28
5382,3798,0,"@chl: btw, I meant to add that it's shame PLS isn't more noted here, since there seems to be a lot of exciting stuff happening around it, especially with tools being developed (e.g. SmartPLS in addition to plspm).  I read some of Wold's work a while back and some of his ideas are only just being realized (e.g. ""having a conversation with your data"").  I really need to set aside some time to explore it more.",2010-10-20T16:47:53.263,251,CC BY-SA 2.5,
5383,3810,1,The usual way to accomplish this is to do the PCA on the correlation matrix instead of the covariance matrix.,2010-10-20T16:52:19.620,919,CC BY-SA 2.5,
5384,3766,0,Could you please tell me if you still believe that the best one is Intra-class correlation? Thanks a lot.,2010-10-20T16:54:31.087,1564,CC BY-SA 2.5,
5385,3766,0,"@Shadi Huh, that makes a difference; because here you might also be interested in assessing whether your 6 similarity matrix (and not six series of measurement) present some form a variance attributable to the raters. Provided you consider a stacked version of your pairwise similarity (a long vector), the ICC remain applicable; otherwise there exist methods to assess the comparability of (dis)similarity matrix but I feel this should be clarified either in your question or best, in a new question (and mods could close this one) so that others may contribute. Let me ask the mods first.",2010-10-20T17:02:52.613,930,CC BY-SA 2.5,
5386,3804,0,"Hello Harlan, can your details be translated in mathematical notation? It would be very helpful to understand the details (for me).",2010-10-20T17:07:40.033,1307,CC BY-SA 2.5,
5387,3798,0,"@ars Do you want a list of recommend readings? I also worked with Arthur Tenenhaus who submitted a nice paper with his father (yes, Michel Tenenhaus) to Psychometrika: They are unifying all two-block methods (PCA, CCA, PLS, inter-battery, etc.) thanks to a very neat rewrite of the argmax constraint. I've been playing myself with penalized PLS/CCA (L1/L2) in genomics, but I feel it will bring more interesting on my biomedical data.",2010-10-20T17:14:12.933,930,CC BY-SA 2.5,
5388,3801,0,> you could add a link to Efron's bootstrap paper**s**. It's as good a place as any to start.,2010-10-20T17:22:40.057,603,CC BY-SA 2.5,
5389,3780,0,"@whuber you are correct; I will edit the question: the words are of any length up to $k$. This is how Scrabble is played, but the question should be self contained.",2010-10-20T17:24:30.677,795,CC BY-SA 2.5,
5390,3762,2,"@Shadi Following my latest comment, I asked for closing this question so that you can reformulate a new one by adding precision on your design, especially the fact that you actually have 6 similarity matrices instead of 6 series of measurement. This way, others may provide useful insights into this question. You can still link to this question, but I really feel it call for a new thread with your added clarifications so that everyone can contribute.",2010-10-20T17:34:06.517,930,CC BY-SA 2.5,
5392,3809,0,"But that's two parameters to set for one dependent variable! I want to keep the underlying beta-binomial structure in $prior_1$, and just update it, perhaps by shifting the mean without changing the variance, to give $prior_2$. And I want to do it in a principled way, as I only 20% trust that scalar anyway...",2010-10-20T17:57:19.297,6,CC BY-SA 2.5,
5394,3801,0,"@kwak Ok, yet it is easier to put a link to Wikipedia http://en.wikipedia.org/wiki/Bootstrapping_(statistics)#References",2010-10-20T18:06:27.470,,CC BY-SA 2.5,user88
5395,3804,0,"added some notation, hope it helps clarify!",2010-10-20T18:16:37.147,6,CC BY-SA 2.5,
5396,3799,0,"N and k can be huge, but computational power is not a problem.",2010-10-20T18:37:28.580,1643,CC BY-SA 2.5,
5397,3809,0,@harlan See edited answer.,2010-10-20T18:42:22.690,,CC BY-SA 2.5,user28
5400,3761,0,"Thanks, you are right that FNN falls into the category of neuro-fuzzy systems.  The links you gave are helpful.  I think I will contact the author of the paper to see if there are closer implementations.",2010-10-20T18:58:56.757,1127,CC BY-SA 2.5,
5401,1251,0,@Harlan: I agree. Although if I had many more points I would probably use a boxplot.,2010-10-20T19:00:39.260,8,CC BY-SA 2.5,
5402,3813,1,or *biclustering* (+1).,2010-10-20T19:09:37.063,930,CC BY-SA 2.5,
5405,3814,0,Does it extend to justifications received in response to an initial review (where minor and/or major revisions were asked)?,2010-10-20T19:14:34.573,930,CC BY-SA 2.5,
5406,3814,0,"@chl: Yes, why not.",2010-10-20T19:19:20.240,8,CC BY-SA 2.5,
5411,3816,2,"+1 for me. This frustrates me, especially when they cite the wrong thing and I've provided the relevant details on how to cite the packages",2010-10-20T19:36:52.963,1390,CC BY-SA 2.5,
5412,3818,7,"Huh... You should add a little bit of contextual information here, e.g. at least specify your working hypotheses.",2010-10-20T19:39:55.223,930,CC BY-SA 2.5,
5414,3818,3,Definitely needs more information.,2010-10-20T19:41:23.753,5,CC BY-SA 2.5,
5416,3821,1,s.d. is shift invariant.,2010-10-20T19:53:44.827,603,CC BY-SA 2.5,
5417,3821,0,Thanks Srikant.  Do you have an idea on how to define the transformation I gave in my example?,2010-10-20T19:54:01.493,253,CC BY-SA 2.5,
5418,3821,1,@Tal Y = X + 1?,2010-10-20T19:55:00.057,,CC BY-SA 2.5,user28
5419,3817,5,I'm a little curious about the stepwise regression bullet. What makes stepwise regression so bad? Is it the data dredging and multiple comparisons issue?,2010-10-20T19:56:12.030,1118,CC BY-SA 2.5,
5421,3817,18,"The problem is that stepwise procedures completely invalidate all the assumptions and preconditions for ""normal"" inferential statistics based on p values, which are then badly biased (downwards towards being ""more significant""). So basically, the answer is ""yes"", with the caveat that one could in principle correct for all these multiple comparisons (but which I have never seen done). I believe strongly that this is the single most important reason why I see so much research in psychology that cannot be replicated - which in turn leads to a huge waste of resources.",2010-10-20T20:04:48.097,1352,CC BY-SA 2.5,
5422,3821,2,You missed a=-1 ;-),2010-10-20T20:07:16.167,,CC BY-SA 2.5,user88
5423,3820,0,"Actually, you cannot change *one* variable and still keep the same standard deviation. In your example, after all, you are not changing *one* variable but *three*.",2010-10-20T20:08:41.380,1352,CC BY-SA 2.5,
5424,3822,3,"Still asking for a Matlab solution? In this case, I let you add this tag. And feel free to add or delete the edits I made if they don't exactly reflect your design.",2010-10-20T20:10:20.080,930,CC BY-SA 2.5,
5425,3821,2,"The standard deviation will not change if you simply reorder your data, but this does not seem to be too helpful here...",2010-10-20T20:10:42.700,1352,CC BY-SA 2.5,
5426,3821,0,@mbq Good point!,2010-10-20T20:13:28.093,,CC BY-SA 2.5,user28
5427,3821,2,"@Srikant: if we expand our search from linear transformations to analytic transformations (i.e., transformations that can be represented by their Taylor series), I think one could prove that all higher order terms need to vanish for the standard deviation to be unchanged. That is, the only analytic transformation that preserves sd would be adding or subtracting a constant to all values. Might be a nice exercise for statistics undergrads ;-)",2010-10-20T20:14:54.813,1352,CC BY-SA 2.5,
5428,3821,0,@stephan I was actually thinking of the same idea but it seemed too much tex typing so I abandoned the idea.,2010-10-20T20:16:31.300,,CC BY-SA 2.5,user28
5429,3818,1,And a question mark... fixed.,2010-10-20T20:18:51.703,,CC BY-SA 2.5,user88
5431,3817,10,"@Stephan: I agree, stepwise is a bad idea.  Though, while they may have not made it to psych methods yet, but there are a variety of selection procedures that adjust for bias related to overfitting by adjusting estimates and standard errors.  This is not typically thought of as an issue of multiple comparisons. They are known as shrinkage methods.  See my response in this thread <http://stats.stackexchange.com/questions/499/when-can-you-use-data-based-criteria-to-specify-a-regression-model> and Harrell's ""Regression Modeling Strategies"" or Tibshirani on the lasso.",2010-10-20T20:34:42.767,485,CC BY-SA 2.5,
5432,3820,0,"@Stephan I understand the question as referring to an univariate distribution, hence one variable with multiple observations. Am I missing something?",2010-10-20T20:34:50.210,930,CC BY-SA 2.5,
5433,3820,1,"@chl - I'd say it's more probable that *I* am missing something. Tal asks about ""changing one *value*"", and I should have written about ""values"", not ""variables"" in my comment, where ""value"" to me sounds much like ""observation"".",2010-10-20T20:38:47.067,1352,CC BY-SA 2.5,
5434,3817,5,"@Brett Magill: +1 on that, and yes, I know about shrinkage and the lasso. Now all I need is some way to convince psychologists that these make sense... but people have been fighting with very limited success just to get psychologists to report confidence intervals, so I'm not too optimistic about psychologists' accepting shrinkage in the next twenty years.",2010-10-20T20:42:34.620,1352,CC BY-SA 2.5,
5435,3820,1,@Stephen In the example he changed 2 to 5: only one value was altered.,2010-10-20T20:47:54.913,919,CC BY-SA 2.5,
5436,3821,1,"@Stephen Not true for analytic transformations or even the most general transformations.  Apply *any* transformation, such as a Box-Cox transformation.  Determine the new variance.  If it exists and is nonzero, rescale the result to match the original variance.",2010-10-20T20:50:20.383,919,CC BY-SA 2.5,
5437,3820,1,"@Tal I changed ""roles"" to ""rules"" in your question and hope that was what you intended.",2010-10-20T20:57:13.517,919,CC BY-SA 2.5,
5438,3820,0,"@chl It's univariate all right but I take it that he is asking about the *observations,* not the variable.",2010-10-20T20:58:03.950,919,CC BY-SA 2.5,
5439,3820,0,"@whuber I understood it in the same way (or I think so): we have a series of observations (= observed numerical values for a given variable), is there a way to keep the same SD by altering one or more of these values?",2010-10-20T21:08:38.413,930,CC BY-SA 2.5,
5440,3820,0,"@chl We take it in the same way.  Although it is based on a different interpretation, Srikant's answer is nevertheless an interesting response.  As the comments afterwards show, though, it doesn't lead to very interesting solutions: there are too many ways one can change a random variable while preserving its variance.",2010-10-20T21:13:51.623,919,CC BY-SA 2.5,
5441,3823,20,"Reminds me that in my early days as a referee i spent *far* too long reviewing a statistical paper that was eventually rejected by that particular journal, but the other referees and I suggested a more useful application for the method, and I also sketched an algebraic proof to replace an unsatisfactory simulation study in the manuscript. The authors have since got two published papers out of it. I'm not *annoyed* by that, but an acknowledgement such as ""we thank referees of an earlier version of the paper for helpful comments"" would have been good manners.",2010-10-20T21:23:38.027,449,CC BY-SA 2.5,
5442,3820,0,"@whuber: 'Interesting' is subjective. I believe that both interpretations are equally valid, interesting in their own right.",2010-10-20T21:44:50.087,,CC BY-SA 2.5,user28
5443,3823,1,"@onestop Yes, I can imagine how disappointing such a situation might be...",2010-10-20T21:56:10.877,930,CC BY-SA 2.5,
5445,3812,0,Thanks to all of you for your very good comments and suggestions.,2010-10-20T22:08:02.857,1647,CC BY-SA 2.5,
5446,3712,0,"Estimates of what? Using a version of the lincom statement above, you get the treatment effect for each quintile (the estimate of the treatment effect for the first quintile is just the coefficient on treatment).",2010-10-20T22:08:24.037,401,CC BY-SA 2.5,
5449,3792,0,"My background is civil/hydraulic engineering, i try to understand statistical discussions :)",2010-10-21T00:13:10.170,1637,CC BY-SA 2.5,
5451,3831,0,"the velocity components (3D) belong to a turbulent flow. Although the trend follows a sinusoidal pattern, there are high-frequency variations. in my case the trend follows 1/12hr frequency but measurement frequency is 1 Hz, i.e. if at time t speed is 20cm/s, speed at t+1 the speed might have any value saying 20+-5 cm/s. Hence, essentially i cannot predict the next value in a turbulent field.",2010-10-21T01:11:11.963,1637,CC BY-SA 2.5,
5452,3831,0,could you please explain more about checking ID condition?,2010-10-21T01:11:48.547,1637,CC BY-SA 2.5,
5454,3831,0,"Well, I do not know enough about your context to give a sensible comment but you have to model the velocity vector as a time series of some sort with an additive error term which you assume is iid. You estimate the model and test if the residuals are iid. Perhaps, you could ask another question with some context of the data and your goals are as far as data analysis is concerned. Perhaps, the issue of iid or not is not that relevant given what you want to achieve?",2010-10-21T01:39:33.467,,CC BY-SA 2.5,user28
5455,3828,1,+1 Log-linear sounds good given the plentiful requirements! :)  I'd also recommend Agresti's text on categorical data analysis.,2010-10-21T03:24:56.497,251,CC BY-SA 2.5,
5458,3809,0,"@Srikant, a (hypothetical) Bayesian will have strong disagreements with your answer. She would have done something like this: prior  $\propto f(\alpha_1,\beta_1|-) \alpha + f(\alpha_2,\beta_2|-) (1-\alpha)$ and then put prior on $\alpha$. However, your answer will be a little less flexible than the Bayesian's answer.",2010-10-21T04:16:41.607,1307,CC BY-SA 2.5,
5459,3823,24,"A few weeks ago I was given a paper to review and found that 85% of it had been published in another journal...by the same authors.  That, too, is still considered plagiarism.  For the last several years I have routinely submitted chunks of papers--especially abstracts, introductions, and conclusions--to Web search engines *before* doing any review.  I want to be sure the work is original before I invest any time in reading it.",2010-10-21T04:40:42.427,919,CC BY-SA 2.5,
5460,3794,0,@whuber I kept my answer pretty software-neutral. It's good to have the Excel implementation.,2010-10-21T05:03:27.023,183,CC BY-SA 2.5,
5461,3794,1,"@Jeromy I see what you mean.  I was responding to your comment in answer to Alex's question.  Of greater interest to me is the idea that Excel can be used as a prototype for commands in R, Mathematica, (and even APL if anyone remembers it), provided you use its array-oriented procedures.  Excel itself is pretty bad as a statistical tool but good for rapid prototyping because it's easy to see and correct one's mistakes.",2010-10-21T05:09:15.183,919,CC BY-SA 2.5,
5463,3817,10,"I'd also argue that in psychology maximising prediction is not typically the theoretical aim, yet stepwise regression is all about maximising prediction, albeit in a quasi-parsimonious way. Thus, there is typically a disconnect between procedure and question.",2010-10-21T06:28:49.813,183,CC BY-SA 2.5,
5464,3794,0,@whuber good point,2010-10-21T06:47:18.210,183,CC BY-SA 2.5,
5465,3779,0,"As far as I remember Scrabble does not allow one letter words, so at least that part of the problem is solved ;)",2010-10-21T07:07:57.473,582,CC BY-SA 2.5,
5466,3821,0,"To Whuber - ""rescale the result to match the original variance"" - here's a thought, thanks!",2010-10-21T07:11:07.757,253,CC BY-SA 2.5,
5467,3820,0,"Thanks everyone for the replies.  I meant ""observations"" indeed.  I'm honored to be able to converse with all of you through here.",2010-10-21T07:13:41.027,253,CC BY-SA 2.5,
5468,3825,0,Amazingly detailed answer Whuber - thank you very much!  This obviously leaves open the questions of non additive transformations - but for my curiosity needs - I am satisfied.  Thanks again!,2010-10-21T07:16:47.340,253,CC BY-SA 2.5,
5469,3833,0,"I like this. I would add that (1) the OP is likely to be interested in studying asymmetrical relationships between all three variables, so that a SEM approach would make sense; proceeding this way would also (2) allow to account for specific measurement error at the level of the scales.",2010-10-21T07:27:34.010,930,CC BY-SA 2.5,
5470,3818,0,"(1) presumably you have three groups of variables and within each group, you have multiple variables (e.g., perhaps you have the Big 5 factors of personality within the personality group); (2) calling your variables categorical makes people think of 'unordered categorical' when the variables are probably best treated as numeric. Thus, in addition to providing more information, you may wish to change the title of your question.",2010-10-21T08:14:28.030,183,CC BY-SA 2.5,
5472,3827,0,"+1 esp as you gave a link, and hence traceability and attribution!",2010-10-21T08:43:51.230,449,CC BY-SA 2.5,
5475,3827,0,@onestop Any idea for the difference between the two series of results?,2010-10-21T09:58:02.370,930,CC BY-SA 2.5,
5477,3840,1,I would suggest you to work on clarity of the question; this helps in getting answers.,2010-10-21T10:56:34.243,,CC BY-SA 2.5,user88
5478,3840,0,"@mbq, i have updated to my post",2010-10-21T11:15:01.460,1655,CC BY-SA 2.5,
5479,3795,0,"@onestop, with graphviz I was not able to draw an arrow to the center of a line without ""cracking"" the arrow that is pointed to. See my question at stackoverflow http://stackoverflow.com/questions/3718025/graphviz-dot-how-to-insert-arrows-from-a-node-to-center-of-an-arrow",2010-10-21T11:33:50.717,767,CC BY-SA 2.5,
5481,3689,0,"One of the best ways to improve a forecast is to average different forecasts (from different models, or from different experts or both...). You may want to look around on forecastingprinciples.com or look through past issues of the International Journal of Forecasting.",2010-10-21T13:02:54.803,1352,CC BY-SA 2.5,
5482,3488,1,"Other posters have noted that stationarity is important here. If both series have a linear upwards trend (one kind of nonstationarity), they will be correlated - but all the correlation may be due to the common trend, which may or may not be what we are interested in.",2010-10-21T13:06:39.010,1352,CC BY-SA 2.5,
5483,3841,0,"I am dealing with a similar problem, I guess you can use `pgmm` from **plm** package but as your response variable is binary I don't know exactly how to do it. Maybe others can elaborate... (And yes you are right: my understanding is whenever you have an endogenous variable, in this case the lagged value, you can't use REML to estimate because it is biased, so you need to use GMM.)",2010-10-21T13:08:39.447,1443,CC BY-SA 2.5,
5484,3846,0,"I'm sorry, it does not. I don't know how 2(k-1)sum(vi(di-kvi)) can become sum(di-kvi)^2 after k was estimated. Obviously, I have failed to grasp the significance of the constraint imposed. Please explain the significance of the constraint's role in solving the equation...Thank you.",2010-10-21T13:21:25.237,1636,CC BY-SA 2.5,
5485,3818,2,I'm inclined to close this question if further information isn't provided. At present it's impossible to see what is being asked.,2010-10-21T13:43:57.840,8,CC BY-SA 2.5,
5486,3791,0,"hi S.  yes we did transformed the data, then we standardize against the largest value, then transformed the standardized value",2010-10-21T13:49:28.767,1627,CC BY-SA 2.5,
5487,1966,0,"your experience -- has run on 1k / 100k points, in 1d / 2d / 3d -- would also be useful.",2010-10-21T14:16:47.237,557,CC BY-SA 2.5,
5489,3791,0,but was your design a Randomized Complete Block Design?,2010-10-21T14:32:09.557,1307,CC BY-SA 2.5,
5490,3845,0,I haven't used this package extensively myself. I think individual effect is indeed what you want. Two step will yield smaller standard errors (see the paper i linked to in my original answer). The number of instruments should be smaller than the number of years-1 you have. Try 2:9 (i think you have 9 years per region). You could also start by using the gretl implementation (which is simpler to use) and come back to R whence you have mastered the model a bit.,2010-10-21T14:33:34.897,603,CC BY-SA 2.5,
5493,3825,0,"@Tal I hope you're satisfied, because I can't do better than this: *every* transformation is additive!  Consider replacing x_i by arbitrary z_i; just define y_i = z_i - x_i.",2010-10-21T14:53:51.797,919,CC BY-SA 2.5,
5494,3807,0,Thanks @whuber!  That's really helpful and I'm going to try it out.,2010-10-21T14:54:07.553,1641,CC BY-SA 2.5,
5496,3793,0,"@whuber, i don't understand the comment just above this one...it sounds like its in reply to something (that I don't see.)    (I also couldn't figure out how to contact you directly to ask that...)",2010-10-21T14:59:33.407,1641,CC BY-SA 2.5,
5497,3793,0,"Members with sufficiently high reputation have the option to ""close"" a question.  This is typically done when a question is vague or off-topic.  It takes five votes to close, but before all five votes are in they are anonymous.  In this case somebody cast a vote shortly after your question appeared.  It's considered polite to explain such votes and to provide constructive suggestions for making the question acceptable, but that courtesy was not offered in this case.",2010-10-21T15:04:07.123,919,CC BY-SA 2.5,
5498,3845,0,"@kwak but the model specification is correct, isn't it? Btw, why the numbers of instruments should be smaller than years-1? If you look the example of **plm** vignette with EmplUK data set (p.23) they use 2:99 (I don't really understand why)?  I came across the following formula for number of instruments (T-1)*(T-2)/2+(T-3)...I'll have a look at gretl, thx!",2010-10-21T15:08:19.040,1443,CC BY-SA 2.5,
5500,3849,3,You will likely get excellent and authoritative answers to this question on the stata list (http://www.stata.com/statalist/ ).  It is THE place to go with issues about how Stata works.,2010-10-21T15:11:22.223,919,CC BY-SA 2.5,
5501,3849,1,i would agree with @whuber but no harm in asking here either. The question is on-topic as far as this site is concerned.,2010-10-21T15:15:20.060,,CC BY-SA 2.5,user28
5502,3809,0,"@suncoolsu Sure you can do that as well. However, if you choose the prior for $\alpha$ to be very tight around 0.8 then your suggestion essentially collapses to mine. However, I agree imposing a prior on $\alpha$ is a bit more flexible than assuming that it is 0.8.",2010-10-21T15:29:53.990,,CC BY-SA 2.5,user28
5503,3725,0,"@ronaf Allegedly Excel 2010 now has both, COVARIANCE.P and COVARIANCE.S.  However, the documentation is so dismal one cannot really be sure!  (In the older versions if you know about covariance at all you're probably knowledgeable enough to be able to multiply the result by n/(n-1) in the formula.)  For more info visit http://office.microsoft.com/en-us/excel-help/statistical-functions-reference-HP010342920.aspx",2010-10-21T16:36:14.737,919,CC BY-SA 2.5,
5504,3779,1,"@nico good point, but I think this is only for mid-game. 1 letter words either don't require one to play a letter, or would allow one to place a single letter anywhere on the board, both clearly unacceptable. However, I was thinking of the opening move. In fact, the question can be compactly stated, for those familiar with Scrabble, as ""what is the probability that the first player will have to pass?""",2010-10-21T16:43:55.547,795,CC BY-SA 2.5,
5506,3856,2,what type of data are you working with? From your examples I assume you are dealing with time series?,2010-10-21T17:57:52.243,582,CC BY-SA 2.5,
5507,3827,1,"@chi Oh dear, I think -sizefx- contains a coding error! It's ignoring the values of one variable for which the other variable is missing (the problem is with the use of -marksample-). I'll contact the author. I've never used it before myself (i'm relieved to say...)",2010-10-21T19:06:52.657,449,CC BY-SA 2.5,
5508,3827,0,"You also make a good point that it would be better if -sizefx- supported a ""by()"" option. I'm also not sure why it's referring to Cohen's d as Hedges' g. The latter really involves a more complex bias correction. All in all, i'd advise against using -sizefx- for the time being!",2010-10-21T19:21:36.567,449,CC BY-SA 2.5,
5509,3864,0,"Agreed that things need to be user friendly.  Since people get very protective over their working practices, any changes must make people's lives easier or they'll fail.",2010-10-21T20:13:18.803,478,CC BY-SA 2.5,
5510,3863,4,Absolutely agreed on the version control.  I use it; as do a substantial proportion of the developers and statisticians.  (I'd like to see 100% adoption but that's another pipe-dream for now.)  The hard bit is getting non-techies to use it.  Any ideas appreciated.,2010-10-21T20:18:11.890,478,CC BY-SA 2.5,
5511,3793,0,"Your question makes sense, from a statistical POV. I don't understand the vote to close, given that there are some very basic questions for which browsing online help would have suffice (and is educative)... and those questions were never voted down or voted to be closed. Anyway, that's community life. However, may I also remind you that you can also upvote the question you accepted as a the correct one.",2010-10-21T20:48:56.443,930,CC BY-SA 2.5,
5512,3827,0,"@onestop (+1) Good to hear that you spotted the problem (not enough Stata experience myself). Anyway, the distinction between Cohen's d and Hedges's g is rather confusing, even on Wikipedia (I admit this is not really a reference, though). In addition to not supporting a `by()` option, no CIs are computed (we'd need to rely on a non-central $t$ distribution, or use some kind of bootstrap approach).",2010-10-21T21:02:58.003,930,CC BY-SA 2.5,
5513,3862,1,Excellent links.  I think two important messages for me to pass on are: we need more automated data checking and I need to start explaining about separating data entry and data presentation.,2010-10-21T21:06:04.643,478,CC BY-SA 2.5,
5514,3853,2,"I can understand that reading online help might seem a tiresome activity at first sight (still, it's very educative and often help to capitalize knowledge on a particular software), but could you at least accept answer(s) you find helpful for your ongoing R activities?",2010-10-21T21:09:55.990,930,CC BY-SA 2.5,
5515,3863,2,"@Richie Cotton: I don't know why, but version control seems to be a difficult concept for non-techies to grasp. People continue to just make some changes to a file, rename it and send it via email. How I hate those ""PaperDraftCorrectedByJohnRevision3RewroteByLeslie-NewVersion3.doc"" files...",2010-10-21T21:14:05.317,582,CC BY-SA 2.5,
5517,3868,1,"Have you tried dropping that zero from the analysis?  I don't think zero is a part of the gamma distribution.  Also: are you really sure it's zero, and not just very, very small?",2010-10-21T23:57:17.420,71,CC BY-SA 2.5,
5518,3846,0,"""The idea is the following insight: Suppose that we choose k such that the above constraint is satisfied then it immediately follows that:
                                          ‚àëi(di‚àílvi)2=‚àëi(d‚àíkvi)2+(k‚àíl)2‚àëiv2i                                                      ""                                                                Could you please show the intermediate steps? I really cannot see how that equation follows immediately from satisfying the constraint. Sorry for the inconvenience caused.",2010-10-22T00:36:17.620,1636,CC BY-SA 2.5,
5519,3252,0,I'm not quite sure why the size of your sample matters. Can you offer anymore specific reasoning as to why you think it is different for small n than it is for big n?,2010-10-22T03:39:10.140,1036,CC BY-SA 2.5,
5520,1116,0,"Although this is the only Paul Allison text I have read, I would like to say various other texts of his have come highly recommended for me to read. Even if you don't use SAS (I have in the past so I was familiar with the code, although when I read this book I had completed migrated to other software) it is a really excellent book on survival analysis.",2010-10-22T03:47:03.580,1036,CC BY-SA 2.5,
5521,3650,0,"Both of the answers by Srikant and whuber were equally informative in helping me answer this question. I wish I could give both an accepted checkmark, but I chose whubers response simply for the added context in helping to demonstrate how I would integrate given my prior distributions.",2010-10-22T05:00:40.453,1036,CC BY-SA 2.5,
5522,3651,0,"@Srikant, I assume it would be inappropriate to use the observed 100 realizations for my prior distributions in this context? Hence the need for ""priors"" to begin with.",2010-10-22T05:02:33.790,1036,CC BY-SA 2.5,
5523,3252,4,"@Andy Because then it becomes very difficult to consider an holdout sample and/or class imbalance with very limited sample size ($13<n<25$) generally yields larger classification error rate when applying CV; some individuals might be considered as outliers when studying bivariate distributions; and measures gathered on instruments with their own measurement error are less reliable (small $n$, large $\sigma$). In a certain sense, it is sometimes difficult to disentangle an unexpected relationship from an artifact.",2010-10-22T05:29:38.510,930,CC BY-SA 2.5,
5524,3876,2,Should one set lmer's REML argument to FALSE when generating those models since they'll eventually be compared using the anova() function?,2010-10-22T05:47:06.053,364,CC BY-SA 2.5,
5525,3875,4,IMHO t-test may not be a good idea here,2010-10-22T06:52:40.040,1307,CC BY-SA 2.5,
5526,3876,7,"When comparing models using likelihood-ratio tests, you can compare different *random effects* structures using REML (restricted/residual maximum likelihood, as above), but you must use ML (maximum likelihood) to compare different *fixed effect* models.",2010-10-22T07:36:08.553,449,CC BY-SA 2.5,
5527,3872,0,"How are you quantifying the results? Numbers of events, or continuous measurements (on a scale with a true zero, i hope) or...?",2010-10-22T07:39:48.163,449,CC BY-SA 2.5,
5528,3877,0,"@onestop (+1) Nice follow-up. My understanding of the question was ""how to calculate an SMD in the case of a logistic regression?"". Maybe worth clarifying that this is indeed the case.",2010-10-22T07:57:54.167,930,CC BY-SA 2.5,
5529,3871,0,"Thanks Rob and Srikant for your explanation. I understand independent events now. Just out of curiosity, if I were to draw the Venn Diagram of aforementioned events, are there any characteristics that I can look for to reliably identify independent events?",2010-10-22T08:15:53.343,1636,CC BY-SA 2.5,
5530,3848,0,"Thanks for your input. Will test this approach and will update. Also, do you think, i can use Kolmogorov-Smirnov Test in my case. Please see the plot, http://img254.imageshack.us/img254/9645/25971376.gif",2010-10-22T08:40:08.237,1655,CC BY-SA 2.5,
5532,3871,0,"@Sara No, You cannot. Venn diagrams will only help you write down the formulas for the various possible relationships between the events (e.g., $P(A \cup B) = P(A) + P(B) - P(A \cap B)$. But, the diagram itself will not tell you anything about the probabilities of the events under consideration.",2010-10-22T09:34:38.537,,CC BY-SA 2.5,user28
5533,3871,0,@Srikant: Noted. Thank you so much for your assistance.,2010-10-22T09:44:16.360,1636,CC BY-SA 2.5,
5534,3847,2,"I hope you don't mind, but I added the output of your commands.",2010-10-22T10:16:36.597,8,CC BY-SA 2.5,
5535,3847,0,@csgillespie no problem :),2010-10-22T10:52:37.140,1443,CC BY-SA 2.5,
5536,3252,0,"I think I can understand that sentiment if what your interested in is solely classification. I think for causal inference the problems with data snooping are the same (i.e. the problems aren't solved by increased power to identify relationships). I'll try to formulate this opinion into an answer. I may ask a question on the main forum in the meantime about the use of cross-validation for causal inference, as I have not come across any work in my field that does this.",2010-10-22T12:48:13.493,1036,CC BY-SA 2.5,
5538,3833,0,Thank You Jeromy for clearing the concept. I am really grateful. Can you please explain PCA.,2010-10-22T13:12:10.107,1649,CC BY-SA 2.5,
5540,3252,1,"@Andy Thanks. Hopefully, your question will receive a lot of interesting answers.",2010-10-22T13:40:25.370,930,CC BY-SA 2.5,
5541,3865,0,"Thanks @Andy W, looks relevant but Wiley want $$. ""Nature of my data"": messing about with Kdtree variants / KNN; as I said, looking for an overview.",2010-10-22T13:45:08.010,557,CC BY-SA 2.5,
5542,3891,0,depends on which package list ? http://www.stat.ucl.ac.be/ISdidactique/Rhelp/doc/html/packages.html,2010-10-22T14:15:05.597,603,CC BY-SA 2.5,
5544,3891,0,@kwak Your link must be to an outdated mirror.,2010-10-22T14:17:41.510,5,CC BY-SA 2.5,
5546,3877,0,"Thanks, these answers are incredibly helpful.  To clarify (hopefully), I intend to calculate the Cohen's d statistic for the difference in mean values of the predictions generated by a logit model.  I suspect that there are simpler ways to estimate the effect size of a logit model, but I want to compare results to a meta-analysis reporting Cohen's d.  Thanks again.",2010-10-22T14:28:42.293,,CC BY-SA 2.5,Fred Vars
5547,3891,0,> Ran/build on latest R (here on ubuntu). Thanks.,2010-10-22T14:30:59.200,603,CC BY-SA 2.5,
5548,3865,0,"@Denis, I have added in a link to the pdf. Often times if you search for the publication on Google Scholar a version is posted free to the public. The supplementary material I posted is free as well, and the R code would be insightful even without access to the paper. The description ""messing about with Kdtree variants / KNN"" is not specific enough to be helpful. Based on that I imagine what I quoted is not that useful unless you are explicitly interested in geographic data.",2010-10-22T14:31:23.613,1036,CC BY-SA 2.5,
5549,3886,0,"> Can you add a link (paper, explanation, hypothesis,...) to the denominator used in the first one ? (i.e. your $\hat{\sigma}_1$) - thanks.",2010-10-22T14:34:13.370,603,CC BY-SA 2.5,
5550,3720,0,"@kwak, When I get the time I will post the question on the role of regression model assumptions when one is solely interested in prediction. Feel free to post it yourself in the meantime though if you would like.",2010-10-22T14:44:15.703,1036,CC BY-SA 2.5,
5551,3884,0,Some very interesting points here.  Convincing people to simplify and standardise their spreadsheets is likely to be more successful than getting them to abandon them.  Also I had no idea that version control could integrate with Excel.  Nice to know.,2010-10-22T14:46:21.737,478,CC BY-SA 2.5,
5552,3885,2,"Don't these become unbiased only in conjunction with additional distributional assumptions?  For example, the relationship between the range and the SD *has* to depend on the shape of the distribution.  In that context, it's worth observing that many more unbiased estimators of SD have been studied, such as those based on linear combinations of quantiles and ranks.",2010-10-22T15:06:59.537,919,CC BY-SA 2.5,
5553,3872,1,"@pom I hope you can clarify this query.  In addition to the issues raised by @onestop, your modification of both ""samples"" and ""models"" by the word ""independent"" makes one wonder about your precise meaning and your synonymous use of ""ratio"" and ""difference"" in the same sentence raises questions about what you mean by those words.  One possible interpretation is that each sample consists of a set of ratios; another interpretation is that you are estimating some kind of statistic in each sample, taking the ratio of those two numbers, and want to compare it to some standard value (such as 1.0).",2010-10-22T15:13:55.853,919,CC BY-SA 2.5,
5554,3871,1,"@Sara, if you look at a probability density function http://en.wikipedia.org/wiki/Probability_density_function of the two events fxy then you will get a rectangular distribution if they are independent.",2010-10-22T15:18:51.173,1673,CC BY-SA 2.5,
5555,3877,1,"Thanks Fred. I've looked up some old refs and an alternative way to convert a log-odds ratio (i.e. the coef for a binary variable in a logistic regression model) to an effect size comparable to Cohen's d is to multiply the logOR by ‚àö3/œÄ. See:

Hasselblad V, Hedges LV.  Meta-analysis of screening and diagnostic tests.  Psychological Bulletin 1995; 117:167-178. http://content.apa.org/journals/bul/117/1/167

Chinn S.  A simple method for converting an odds ratio to effect size for use in meta-analysis.  Stat Med 2000;19:3127-3131. http://www3.interscience.wiley.com/cgi-bin/abstract/75500445",2010-10-22T15:22:06.093,449,CC BY-SA 2.5,
5556,3894,2,That wouldn't measure dispersion (no dimensionless value possibly could): it's a measure of *shape* for a non-negative variable.  The range itself is a (poor) measure of dispersion.,2010-10-22T15:23:43.033,919,CC BY-SA 2.5,
5557,3894,0,@whuber: range is a difference between max and min values.,2010-10-22T15:25:08.837,219,CC BY-SA 2.5,
5558,3894,4,"I think everyone knows what the range is, but there are two possible areas in which it can be applied: to a *sample* or to a *distribution*.  In the latter case the range is not often used because the range is often infinite.  Thus I assumed you were asking about a sample.  Regardless, in either case the ratio of the range to a central statistic cannot measure dispersion.",2010-10-22T15:30:30.230,919,CC BY-SA 2.5,
5559,3896,1,"That's a reasonable guess.  It still does not measure dispersion,though!",2010-10-22T15:31:17.513,919,CC BY-SA 2.5,
5560,3868,4,"@Matt Dropping a zero is usually not a good idea unless the dataset is so large that losing an extreme value will make no difference in the analysis.  Your implicit suggestion of treating it as very small is good, but then the result can be sensitive to the value chosen.  One could check for that in various ways.  A more formal approach is to treat the zero as a left-censored value (censored perhaps at the second lowest value) and use methods that handle censored or interval-valued data (which includes ML).",2010-10-22T15:35:35.303,919,CC BY-SA 2.5,
5563,3070,0,"another oddity of Matlab: `std(randn(1))` returns `0`, not `nan`. This makes utterly no sense.",2010-10-22T16:08:29.657,795,CC BY-SA 2.5,
5564,3803,0,"@whuber I really want to see the PIE solution, but it looks like MC would have to suffice.",2010-10-22T16:10:59.347,795,CC BY-SA 2.5,
5565,3865,0,Thanks again @Andy W. Does my wish for *both* analysis and synthesis make sense ?,2010-10-22T16:12:44.760,557,CC BY-SA 2.5,
5566,3871,1,"@Sara, the wiki is not perfect for the multivariable case. There are better sources. but a prob density function of 2 variable can be very interesting.",2010-10-22T16:15:21.077,1673,CC BY-SA 2.5,
5567,3779,0,"@nico Thank you for that clarification.  Theoretically a similar issue pertains in dictionaries containing all possible two-letter combinations as words: when that's the case, any hand of 2 or more letters automatically contains a word.  @shabbychef's comment about mid-game shows how irrelevant the original question is to most of Scrabble, because in mid-game you have available an array of word parts (prefixes, suffixes, and even middle sections) in addition to the 7 letters in your hand.  This greatly increases the chances of being able to make a word.",2010-10-22T16:29:30.057,919,CC BY-SA 2.5,
5568,3897,1,With one df the range is not even an approximation--it's directly proportional to the SD :-).,2010-10-22T16:31:05.583,919,CC BY-SA 2.5,
5569,3865,0,"@Denis, yes your distinction between analysis and synthesis make sense given your description in the question (although I would use the word ""simulate"" instead of synthesize). I'm not sure if reducing clustering to a single coefficient for simulation will be all that insightful, and the geographic coefficients for clustering will likely be harder to implement in higher dimensional space and would probably take custom coding. The more refined a question the more applicable answers you will receive are.",2010-10-22T16:33:14.127,1036,CC BY-SA 2.5,
5570,3873,0,"This is such a nice summary!  I am grateful for your effort in sharing it with us.  I can't help upvoting it despite your appeals not to :-).  (I don't think it will change your rep, anyway, and there is a lot of merit to having it appear close to the original question if it does get substantially upvoted.)",2010-10-22T16:38:54.270,919,CC BY-SA 2.5,
5571,3887,0,Python isn't widespread in our organisation but it looks like an interesting project.  I'll see if I can pinch some ideas on how things should be done from their documentation.,2010-10-22T17:02:17.420,478,CC BY-SA 2.5,
5572,3829,0,Agree -- struggling to understand what the author(s) meant before even evaluating the scientific content is really annoying.,2010-10-22T17:04:53.940,1355,CC BY-SA 2.5,
5573,3880,4,I don't really understand why this option was added to `hist()` (`labels=T`) and not `barplot()`.,2010-10-22T18:07:58.037,930,CC BY-SA 2.5,
5574,3883,1,(+1) This is the hard way :) I like it.,2010-10-22T18:08:58.240,930,CC BY-SA 2.5,
5575,3877,0,"@Fred Still, it is not clear (sorry for that!) if your model includes a single predictor or more. In the latter case, SMDs are good summary statistics but they are not adjusted for other covariates of interest. @onestop I think there's a typo and you meant ""the coef relating a predictor to the binary outcome"" instead of ""the coef for a binary variable..."".",2010-10-22T18:18:45.753,930,CC BY-SA 2.5,
5577,3902,2,The easy way to find out is to feed it some simple datasets having ties!,2010-10-22T21:12:33.137,919,CC BY-SA 2.5,
5578,3891,1,That mirror was last updated in October 2003. It's better to use http://cran.r-project.org/web/packages/,2010-10-23T00:10:01.837,159,CC BY-SA 2.5,
5580,3895,0,"""Orthogonal regression"" is another thing one might encounter when looking for methods to deal with contaminated abscissas and ordinates.",2010-10-23T02:31:14.540,830,CC BY-SA 2.5,
5581,3833,0,"@Neelam no problems. PCA = Principal Components Analysis, a data reduction technique: see this excellent tutorial by Lindsay I Smith: http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf",2010-10-23T02:40:22.643,183,CC BY-SA 2.5,
5582,3907,0,I think the answers to this question will be helpful to you. http://stats.stackexchange.com/q/3611/1036,2010-10-23T02:43:04.073,1036,CC BY-SA 2.5,
5583,3907,1,"Have you seen the August issue of The American Statistician?  The article ""Two Pitfalls in Survival Analyses of Time-Dependent Exposure"" covers exactly this issue.  Abstract at  http://pubs.amstat.org/doi/abs/10.1198/tast.2010.08259",2010-10-23T02:44:43.033,919,CC BY-SA 2.5,
5586,3885,0,"Yes, indeed. Under the normality assumption.",2010-10-23T07:21:27.607,339,CC BY-SA 2.5,
5587,3907,0,"@whuber Sorry, I can't see why that AmStat article is relevant. Where is the time-*dependent* exposure in the question?",2010-10-23T07:25:01.637,449,CC BY-SA 2.5,
5588,3877,1,"@chl Yes, thanks for the correction. Was a bit more than a typo to be honest, more of a thinko. This conversion requires some assumptions, of course - i think they amount to requiring that the predictor should have (approx) the same variance in those with and w/o the outcome after a transformation, if required, to make the distribution of the predictor approximately normal/logistic (strictly logistic, but the two are hard to distinguish without a huge dataset).",2010-10-23T07:38:39.607,449,CC BY-SA 2.5,
5590,3886,0,"1. Quality Control and Industrial Statistics - Duncan
2. Statistical Quality Control - Grant, Leavenworth
3. Introduction to Statistical Quality Control - Montgomery",2010-10-23T08:51:34.830,339,CC BY-SA 2.5,
5591,3912,0,"Why do you say hypothesis tests are often meaningless for odds ratios, any more than any other effect estimate? I'd stress instead that confidence intervals are more useful than standard errors for odds ratios and other estimates with asymmetric sampling distributions in finite samples.",2010-10-23T08:53:17.850,449,CC BY-SA 2.5,
5592,3914,0,Thanks. But what about frequentist confidence intervals specifically? Are there any circumstances at all where they would be relevant?,2010-10-23T09:33:52.977,1393,CC BY-SA 2.5,
5593,3895,0,+1 This is yet a niche only in statistics; more complex least squares methods (not only adding X variability but also different penalties for points based on error approximations) are common in experimental physics; ROOT framework has dozens of such.,2010-10-23T11:38:36.590,,CC BY-SA 2.5,user88
5594,1280,4,http://imgur.com/0dsVC.gif,2010-10-23T11:59:06.837,830,CC BY-SA 2.5,
5595,3912,0,"@onestop Well, I was partly thinking of what you say about ""asymmetric sampling distributions..."" (and it seems I was not so clear), but also of the fact that in epidemiological studies we are generally most interested in CIs (that is, how precise is our estimate) than HT.",2010-10-23T12:33:33.110,930,CC BY-SA 2.5,
5596,3918,2,"@Srikant. No! This is how classical CIs bite. Let's assume for simplicity that the amount of cereal filled in a box is normal with mean $\mu$ and variance $\sigma^2$. The confidence interval of $\mu$ is based on its *sampling* distribution which is different. A particular CI may be way off due to sampling errors and then it will have no relation to how the machine performs. If you were to repeatedly sample and repeatedly form CIs then 95% of them would be right, but that is no consolation.",2010-10-23T13:44:59.757,1393,CC BY-SA 2.5,
5597,3920,1,"(+1) `psych::alpha` does a better job than `score.items`, thanks for adding that. I never remember the one I should use!",2010-10-23T13:53:50.730,930,CC BY-SA 2.5,
5598,3918,1,"@Jyotirmoy Of course, a specific CI may be way-off. In other words, there is a 5% chance that the CI does not contain the true value. Nevertheless, the interpretation I gave is consistent with how CIs are actually constructed. We imagine using the method repeatedly and construct the CI such that the probability that the observed CI contains the true value is 0.95. Notice that my answer does not say anything about the probability of where the true value actually lies as that is a statement that can only be made with credible intervals and not confidence intervals.",2010-10-23T14:03:22.307,,CC BY-SA 2.5,user28
5599,3918,1,"@Jyotirmoy Lower/Upper bounds for a $(100-\alpha)$% CI of an observed mean are constructed under $H_0$, where the sampling distribution of a mean (or a difference of means) is the one you assumed depending on your sample ($t$ or $z$ distribution). I found Srikant's answer correct, and his interpretation doesn't seem to go beyond the experiment that was framed. CIs are random variables.",2010-10-23T14:09:23.740,930,CC BY-SA 2.5,
5600,3918,0,"@Srikant. I perhaps misunderstood ""method=machine"" in the answer. I thought you were saying that 95% of all boxes coming out of the assembly line would have weights within the 95% confidence interval derived from a particular sample of the boxes.",2010-10-23T14:43:15.860,1393,CC BY-SA 2.5,
5601,3914,0,"I believe having different priors is a non issue (at least from the objective Bayesian point of view), if it happens that you have different knowledge about the situation at hand. We meed to see the priors as a way of casting our a priori information. I know that it is not simple...",2010-10-23T14:57:15.250,1443,CC BY-SA 2.5,
5602,3914,0,"@Jyotirmoy About bayesian vs. frequentist approaches, interesting points were made here: http://stats.stackexchange.com/questions/1611/do-working-statisticians-care-about-the-difference-between-frequentist-and-bayesi/3370#3370",2010-10-23T15:23:13.577,930,CC BY-SA 2.5,
5603,3925,3,(+1) This remind me of the `pairs.panels` function in the `psych` package by W Revelle.,2010-10-23T20:03:15.373,930,CC BY-SA 2.5,
5607,3924,0,I'm missing something: you have your dataset so you compute its mean *m* and standard deviation *s*.  The value at *z* standard deviations from the mean therefore equals m + z*s for any *z* you like.  Where is the problem?,2010-10-23T21:19:14.023,919,CC BY-SA 2.5,
5609,3927,0,"I've been looking at my data, and I misrepresented the question a little bit.",2010-10-23T21:47:35.743,,CC BY-SA 2.5,Jane
5610,3927,0,@Jane If the added context makes my answer irrelevant I will delete my answer or perhaps edit to fit your context better.,2010-10-23T21:49:08.173,,CC BY-SA 2.5,user28
5611,3924,0,"@whuber Perhaps, the problem is that we cannot assume that the distribution is normal? It seems to me that the values are bounded from below (at 0) and from above (at 255). The normal may not make sense unless the standard deviation is very small and the mean far from the the boundary points. @Dan Could you please clarify?",2010-10-23T21:51:59.663,,CC BY-SA 2.5,user28
5612,3925,0,"Interesting. I did come across that code, but never knew it also existed in the psych package.  I am sure it inspired me in someway when I wrote that post (I should add this to the credits on the post...)",2010-10-23T22:11:25.700,253,CC BY-SA 2.5,
5613,3924,1,"@Srikant That's a reasonable guess.  But nothing in the current statement requires assuming any kind of distribution at all.  Perhaps the question really is about how to *interpret* values of the form m + z*s?  Regardless, although the question makes sense, it requires some amplification in order to elicit an answer that would be genuinely helpful.",2010-10-23T22:26:49.670,919,CC BY-SA 2.5,
5614,3929,1,@Jane Please add the above information to your question using the 'edit' link below the tags and delete this 'answer' as it really does not answer your question.,2010-10-23T22:26:57.187,,CC BY-SA 2.5,user28
5616,3909,1,"So you don't think repeatability in effect estimates can be useful? Although you are not alone in your conception of what proof of causality is , I think it is quite narrow. We will never be able to indefinately prove a causal relationship, even with an experiment, absent all the evidence in the universe. Hence in my opinion the goal is to proffer evidence that whatever relationship we estimate is as close to the truth given the information we do know. Given that don't you think repeatability in prediction from a training set to a hold out sample could be a useful check on inferences made?",2010-10-24T05:48:17.460,1036,CC BY-SA 2.5,
5617,3909,0,"I appreciate your comments as well, and I completely agree that inferences are heavily dependent on logic and the research design.",2010-10-24T05:50:03.480,1036,CC BY-SA 2.5,
5618,3917,0,"Thank you for the reference. So say you are not concerned about model selection, could cross validating the effect estimates of the training data set to the hold out dataset be useful?",2010-10-24T05:54:54.043,1036,CC BY-SA 2.5,
5620,3932,2,"Thank you Whuber.  I have to teach the students with the n-1 correction, so dividing in n alone is not an option.  As written before me, to mention the connection to the second moment is not an option.  Although to mention how the mean was already estimated thereby leaving us with less ""data"" for the sd - that's important.  Regarding the bias of the sd - I remembered encountering it - thanks for driving that point home.  Best, Tal",2010-10-24T07:15:34.927,253,CC BY-SA 2.5,
5621,3922,2,"BTW, A question about *Graphing Likert scale responses* just came across Andrew Gelman's weblog yesterday :) http://j.mp/aBm8mZ",2010-10-24T08:46:37.780,930,CC BY-SA 2.5,
5623,3924,0,"In most cases the values will be nearer the high range (ie. nearer 255). The SD can also vary quite a lot unfortunately. As I mentioned in my post, I'm completely new to stats, so the formula you gave whuber might be what I want. I'll need to play around with various datasets to see if it works or not.",2010-10-24T09:37:32.653,1683,CC BY-SA 2.5,
5626,3942,1,"Thanks onestop - I didn't think there would be an answer from that direction.  Any way to sum-up the intuition, or is that not likely to be possible?  Cheers, Tal",2010-10-24T11:20:45.130,253,CC BY-SA 2.5,
5627,3942,0,"I couldn't do so myself, but a book reviewer summarised the approach in a paragraph in Amer. Stat. in 1993: http://www.jstor.org/stable/2684984. 

I'm not sure it's really practical to use this approach with your students unless you adopt it for the entire course though.",2010-10-24T12:10:18.830,449,CC BY-SA 2.5,
5629,3909,1,"Andy, I've edited my post to address your comments.  Also, I don't mean to suggest that causal inference cannot be done outside of the context of a designed experiment.  Nonetheless, it is more difficult and less certain in observational studies and we shouldn't look to model building procedures to help us with that problem.  Rather, we should try to better understand the problems for which we are attempting to understand causal relationships.",2010-10-24T14:53:16.550,485,CC BY-SA 2.5,
5630,3934,26,"Let's not confuse ""intuitive"" with ""nontechnical"".",2010-10-24T15:38:31.633,919,CC BY-SA 2.5,
5631,3932,3,"@Tal I was writing in your language, not that of your students, because I am confident you are fully capable of translating it into whatever you know will reach them.  In other words, I interpreted ""intuitive"" in your question to mean intuitive to *you*.",2010-10-24T15:40:16.843,919,CC BY-SA 2.5,
5632,3929,0,"@Srikan Vadali For some reason the site wont let me edit my own question, but it does give me an edit button for the question, so there seems to be something strange going on... Sorry about that.",2010-10-24T15:40:58.207,,CC BY-SA 2.5,Jane
5634,3210,0,"I knew of no freeware GWR implementation either until I came across this website recently, http://www.ecoevol.ufg.br/sam/#Graphics . I wouldn't be surprised if others exist though.",2010-10-24T16:03:18.187,1036,CC BY-SA 2.5,
5637,3954,6,(+1) Great that you add some pictures to illustrate the whole thing!,2010-10-24T17:54:28.340,930,CC BY-SA 2.5,
5645,3958,0,"Your question is great, unfortunately it's very similar to a previous (very highly rated) [question](http://stats.stackexchange.com/questions/3). Perhaps you could change your question to just ask about websites?",2010-10-24T21:04:13.887,8,CC BY-SA 2.5,
5646,3210,0,@Andy Thank you!  That's a real find.  This software looks like it reproduces GeoDa's capabilities with the addition of logistic regression and GWR.  I look forward to exploring it.,2010-10-24T21:09:43.460,919,CC BY-SA 2.5,
5647,3798,1,"@ars So, I'd like to suggest the following papers from Father & Son: http://j.mp/dvEDgb, http://j.mp/csD1Yf, http://j.mp/dkEHq5.",2010-10-24T21:11:56.823,930,CC BY-SA 2.5,
5648,3932,1,"Hi Whuber.  Thank you for the vote of confidence :).  The loose of the degree of freedom for the estimation of the expectancy is one that I was thinking of using in class.  The problem is that the concept of ""degrees of freedom"" by itself is one that needs knowledge/intuition.  But combining it with some of the other answers given in this thread will be useful (to me, and I hope others in the future).  Best, Tal",2010-10-24T21:12:58.773,253,CC BY-SA 2.5,
5649,3958,0,"@csgillespie Oh yes, I forgot about this one, sorry!",2010-10-24T21:18:38.333,930,CC BY-SA 2.5,
5650,3940,5,Thanks. I think it would be handy to have built-in functions (with min and max paraemters ala the unif family). It's a bit ugly to have to add function definitions into scripts just to use the discrete uniform distributions the way you would use other standard distributions. The builtin functions also deal with error handling (for example - if parameters aren't integers) and are optimized for speed.,2010-10-25T00:47:13.803,,CC BY-SA 2.5,Joseph Hsieh
5651,3917,0,"It could be, but I'd say that you're basically doing bootstrapping (or some variation thereof) at that point.",2010-10-25T01:46:59.787,303,CC BY-SA 2.5,
5655,3939,5,"This isn't correct for the edge cases. To see this, try running the following command: `table(round(runif(10000, min=0, max=2)))` It's clearly not discrete uniform.",2010-10-25T08:30:36.550,8,CC BY-SA 2.5,
5656,3939,0,"@csgillespie: nicely spotted, I updated my answer :)",2010-10-25T08:32:44.180,582,CC BY-SA 2.5,
5659,3948,5,+1 (although it deserves +3 at least) - very thorough and comprehensive. Made the whole issue much clearer for me. Thanks!,2010-10-25T11:20:54.977,22,CC BY-SA 2.5,
5662,3951,0,"Thanks for the input. Using poly() to compare different polynomial transforms of grade was also suggested to me this weekend on the R-SIG-mixed-models list, and I think this solves things. I've developed code that automates the process that will be included in the next version of my R package ""ez"".",2010-10-25T12:05:46.773,364,CC BY-SA 2.5,
5663,3967,3,"Just to be sure, are you talking about ST as found in clinical trials, e.g. http://en.wikipedia.org/wiki/Sequential_analysis?",2010-10-25T14:07:41.080,930,CC BY-SA 2.5,
5664,3931,46,"I'd like to quote this zinger from the book *Numerical Recipes*: ""...if the difference between $n$ and $n-1$ ever matters to you, then you are probably up to no good anyway - e.g., trying to substantiate a questionable hypothesis with marginal data.""",2010-10-25T14:09:30.380,830,CC BY-SA 2.5,
5665,3888,0,"I don't think a ""ggplot"" tag will be very useful here (maybe mods would have a different opinion).",2010-10-25T14:16:02.897,930,CC BY-SA 2.5,
5667,3917,0,"I agree, I and think there are other things regularly done that reflect this same sort of logic (such as subset specificity tests or non-equivalent dependent variables). I simply posed the question because I imagined more formal treatments existed.",2010-10-25T15:06:40.330,1036,CC BY-SA 2.5,
5668,3909,0,"I agree with pretty much everything you say, except that issues of accuracy and repeatability are essential to make correct inferences in the face of doubt. I can give experts the benefit of the doubt that they are building logical models. Where I am concerned is repeatability of the findings in many observational contexts. Although I agree repeatability does not necessarily account for confounding influences that are best dealt with in experimental settings.",2010-10-25T15:11:45.533,1036,CC BY-SA 2.5,
5669,3970,0,"The reason I posed the question is because we don't have to think of cross validation as solely a way to evaluate a models predictive ability. It is not uncommon to be concerned that a models results (and hence inferences made) are artifacts for many potential reasons. Hence we want to examine the robustness of the findings, and I figured cross validation could be a useful context to examine the robustness of the results.",2010-10-25T15:18:49.283,1036,CC BY-SA 2.5,
5670,3940,2,"Nice answer.  And for the quantiles we can do something like qdu <- function(p, k) ifelse(p <= 0 | p > 1, return(""undefined""), ceiling(p*k))",2010-10-25T15:24:32.287,,CC BY-SA 2.5,user1108
5671,3965,2,"Although these statements make no sense as given, they make perfect sense--and lead to an intriguing question--provided ""probability distribution"" is replaced by ""mean"" throughout.  Is this perhaps what you intend to ask?  Also, is this ""certain numerical range"" specified in advance of the analysis or are you asking how to construct an interval that has a given chance of enclosing at least one of the [means]?",2010-10-25T15:35:36.407,919,CC BY-SA 2.5,
5673,3924,1,"@Dan It would also help if you could tell us why you are doing this. As an aside: doing something like m + z*s may very well end with a upper bound of greater than 255 (depending on your values for m, z and s). Some sense of what you want to achieve (i.e., your goals) would help.",2010-10-25T15:53:13.623,,CC BY-SA 2.5,user28
5674,3884,2,"Concerning the advice not to store redundant variables: this is appropriate for RDBMSes but I would like to suggest that the opposite should be encouraged for spreadsheets.  The latter are so error-prone that mechanisms to detect and correct errors are invaluable.  One of the best consists of redundant information, such as computed fields and statistical summaries.  For example, if column C is the ratio of columns A and B, then an error in a single column in any given row can be detected and usually fixed.",2010-10-25T15:55:25.510,919,CC BY-SA 2.5,
5676,3971,0,"I had suspected there was a 'trivial' algorithm, which generated the identity permutation with probability $r$, and was uniform on all permutations with probability $1-r$, for the case $r > 0$. It looks like that is not exactly correct, and your ""uninteresting"" solution fixes my error.",2010-10-25T16:29:06.120,795,CC BY-SA 2.5,
5677,3884,1,"@whuber : that is what we check in the data control step. You can use that extra column to check fast, but you shouldn't keep it in the final sheet. Formulas in spreadsheets are horror, and the larger the spreadsheet, the more difficult to get the data out of it. Plus, in the case of Excel you'll be fighting the differences between .xls and .xlsx anyway. Be sure that a decision of a manager to update Microsoft Office can break tons of code if you rely heavily on excel files. So: save as csv, and keep these csv files as small as possible.",2010-10-25T16:33:03.637,1124,CC BY-SA 2.5,
5678,3974,2,"What do you mean by 'trivariate'? It would also help if you could to the extent possible use equations to illustrate your situation. Do you mean that you observe $Y$ where $Y = X + Z$, $X \sim Exp(\lambda)$ and $Z \sim N(\mu,\sigma^2)$?",2010-10-25T16:47:16.130,,CC BY-SA 2.5,user28
5679,3884,0,"After spending a significant part of the last 24 years of my career coping with data transmitted in spreadsheets and managing substantial databases, I must respectfully disagree.  There is no such thing as ""control"" over spreadsheets (whether .xls, .xlsx, .wks, .wb*, etc) or even csv files.  The presence of redundant information in such files--even when they're available only in printed form--has many times resurrected some fairly large databases (100k+ records).  Every time this happens I (and my clients) have been grateful for the redundancies.",2010-10-25T17:14:21.963,919,CC BY-SA 2.5,
5680,3975,2,"(+1) Interesting! Which software do you use? Just a remark: There's no indication about absolute values for % or counts, so this seems to allow only a relative interpretation.",2010-10-25T17:45:40.450,930,CC BY-SA 2.5,
5682,3975,0,"Sorry, I didn't read your last sentence (the x-axis is invisible). I'll try another remark: Any chance to get the NA counts visible in the centered view (i.e. distinguish them from neutral)?",2010-10-25T18:11:11.580,930,CC BY-SA 2.5,
5683,3971,0,"@shabbychef You have a fine intuition: picking a permutation uniformly at random gives an expected correlation of zero. To achieve an expectation of r > 0, then, select the identity with probability r and otherwise select any uniformly random permutation (*including* the identity) with probability 1-r. A similar approach (using the reversal of ranks) will yield any expectation down to the minimum possible value for the data (which depends on the data, not just on how many there are). These solutions don't yield the most uniform distributions overall, though: one permutation clearly is favored.",2010-10-25T18:30:48.727,919,CC BY-SA 2.5,
5684,3974,0,"Sorry to use a non-standard term.  The data are multivariate with three variables: R,G,B.  I have the joint J(R,G,B) and I can compute the marginals.  The form of the (univariate) marginals is the Y you defined.",2010-10-25T18:31:11.657,1704,CC BY-SA 2.5,
5685,3976,0,"I know that copulas capture advanced correlation structures; I don't know much else about them.  Unfortunately, I don't think they would be useful to me (for understanding the relationships in the data), nor to scientific colleagues (who would have to have some intuition/experience with copulas to understand my conclusions).",2010-10-25T18:32:23.373,1704,CC BY-SA 2.5,
5686,3974,1,"It would also help if you clarify a bit more what you mean by ""understand this data"". Is there a specific question(s) you are trying to answer or is this just exploratory data analysis? I would suggest editing the question to incorporate your comment and make the question as precise as possible so that useful answers can be given.",2010-10-25T18:36:45.130,,CC BY-SA 2.5,user28
5687,3970,0,sorry for the misinterpretation.,2010-10-25T18:52:46.330,1307,CC BY-SA 2.5,
5688,3924,0,"Unfortunately it's not that easy to explain why I want it without writing a huge article worth of text. The upper bound going greater than 255 isn't a problem. It's the lower bound I need.
I think the m+z*s pretty much answers my question. Thankyou for that. If you post it as an answer, I'll mark it as the answer. Unfortunately I don't have the rep to upvote.",2010-10-25T19:02:44.190,1683,CC BY-SA 2.5,
5689,3967,0,"Yes. There are quite a few variants of sequential testing, including sequential t-tests, but none are used in basic research. I don't see any impediment to their use.",2010-10-25T19:19:09.187,1679,CC BY-SA 2.5,
5690,3971,0,"@whuber yes, I had mistakenly thought that I could also achieve any negative $r$ by this trivial method, by 'reversing' the indices with probability $|r|$. I guess I was tacitly assuming the vector $X$ was symmetric about $0$. The trivial algorithm is unsatisfying, though, as you suspected.",2010-10-25T19:25:01.373,795,CC BY-SA 2.5,
5691,3974,0,"I will try improve the question.  I need to give it a bit of thought.  -If- the data were multivariate normal (and, as follows, normal in the marginals), I would compute r and be done with it.  So, maybe I have two options:  (1) transform the data so they are normal or (2) compute r (via resampling) to assess its validity.  As far as ""understanding the data"":  ideally, I would want to characterize the conditional distributions (not necessarily by closed form equations) ... that is, I'd like to know how the variables affect each other.",2010-10-25T19:29:34.650,1704,CC BY-SA 2.5,
5692,3971,0,"@shabbychef I believe these permutation methods tend to be used more with Spearman (rank) correlation coefficients, where all values between -1 and 1 are attainable.",2010-10-25T20:15:36.690,919,CC BY-SA 2.5,
5694,3961,1,"@kwak Concerning your deleted response: I see vaguely that there is some kind of limited permuting going on until somehow a correlation coefficient below 0.5 (e.g.) is attained, but I cannot quite connect that with the conditions of the original question.  Are you saying you just keep applying ""small"" random permutations until you obtain a correlation within a targeted range of correlations?  If that's so, I think it's a really creative idea but some analysis is needed to show it can actually work.  Regardless, thanks for sharing it (however briefly!).",2010-10-25T20:25:53.773,919,CC BY-SA 2.5,
5695,3977,3,"I don't think that this is on-topic, but I will wait for others to vote to close it.",2010-10-25T20:29:24.767,5,CC BY-SA 2.5,
5696,3977,2,Plenty of answers are at http://www.google.com/search?q=abbreviate+millions .,2010-10-25T20:40:08.123,919,CC BY-SA 2.5,
5697,3884,0,"@whuber : We do data control with extra scripts, looking for the impossible values/outliers/odd cases. That's what I mean with the data control step. This is industry standard btw in companies like SGS and others doing analyses of clinical trials etc. The redundant information that is needed is kept in seperate databases. If one of them fails, the other is needed for the resurrection. In case you don't have a decent backup system, that is...",2010-10-25T20:56:58.570,1124,CC BY-SA 2.5,
5698,3971,0,"@whuber: you read my mind! A solution to this problem could be used to generate a bootstrap sample from a vector with a fixed level of Spearman correlation to any other vector of the same length (modulo degeneracies in that other vector.) In that case, it suffices to think of $X_i = i$ up to location and scaling shifts.",2010-10-25T21:02:29.437,795,CC BY-SA 2.5,
5700,3979,2,"I cannot think of any situation in which molar can be confused with ""millions"". Millions is not generally used by itself, you usually write, for instance, 2M‚Ç¨, 2Mb or 2MŒ©. Moreover, you use Molar next to a chemical name, es. CaCl2 0.5M, so there is no ambiguity.",2010-10-25T21:44:36.983,582,CC BY-SA 2.5,
5703,3975,0,"@chl Thanks. I use JMP, which I get paid to work on. The first one is a stacked bar chart with positive and negative values, which should be possible in lots of tools. NA counts could be done different ways (at one end, split over both ends, in the middle, separate column) and none seems obviously better for most situations.",2010-10-25T23:55:45.120,1191,CC BY-SA 2.5,
5704,3970,0,"No need for apologies. I'm the one suggesting something apparently fringe, and cross validation is apparently always used in the context you suggest.",2010-10-26T00:07:07.783,1036,CC BY-SA 2.5,
5705,3955,1,"I'm surprised you have not received any answers yet (although it is an obviously popular question). Since each of your four bullets are worth a question in and of themselves, how about limiting this question to one of them and post the others in separate questions. They are all really excellent questions that take distinct approaches, and all four are worthwhile on their own.",2010-10-26T00:11:17.587,1036,CC BY-SA 2.5,
5706,3924,0,"@whuber See Dan's comment. Perhaps, you should post your suggestion as an answer for him to accept it. FYI.",2010-10-26T05:02:12.963,,CC BY-SA 2.5,user28
5707,3982,1,Might this be more likely to be find an answer at http://stackoverflow.com/?,2010-10-26T06:26:20.397,449,CC BY-SA 2.5,
5708,2032,1,"This Q has reappeared on the 'active questions' page as it was 'poked' by the 'Community' background process, one of whose tasks is ""Randomly poke old unanswered questions every hour so they get some attention"". I don't think this question *deserves* any more attention. It's community wiki, it received only one up-vote, got a reasonable answer on Aug 26 but the user who asked it was last seen on Aug 24, so it seems unlikely that user is ever going to accept an answer. I suggest it should be closed by a moderator.",2010-10-26T06:40:40.083,449,CC BY-SA 2.5,
5710,3876,0,"Shouldn't time be a random effect since the heart rate measurements are samples taken during the surgery? 

If this is the case, would the following fit make sense (since I'm still reading up on the lmer function and have not quite understood the syntax)?

lmer(heart.rate~treatment+(1|id)+(1+time),data=heart)",2010-10-26T08:10:34.803,1663,CC BY-SA 2.5,
5713,3371,7,"Just to say that my answer may actually look a little bit off-topic since this question has been merged with another one, http://stats.stackexchange.com/questions/3369/difference-between-fa-and-pca (I initially answer to the latter).",2010-10-26T09:16:23.327,930,CC BY-SA 2.5,
5715,3986,0,"Hi chl,Thanksfor the reply..I admit that I got some information from your answer..Let me put my understanding and then you can comment please. (1) I get a hint that P values can go down if your sample size is big...--Is that so ?? To my understanding p values can only show whether or not ur null hypothesis is rejected. (2) I understand now that I need to see difference in AIC values with intercept only and with covariates. I suppose when we say that we want lower AIC we mean for the same dataset. I am getting character character left in my comment so will comment again once you answer please,",2010-10-26T09:48:39.483,1763,CC BY-SA 2.5,
5716,3986,1,"@ayush (1) the test statistics (e.g. Wald) depend on the sample size (the standard error decrease with increasing sample size, and you're likely to get lower p-values with a larger sample). (2) yes, although AIC may be used to compare non-nested models, here I was thinking of it as a way to compare different models of increasing complexity.",2010-10-26T10:35:32.587,930,CC BY-SA 2.5,
5717,3965,0,first of all thanks for pointing out that my question is imprecise. i still need to to a lot practising for that. i modified the question accordingly. bear in mind what i want to do evolves along with what i probably need :),2010-10-26T10:53:03.480,1516,CC BY-SA 2.5,
5718,3986,0,thanks again..I get the essence of the p value now. Some 5mins back I ran a model which is giving me p values below .05 for all the variables but AIC of 28238.407 with intercept only and with covariates 21507.933. I also have a case in which AIC is 16035.xy with intercept only and with covariates 4234.xy. What is your opinion comparing two cases ? Please note that the second model had different variables 25 var while first had 20. so second though had more variables ( 25 comparison to 20) had lower AIC. Though p values werent .05 for all. Please suggest..more to ask after this..Thanks.,2010-10-26T11:33:26.513,1763,CC BY-SA 2.5,
5719,3980,0,"I think we need greater clarity with respect to the timing of interventions and the measurements. If I am reading your question correctly the timing is: A, B, {k1 successes in n1 trials}, B, C, D, {k2 successes in n2 trials}, {k3 successes in n3 trials} etc. I also assume that there are no other treatments beyond the 5 treatments listed. Is the above correct?",2010-10-26T12:43:51.017,,CC BY-SA 2.5,user28
5720,3955,0,"Hi Andy, thanks a lot for your answer. I'll try to rephrase it and focus on a single issue at a time. My initial thought was making it as broad as possible (and avoid repeating the same question afterwards) but given the lack of answers, it seems it is too general for someone to answer.",2010-10-26T12:56:23.727,1694,CC BY-SA 2.5,
5722,3986,0,"@ayush It's difficult to answer about model quality without knowing how variables were selected. The gap in AIC between a model including only an intercept and some covariates gives you an indication about the ""explanatory power"" of those predictors (the residual deviance seems to decrease by a larger extent in the 2nd case you showed, and AIC penalizes for the # parameters as I said in my response). It's by no means a full answer about the relevance of these predictors. I'd suggest you to ask for a more specific question (IMO), e.g. about *variable selection in GLMs* for your specific study.",2010-10-26T13:29:07.270,930,CC BY-SA 2.5,
5723,3982,0,"I agree, this has nothing to do with statistical analysis as stated.",2010-10-26T13:29:23.953,1036,CC BY-SA 2.5,
5724,3993,0,"I will add a more detailed comment later, but yes what I am suggesting is a method of cross validation that is concerned entirely with internal validity.",2010-10-26T13:44:57.863,1036,CC BY-SA 2.5,
5725,3876,1,"The term '(time|id)' on the random effects side tells the function to fit different (linear) slopes for each person. So you can have time as both a fixed effect and a random effect, but they mean different things. Have a look at the sleepstudy example in Douglas Bates' book: http://lme4.r-forge.r-project.org/book/Ch4.pdf",2010-10-26T13:47:09.617,966,CC BY-SA 2.5,
5726,3982,3,"IMO, it does have to do with visualization (which is on-topic), so I'm open to allowing it.  That being said, I'm sure that you will have a hard time getting an answer either here or SO for this question: your best bet is the protovis mailing list.",2010-10-26T13:50:17.320,5,CC BY-SA 2.5,
5727,3982,2,"@Saneef It would be helpful if you could provide a data sample so this is reproducible.  Either make geoPopList available, or else use a smaller data same that can be posted here.",2010-10-26T13:51:44.213,5,CC BY-SA 2.5,
5728,560,1,The link to the paper doesn't work for me. Should it be http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.1675 ?,2010-10-26T14:46:11.670,461,CC BY-SA 2.5,
5729,3982,1,"@AndyW Thanks for a flag, but I agree that it is on topic.",2010-10-26T14:52:52.680,,CC BY-SA 2.5,user88
5730,3955,0,I edited the question to expose the two main issues I'm mostly interested in. I chose to put them in the same question given their tight relationship.,2010-10-26T15:03:35.240,1694,CC BY-SA 2.5,
5731,3993,1,"@chl: Can you explain your statement in the first paragraph on internal v. external validity? In the tradition that I am familiar with: **internal validity** refers to the ability to assert cause and effect relationships among the variables within the particular sample; **external validity** is about the ability to generalize from a sample to other persons, places, and times.  Traditionally, cross-validation is about the latter and thus by the above definition about external validity, whereas you state that it is about internal validity. Did I misunderstand your statement?",2010-10-26T15:08:34.677,485,CC BY-SA 2.5,
5732,3907,0,Is the case that the gene change took place for all participants *before* they enrolled in the registry? The other possibility is that gene changes took place at some random time before/after enrollment but you simply do not know when it took place. Can you clarify this issue?,2010-10-26T15:12:07.830,,CC BY-SA 2.5,user28
5733,3993,1,"@Brett I was thinking of CV as a statistical technique to avoid overfitting or to provide a measure of prediction accuracy on the working sample (hence not necessiraly as a dedicated tool to demonstrate internal validity). I was not very clear, thanks or pointing that. I agree that this is then used to generalize over the sample at hand, but there I think it has nothing to do with causal inference (CV doesn't prove anything about causal links as modelled on the working sample). I share your view on external validity, but to demonstrate it we need other samples, no?",2010-10-26T15:21:36.573,930,CC BY-SA 2.5,
5734,3993,1,"You might clarify that first paragraph.  I think you're trying to say that CV doesn't do internal validity.  That's a matter for other processes.  But, if we've got good internal validity for other reasons, whatever that might be, CV will help estimate that effect more accurately across persons, places, and times--i.e. improve external validity. I still cannot think of any way that CV would help us to make causal claims about relationships between variables--the internal validity question itself--only to help generalize an established causal relationship.",2010-10-26T15:28:48.147,485,CC BY-SA 2.5,
5735,3924,0,"@Srikant I am happy Dan has found something useful in these comments (and that you have helped him in that).  However, I am reluctant to post an answer to a question I just don't understand!  Remember, too--I think I'm recalling this correctly--it is this site's policy not to go to extraordinary efforts to help someone formulate their question, so I'm even more reluctant to restate what I think the question might be asking and then answer the restated question.  Whom would that serve?  Others should feel free to jump in and do this if they want some easy points.",2010-10-26T15:53:42.103,919,CC BY-SA 2.5,
5736,3999,0,"As a hypothetical example, suppose the categories are named ""A"" through ""E"".  Could an object simultaneously have values of 10 for A.Low, 20 for B.High, 50 for B.Medium, 15 for B.Low, and 5 for C.High (and zeros in all other subcategories)?  Or are you treating all five categories independently so that, e.g., the object could have 100 in A.Medium, 90 in B.High and 10 in B.Medium, 50 in C.Low, 10 in E.High (and zeros in the other 10 subcategories)?  In the former case you would seek a single index; in the latter, five index values (one for each of A, B, ..., E).",2010-10-26T15:58:40.693,919,CC BY-SA 2.5,
5737,3971,1,"@shabbychef Then you might want to give some thought to achieving a desired *variance* in the correlation coefficients that will be generated.  You can include that within the constraints and, provided the desired variance is neither too great nor too small, you should still have a nonempty feasible set.  For this application it might be wise to achieve as uniform a distribution as possible.  Alternatively, you can impose a desired variance by means of a penalty term in the objective function, seeking a balance between uniformity and achieving the target values of variance and expectation.",2010-10-26T16:16:45.210,919,CC BY-SA 2.5,
5738,3980,0,"Sorry, I think I did not communicate clearly.  Case 1 had k_11 successes in n_11 trials in one month, then A and B were applied to Case 1, and then it had k_12 successes and n_12 trials in the following month.",2010-10-26T16:47:19.320,,CC BY-SA 2.5,george s
5739,3980,0,"Followup - Case 2 had k_21 successes in n_21 trials in one month, then B,C and D were applied to Case 2, and then it had k_22 successes and n_22 trials in the following month.",2010-10-26T16:48:31.143,,CC BY-SA 2.5,george s
5740,3993,1,"@Brett I think your comments to this question are very pertinent and sum up some of the issues very nicely. I doubt it will help any of the confusion between internal and external validity at this point, but chl's genetic epidemiology example is actually a problem of internal validity not external validity (except for between dataset heterogeneity (or population substructure), but that IMO is of less concern than internal validity in these examples).",2010-10-26T16:52:18.270,1036,CC BY-SA 2.5,
5743,3907,0,"@Srikant Vadali They where born with these genes. There was not change in genes :-) This is the root of the problem/question. I have not seen this spelled out well but I assume the starting time could be birth. But it cannot be unrelated to the outcome event? But what if age is part counted is used as an exogenous variable, I not clear on how this effect the estimation. Does this help. If so I will edit the original question to include this.",2010-10-26T18:06:14.620,1189,CC BY-SA 2.5,
5744,3981,0,"@Srikant, how did u manage to get 1., 2., 3. numbering working. I, for the life of me, could not figure it out!",2010-10-26T18:17:25.953,1307,CC BY-SA 2.5,
5745,3981,1,"Magic! The secret is: Type a space at the start of the following paras: ""Apart from..."" and ""There is a R package..."".",2010-10-26T18:20:39.650,,CC BY-SA 2.5,user28
5746,3907,0,"I do not see any issues in using 'age' as an independent variable while using birth time as the starting time. Think of the following analogy: Suppose that you are investigating the variation in salary increases and you have a hypothesis that the higher someone's income is the lower is that person's salary. In such a situation, the 'income variable' will appear on both sides of your model: In 'Percent salary increase' income will appear as part of the dependent variable and income will again appear as an independent variable in the model.",2010-10-26T18:25:59.123,,CC BY-SA 2.5,user28
5748,3999,0,"Thanks for you answer, yes an object could have A.low = 20, A.medium = 40, B.High = 40 and C.High = 5. All others would be zero. So yes, I am seeking for a single index.",2010-10-26T19:07:43.500,791,CC BY-SA 2.5,
5749,3999,1,"OK, just to be clear, because this is an important detail: is there a natural hierarchy to the categories and subcategories?  Could one assume that the ordering is A.High A.Medium A.Low B.High ... E.Low (or something comparable)? (If so, is there anything at all to prevent us from thinking of these 15 classifications as forming a single *ordinal* variable?) Also, it would help to clarify what the numerical values are intended to mean. *E.g.*, should they be interpreted as fuzzy degrees of membership, as probability of membership, as combined results of multiple measurements, or something else?",2010-10-26T19:24:23.930,919,CC BY-SA 2.5,
5750,3976,2,"@Mark In light of the revised question, I agree that looking at a copula-based description is a good suggestion.  There's nothing terribly difficult about copulas: because you have the marginals, you can apply the probability integral transform to make the three variables (R, G, B) uniform on [0,1].  A copula merely describes their multivariate distribution in these terms.  In effect, it eliminates the parameters from the marginals to allow you to focus on describing their multivariate dependencies.  It affords the non-parametric approach you're asking for.",2010-10-26T19:31:53.173,919,CC BY-SA 2.5,
5751,3987,1,"Basic biomedical researchers do interim analyses all the time, they just don't declare them because they don't even know that it matters! I have surveyed researchers at a national congress and found that more than 50% did not know that the control of error rates from Student's t-test is dependent on a pre-determined fixed sample size. You can see evidence of that in the sometimes erratically varying sample sizes used.",2010-10-26T19:44:16.463,1679,CC BY-SA 2.5,
5752,3999,0,"For example: if I am interested in the overall urbanization of a state, my five categories would be: roads, residential areas, industrial areas, airports and intense agriculture. Each of the categories is divided into 3 subcategories (low, mid, high) with the percentage of area falling into these categories. Now I would like to express the degree of urbanization in one number without loosing to much detail. Maybe I should get rid of the subcategories? Thanks a lot for your effort.",2010-10-26T19:44:48.650,791,CC BY-SA 2.5,
5753,3987,0,Some of the disadvantages that come from the complexities of sequential designs come specifically in the design of the analyses rather than in their implementation. Perhaps we could have a set of pre-canned designs for small sample basic experiments.,2010-10-26T19:49:06.993,1679,CC BY-SA 2.5,
5754,3993,2,"Brett's definition between internal and external validity is accurate, but for our purposes it will help to define it in different terms. External validity is only concerned with the sample and how that sample relates to other populations. Internal validity is concerned with various aspects about the effects estimated and the constructs used to estimate those effects.",2010-10-26T20:06:13.893,1036,CC BY-SA 2.5,
5755,4002,0,"""However, if you use the starting time as the time at which they enter the registry....bias your conclusions."" Thanks for your answer. I agree with your answer I am trying to convince others that using time of enrollment is wrong as it is not related to outcome or genetics. Age could be the endog which is equivalent to starting the survival interval at birth.  Do you have a reference that would indicate that using time of enrollment is wrong.",2010-10-26T20:06:26.323,1189,CC BY-SA 2.5,
5756,3998,0,"Thanks Shane, I have not had any esperience analysing timeseries i tried briefly dxts = as.xts(as.POSIXct(ae$date_time)) but had no luck any pointers?",2010-10-26T20:14:48.017,1716,CC BY-SA 2.5,
5757,3993,0,"@Andy @Brett I fully agree with Brett's definitions. My point was that CV won't help confirming an hypothesized conceptual model, this one contributing to internal validity. But I must admit that it wasn't clear in my original answer. I'll update it when I get back to my computer.",2010-10-26T20:31:10.860,930,CC BY-SA 2.5,
5758,3976,0,"Thanks for sticking with it through the reformulation.  I will look into copulas.  Will copulas allow me to compare marginals and conditionals from different Js:  $J_1$, $J_2$, ...  I might also wish to compare $R_1|G_1$ (from $J_1$) and $R_2|G_2$ in some way.  The different joints are measurements on individuals in experimental conditions.  Some of the measurements are repeated measurements of the same individual).",2010-10-26T20:31:47.003,1704,CC BY-SA 2.5,
5760,3976,0,"@Mark Copulas will be useful for identifying, creating, and comparing models of joint distribution, but I don't think they will help in formal tests of hypothesis concerning those models.  Consider asking another question in which you reveal more about the nature of the data and how they are collected, because that ought to play an important role in the modeling and choice of analytical techniques.",2010-10-26T21:00:40.597,919,CC BY-SA 2.5,
5761,4003,0,"Many thanks for this detailed answer, it makes a lot of things clearer and I think I will follow your suggestion regarding PCA.",2010-10-26T21:06:12.213,791,CC BY-SA 2.5,
5763,4002,2,"@Vincent At risk of being repetitive, I would refer you again to the TAS article (""Two Pitfalls in Survival Analyses..."") because the authors address *precisely* this issue.  They use ""multistate modeling"" to examine ""specific types of survival bias such as *length bias* and *time-dependent bias*.""  You should be particularly concerned about length bias, which is a form of sample selection bias.  (What about people who died before they had a chance to ever make it into your study?)  This article, as is typical for TAS, has a good bibliography for further research.",2010-10-26T21:33:35.970,919,CC BY-SA 2.5,
5764,3907,0,@onestop See my comment to Srikant's answer.,2010-10-26T21:34:12.897,919,CC BY-SA 2.5,
5765,3976,0,"Will do.  It's always difficult to get the right abstraction of a problem.  These two questions also seem to get at distribution free, multivariate analysis:  http://stats.stackexchange.com/questions/4/assessing-the-significance-of-differences-in-distributions and http://stats.stackexchange.com/questions/1927/is-there-an-accepted-definition-for-the-median-of-a-sample-on-the-plane-or-highe",2010-10-26T21:35:25.313,1704,CC BY-SA 2.5,
5766,3976,0,"I'm reading over some of the terminology regarding copulas.  I'm missing a ""process"".  For example:  with an empirical joint and empirical marginals and a distribution for the marginals, what do I do?  It seems that I need a transformation function to take my marginals to uniform distributions and then a ""copula function"" (term?) to combine those to for a distribution which (hopefully?) models the joint.  It may turn out that I'm more interested in the pairwise marginals (RG, RB, GB) than I am in the full joint.",2010-10-26T21:39:59.270,1704,CC BY-SA 2.5,
5768,4002,0,"@Vincent No, I do not know of any reference. Perhaps, the reference pointed out by whuber may help. Alternatively, you can perform a simulation study and demonstrate what the right choice is. By the way, if people enroll roughly at the same age then the issue is not relevant as enrollment time and age will be synonymous. The issue of incorrect estimates becomes more prominent as enrollment times vary a lot vis-a-vis age. Or, at least that is my intuition.",2010-10-27T00:55:49.127,,CC BY-SA 2.5,user28
5769,3643,0,"@rob - if i catch your drift, i think we are on the same page regarding actual vs asymptotic parameters. we seem to agree that the latter are what really matter.",2010-10-27T03:01:12.623,1112,CC BY-SA 2.5,
5770,4010,1,"You say this sequence 'may represent the correlation between a couple of measurements'. Do you have the data for the measurements? If there are only a couple of measurements, how can you derive a sequence of *N* correlations from them?",2010-10-27T06:48:08.893,449,CC BY-SA 2.5,
5771,3992,0,"I'm using Mathematica for developing the codes I need, how can I use what I downloaded from robustbase website?",2010-10-27T08:24:29.117,1637,CC BY-SA 2.5,
5772,4006,0,"(+1) Thanks for reiterating the need to read the R vignette. Nothing's missing there. And for a more thorough understanding of it, all preprint/draft papers that sit on Jan's website, http://gifi.stat.ucla.edu/",2010-10-27T09:28:46.370,930,CC BY-SA 2.5,
5774,4006,0,"@chl Yeach, infamous Cuddy Valley...",2010-10-27T09:48:26.767,,CC BY-SA 2.5,user88
5775,3992,0,You can find the source of the function in this thread: http://stats.stackexchange.com/questions/4017/translate-with-rcpp and try to translate from there to mathemtica.,2010-10-27T10:02:57.763,603,CC BY-SA 2.5,
5777,4011,0,"Which comment of mine confused you? By the way, I am not the OP. Just wanted to clarify as it feels as if your response including the last line is addressed to me.",2010-10-27T12:07:59.080,,CC BY-SA 2.5,user28
5779,2697,5,"Okay: the Eigenvalue associated with each principal component tells you how much variation in the data set it explains (in my example, how clearly it separates your bottles into groups).  They are usually expressed as a percentage of the total variation in the data set.  As for the Eigenvectors, well, that's where as claws said I follow the output of an analysis like a machine ;)  In my head, they are related to how you rotate Vince's mobile to its 'best' orientation, but this might not be the right way to think of them.",2010-10-27T13:07:34.637,266,CC BY-SA 2.5,
5780,4018,0,"@Dirk:> a) there are plenty of implementation of median() (say 'pull' in package pcaPP) so it's fair game. b) you mean one won't notice a sizable increase in running times ? c) okay, but i think the issue with this code is not really the translation to C++, rather the idea of calling some R functions [pnorm, dnorm,...] in C++ (of course i can be really wrong) d) can you provide the link to your mailing list ?",2010-10-27T13:21:12.227,603,CC BY-SA 2.5,
5782,4018,0,"Can we please split the sub-questions off one by one?  A) you can call R function from C++ -- for convenience but not necessarily speed.  See the examples/ in Rcpp.  B)  I said no such thing.   C)  That is a all easy since Rcpp 0.8.7, see the 'Rcpp sugar' docs, posts on Rcpp-devel and our recent presentations.  D) It hangs off the R-forge page; just google for 'rcpp-devel'.",2010-10-27T13:25:05.713,334,CC BY-SA 2.5,
5786,4018,0,"@Dirk to A)B)C)D->thanks i'll read more. Would you know of a directory/listings of examples of simple R functions together with there translation in C++ (i know there are C++ funciton in any library but i would prefer modern examples, that is, ones that use as much as possible the features of Rcpp).",2010-10-27T13:36:23.407,603,CC BY-SA 2.5,
5788,4018,0,"Did you see examples/ within the Rcpp package?  Also, some of the HPC tutorials are by now 'too old' for current Rcpp practice. Make sure you lick a recent one -- say from useR this summer.",2010-10-27T13:41:16.710,334,CC BY-SA 2.5,
5789,4018,0,"The HPC tutorial i saw was over a year ago (btw thanks for these and the rest of your contribs), so maybe there is more there. I saw the examples in '/Rcpp/inst/examples/functionCallback/' but that's really not much. I realize there may be more spread around on your mailing list (and on your HPC slides) but that's too sparse. Would you have learned regression if it were taught that way (couple of contrived examples on slides and ML)?",2010-10-27T13:56:10.700,603,CC BY-SA 2.5,
5790,4018,1,"1) Start at http://dirk.eddelbuettel.com/presentations.html and work your way down.  2) There are six subdirectories to examples/ so I am unsure why you focus on one.  3)  There are 770+ unit tests that double as examples if you care to look closely enough.  4) There are eight (8) vignettes in the Rcpp package.  5) We authored a few other packages that use Rcpp, you could look at those too.  6) Lastly, CRAN lists fifteen packages depending upon Rcpp -- these all are examples too.",2010-10-27T14:00:36.777,334,CC BY-SA 2.5,
5792,4020,0,"Thank you for your answer!

But I have some doubts. The problem is that A always outperforms B if initial params and learning/validation/testing sets are the same; but it doesn't neccessarily hold
if they differ.
See question update.",2010-10-27T14:13:14.413,1725,CC BY-SA 2.5,
5794,4019,0,"It would be helpful if you could give a bit more detail about the nature of the machine learning algorithms, in particular what is meant by ""randomly generated initial approximation (parameters)"" - does this mean something like a random set of initial weights for a neural network, or do you mean somplething like a random choice of kernel parameters for an SVM?  The correct evaluation protocol may depend on the nature of these ""parameters"".",2010-10-27T14:32:21.077,887,CC BY-SA 2.5,
5796,4018,1,"Dude: There is a mailing list for the project you are interested in. *All* our documentation suggests to ask on the mailing list. So why-oh-why do you keep piling on here?  Can we **please** stop that now.  Lastly, your 'too superficial' would require some backing up.  I will gladly review patches, just don't post them *here*. Ok?",2010-10-27T14:38:48.417,334,CC BY-SA 2.5,
5797,4017,3,"I *strongly disagree* with your edited title and added/edited question.  You are simply mistaken if you consider Rcpp to be a code compiler, or when asking us to rewrite code for you.",2010-10-27T14:41:37.197,334,CC BY-SA 2.5,
5798,4023,0,The Table did not come out right. Every odd number is a count from group 1 and every even number is the respective count from group 2,2010-10-27T14:42:40.753,1727,CC BY-SA 2.5,
5800,4018,2,"@kwak: Responding to ""It's something that should be outsourced to the community"": I look forward to seeing your contributions as you work through these examples yourself.",2010-10-27T14:45:56.143,1657,CC BY-SA 2.5,
5801,4019,0,"This initial approximation is a couple of matrixes filled in with random values drawn uniformly & independently from [0;0.02]. (In fact, I'm comparing matrix factorizations via SGD with different loss functions; this matrixes are initial approximation of factors). So, it's closer to weights for NN",2010-10-27T14:46:21.747,1725,CC BY-SA 2.5,
5802,4017,0,"@Dirk:> sure, what would a suggested title be (the older one?). I don't really care about this particular function I'm interested in learning ways to make my codes run faster. If you have another example, please post it. I'll happily close this one.",2010-10-27T14:49:33.550,603,CC BY-SA 2.5,
5803,4018,0,@Joshua Ultrich:> Sure: my question was my initial contribution to the process. Let's see if we can keep this ball rolling.,2010-10-27T14:52:40.613,603,CC BY-SA 2.5,
5804,4023,0,I've reformatted your question. Is the table now correct?,2010-10-27T15:04:31.373,8,CC BY-SA 2.5,
5805,4019,1,"In that case, I'd probably use the bootstrap, but generate a different initial matrix for each iteration, so that the resampling procedure averages over the variability due to the partitioning of the dataset and due to the initialisation, but I would use the same initialisation in each iteration for both methods.  I'd then use the Wilcoxon signed rank test for performance evaluation (fewer assumptions).",2010-10-27T15:28:30.713,887,CC BY-SA 2.5,
5807,4025,1,Crossposting between here and StackOverflow is discouraged.,2010-10-27T15:47:25.007,334,CC BY-SA 2.5,
5808,4025,2,I would suggest that this would be the more appropriate place for this question.,2010-10-27T15:48:40.180,8,CC BY-SA 2.5,
5810,4025,0,sorry for crossposting.i didn't know here when i posted the question to stackoverflow.somebody suggested here and i posted here then. thanks.,2010-10-27T15:50:50.197,,CC BY-SA 2.5,Yuan
5811,4024,0,"Thank you for suggestions. Logically, it is not quite possible to merge phenotypes as each of them is a unique combination of three recorded parameters. Since each of these parameters can go ""up"", ""down"" or stay ""unchanged"" as a result of a mutation, so there can be 3^3=27 distinct phenotypes. In the example above I removed those phenotypes for which both groups scored ""0"", so there were only 21 of them. I do see the prevalence of certain phenotypes but i would like to have some statistical proof that distributions of such phenotypes in various groups of mutants is similar (or not). Thank you!",2010-10-27T16:03:56.497,1727,CC BY-SA 2.5,
5812,4019,2,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.3325,2010-10-27T16:26:57.720,511,CC BY-SA 2.5,
5814,4020,2,"Note that if you adjust parameters of an algorithm after seeing its performance on the test set, test set error is no longer guaranteed to be an unbiased estimate of true error.",2010-10-27T16:41:42.403,511,CC BY-SA 2.5,
5815,4020,0,"Yaroslav! Actually, I've got learn,validation and test sets. I use validation set to tune some params like regularization term weight and optimization step (via simplex [meta]optimization), while initial weights (the parameters I was talking about before) are not adjusted at all... surely, the test set is used only to calculate quality metrics",2010-10-27T16:51:31.513,1725,CC BY-SA 2.5,
5816,4026,0,"unfortunately, i have no background in statistics so i don't know what to expect from a ""MLE"" output. sorry for that. so, when i print model$table, i get something like below. is this MLE?                                                          
            Sepal.Length                                               
Y             [,1]      [,2]                                           
  setosa     5.006 0.3524897                                           
  versicolor 5.936 0.5161711                                           
  virginica  6.588 0.6358796",2010-10-27T17:18:24.923,,CC BY-SA 2.5,Yuan
5818,4029,1,"Interesting, Fisher's test crashed on R.",2010-10-27T18:51:38.150,930,CC BY-SA 2.5,
5819,4030,0,"Hope you don't mind but I added a link to ""VECM"". Feel free to correct.",2010-10-27T18:57:52.653,930,CC BY-SA 2.5,
5820,3982,0,"Is there a reason why the code you link to on the example page doesn't work? From my reading of the pv.Geo.scale source it appears that it supports setting the domain like pv.linear.scale does... What happens when you try that approach and attempt to zoom with the mousewheel?

Please update your question with a simple reproducible example like Shane asked for as well; if you can do that I'd be happy to take a look.",2010-10-27T19:26:32.613,1730,CC BY-SA 2.5,
5822,4026,1,"@Yuan I don't know, because simple ""MLE"" can mean different things... Try reading ?naivebayes, it is quite detail description what is what in table, maybe you could find something familiar there. Or try to specify what you need in more detailed way -- what is your desired use, for what you need it?",2010-10-27T20:06:18.430,,CC BY-SA 2.5,user88
5823,4020,0,"mbq, I'm not sure if I was clear enough =| I do not want to nail this weight",2010-10-27T20:07:09.530,1725,CC BY-SA 2.5,
5824,4020,0,@bijey It is only that you will get a stronger result showing that A>B in unpaired test.,2010-10-27T20:12:31.387,,CC BY-SA 2.5,user88
5825,4020,0,"...I do understand that it's better to show that A>B in unpaired test. But I'm afraid, it is very unlikely to show since (informaly) [quality(A) - quality(B)] << variance(quality(A))",2010-10-27T20:17:24.543,1725,CC BY-SA 2.5,
5826,3955,1,I'm still failing to understand why having one estimate with a small error makes an approach 'unusable'.,2010-10-27T20:43:00.453,449,CC BY-SA 2.5,
5827,3993,1,"@Brett I made some edits; I hope it helps clarifying my initial thoughts, but let me know. I could also remove any reference to CV wrt. internal/external validity if this happens to be unclear, although I think you were more concerned with CV and external validity (i.e. generalizing on new samples) while I was envisioning CV as part of model validation (hence, on the working sample).",2010-10-27T20:44:16.027,930,CC BY-SA 2.5,
5828,3909,0,(+1) My apologies. It seems I also forgot to upvote your very nice answer. Already voted up your helpful comments.,2010-10-27T20:56:04.923,930,CC BY-SA 2.5,
5829,4029,0,"Cannot upvote more, sorry. It seems I hadn't increase the wksp enough :)",2010-10-27T20:59:27.243,930,CC BY-SA 2.5,
5831,4024,1,"@Membran Aggregation doesn't have to be meaningful: you're free to combine bins any way you please.  A subtle problem, though, is that *post-facto* aggregation casts the p-values in doubt; the aggregation ought to be independent of the data.",2010-10-27T21:16:24.403,919,CC BY-SA 2.5,
5832,4020,1,"I thought so; then stick to paired test, yet don't hide this fact. This is because in a sense you can say that they are equivalent -- B can be better than A when started with luckily picked seed.",2010-10-27T22:02:35.957,,CC BY-SA 2.5,user88
5834,4029,0,"Isn't it that Fisher's ""exact"" test actually addresses slightly different question: ""...it is used to examine the significance of the association (contingency) between the two kinds of classification"" (wiki page). In my case I sought to confirm (or refute) the hypothesis that distributions of phenotypes between 2 groups are similar (equal). When I found that online test (see the first post) named ""Chi-square test for equality of distributions"" I thought it was precisely for my problem...",2010-10-27T22:38:24.300,1727,CC BY-SA 2.5,
5835,2909,0,That would require about a million simulated random walks...,2010-10-27T22:40:44.953,919,CC BY-SA 2.5,
5836,4029,0,"Also, if you think that mentioned version of Fisher's test is fine for comparing two distributions, can it also be used for checking uniformity of distribution (i.e. to say that phenotypes within one group were distributed non-uniformly between a finite number of possible phenotypes)? One can do this even in Excel using CHITEST function, but what if I have a distribution similar to the ones above, with lots of phenotypes observed less than 5 times?",2010-10-27T22:42:57.687,1727,CC BY-SA 2.5,
5838,4010,0,"The measurements may be many time series, then we can derive the correlation between each two of them.",2010-10-28T02:56:43.877,,CC BY-SA 2.5,Jason Xin Liu
5840,4025,1,"At the very least, you should provide the link to the other posting.",2010-10-28T03:20:28.063,183,CC BY-SA 2.5,
5842,4029,0,"@Membran # 1: It is a *slightly* different question as Fisher's exact test conditions on both sets of marginal totals. This seems something of an academic statistical nicety to me though, and I'm a statistician in academia. (BTW could you clarify to *which* wiki you refer?)



@Membran #2: I would not call the conditional exact test ""Fisher's exact test"" in the case of a one-way table, but such a test should be possible.and  I would have thought more straightforward for one-way tables, but I can't currently find software to assist and I don't have time to perform the calculation without.",2010-10-28T04:07:33.330,449,CC BY-SA 2.5,
5843,4011,0,"There is no change in the genes, they are born with them. Therefore entering the registry does not increase risk. I don't expect a change in behavior but it is possible. There care does not change it is just tracked in the registry.",2010-10-28T04:16:02.493,1189,CC BY-SA 2.5,
5846,3992,0,"Ok, I installed the software, I used the scan to read my data: mydat<-scan(""C:\\mydat.txt"") then Qn(mydat) but an error message appears telling me: Qn function is not found or for scaleTau2(mydat) scale2Tau2 function was not found. I guess I need to upload a package first, but which one?",2010-10-28T07:16:56.320,1637,CC BY-SA 2.5,
5848,3723,1,"The uncertainties package does apply error propagation theory (it was created just for that), *and* can take covariances as an input (http://packages.python.org/uncertainties/user_guide.html#correlated-variables). :)",2010-10-28T08:03:33.787,1735,CC BY-SA 2.5,
5849,3715,1,"@cool-RR: Just a small technical remark, to make things precise: the factory *function* `ufloat()` creates a `UFloat` *object*.  Case does matter, here (the two names were chosen to be close to each other on purpose).  :)",2010-10-28T08:18:53.523,1735,CC BY-SA 2.5,
5850,3874,0,"Before I update the answer, is treatment a repeated factor? i.e., does each subject receive both treatment ""a"" and treatment ""b"" or is this a between-subjects factor?",2010-10-28T08:46:23.030,966,CC BY-SA 2.5,
5851,4041,1,"(+1) Nice `plyr` package, isn't it? :)",2010-10-28T08:47:11.533,930,CC BY-SA 2.5,
5852,3874,0,Treatment is a between-subjects factor. Each subject only receive 1 kind of treatment. I've coded the two treatments as 1 and 0 and set treatment as a factor variable.,2010-10-28T09:08:00.190,1663,CC BY-SA 2.5,
5853,3955,0,"Thanks for the comment. I'm not saying the aproach (trend calculation) was unusable, but its error calculation could have a potential problem. Given that I got no answer so far, I'll remove that item from the question hoping it gets easier to answer.",2010-10-28T10:05:16.033,1694,CC BY-SA 2.5,
5859,4041,0,"This works great. Thanks for pointing out the plyr package! Could you please explain the "".(group)"" syntax?",2010-10-28T14:31:51.303,439,CC BY-SA 2.5,
5860,4042,1,"Nice, thanks! I've been experimenting with `by`, but couldn't figure out how to transform the result into a data frame.",2010-10-28T14:38:13.680,439,CC BY-SA 2.5,
5861,4045,0,"Thank you dirk. I suspected `zoo` would have something on that line, I'll have a look and see if it fits my needs. I assume it would work also for ""relative"" times? Each of my time series arbitrarily starts at 0s and ends at ~3500s, so I don't have precise dates.",2010-10-28T14:42:29.247,582,CC BY-SA 2.5,
5863,4045,0,I think so for as long as you can compute some indexing -- something the to-be-averaged items have in common.,2010-10-28T14:55:50.613,334,CC BY-SA 2.5,
5865,3056,0,"+1 That's a pretty direct answer, considering these functions implement the formulas in the paper!",2010-10-28T15:43:07.510,919,CC BY-SA 2.5,
5866,4013,0,Not because it answers your question but because they are connected: http://stats.stackexchange.com/questions/665/whats-the-difference-between-probability-and-statistics/675#675 (I really like the answer of Mark/Peter)  and http://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability,2010-10-28T17:13:07.430,223,CC BY-SA 2.5,
5869,4054,0,"@Srikant Could you explain how you derive the ""trivial answer""?  (There must be some unstated assumptions behind it.)  And to what theorem do you refer when you conclude that ""eventually everyone will be connected""?  That's not at all obvious!",2010-10-28T20:26:45.380,919,CC BY-SA 2.5,
5870,4054,0,"@whuber I am assuming that the network size is fixed. The question states: A person picks another person at random to make a connection and presumably this is an ongoing process. Thus, as time goes to infinity everyone should be connected. No theorem, just intuition. Perhaps, I am using imprecise language.",2010-10-28T20:29:30.320,,CC BY-SA 2.5,user28
5871,4055,0,"Note that I do not interpret the question to state that everyone chooses *exactly one* other person to connect to--that would lead to a sterile theory because the number of connections would be determined, not random.  Instead I have interpreted it to state that everyone when they enter the network chooses connections randomly among the n others, winding up with anywhere from 0 through n connections total.  The assumption on the variances is assured when there is a limit on the number of connections any newcomer will make *and* that number has some ""minimal"" randomness.",2010-10-28T20:30:02.363,919,CC BY-SA 2.5,
5872,4053,1,"Clarification: Is the question about the ""total number of connections for the whole group"" or ""the total number of connections for one person""? My answer implicitly assumes the latter.",2010-10-28T20:35:26.960,,CC BY-SA 2.5,user28
5873,4054,0,"@Srikant I am still confused, because after a long time, ""Prob(No of connections=n)"" equals 1 when n = 3 and otherwise is always zero.  After all, when ""everyone should be connected"" the number of connections equals n(n-1)/2.  I suspect you may have several different random processes in mind at the same time.  It could help to disclose the assumptions you are making and be a little more precise.",2010-10-28T20:43:52.877,919,CC BY-SA 2.5,
5874,3056,0,"It seems like this package computes the _expected_ max drawdown based on the paper, but does not compute the CDF. The paper gives 'shortcut' results, IIRC, to compute that expectation.",2010-10-28T22:39:41.790,795,CC BY-SA 2.5,
5875,4053,1,*Riley* distribution? That's a new one on me. Do you have a reference or link?,2010-10-28T22:48:45.217,449,CC BY-SA 2.5,
5876,4055,0,I'm a bit confused about $X_{i}$ and the variance. Does this suggest people have an intrinsic variance?,2010-10-28T23:56:26.017,1036,CC BY-SA 2.5,
5877,4011,0,sorry for addressing the wrong person. I understand that the gene doesn't change in their life time my English is failing me here I thought you used that  term as gene mutation. But regardless of that thanks for clarifying that you think that being in the registry doesn't change their risk.,2010-10-29T00:10:45.597,10229,CC BY-SA 2.5,
5878,4053,3,"""Rayleigh"" maybe?",2010-10-29T01:11:44.343,919,CC BY-SA 2.5,
5879,4055,1,"@Andy Not people: the number of connections made.  The important thing is that there should be a good chance that the number of connections made by individuals actually varies and doesn't settle down to a constant.  When that happens, the limiting distribution (of the number of connections) is determined by the finite number of initial connections that do vary, so it's not possible to approach a Normal distribution asymptotically.",2010-10-29T01:14:14.667,919,CC BY-SA 2.5,
5880,3056,0,"@shabbychef Sorry, I missed that nicety.  I see how obtaining the entire CDF can be more useful than just knowing the expectation. (Financial risk is much more than just expected losses...)  But now I feel a little bit better about the work I did to approximate the CDF!",2010-10-29T01:17:31.153,919,CC BY-SA 2.5,
5881,4033,1,"this is really great, and should go a long way towards the CDF. Above and Beyond badge material.",2010-10-29T02:21:25.783,795,CC BY-SA 2.5,
5882,4015,0,Thank you Henrik - the distinction between the definitions (and your thoughts about them) was helpful.,2010-10-29T06:48:33.720,253,CC BY-SA 2.5,
5883,4050,1,"Thank you C.p, you make an interesting point.  Although, from the perspective of Henrik's answer above, you are still in the realm of inductive, since the statistical reasoning you describe is one that involves uncertainty.",2010-10-29T06:51:03.607,253,CC BY-SA 2.5,
5884,4041,2,"aix - sure.  It means ""split the data by the variable between .(), and on each subset perform the function"".  In order to have it include more variables, you should simply use this syntax:  .(var1, var2, var3) .  Which is like cutting your data by each combination of levels of var1, var2 and var3.  And on each cut to perform your function.  This package is maintained by Hadley (also th author of ggplot2), so I trust it will keep developing.",2010-10-29T06:54:01.873,253,CC BY-SA 2.5,
5885,4041,2,"Oh, and BTW, you could also use plyr with a parallel computing on several cores (almost automatically), see: http://www.r-statistics.com/2010/09/using-the-plyr-1-2-package-parallel-processing-backend-with-windows/",2010-10-29T06:55:37.453,253,CC BY-SA 2.5,
5886,4041,0,@chl - indeed :),2010-10-29T06:57:21.570,253,CC BY-SA 2.5,
5887,3989,1,"OK, the answer for the guys who will read this later: if you just want to calculate a robust scale estimator for a piece of data 1-install latest version of R 2-install the robustbase package 3-ready to go! but if you are developing a code outside this environment you need to use weighted high medians to minimize required calculations for Sn or Qn.",2010-10-29T07:28:11.610,1637,CC BY-SA 2.5,
5888,3992,0,"thanks kwak, after all i could do test my code with R results. :)",2010-10-29T07:32:38.830,1637,CC BY-SA 2.5,
5889,2998,1,"Hi Jeromy.  Q: what does it mean to take the dist of abs(cor(...)) ?  Isn't a cor matrix is one where a larger number indicate more of a relation, while in a distance matrix the opposite is true?  (I suspect I am missing something, could you please explain?)",2010-10-29T07:47:46.923,253,CC BY-SA 2.5,
5893,4064,0,"(+1) I looked for this one, but cannot found it in the `ape` package!",2010-10-29T09:56:03.663,930,CC BY-SA 2.5,
5894,4062,1,What is the advantage of a polar representation (aside from saving space)? It looks to me like it is trickier to look at.,2010-10-29T09:56:08.057,582,CC BY-SA 2.5,
5895,4062,1,@nico It is more cool (-;,2010-10-29T10:08:04.147,,CC BY-SA 2.5,user88
5896,4062,1,It is also useful when you don't have one stem...,2010-10-29T10:29:09.613,253,CC BY-SA 2.5,
5897,4064,0,Bingo.  That's what I was looking for.  I wonder if there is something similar in ggplot2...,2010-10-29T10:33:18.453,253,CC BY-SA 2.5,
5898,2998,0,"@Tal My understanding is that the dist function takes the euclidean distance of the matrix passed to it, which in this case was an absolute correlation matrix. Perhaps, a more literal conversion from the correlation matrix would be the as.dist() function which takes an existing distance matrix: e.g., as.dist(1-abs(cor(na.omit(x)))). see ?dist",2010-10-29T11:33:17.520,183,CC BY-SA 2.5,
5899,2998,2,using a 1-abs(cor...) would make more sense to me :),2010-10-29T12:06:56.137,253,CC BY-SA 2.5,
5900,3992,0,"@K-1:> oh, i was going to respond...then i somehow forgot. Sorry about that. So it worked ?",2010-10-29T12:10:56.723,603,CC BY-SA 2.5,
5901,4063,0,Good point. I've only come across the converse: assuming an effect is linear when it isn't can lead to spurious statistical evidence for multiplicative interaction terms.,2010-10-29T12:23:44.897,449,CC BY-SA 2.5,
5902,4045,1,"Dirk, unfortunately aggregate did not solve my problem, as I want to resample at essentially the same number of points of the original trace. However I found the `reglin` function in `pastecs` that does exactly what I need! http://rgm2.lab.nig.ac.jp/RGM2/R_man-2.9.0/library/pastecs/man/reglin.html",2010-10-29T12:30:08.070,582,CC BY-SA 2.5,
5903,3205,0,"(+1) because it helped me a lot to find what I actually want to know. Thank you ! Maybe this question can be marked as community wiki, because I dont think there is a definite answer.",2010-10-29T12:43:51.130,264,CC BY-SA 2.5,
5904,229,4,"@drknexus Your intuition is correct.  However, extremes (and near-extremes) of sampling distributions are somewhat special: their values are constrained by the mass of data on one side of them, whereas--for parent distributions with infinite tails--there is no constraint at all on their values on the other side.  Thus, for instance, the distribution of a maximum (from a distribution with no upper bound) is positively skewed.  This *increases* its expectation relative to the corresponding percentile.",2010-10-29T13:32:30.693,919,CC BY-SA 2.5,
5907,3997,2,"Your code doesn't work because the components of a POSIXlt object are numeric/integer, not character.  But as Shane said, you really should be using a time-series class...",2010-10-29T15:29:53.203,1657,CC BY-SA 2.5,
5909,4067,0,Elegant solution Joshue.  Do you think there are cases when one solution is better then another?,2010-10-29T16:05:36.667,253,CC BY-SA 2.5,
5910,4067,2,"I think it's a matter of preference.  My example is essentially what `plyr` does but it gives you finer control, though it's not nearly as clean.  My opinion would change if one solution had a better time/memory profile.  I haven't compared them though.",2010-10-29T16:08:37.660,1657,CC BY-SA 2.5,
5912,4069,0,"aha, I have not worked with R enough to spot the `NA` vs `NaN` distinction. Nice that it has both...",2010-10-29T18:22:36.100,795,CC BY-SA 2.5,
5913,4069,0,"Of course a single number has a variance!  It's the mean square deviation from its mean, namely zero.  Perhaps you meant to say a dataset with one number cannot have a *sample* variance?",2010-10-29T18:23:55.827,919,CC BY-SA 2.5,
5914,4068,0,Did you mean to ask about the population variance definition or about the *sample* variance definition?  There's no issue with the former.,2010-10-29T18:27:17.260,919,CC BY-SA 2.5,
5915,4070,3,"Actually, a single observation of a random variable often provides information about that variable's variance.  It depends on the possibilities you allow.  For instance, a single observation from a Uniform, Poisson, or Exponential distribution lets you estimate its single parameter, which gives you an estimate of the variance.",2010-10-29T18:34:01.917,919,CC BY-SA 2.5,
5916,4068,0,@whuber I think what Matlab is doing is the following. When it encounters a scalar it reports the population variance by default. When it encounters a vector it reports the sample variance by default unless you ask it to report population variance. In either context they use same function: `var()`.,2010-10-29T18:39:11.193,,CC BY-SA 2.5,user28
5918,4068,0,"@Srikant That may be, but it does not address my request for clarification.  I suspect there may be a typographical error in the last line.  The var() function appears to be returning the correct values of population variance but not the correct values for *sample* variance.",2010-10-29T18:47:47.610,919,CC BY-SA 2.5,
5919,4071,0,"You have restated the problem without offering a resolution.  The concern is that this convention can lead to wrong answers when the variance estimate is used later.  It certainly delays the stage at which a failure is recognized.  (Think about what happens if you try to use this ""variance"" in a t-test, for instance.)",2010-10-29T18:50:13.783,919,CC BY-SA 2.5,
5920,4068,0,"@whuber Actually, `var(randn(1),0)` weights the variance by 0 and hence the output is 0. The second element is used to do a weighted average if it is different from 1. See: [Mathworks help for `var`](http://www.mathworks.nl/help/techdoc/ref/var.html)",2010-10-29T18:52:11.457,,CC BY-SA 2.5,user28
5921,4071,0,"@whuber Not exactly. The OP is thinking that `var` returns the sample variance for a scalar whereas it returns the 'population variance'. Whether it is a problem or not depends on what type of error handling routines they have internally. However, I agree that their convention is susceptible to errors.",2010-10-29T18:57:37.223,,CC BY-SA 2.5,user28
5922,4068,0,"@srikant, the 'weight' is a red herring: mathWorks wanted to add weighted variance without boogering the API, so they added this odd option. At least, that is how I see it.",2010-10-29T19:07:06.697,795,CC BY-SA 2.5,
5923,4071,0,"@Srikant In order to determine who is more confused, you or I, I have learned that we both are!  Wikipedia terms the formula with 1/(n-1) the ""sample variance"" (at http://en.wikipedia.org/wiki/Variance ) whereas Mathworld clearly uses 1/n at http://mathworld.wolfram.com/SampleVariance.html .  However, the OP uses ""sample variance"" in the first sense when remarking that ""the sample variance is not dividing by 0=n‚àí1"".  In this sense *there is no such thing* as ""sample variance"" for a dataset of one value (a ""scalar""), so your characterization of what the OP is ""thinking"" looks incorrect.",2010-10-29T19:10:53.510,919,CC BY-SA 2.5,
5924,4068,0,"@Srikant Excellent!  What you have found is that the OP has mischaracterized the meaning of the second parameter.  *That* is a fine resolution of the question.  Why don't you point that out in your answer?  (Regardless, I think Matlab is using a terrible convention, even though they clearly document it.)",2010-10-29T19:14:07.200,919,CC BY-SA 2.5,
5925,4068,2,"@shabbychef: The page Srikant has referenced clearly documents the behavior you have noted.  In particular, it does not maintain that var() returns the 1/(n-1) version of the variance when n=1.  So the issue is not that Matlab is *wrong*, per se.  (From a software design point of view this implementation of var() is terrible because it tries to do too much in a single function and consequently risks misunderstanding and erroneous results by unvigilant users.)",2010-10-29T19:17:27.917,919,CC BY-SA 2.5,
5926,4073,0,Can anyone figure out why link #2 is not working correctly?  Maybe there's a problem with using square brackets elsewhere in the text?,2010-10-29T19:46:02.623,919,CC BY-SA 2.5,
5927,4073,0,Looks like my edit overlapped with shabbychef. Hope we did not break anything.,2010-10-29T19:58:35.743,,CC BY-SA 2.5,user28
5928,4073,0,there was a space in the URL. fixed.,2010-10-29T19:58:41.797,795,CC BY-SA 2.5,
5929,4073,0,"Thanks guys.  You are not only smart and sharp-eyed but helpful, too.",2010-10-29T20:01:53.320,919,CC BY-SA 2.5,
5930,4072,1,"Fred, a couple of questions: (1) when you say 'commitment' here, is that meaning 'the trait of sincere and steadfast fixity of purpose' or 'the official act of consigning a person to confinement'? (2) If the latter, are you really saying people will be confined or not based on the predictions from your model? If so I think I might wish to seek legal advice before posting an answer!

By the way, you might like to take a quick look at the latter paragraphs of the section 'How do I ask questions here?' in the site FAQ:http://stats.stackexchange.com/faq",2010-10-29T21:28:56.190,449,CC BY-SA 2.5,
5931,4056,1,@suncoolsu It's a shame we cannot vote up the update...,2010-10-29T21:47:18.193,930,CC BY-SA 2.5,
5932,3987,1,"@Michael About ""fake"" interim analyses (looking at p-values while study is still in an evolving stage): it looks like it is a misappropriate use of statistics, no more.",2010-10-29T22:24:19.280,930,CC BY-SA 2.5,
5933,3992,0,"yes, it worked. It's pretty faster than my code in mathematica. Now i should look for a way to connect mathematica and R kernels together. thanks for the helpful comments.",2010-10-30T00:42:18.697,1637,CC BY-SA 2.5,
5936,4039,1,+1 thanks for pointing out how to do this with the uncertainties package.,2010-10-30T02:48:40.293,251,CC BY-SA 2.5,
5939,3912,0,"+1.  This reminds me that I've been using your scripts to learn asymptote by jumping in and changing stuff around, trying different things.  Thanks again for that, *very* helpful to get started.",2010-10-30T03:07:47.273,251,CC BY-SA 2.5,
5940,4068,0,"Yes, the behaviour is documented in the help for `var`, but the API for this function is annoying at best, as you point out. My question is _why_ do they do this? You simply cannot ask the Matlab `var` command to 'weight by zero'/give sample variance for  a scalar, it refuses to answer the question, pretending the user has asked a different question. What is the sensible answer?",2010-10-30T03:10:10.533,795,CC BY-SA 2.5,
5941,2998,0,"@Tal Yes. I agree. I did a quick look. In this case the vector of distances (euclidean of abs cor with 1-abs cor) correlate around .96, so it doesn't make much difference.",2010-10-30T05:13:12.943,183,CC BY-SA 2.5,
5942,4056,2,"@chl .. I agree, I would have would like to give (+2) :-), actually I never thought so deeply about this question until this update.",2010-10-30T07:38:38.533,1307,CC BY-SA 2.5,
5943,4062,3,"@mbq: you missed a ""good"" pun there... you could have said ""it's more *fan*"" :)",2010-10-30T08:22:48.633,582,CC BY-SA 2.5,
5944,3912,0,"@ars Actually, I seem to remember that this picture was made with PStricks. Anyway, a good starting point for Asymptote is http://www.piprime.fr/asymptote/.",2010-10-30T08:59:36.117,930,CC BY-SA 2.5,
5945,4069,0,"You'll notice that I said it can't have a *population variance*, not that it cannot have a variance.  Clearly it has a sample variance, which is indeed trivially zero.",2010-10-30T09:07:41.183,1739,CC BY-SA 2.5,
5946,319,11,"I agree wholeheartedly about the use of simulation for explaining this.  But just a small note on the example at the end: I find that people (not just students) do find it difficult to distinguish for any particular distributional assumption, e.g. the poisson, between being *marginally* poisson distributed and being *conditionally* poisson distributed.  Since only the latter matters for a regression model, a bunch of dependent variable values that aren't poisson need not necessarily be any cause for concern.",2010-10-30T09:19:39.623,1739,CC BY-SA 2.5,
5947,2998,0,"wait a sec, are we talking about building a dist on the cor matrix?  dist(1-abs(cor(...)))?  Or about looking at the cor matrix AS a dist matrix? as.dist(1-abs(cor(...)))   The second option makes sense to me.  The first one, I am not sure what it means - if you do, I'd be glad to know :)",2010-10-30T09:44:15.177,253,CC BY-SA 2.5,
5948,4077,0,"Guys,thanks for the answer. I have a data to analyse. How do i attach data to this. Lots to ask you people..Thanks in anticipation. Expecting a prompt reply.",2010-10-30T10:18:06.417,1763,CC BY-SA 2.5,
5949,4057,1,"Personally, just to be sure, I'd polish off an initial approximation derived from CF with Newton-Raphson. Even then, based on some experiments I've done, I'm not convinced of the virtue of carrying more than three or so terms of an expansion.",2010-10-30T10:18:49.237,830,CC BY-SA 2.5,
5950,4077,4,"Huh? You said in the question you'd already computed the t-statistic, and chl has provided sample R code. What more do you want? By the way, I'm not sure you have any right to expect or request a prompt reply; we don't get paid for this you know.",2010-10-30T10:45:04.057,449,CC BY-SA 2.5,
5951,3916,1,"Thanks for this! I'm reading your code and it's helping me figure out the R syntax a lot.  One thing that's giving me trouble is the line `freq.resp <- raw.resp/apply(raw.resp, 1, sum, na.rm=T)`.  The error is that ""dim(X) must have positive length"" while dim(raw.resp) is NULL.  Could it be that since my data doesn't have all options with positive frequencies, my tables aren't all the same length?  How can I fill in the zeroes in my `table` invocation?",2010-10-30T12:03:28.360,1681,CC BY-SA 2.5,
5952,3916,0,"@Matthew Yes, it is very likely that this is indeed the source of the problem (well, you're learning R quickly, I appreciate). So, if one response category is not observed for one or more items, then the length of the table will be < 4, and `freq.resp` will throw an error. I will fix the code (by tomorrow), this will also show you how to concatenate irregular tables with little code.",2010-10-30T12:47:56.597,930,CC BY-SA 2.5,
5953,4064,0,"@Tal No official support for tree structures in ggplot2. Look at this Google group thread, http://j.mp/c85l5l (but it's definitively not circular).",2010-10-30T12:52:51.447,930,CC BY-SA 2.5,
5954,4077,1,"@ayush For your preceding question, I provide a complete answer to your question (IMHO) -- then I gave some follow-up to your comments before stopping when I thought you were asking for another question which is not the purpose of comment option here. So, I would suggest that either you clearly state if your question relates to theoretical consideration or applied data analysis (in the latter case, give us a reproducible example) or separate your questions. BTW, you still have the option to accept answers that you find useful (again, wrt. your original question, not the comments that follow).",2010-10-30T13:01:11.667,930,CC BY-SA 2.5,
5957,4050,0,"Please see the (hopefully understandable) update to my answer, where I try to address the issue brought up here.",2010-10-30T14:17:08.973,442,CC BY-SA 2.5,
5958,4050,0,"@Henrik  That is clearer (to me at least).  Just a little niggle:  It's not quite the case that ""the same reasoning applies for probabilities in classical p-value and Bayesian statistics"".  The latter *will* give you single event probabilities, e.g. the probability that the true mean is between some value and some other value (although your other caveats all apply) while 'classical' frequentist methods such as confidence intervals will not even do that, despite the fond and widespread hope that they do.  Their interpretation is indeed as you describe it.",2010-10-30T14:29:56.523,1739,CC BY-SA 2.5,
5959,4015,0,"Your update was clear and to the point.  If I could have given you another (+1), I would.",2010-10-30T14:44:53.323,253,CC BY-SA 2.5,
5960,4078,1,+1 for the creative (and insightful) interpretation and explaining the distinction.,2010-10-30T15:11:23.307,919,CC BY-SA 2.5,
5961,4072,0,"@onestop, there actually is a venue through which a person can be ""civilly committed"" based on future dangerousness to themselves or others. Here's a website that has a good description of what civil commitment is, http://www.oregoncounseling.org/LawsRights/CivilCommit.htm . It also is often used against sexual offenders, see http://www.nytimes.com/2010/05/18/us/politics/18offenders.html . Although I'm not familiar with any ""legal"" standards determined through predictive modeling. See my comment to Fred directly.",2010-10-30T15:17:38.187,1036,CC BY-SA 2.5,
5962,4072,0,"@Fred, although you've provided a description well enough to be able to answer your question, given onestop's and mine question over what exactly you are modelling I think it would be nice if you could clarify your design a bit more precisely. In your question do you really mean ""legal standards"" of commitment are determined by some type of logistic model (and if you do I'd like to know what State (or State entity) you are referring to? Or is this model simply evidence of whether someone should be committed.",2010-10-30T15:23:16.963,1036,CC BY-SA 2.5,
5963,4072,0,"@Fred, Also could you clarify if you mean civilly committed? I am aware of no circumstances where a legal statute has a calculated measure of risk in guiding the laws decision. Although it could be used as evidence in a parole hearing it wouldn't be used in any standardized way that I am aware.",2010-10-30T15:25:43.013,1036,CC BY-SA 2.5,
5964,4077,2,"@ayush Ah, and I just realize that you **never** vote up any of the answers that were provided to you (though you have enough rep now).",2010-10-30T15:31:02.927,930,CC BY-SA 2.5,
5966,4072,0,@Andy W #1: Seems the term 'civil commitment' is a rather wonderful example of our two nations' separation by a common language! e.g. http://www.guthriecastle.com/terms-and-conditions/,2010-10-30T17:18:12.633,449,CC BY-SA 2.5,
5967,4072,0,"@onestop, yes I am speaking entirely in the context of USA law. I hope marriage is not considered so undesirable that one refers to it as recidivism!",2010-10-30T17:45:44.783,1036,CC BY-SA 2.5,
5968,319,1,I have to confess that I didn't know that.  I've really appreciated your comments around this site over the past few days of your membership - I hope you'll stick around.,2010-10-30T18:29:32.153,71,CC BY-SA 2.5,
5970,4079,9,"+1 for bringing out the key ideas: (1) we can guarantee the means will differ when the datasets are this large and (2) some other analysis is likely to be more appropriate and useful.  But because we don't know about the purpose of the analysis, we should be cautious about making specific recommendations.",2010-10-30T20:45:54.120,919,CC BY-SA 2.5,
5971,4069,1,"I have come to realize that our conversation derives from two different understandings of ""population"" and ""sample"" variance. The terms are confused on the Internet, so I think we're both blameless and that we may be in agreement. I was attempting to use the convention adopted by the OP, whose terminology indicates the ""sample"" variance divides by n-1 while the ""population"" variance divides by n.  Incidentally, your comment seems (correctly) to allow for *three* variances: that of a random variable, the unbiased estimator thereof, and that of a set of independent realizations of it.",2010-10-30T20:55:35.840,919,CC BY-SA 2.5,
5972,1171,0,May I ask what your intention is in offering a bounty on someone else's question when you have already provided a good answer?,2010-10-30T21:02:13.290,919,CC BY-SA 2.5,
5974,4070,0,"@whuber thanks for the pointers. Actually, I was thinking more on the lines of sample variance. What I meant to say was, estimating sample variance may not be appropriate with one sample. It stands corrected now.",2010-10-30T22:30:04.667,1307,CC BY-SA 2.5,
5975,4064,0,"Hello chl, thank you for the link.  I'll respond there with reference to this code as well...",2010-10-30T23:16:08.900,253,CC BY-SA 2.5,
5976,3987,1,"@Chi On one level, yes, undeclared and uncorrected interim analyses are inappropriate (but it is done in ignorance, an ignorance that I believe points to inadequacies in the methods of teaching statistics to basic biomedical researchers...). However, if we consider it at a meta level, then it is possible to find some partial justifications. Many experiments involve such small samples that an increased false positive error rate may be a reasonable tradeoff for more power. Convention precludes a declared level of alpha higher than 0.05.",2010-10-30T23:25:03.780,1679,CC BY-SA 2.5,
5977,3987,0,"I note in this context that basic biomedica researchers do not work in an exclusively Neyman-Pearson approach, even if statements that ""results where P<0.05 were considered significant"" might suggest otherwise. If we stay within the confines of Fisher's significance testing wherein considerations other than the achieved P value can be incorporated into decisions of how to deal with the test results, perhaps interim analyses might not be so bad. However, it is certain that a designed sequential test would be superior to an undesigned one.",2010-10-30T23:29:49.427,1679,CC BY-SA 2.5,
5978,2998,1,@Tal the dist option: each variable has a vector of absolute correlations. Variables with more similar vectors would have smaller euclidean distances: http://en.wikipedia.org/wiki/Euclidean_distance,2010-10-31T02:21:04.380,183,CC BY-SA 2.5,
5979,3916,0,@Matthew I've added a fix. Should work with dichotomous items too. Let me know if this is now ok. http://gist.github.com/642219,2010-10-31T09:04:18.033,930,CC BY-SA 2.5,
5980,1171,0,"I was taking an experimental approach to a question i've just discovered is answered at http://meta.stackexchange.com/questions/16065/how-does-the-bounty-system-work, namely 'Can I award a bounty to my own answer?'",2010-10-31T09:06:11.303,449,CC BY-SA 2.5,
5981,4082,0,"(+1) A good paper, thank you. I also found this article interesting, Conditions for factor (in)determinacy in factor analysis (Krijnen et al.), http://j.mp/dwo7c8.",2010-10-31T09:39:04.550,930,CC BY-SA 2.5,
5983,3987,0,"@Michael Yes, you made a good point. This reminds me of Rothman arguing against a strict correction for multiple testing at the expense of Power considerations. However, planned *vs.* unplanned comparisons, as well as pitfalls associated to what I called ""fake"" interim analysis, should be part of every courses on DoE or RCTs.",2010-10-31T10:39:57.157,930,CC BY-SA 2.5,
5984,3992,0,"@K-1:> if you look at the code/formula of the Tau estimator (type ""scale2tau2"" in R), it's basically just a weighted mean (with weights set by the median/MAD  and cut of to 0 below a threshold). This should run fast on anything.",2010-10-31T11:38:45.060,603,CC BY-SA 2.5,
5985,3946,0,"Thanks chl for the answer, I accepted it for the sheer scope of it.  Best, Tal",2010-10-31T13:34:32.593,253,CC BY-SA 2.5,
5987,3945,1,"I think the thanks are due to Roger Newson, as I'm just quoting from his article.",2010-10-31T15:52:25.477,449,CC BY-SA 2.5,
5988,4011,0,"@Vincent, even if their care does not change, becoming part of a register will probably lead to changes in how they represent their health to themselves, and thus cause changes in their behaviour /end(somewhat irrelevant psychology tangent}",2010-10-31T15:56:31.287,656,CC BY-SA 2.5,
5989,1171,0,".. but i forgot i'd have to wait 24 hours before i could award the bounty, during which time the question would be 'featured'. Sorry about that! Now i've learnt of the existence of the Community FAQ for the original trinity (or triumvirate, or whatever it's called) of SE sites i won't need to experiment again. http://meta.stackexchange.com/questions/7931/",2010-10-31T16:02:04.720,449,CC BY-SA 2.5,
5990,4086,2,"Are you sure you want to limit yourself so severely?  If this problem has any practical application, then by the time you know the count at time x you also know all previous counts.  Why not use them to help with the prediction?",2010-10-31T16:21:39.610,919,CC BY-SA 2.5,
5991,4086,0,Indeed. You're correct. Thanks for pointing that out.,2010-10-31T16:30:38.363,485,CC BY-SA 2.5,
5992,2629,7,"Just came across this post from A Gelman, *How does statistical analysis differ when analyzing the entire population rather than a sample?*, http://j.mp/cZ1WSI. A good starting point about diverging opinions on the concept of a ""super-population"".",2010-10-31T17:20:06.970,930,CC BY-SA 2.5,
5993,2629,2,@chl: interesting -- reminds me that Gelman had a discussion of finite/super population inference being comparable to fixed-/random-effects in his paper on ANOVA [ http://www.stat.columbia.edu/~gelman/research/published/econanova3.pdf ].,2010-10-31T17:53:26.807,251,CC BY-SA 2.5,
5994,4074,0,"Thanks for the comments and answer. First on the comments: I do mean civil commitment (ie, confinement).  I am not proposing a model to be used, but rather using a model to illustrate shortcomings in current practice.  The statistics are generally used as evidence, not alone determinative.  Some jurisdictions do define sufficient future dangerousness quantitatively (eg, more likely than not).",2010-10-31T18:12:41.967,,CC BY-SA 2.5,Fred Vars
5995,4074,0,"Based on the answer and my own research, I've written new code that generates plausible results. With a little trepidation (I read the FAQs and still think the question is appropriate, but my apologies if I'm missing something), here it is (feedback welcome) (0.03... is the constant from the logit model): .predict sr2, xb
.gen byte sr2C1R1=0 if sr2!=.
.gen sr2w = (0.0310417 + sr2 - 0.67448975*stdsr1)
.replace sr2C1R1=1 if 1/(1 + exp(-sr2w)) > .75 & sr2!=. & sexrecS!=.
.sum sr2C1R1 if sr2!=. & sexrecS!=.",2010-10-31T18:20:39.413,,CC BY-SA 2.5,Fred Vars
5996,3877,0,"Thanks again for the helpful comments.  @chl The model has several independent variables.  Does that raise problems for my proposed method of calculating the Cohen's d on the difference in mean values of the logit predictions for ""hits"" and ""misses""?",2010-10-31T18:30:15.720,,CC BY-SA 2.5,Fred Vars
5998,4070,0,@whuber But this is not the case here; this function is documented to return variance/(n-1) and is returning 0 for 0/0 symbol.,2010-10-31T19:51:27.377,,CC BY-SA 2.5,user88
5999,4071,0,"+1 As I know MATLAB, I think your hypothesis is (sadly) true.",2010-10-31T19:53:59.873,,CC BY-SA 2.5,user88
6000,4070,0,"@mbq You missed the disclaimer in the documentation: ""For N=1, V is normalized by N.""  That's an elliptical way of saying ""For N=1, a value of 0 is returned.""  The point I was making with my comment actually is about something else: it addresses @suncoolsu's assertion that ""you are trying to extract information that is *NOT* there"" in the case N=1.  On the contrary, a single observation from a random variable indeed *does* provide information about its variance in many practical applications.  (However, I am *not* saying that the variance is best estimated by some kind of variance formula!)",2010-10-31T20:23:53.770,919,CC BY-SA 2.5,
6001,4074,0,"@Fred Questions specifically about how to code such-and-such are of marginal interest, but you actually have presented a statistically interesting question: how does one compute a prediction interval for logistic regression?",2010-10-31T20:30:08.270,919,CC BY-SA 2.5,
6002,4074,0,@Fred Your calculations still look incorrect.  You seem to be mixing up predicted probabilities and standard errors of their logits.,2010-10-31T20:32:25.303,919,CC BY-SA 2.5,
6003,4070,0,@whuber Fair enough; yet this does not justify this behavior.,2010-10-31T20:42:12.873,,CC BY-SA 2.5,user88
6004,4070,0,"@mbq I agree with you completely.  Sadly, even Mathematica has troubles with singleton datasets (in its ""HypothesisTesting"" package: it issues error messages but actually returns a confidence interval of [0,0] when asked for a CI for the variance of a single observation).  It seems that if you want to handle statistical estimators correctly you really need to use a true statistics package and not just some wannabe add-on to Matlab, Mathematica, Excel, or whatever.",2010-10-31T20:48:11.540,919,CC BY-SA 2.5,
6007,4088,0,"(+1) Really neat R code. Of note, it will not work if $n$ is odd.",2010-10-31T22:24:02.550,930,CC BY-SA 2.5,
6008,3877,0,"@Fred No. Just to say that in this case, you show crude estimates of effect size since they are not adjusted for other covariates. Odds-ratio are better in this respect, since they are adjusted.",2010-10-31T22:34:54.283,930,CC BY-SA 2.5,
6010,2629,0,+1 I just came back to this again (through google).  I think that your answer is spot on.,2010-10-31T23:37:58.750,5,CC BY-SA 2.5,
6011,3982,0,"I don't see this as even being on topic. Sure, it's for a visualization app, but the question has to do with a mechanical detail of a particular programming environment.  It is unlikely to lead to any answer or comments of any relevance to data visualization.",2010-11-01T02:36:45.707,919,CC BY-SA 2.5,
6013,4082,0,my google search for 'Heywood case definition' had been rather unsatisfactory. I am glad to see that such a google search now links to this question.,2010-11-01T04:31:00.260,795,CC BY-SA 2.5,
6014,4088,0,"@chl Thanks!  But I think it will work. The task was to give samples of size 10 from a set of datapoints. Assume n = length(datapoints). The code gives the maximum number (n %/% 10) of such samples. The first corner case is n<10 (anyway ruled out in the problem statement by describing the dataset as 'large', i.e. n>10). In that case you get the datapoints back and a warning (not an error). The second corner case is if there are dangling elements (when n %% 10 != 0). Then you get as many samples as possible and a warning (not an error). Odd n situations are subsumed in one of these two cases.",2010-11-01T08:44:35.887,1739,CC BY-SA 2.5,
6015,4088,0,"It seems the first element of the list is of length 11, not 10, and `sum(unlist(lapply(sample, length)))` return the length of `datapoints` (which I set to 1001).",2010-11-01T09:05:27.143,930,CC BY-SA 2.5,
6016,4088,0,@chl Damn! You're quite right.,2010-11-01T09:37:07.880,1739,CC BY-SA 2.5,
6018,4077,0,@chl-- yeah..even I realize this fault of mine and shall rectify this for sure in posts to come..Thanks for pointing this out..Consider me for some days a naive amateur..,2010-11-01T10:27:41.067,1763,CC BY-SA 2.5,
6019,4077,0,"@onestop..I realize that you people are not paid for teaching me statistics..It seems like my comment has offended you ( though I can tell you for sure I didnt mean to )..Never mind..here for a common cause called ""stati-tarian"" analogous to humanitarian.",2010-11-01T10:30:20.063,1763,CC BY-SA 2.5,
6020,4079,0,Thanks Gaetan..got you..I think what I take away from this is that standard deviation is a better measure when you have large samples like mine..please let me know if I missed anything.,2010-11-01T10:43:36.723,1763,CC BY-SA 2.5,
6021,4100,0,"+1 but Parametric IRT models make strong assumptions about unidimensionality (not obvious in the question) and local independence. Anyway, in this particular case, I would favor some kind of explanatory IRT model, like the LRRM, as explained in de Boeck & Wilson's book: person-specific covariates can be directly tested within such a model, no need to consider weights of any kind.",2010-11-01T10:56:14.490,930,CC BY-SA 2.5,
6026,4100,0,"@chl True those *are* strong unidimensionality assumptions. That's why I suggested the Mokken procedures to see how reasonable they were.  I should have put that part first not second.  And it's quite true Joris doesn't say anything about a scale - I'm probably overgeneralizing from the survey problems I see there...  Finally, with more than a handful of respectably informative items, the explanatory IRT models I've seen (i.e covariates on theta) tend to generate the same coefficients as the regression part of a two step irt + regression procedure. Gotta ref for that someplace if you want.",2010-11-01T12:00:17.657,1739,CC BY-SA 2.5,
6027,3916,0,"Thanks so much for holding my hand.  Your function gives me an error: ""Error in raw.resp[i, names(tmp)] <- tmp : subscript out of bounds"", but oddly enough, I could run each line of the function without problems.  So I have a working report.",2010-11-01T12:29:15.047,1681,CC BY-SA 2.5,
6028,4104,0,"@Max It seems like a good question. I'm not sure about the ""data-mining"" tag, though. I'd suspect that there should be some kind of appropriate statistical (parametric) model for the data and question at hand.",2010-11-01T13:05:19.473,930,CC BY-SA 2.5,
6030,2998,1,"@Tal I'll second Jeromy on that point. Correlations (or $1-r$) are useful metric for clustering or PCA, and sometimes $1-|r|$ are preferred simply because a negative correlation may be considered as informative as a positive one. Taking the complementary value just amount to work with dissimilarities rather than similarities (the same consideration applies in MDS).",2010-11-01T13:22:34.797,930,CC BY-SA 2.5,
6032,4104,0,@chl I tagged it data-mining because I was thinking about finding some rules and patterns in the data. Wouldn't that make it a data mining technique? The workout is just an example. In general I would like to know if there exist some algorithm that can deal with temporal cause-and-effect data. But I also would like to know if there's some specific statistical model for this particular example.,2010-11-01T14:10:50.897,255,CC BY-SA 2.5,
6036,4104,0,@Max A simple initial approach is to posit an exponential decay in effectiveness over time.  The decay rate becomes a parameter in a model relating cumulative workout effect (as weighted by the decay) to outcome measures.  If the decay is not huge and not zero and you have sufficient data you can move on to exploring other forms of decay.  This is really in the spirit of exploratory data analysis.,2010-11-01T14:45:37.920,919,CC BY-SA 2.5,
6037,4104,0,"@Max How large is your dataset?  If it's relatively small, what you're doing might be better characterized as ""exploratory data analysis"" rather than ""data mining.""",2010-11-01T14:46:38.200,919,CC BY-SA 2.5,
6039,4104,0,"@whuber Depending on how the data is presented, the dataset is from several tens to about a hundred records.",2010-11-01T15:59:06.237,255,CC BY-SA 2.5,
6040,4079,1,"ayush...  You are right.  That is basically it.  And, this is because your standard error will become so small (due to the large sample size).  This in turn overstates the statistical distance between your test and control groups.  And, causes you to ultimately run into a Type I Error (uncover a difference that is so small as to be immaterial).  This is a common problem in hypothesis testing with large samples.",2010-11-01T16:17:15.013,1329,CC BY-SA 2.5,
6041,4082,0,"So it looks like this phrase is _not_ used for the case of numerical issues (loss of precision) causing a negative variance estimate, but is not ambiguous.",2010-11-01T16:39:03.887,795,CC BY-SA 2.5,
6042,4082,1,"@shabbychef The trick with searches is to focus them with appropriate keywords.  ""Case"" and, to some extent, ""definition"" don't accomplish much.  Including ""negative"" and ""variance"" encouraged Google to cough up more relevant material ;-).",2010-11-01T17:05:21.740,919,CC BY-SA 2.5,
6043,4104,0,@Max Thanks: that indicates this is EDA.  One implication is that you shouldn't hope for too much; a pattern will be difficult to detect unless it is extremely strong.  Another implication is that graphical and robust techniques (key elements of EDA) are likely to be helpful.,2010-11-01T17:07:18.337,919,CC BY-SA 2.5,
6044,4109,0,"I see one problem with this model. It is assumed that the effect of a workout is positive and descreases with the time. In fact at first the effect is negative, then it moves to a positive range and then decreases to zero. Other than that I believe that this procedure can generate some interesting results. Thanks.",2010-11-01T19:46:16.700,255,CC BY-SA 2.5,
6045,4109,0,"@Max It is assumed only that the effect, whether positive or negative, decays at a constant rate over time.  If you are sure that the change in effect over time is more complex than that, you can incorporate that assumption.  (E.g., Y could be the sum of two terms [presumably of opposite signs] having two different decay rates.)  This will introduce additional parameters that play the role of \theta and it could complicate the model sufficiently that you might have to rely on ML methods.  I think you'll find you need lots of data!",2010-11-01T19:56:30.417,919,CC BY-SA 2.5,
6048,4111,6,You could also use other anecdotal evidence (e.g. amount of garbage generated compared to normal).,2010-11-01T20:56:01.410,5,CC BY-SA 2.5,
6049,4115,3,"For illustration, here are some additional papers: http://j.mp/cBJ0hI, http://j.mp/deMrA8, http://j.mp/dhwcrv, http://j.mp/cyVx0m.",2010-11-01T21:03:12.483,930,CC BY-SA 2.5,
6051,4111,2,But Stewart specifically asked to leave the place cleaner then they got it :),2010-11-01T21:13:08.620,253,CC BY-SA 2.5,
6052,4111,2,"DC metro ridership might factor in.  Also overflow from the mall, when they stopped letting people in, might place a minimum -- assuming one has reasonable historical estimates of how many fit in the mall.",2010-11-01T21:23:51.023,251,CC BY-SA 2.5,
6053,4111,0,"@Shane @ars yes, you are right.  Now that you mention it, there should be other correlates running around, too, which may do a pretty good job of predicting crowd size.",2010-11-01T22:26:51.300,,CC BY-SA 2.5,user1108
6054,4116,0,"Thanks.  This was essentially just what I was looking for.  (I should have googled crowd size instead of population size, yours is there with a bunch of others).",2010-11-01T22:31:05.553,,CC BY-SA 2.5,user1108
6055,4117,0,"Thanks, I had wondered how police make their guesstimates and if you ever run across that information I would appreciate your posting it back here, for the record.",2010-11-01T22:32:59.280,,CC BY-SA 2.5,user1108
6056,4118,0,"Yes, I agree that it is a harder problem than many would like to admit.  In fact, it's not even trivial to define what it means to ""attend""; do we mean just stopping by for a second, staying all day long?, etc.  I think the actual problem is too hard to solve exactly, and I am happy to hear about first order approximations.  :-)",2010-11-01T22:37:04.113,,CC BY-SA 2.5,user1108
6058,4122,0,"I know that E(XY) is 1 (this is the correlation), so E(X^2)+E(Y^2) must equal 2 but I'm having trouble showing this.",2010-11-01T23:29:57.937,1395,CC BY-SA 2.5,
6059,4122,3,Can you connect E(X^2) to the variance formula?,2010-11-01T23:31:03.977,,CC BY-SA 2.5,user28
6060,4122,0,"Ahh so E(X^2) = Variance(X) when the mean is 0, and so variance is 1.",2010-11-01T23:33:34.940,1395,CC BY-SA 2.5,
6062,4120,0,Does the Y column give the means?,2010-11-02T00:12:38.103,159,CC BY-SA 2.5,
6064,4125,0,What have you done so far?,2010-11-02T01:30:33.687,,CC BY-SA 2.5,user28
6065,4125,0,I don't know where to start.,2010-11-02T01:38:40.100,1395,CC BY-SA 2.5,
6066,4125,0,I know the simple regression line is y=B0 + B1x,2010-11-02T01:39:03.763,1395,CC BY-SA 2.5,
6067,4125,1,"And I guess by the average x and y is E(x) and E(y), not sure how to link that to the regression line though.",2010-11-02T01:40:20.457,1395,CC BY-SA 2.5,
6068,4125,0,"The regression line is the line that minimizes the sum of squared errors. Knowing that, and a basic knowledge of calculus, find the values of B0 and B1 that minimize that sum of squared errors. The rest requires a little bit of high school level algebra.",2010-11-02T02:06:22.780,1118,CC BY-SA 2.5,
6071,4125,0,"Starting with the definition is good.  You can carry out the demonstration without calculus, though: if the regression line does not go through the point of averages, you can raise or lower it by some amount to make it pass through that point.  In so doing, you will lower the sum of squares of residuals by *n* times the square of the amount of raising or lowering (which is a one-line algebraic calculation).  If the line was a least-squares line the sum of squares cannot be lowered, demonstrating the shift must have been zero.",2010-11-02T05:52:22.713,919,CC BY-SA 2.5,
6072,4125,0,"@Justin: Are you doing regression or correlation?  In the former, the x's are fixed (not random variables) so one usually does not refer to E[x].  Moreover, y is presumed to depend on x, so E[y|x] makes sense, but E[y] would not usually be considered.",2010-11-02T05:54:56.173,919,CC BY-SA 2.5,
6076,4131,0,"Quick clarification: Do you want to assume that exactly one functional form should fit all of the participants, or do some participants have one form and some another?  If one of the functional form fits them all, do the parameter values get to vary across participants, or not?",2010-11-02T09:01:55.597,1739,CC BY-SA 2.5,
6077,4131,0,@Conjugate prior: No. It is quite clear from the data that the best functional form varies across participants. I am running separate nonlinear regressions.,2010-11-02T09:07:25.803,183,CC BY-SA 2.5,
6078,3259,0,"I am not an expert but I found the sentence ""controlling for associated subscores when devising cut-off scores on each dimension of a measurement scale"" a bit esoteric. Can you give me one more line of explanation (otherwise I found it difficult to understand the question) ?",2010-11-02T12:38:29.587,223,CC BY-SA 2.5,
6079,4065,0,"wrote ""the overall precision of the scores should be e.g. 0.1"" do you mean you want the score to be a random variable with mean 0.1 ? why don't you generate a classification problem (say with gaussian variables) and use something fast and simple (like LDA) to solve it ? can you tell us more on your motivations ?",2010-11-02T12:45:55.800,223,CC BY-SA 2.5,
6082,1654,0,"Cool - I didn't know about the KFAS package.  There's also the dlm and dse for state space-ish approaches, and an general overview for R users here: http://cran.r-project.org/web/views/TimeSeries.html",2010-11-02T13:19:29.190,1739,CC BY-SA 2.5,
6084,2825,25,"If you're going to use R, I'd recommend embedding your R code in an Sweave document that produces your report.  That way the R code stays with the report.",2010-11-02T14:00:05.077,319,CC BY-SA 2.5,
6085,4139,0,"But, your answer gives the probability that $P(X_i >= X_j)$ for a specific value of $j$. It does not take into account the possibility that $X_i < X_k$ for some $k \ne j$. My intuition suggests that the answer is $\frac{1}{p}$ where $p$ is the dimension of the random vector.",2010-11-02T15:30:21.590,,CC BY-SA 2.5,user28
6086,4139,0,My intuition is flawed! It cannot be $\frac{1}{p}$. A simple simulation shows otherwise.,2010-11-02T15:52:05.510,,CC BY-SA 2.5,user28
6087,4139,0,@Srikant oups ... I have updated the answer... but I won't go any further than n=2 (the idea is here but there is too much calculation for me :) ),2010-11-02T16:08:33.713,223,CC BY-SA 2.5,
6088,4142,2,"Instead of controlling type I error while minimizing type II you can minimize the sum of type and type II error (or any combination)... in this case you don't use the likelihood ratio like in Neymann pearson but it's as easy (looks like bayes rule with uniform prior). Anyway, recall that under the null your p-value is uniform (can be close to 0.05) also, I don't really like your procedure.",2010-11-02T17:12:35.520,223,CC BY-SA 2.5,
6090,4143,0,"Thanks for your comment. The reason why I want to be using the result from both the test is that, I have some evidence that by choosing a particular null hypothesis, the test result is actually getting biased towards what the null hypothesis tests. Thanks for the paper link though - will go through it.",2010-11-02T17:26:37.520,528,CC BY-SA 2.5,
6091,4143,0,"@Samik in a way, when you do a hypothesis test using p-values, you are assuming the null to true and thus the bias for the null. Its just a suggestion: try using Bayes Factor, may be that will remove the ""biasness"" you observe (although, I must remark, its not a guarantee that this will work)",2010-11-02T17:38:35.823,1307,CC BY-SA 2.5,
6092,4139,2,"This is a nice try but unfortunately Part 2 is incorrect (even after interpreting \Phi as the Gaussian probability in the *upper* tail): the right answer has to involve an *integral* of the Gaussian CDF, not the product of two of them (as the iterated expectation indicates).  Alternatively, consider what happens when both variances are 1, the correlation coefficient approaches 1, and u1=u2: the joint probability that X1>u1 and X2>u2 should converge to Pr[X1>u1] (due to near-perfect correlation), but on the contrary the first factor in the right hand side goes to zero.",2010-11-02T18:06:06.373,919,CC BY-SA 2.5,
6093,4142,2,"Could you clarify what it means for null hypotheses to be ""reversed""?  E.g., if one null is that two means are equal, do you take the other null to be that they are unequal?  (This particular example leads to trouble...)",2010-11-02T18:08:37.463,919,CC BY-SA 2.5,
6094,4143,0,"thanks, will read up on that. I have also added the actual context of the question in the OP.",2010-11-02T18:19:51.390,528,CC BY-SA 2.5,
6096,4139,0,"@whuber I should not try to write anything else today, to many mistakes in a few hours ;) ... anyway I have updated the answer but it is not friendly anymore... maybe there is a way to make the calculation in the particular case of the question!",2010-11-02T18:40:35.253,223,CC BY-SA 2.5,
6097,4144,1,"I didn't have time for more than a pithy answer this morning. I was planning to expand on it this evening but you've done the the job very nicely, whuber.",2010-11-02T18:49:31.560,449,CC BY-SA 2.5,
6098,4074,0,"@whuber.  Thanks again.  I agree that something isn't right.  I'll keep working on this, but I would appreciate any guidance you (or anyone else) could provide.",2010-11-02T18:58:27.473,,CC BY-SA 2.5,Fred Vars
6099,4139,0,@robin Thank you!  This is a difficult calculation on which you have made excellent progress.  (I already upvoted it so unfortunately I can't do so again...),2010-11-02T19:34:18.637,919,CC BY-SA 2.5,
6100,4074,1,@Fred Ignore the probabilities: work entirely in logits.  That will bring you closer to familiar OLS and ML calculations and lets you avoid all that stuff with logarithms and exponentials.  All that's required is to convert your threshold of 0.75 into its logit in order to make comparisons.,2010-11-02T19:40:20.593,919,CC BY-SA 2.5,
6102,4140,0,"Probably I didn't understand something, but can you please clarify how can you simulate ""real"" CDFs? (I am assuming you are doing a real life data analysis, as you mention the analysis over a genome)",2010-11-02T20:01:26.807,1307,CC BY-SA 2.5,
6103,4147,1,Thank you for sharing this!  (I like your style of making all parameters variables and clearly commenting them.)  I'm all out of votes today :-( so I'll have to wait to upvote it.,2010-11-02T20:03:32.343,919,CC BY-SA 2.5,
6104,4140,0,"@suncoolsu: I guess it was a bad choice of words. I don't simulate real data, I simulate the experiment under the assumption the genome has nothing special in it.",2010-11-02T20:12:01.943,634,CC BY-SA 2.5,
6105,4147,2,"@Srikant Just a few words about R code (per your request): you can use `replicate()` whenever you want to call a given function like `rbinom()` $k$ independent times; the `{*}apply()` family functions are very useful to work with matrix, data.frame or list and make a call to the same function by column or row; it is recommended to avoid the underscore for variable naming (and use a dot instead), but it is subject to debate. Otherwise, I also like simple and explicite R code like yours because it will be understandable to everybody. I gave my +1.",2010-11-02T21:51:42.987,930,CC BY-SA 2.5,
6106,4147,0,"@chl Thanks. I am using these questions to learn R! So, feedback such as yours is valuable.",2010-11-02T21:56:05.153,,CC BY-SA 2.5,user28
6107,4147,0,"@Skrikant LOL I was just adding: ""BTW I like the way you learn R!""",2010-11-02T21:59:07.403,930,CC BY-SA 2.5,
6108,3259,0,"@robin Yes, basically I meant: we have $j=4$ scores (e.g. anxiety, impulsivity, neuroticism, sensation seeking) and we want to find a cut-off value $t_j$ (i.e. ""positive case"" if $x_j>t_j$, ""negative case"" otherwise) for each of them. We usually adjust for other risk factors like gender or age when devising such cut-off (using ROC curve analysis). Now, what about adjusting impulsivity (IMP) on gender, age, *and* sensation seeking (SS) since SS is known to correlate with IMP? In other words, we would have a cut-off value for IMP where effect of age, gender and anxiety level are removed.",2010-11-02T22:54:09.403,930,CC BY-SA 2.5,
6109,4149,1,"Further to this, you might want to check out [What Every Computer Scientist Should Know About Floating-Point Arithmetic, by David Goldberg](http://docs.sun.com/source/806-3568/ncg_goldberg.html) for advice on how to deal with issues of numerical representation.",2010-11-02T22:58:45.020,179,CC BY-SA 2.5,
6110,4148,0,"@Srikant: Thanks for the link to the notes. I am actually trying to detect (D) [I have separate tests for (B) and (C) which I run before (D)] and from that perspective it does seem like one test is reverse of other. Other than that, do you also have a comment about the procedure itself that I was thinking of using?",2010-11-02T23:19:18.407,528,CC BY-SA 2.5,
6111,4149,0,@Rob: Thanks for the answer. I guess you are then implying that it is alright to scale the series before doing the analysis.,2010-11-02T23:27:14.213,528,CC BY-SA 2.5,
6112,4149,0,@fmark: Thanks for the comment - I am pretty familiar about that material actually.,2010-11-02T23:27:38.393,528,CC BY-SA 2.5,
6113,4149,0,"@Samik: For linear models such as Gaussian ARIMA processes, yes.",2010-11-02T23:46:35.830,159,CC BY-SA 2.5,
6114,4149,4,"Within vast limits, scaling makes no difference whatsoever for floating point calculations: it simply amounts to a shift of exponent with no loss of precision.  Where scaling can help is where a calculation involves sets of data that are themselves on different scales.  My guess is that the time series formulae are using some time measurement (milliseconds?  years? just integral steps?) that may have a hugely different range than the range of the data.  Good stats SW will internally scale its matrices to avoid loss of precision; this could explain the differences between FP and Minitab.",2010-11-02T23:49:32.487,919,CC BY-SA 2.5,
6115,4152,1,"Thanks suncoolsu; I looked into fitdistr but I've found that a mixture model fits my data better than a univariate distribution.  I'm not sure how to use fitdistr for a multivariate distribution while constraining the sum of the weights to 1 - any tips?  I think my problem is primarily in how to optimize a multivariate function subject to constraints.  Thanks, Robert",2010-11-03T00:29:18.067,,CC BY-SA 2.5,robert1
6116,4150,3,"@robert1 If you could disclose the purpose of this fit you will increase your chances of getting really effective responses.  Why do you need a parametrization instead of, say, the empirical distribution function (as revealed by the sample quantiles)?  Do you have any theoretical expectations concerning the shapes of the mixture components (or concerning why one might expect this to a be a mixture)?  What might be the consequences of uncertainty in the parameters?  (That uncertainty will be *huge* unless your distribution is sharply multimodal.)",2010-11-03T00:45:20.233,919,CC BY-SA 2.5,
6117,188,1,+1 for the last paragraph.  With a minimum of technicalities it conveys the idea well.,2010-11-03T00:52:31.300,919,CC BY-SA 2.5,
6118,4152,1,"@robert1 Sorry for the previous mistake. I thought you wanted to fit _a_ parametric distribution and _not_ mixtures. OK, before I suggest anything, can you please tell what your data set looks like - I mean how do you get your data, do you have any guesses about the number of mixtures there will be ... and other similar aspects.",2010-11-03T01:32:00.713,1307,CC BY-SA 2.5,
6119,3912,0,"@chl, this may be off-topic, but can you please tell me if you made these graphs in R?",2010-11-03T01:41:43.713,1307,CC BY-SA 2.5,
6120,4150,0,"I'm modeling liability - in a personal auto context (let's temporarily assume there is no insurance limit); the amount of liability is almost certainly a mixture - you have a ""fender bender"" process, a ""third party severely injured"" process; a catastrophic process (""DWI + injury involving school bus w/ multiple youth injuries"").  I'd like to have a parametrized distribution for use with simulating future loss and certain other applications.  My sample data is small relative to the population, but I know this type of process is long tailed so I am thinking logn/pareto/gamma type distributions.",2010-11-03T01:45:34.103,,CC BY-SA 2.5,robert1
6121,4154,1,"The EM algorithm looks promising, thank you.  I'll have to become better acquainted with it - I'd be interested in exploring any R packages for this.",2010-11-03T01:56:14.497,,CC BY-SA 2.5,robert1
6122,4155,1,Do you mean ARIMA rather than X-12-ARIMA? The latter is a decomposition method not a forecasting method.,2010-11-03T02:56:47.853,159,CC BY-SA 2.5,
6123,4130,0,"I'm marking this as correct as it is short and sweet.  However, see whuber's lovely longer explication as to WHY it is correct.  And, indeed, in fooling around with the problem, weighted by n produces the correct parameter estimates and relatively similar SE and p values to working with simulated data.",2010-11-03T04:48:30.147,101,CC BY-SA 2.5,
6126,4157,4,"Re your e.g.: It's a common misconception that a mixture of two Gaussians is necessarily bimodal. A mixture of two Gaussians with equal standard deviations is bimodal only if the difference in their means is at least twice their standard deviation. See: Schilling, Mark F.; Watkins, Ann E.; Watkins, William (2002). ""Is Human Height Bimodal?"". *The American Statistician* 56(3): 223‚Äì229. http://dx.doi.org/10.1198/00031300265",2010-11-03T07:15:11.680,449,CC BY-SA 2.5,
6129,4154,1,"@Skrikant The [mclust](http://cran.r-project.org/web/packages/mclust/index.html) package should work here. To reproduce the example on Wikipedia (two-cluster solution), you will have to fix parameter `modelNames` (shape of covariance matrices) or `G` (No. clusters). Otherwise, the returned solution (i.e. minimizes BIC) is a three-cluster solution. Maybe you could add it to your original answer, for illustration purpose.",2010-11-03T08:26:04.860,930,CC BY-SA 2.5,
6130,3912,0,"@suncoolsu No, it is LateX + PSTricks. Here is the code if you'd like to play with it, https://gist.github.com/660876 (it's old and pretty ugly for sure). In R, it should not be very difficult to reproduce, though.",2010-11-03T08:32:37.077,930,CC BY-SA 2.5,
6132,4154,0,"@Skrikant After browsing the site, I also came across @csgillespie's example with [mixtools](http://cran.r-project.org/web/packages/mixtools/index.html), http://stats.stackexchange.com/questions/899/separating-two-populations-from-the-sample/1010#1010.",2010-11-03T09:09:57.727,930,CC BY-SA 2.5,
6135,854,1,"Why not?  Because Fisher's Exact Test it assumes that data generating process has both sets of marginal totals fixed but actually occurring processes seldom do.   As Hadley points out, we need to know about that generating process.",2010-11-03T12:59:32.180,1739,CC BY-SA 2.5,
6136,4169,3,"That's nice, Mike, but *what does the paper say?*",2010-11-03T14:17:32.643,919,CC BY-SA 2.5,
6137,4117,0,"Well, I asked, but he didn't remember. On a lighter note, Rob Hyndman has a story about the way headline numbers are arived at. Scroll down a bit on http://robjhyndman.com/researchtips/statistical-jokes/",2010-11-03T15:09:04.370,1766,CC BY-SA 2.5,
6139,4177,0,"Stochastic processes are also very useful in the industry (think Wall Street, financial industry).",2010-11-03T15:37:20.187,,CC BY-SA 2.5,user28
6140,4177,1,@Srikant-vadali: Good point. I suppose I should add that I did a PhD in Stoc Proc and have found it very helpful in my new field of systems biology.,2010-11-03T15:45:15.260,8,CC BY-SA 2.5,
6141,4065,0,"@Robin: No, I am talking about ""precision"" as measure of the quality of a binary classification model (see e.g. http://en.wikipedia.org/wiki/Sensitivity_and_specificity here). Regarding your second question: I require the scores for a simulation where multiple binary classification models are combined (more in the area of data processing than Data Mining). It is hard to explain the task in details, but I am pretty sure I need these scores :D",2010-11-03T16:02:34.667,264,CC BY-SA 2.5,
6142,4178,0,I'd appreciate if anyone with more skills in latex could edit the calculation in 4. so that it is more readable.,2010-11-03T16:04:53.260,264,CC BY-SA 2.5,
6143,4176,0,Surely p^ is 7/100 (or however many trails were used in the original test)?,2010-11-03T16:09:03.253,229,CC BY-SA 2.5,
6144,4174,0,"The iteration numbers are surely just identifiers, and should not be used as numbers in computations. How many iterations did you do in total in your original trial?",2010-11-03T16:11:19.397,229,CC BY-SA 2.5,
6145,4176,0,"No I don't think so. I took the question to mean that he ran device until he reached a failure. So it failed on 100 iterations, 22 iterations, ...",2010-11-03T16:14:51.210,8,CC BY-SA 2.5,
6147,4174,0,@James I could be wrong but if the iterations are simply identifiers then most people will naturally report them in an increasing order like so: 22 24 36 44 ... The fact that the numbers are reported in an apparently random sequence suggests that each number represents the failed iteration number of 7 separate tests.,2010-11-03T16:26:56.213,,CC BY-SA 2.5,user28
6150,4175,1,What's the purpose of the experiment?  The answer to that determines the answers to all your questions.,2010-11-03T16:43:43.987,919,CC BY-SA 2.5,
6152,4175,0,"The purpose of the ""experiment"" was to test the claim that subjects can score above chance. Hence H1.",2010-11-03T16:47:18.287,1614,CC BY-SA 2.5,
6154,4176,0,Then shouldn't each test have its own failure probability?,2010-11-03T16:56:13.533,229,CC BY-SA 2.5,
6155,4174,0,"I take your point, but I'm still a little unsure of the value of the mean of these numbers.",2010-11-03T16:57:26.433,229,CC BY-SA 2.5,
6156,4176,0,Each test does have it's own failure probability - it's 1. We've got the data to prove it ;) Anyway I'm assuming that each test is a replicate from the Geometric distribution.,2010-11-03T16:59:33.463,8,CC BY-SA 2.5,
6159,4180,0,thank you so much for this thoughtful answer. I have included an **Update** section in my question that attempts to address your concern as well as ask an additional question.,2010-11-03T17:54:52.130,1786,CC BY-SA 2.5,
6161,4174,0,"@Srikant: Yes, your interpretation is spot on.   To James' question, I think it would be fair to say that with this given device and software test bench, the device fails *on average* at the 55th iteration.  Does that not sit will with you?",2010-11-03T18:01:03.233,1786,CC BY-SA 2.5,
6162,4176,0,@csgillespie: Thank you for taking the time to write this answer.  I have added an **Update** section to my original answer.  One more question I have for you is if there is an underlying reason why we believe failures rates to be exponentially distributed or are we just back fitting the empirical data we see from known failures to a distribution that fits?,2010-11-03T18:21:01.953,1786,CC BY-SA 2.5,
6163,4179,0,"What is this doing exactly? Im not familiar with R-code but if I had to guess it is using the original 7 figures to extrapolate out the exponential series with 100,000 data points?",2010-11-03T18:26:52.417,1786,CC BY-SA 2.5,
6164,4181,2,"+1 Very nice!  This problem *had* to require numerical integration (except for special cases such as all means being equal), so the availability of this package and the reduction of one in dimensionality are both useful contributions.  By the way, as a check you can compute P(Y>X and Y>Z) = 0.3200386 and P(Z>X and Z>Y) = 0.5353126; the three results sum to 0.9999999, which is reasonably close to 1.",2010-11-03T18:28:49.287,919,CC BY-SA 2.5,
6165,4179,0,I edited the code with comments (which in R are after a #),2010-11-03T18:59:55.947,1766,CC BY-SA 2.5,
6166,4176,0,"@SiegeX: no, we're not playing fishing games here.  When the probability of failure is constant over time the distribution of failure times will be exponential.  Alternative theories of failure probabilities lead to other predictions of the failure time distribution.  You have to be prepared to be wrong about your assumptions!  (That's one reason to check them using techniques like probability plots.)",2010-11-03T19:05:23.890,919,CC BY-SA 2.5,
6167,4176,3,"Assuming a geometric distribution is assuming that the failures are independent. In other words, on any trial the device fails with probability p, independent of the other trials (like tossing a coin). This is a reasonable starting point, but if the failure is due to something like a memory leak, then the longer the device runs, the more likely it is to fail, and the assumption of independent failures would be invalid.",2010-11-03T19:06:01.297,247,CC BY-SA 2.5,
6168,4182,0,"That's a clever approach, but it seems biased to me, for two reasons.  The first is that the occurrence of a failure in any experiment precludes the possibility of any more successes, calling into question the independence of failure and success which is assumed by Fisher's Exact Test.  In other words, how can you model these results as if they were like drawing balls out of an urn?  The second is the censoring of the last value at 223 has to be handled a little more carefully.  (This problem can be surmounted.)",2010-11-03T19:11:08.750,919,CC BY-SA 2.5,
6170,4181,0,@whuber Thanks.  Thanks also for following through with P(Y>X and Y>Z) and P(Z>X and Z>Y); that didn't occur to me.,2010-11-03T19:14:24.200,,CC BY-SA 2.5,user1108
6171,4179,0,"PS: Of course, this is not really the worst case (which would be a draw of 7x100), but then you wouldn't have to use bootstrap, because you would ignore the other observations.",2010-11-03T19:15:42.907,1766,CC BY-SA 2.5,
6172,4164,1,I think the question isn't fully specified and so it is hard to tell exactly what the OP meant without asking him directly.  Maybe instead of drafting a solution to a hypothetical problem I should have asked for clarification.  Why not do that now?  :-),2010-11-03T19:16:37.240,,CC BY-SA 2.5,user1108
6173,4138,0,"@gregor What is ""l"" in this problem?  Is it fixed (but arbitrary), or something else?",2010-11-03T19:17:28.097,,CC BY-SA 2.5,user1108
6174,4175,0,"Just to be clear (since I have no idea what ""H1"" means): are you testing the claim that the *average score* in a *population* is better than 1/5 or that there exist *particular subjects* who have a tendency to score better than 1/5?",2010-11-03T19:32:23.887,919,CC BY-SA 2.5,
6175,4174,0,"@SiegeX Your last approach has the right flavor, but please don't use the formulae on the Wikipedia page: they are only for normally distributed data.  It is rare for failure time data to be normal.  They typically are positively skewed.  This implies the normal theory upper prediction limits can be (way) too low.  BTW, a lower prediction limit is irrelevant here--although the fact that it is hugely negative is a clear indicator of how bad the normal theory methods are for these data.  To adjust the equation, see the reference I provided in my answer.",2010-11-03T19:35:41.480,919,CC BY-SA 2.5,
6176,4175,0,The experimental hypothesis (H1) is that subjects in the study will score above chance. The null hypothesis (H0) assumes a binomial distribution for each subject. I tried to make that clear in the question.,2010-11-03T19:38:25.937,1614,CC BY-SA 2.5,
6177,539,28,"This question and its responses remind us of how crude and limited this antiquated division of variables into categorical-ordinal-interval-ratio really is. It can guide the statistically naive, but for the thoughtful or experienced analyst it's a hindrance, an obstacle in the way of *expressing variables in ways that are appropriate for the data and the decisions to be made with them.* Someone working from this latter point of view will freely move between categorical and ""continuous"" data representations; for them, this question cannot even arise! Instead, we should ask: how does it help?",2010-11-03T20:29:39.853,919,CC BY-SA 2.5,
6178,4175,0,"@RSoul. As whuber as has already said, is your $H_o$: mean score = 1/5 and $H_1$: is mean score $\ge$ 1/5 ?",2010-11-03T20:43:17.630,1307,CC BY-SA 2.5,
6179,4175,0,Okay. That's a good question. I'll just put down my copy of Feller.,2010-11-03T20:48:20.530,1614,CC BY-SA 2.5,
6180,4175,0,There was a stray zero in the question which may have confused things. I am testing the claim that subjects will score significantly above mu.,2010-11-03T20:55:43.533,1614,CC BY-SA 2.5,
6181,539,1,"@whuber (+1) At the very least, it seems difficult to optimize measurement reliability and diagnostic accuracy at the same time.",2010-11-03T21:02:30.223,930,CC BY-SA 2.5,
6182,4175,0,"@RSoul Sorry to keep asking questions, but I do have a lot.  Is this really a die or is it a pseudorandom number generator?  (Charles Tart famously ""demonstrated"" ESP by virtue of a flawed RNG!)  If it's a die, is it just one or are there several copies of it?  (With a physical die, it's doubtful all outcomes have probability 1/5, so your experiment needs to pay attention to learning effects and nonuniformity of outcomes.)  What is your justification for looking solely at the one-sided alternative?  How are the subjects selected for testing?  (Answers to these can guide resampling methods.)",2010-11-03T21:10:05.683,919,CC BY-SA 2.5,
6183,4175,0,"The real experiment is not a dice experiment. It is computer based, but tbh I didn't want to go into the details. The information given should really be more than enough. I was essentially testing chance and finding chance. I realise it seems an odd thing to do.",2010-11-03T21:19:35.093,1614,CC BY-SA 2.5,
6184,4182,0,"Your website's abstract seems spot on to the problem I'm facing here.  Thank you for taking the time to put together this page.  Prior to the 223 continuous run, I also had two additional continuous runs of 102 and 60 which I stopped early for time consideration.  There has been no failure between the 102, 60 and 223 runs so I think I can add those up for the figure in your ""New Version"" Successes cell.",2010-11-03T21:34:06.980,1786,CC BY-SA 2.5,
6185,4182,0,"@whuber: Actually, I think your first concern doesn't apply in this situation because once the device fails there can no longer be any more successes *for that test* because the device hard locks and requires me to cycle the power to bring it back to a normal state.  There is still the concern about me censoring the data, however.",2010-11-03T21:38:10.410,1786,CC BY-SA 2.5,
6186,4175,0,"BTW I was trying to keep it simple, not obfuscate.",2010-11-03T21:53:00.433,1614,CC BY-SA 2.5,
6187,4179,0,"Thank you for the comments, I understand it much better now.  However, could you put into layman's term what ""quantile(p.vec,probs=0.01) #calculates the 1%-quantile of the probs"" is essentially telling us? I see that we get a value of 0.0122 for the 1% quantile. How do I interpret that?",2010-11-03T22:46:44.700,1786,CC BY-SA 2.5,
6190,4180,0,"I've added an **UPDATE 2** section that attempts to use the equations in the *Normal Based Methods in a Gamma Distribution* paper by Krishnamoorthy, Mathew, and Mukherjee.  Do you agree with my results?  If so, I intend to get more failure data by removing the fix then testing the new data set against an exponential probability plot to ensure they fit the exponential distribution.  I'll then update my R code with the new dataset to get my new upper limits.",2010-11-04T01:51:41.727,1786,CC BY-SA 2.5,
6191,4180,0,"@SiegeX Good job!  Using equation (2) of the paper with your data and your confidence levels, but using a different computing platform as a check, I obtain UPLs of 42.5763,50.1065,58.4814,71.5132,83.4132,121.271,143.914,220.331.  That looks very close to what you have plotted.",2010-11-04T02:13:49.787,919,CC BY-SA 2.5,
6192,4186,1,"You are correct: the formula is an elementary plane geometry exercise.  The lines create three triangles at the left, top, and right.  The left and right ones are similar, whence the estimated mode partitions its bin in the same proportion as the bases of these triangles.  In the notation of the formula, one base is *fm-f1* and the other is *fm-f2*, so the fraction of the way to go across the bin equals *(fm-f1)* / (sum of the two bases).  The interpolated value is obtained by starting at the left endpoint *L* and adding this fraction of the bin width *h*.",2010-11-04T02:32:37.267,919,CC BY-SA 2.5,
6193,4187,0,Please verify that the parentheses I inserted into the shrinkage formula are where you intended them to be.,2010-11-04T04:10:29.143,919,CC BY-SA 2.5,
6194,4165,9,It's a little discouraging that any teacher would be so frankly ignorant of the applications of their field.,2010-11-04T04:27:56.093,919,CC BY-SA 2.5,
6195,4193,0,"(+1) .. good question. The Bayesian Central Limit Theorem is called the Laplace Approximation ie the posterior behaves ""more or less"" like a normal distribution. (formally posterior converges in distribution to a normal distribution)",2010-11-04T06:17:32.527,1307,CC BY-SA 2.5,
6196,2635,0,I attempted to make it a better title; feel free to change it to something more appropriate,2010-11-04T06:25:39.737,183,CC BY-SA 2.5,
6197,4182,0,"I agree that my approach is not completely correct for the data you actually have. I suspect that the error is small, especially in comparison to worries about non-independence of successive trials if the hardware is e.g. failing partly due to the build up of heat. I don't know this for sure, though. I personally would be happier if I could run enough trials to come up with a tail probability of 1E-6 or smaller, or if I could run experiments to find a way to provoke a hardware failure 100% of the time and then file a bug report the hardware guys could work on.",2010-11-04T06:29:48.843,1789,CC BY-SA 2.5,
6198,4194,0,"To add to this answer, you need not marginalize the other variables as originally stated in the question. All you need to do is to 're-organize' $Pr(X_i|X_{i-1},X_{i+1})$ such that you recognize the result as a known pdf and you are done. As long as you are able to re-organize the above everything else (i.e., all other constants, the integral in the denominator etc) will equal the appropriate constant for the pdf to integrate to 1.",2010-11-04T09:15:29.453,,CC BY-SA 2.5,user28
6199,4179,1,"The quantile is a rank-based value. One you probably know is the median: You sort your observations according to size, and the median is the observation in the middle (say, you have the income of 100 people, then the median income is the 50th-highest income). Now generalizing, the 1%-quantile is the probability at 1% of the observations. The advantage of a quantile is that it is non-parametric, you don't have to make assumptions about the underlying distribution, and that it is less biased than the quantile-function of a symmetrical distribution when you have skewed data.",2010-11-04T09:16:26.810,1766,CC BY-SA 2.5,
6200,4186,0,@Srikant: Thanks. I know it is only an estimate but is there any reason why the mode must be at the intersection of the two diagonal lines and not somewhere else?                                                           @whuber: Thanks. Your comments was very helpful in understanding the answer.,2010-11-04T09:55:56.953,1636,CC BY-SA 2.5,
6201,4187,1,Shouldn't the 2nd part of your question (with a binary variable) be a separate question?,2010-11-04T11:45:06.007,930,CC BY-SA 2.5,
6202,4138,0,@gregor are you interested in some particular value for $\Sigma$ ? do you need to calculate a limit when the dimension is large ? do you need to simulate things?,2010-11-04T12:15:48.267,223,CC BY-SA 2.5,
6204,4169,0,"@whuber: possibly you missed the link to the paper posted in my response. If not, possibly you didn't bother to look at it, in which case the gist is that I find that the EM algorithm performs well in some cases (lots of data, reasonable proportion of reasonably concentrated Von Mises data), not so well in others (little data, low proportion of Von Mises, Von Mises with low concentration).",2010-11-04T12:30:32.380,364,CC BY-SA 2.5,
6205,2495,0,"I don't thing consistency can have any sense in a goodness of fit test (such as KS test).  You said ""with asymptotic power the alternatives are changing"" do you have an idea of how they change (different way of changing them will lead to different conclusions)? ""Under some regularity conditions"" is it easy to meet these regularity conditions in the case of Kolmogorov Smirnov test ? Did you find something about that in Lehmann's book ?",2010-11-04T12:33:44.750,223,CC BY-SA 2.5,
6206,2495,0,"1) Sure, it can.  Let H0: X ~ N(0,1), suppose X1,...Xn ~ N(1,1), and let n go to infinity.  That's consistency for the alternative $N(1,1)$.  The KS-test is nonparametric so I bet it would be lots of fun to show consistency for all possible alternatives.  2) Yes, they change on the order of $\sqrt n$, say.  3) I'm not quite clear on the question(s): are you wanting to show the asymptotic power of the KS-test is 1?  The first step would be to find a definition of asymp power that makes sense for a nonparametric test.  I don't happen to know of one, in Lehmann's book or elsewhere.",2010-11-04T13:30:35.943,,CC BY-SA 2.5,user1108
6207,4169,6,"This issue has come up on other SE sites, too. The point is that answers should be stand-alone, for many reasons. The intent is to make our threads a source of information, not just some gateway of links to possibly evanescent material. The response ""an answer is at such-and-such a place"" is appropriate when the answer is simple, standard, and easily reachable from a search (e.g., has a good Wikipedia article).  Otherwise, *especially* when the answer reflects a significant investigation like yours, please at least provide a summary. There's no need to attack me for making this reminder.",2010-11-04T14:05:16.430,919,CC BY-SA 2.5,
6211,4196,0,"What kind of inference are you making?  E.g., do you view the discrete values X(t1), X(t2), ... as a sample from a continuous set of values about which you would like to draw conclusions, or do you view this time series as a sample from many possible such series for a particular combination of car/driver/conditions/location, or do you perhaps view it as a sample from possible time series for many cars and drivers?",2010-11-04T14:21:02.353,919,CC BY-SA 2.5,
6213,4178,0,"since this is exactly what I wanted, I will mark it as "" the answer"". However, I am still open for suggestions and ideas.",2010-11-04T14:54:33.637,264,CC BY-SA 2.5,
6214,4206,0,"Thanks for the links! From what my understanding is worth though, these links discuss the distribution of the fit parameters (in my clumsy notation, p), not the distribution of the fit function itself at a fixed value of the argument. Or do I miss something simple?",2010-11-04T15:22:59.560,1197,CC BY-SA 2.5,
6215,4186,0,"@Sara The mode in general depends on how the data are grouped, so the best you can hope for is some reasonable estimate. I suspect this formula may originally have been derived by assuming numbers were drawn independently from a unimodal distribution and then grouped somehow. On the average the interpolated value ought to depart less from the mode of the underlying distribution than some other estimate (such as the midpoint or one of the endpoints of the largest group). When you know more about the distribution--e.g., if it's Normal--then this interpolator is not necessarily best on average.",2010-11-04T15:30:03.110,919,CC BY-SA 2.5,
6216,4156,0,But doesn't the cap indicate a possibly strong heteroscedasticity (and skewness) in the residuals?  That would suggest using a non-linear re-expression of the customer counts.,2010-11-04T15:50:10.277,919,CC BY-SA 2.5,
6218,4174,0,"@SiegeX OK, that does make more sense, seems like your data could be drawn from an overdispersed poisson process",2010-11-04T16:45:02.050,229,CC BY-SA 2.5,
6221,1476,2,"What I find interesting about this paper is that the *perspective* is Bayesian, but the hierarchical modeling approach offered to replace corrections for multiple comparisons does *not* require you to be Bayesian.",2010-11-04T17:35:17.963,1739,CC BY-SA 2.5,
6224,4206,0,"You're right, I didn't read your question properly. Once you've got an estimate and standard error for p from asympototic theory, however, an estimate and standard error for any function of p follows straightforwardly using the delta method. 
http://en.wikipedia.org/wiki/Delta_method",2010-11-04T18:43:55.740,449,CC BY-SA 2.5,
6226,4206,0,"Let me make sure I got it right. So, basically, once I have the best-fit p0 and its variance dp, I take f(x,p0+dp), Taylor expand it, and identify the deviation from f(x,p0) as a variance of f(x,p0) itself, is this right?",2010-11-04T19:40:37.640,1197,CC BY-SA 2.5,
6227,4213,1,"+1 this is neat, I'd forgotten about the vcdExtra package.",2010-11-04T20:01:12.337,251,CC BY-SA 2.5,
6228,4212,1,"I should have pointed out that this ordering is not necessarily transitive!  E.g., on a nine-point scale (0,2,0,2,0,0,0,0,2) beats (2,0,0,0,0,2,0,2,0) beats (0,0,2,0,2,0,2,0,0) which beats the first one!  See http://en.wikipedia.org/wiki/Nontransitive_dice .",2010-11-04T20:17:35.957,919,CC BY-SA 2.5,
6229,4156,0,"@whuber. Yes. But non-linear re-expression of X is really a different problem. The ""right"" way to do this is to assume a latent process Y that is partially observed and try to model that using a non-linear state space model. But I figured that was more complicated than the OP would want. My suggestion will work ok provided the cap is not hit very often. Otherwise it will underestimate the forecast variance.",2010-11-04T21:06:31.107,159,CC BY-SA 2.5,
6230,4196,0,"@whuber: in this case, it's really a single car making measurements so my hunch is that it's not independent : am I right ? I will, at one point, be in a ""crowdsourcing"" case where a lot of independent cars will be driving and I will be looking at what's happening in the same spots : in that case, can I consider them independent even though they should be measuring the same thing ? I think so but I'd like to be extra sure.",2010-11-04T21:12:40.163,1784,CC BY-SA 2.5,
6231,4176,0,@PeterR: Does this assumption of independence in failures not apply to a normal/Gaussian distribution?,2010-11-04T21:19:33.873,1786,CC BY-SA 2.5,
6232,4196,0,"""Independence"" makes sense only after you have adopted a chance model for the data.  Several good models are appropriate.  Which one(s) to use depend on the decisions you plan to make.  So: it seems you wish to estimate the proportion of time spent moving, stopped, or off.  Will you use these proportions to characterize (a) the history of a single ride, (b) conditions along a route traveled by many cars at the same time, (c) a route to be traveled extensively in the future, or (d) some other generalization of the data?",2010-11-04T21:55:50.983,919,CC BY-SA 2.5,
6233,4214,2,What would be the purpose of (or what would you conclude from) a meta-analysis on studies involving the same outcome but different predictors or risk factors?,2010-11-04T22:19:46.987,930,CC BY-SA 2.5,
6235,3332,0,"Yes, there's a [prim](http://cran.r-project.org/web/packages/prim/) package, with an R vignette: [Using prim for bump hunting](http://cran.r-project.org/web/packages/prim/vignettes/prim-2d.pdf). It's not obvious to me how it will work in this case, though.",2010-11-04T22:36:05.603,930,CC BY-SA 2.5,
6237,4212,0,"I've already +1, but I wonder (maybe a naive question) how it would generalize to more than two items? (btw, shouldn't ""integral"" (3rd ¬ß) actually read ""integer""?)",2010-11-04T22:52:48.680,930,CC BY-SA 2.5,
6239,4214,1,"Hi chl,  If I where to send 100 researchers to study different risk factors for the same disease, and 5 of them came back with a P value bellow 5% (but not much less).  Would it be relevant to somehow take all the studies into account? (I believe on the practical level, the answer is no.  But on a more hypothetical level - I am not sure).  From the little I know of meta analysis, it would be relevant only if there was an overlap in risk factors, otherwise, it doesn't offer much (at least from the little I know).  Is that what you where aiming at?",2010-11-04T23:34:41.670,253,CC BY-SA 2.5,
6241,4155,0,"Yes, I mean ARIMA, though I've found using the ARIMA within the X12-ARIMA package to be more convenient than the various ARIMA packages in R. (The diagnostics, the ease of adding variables like AO2004.Jan, etc. The actual workflow is a bit more awkward, but it seems to be a win to me.) So, to your point, there's X12-ARIMA the package, and X12-ARIMA the method, and I didn't think to differentiate.",2010-11-04T23:46:27.397,1764,CC BY-SA 2.5,
6243,4156,0,"You're right, whuber's suggestion is currently beyond me. Unfortunately. Though, I am trying to understand State Space models as we speak and figure that's eventually my answer. (To account for a capacity cap and to provide for trend prediction.) Somehow, the SS concept seems simple but figuring out what the actual matrices accomplish... I've started watching a great SS class from Stanford on Youtube, and there are two or three R packages to choose from.",2010-11-04T23:53:27.513,1764,CC BY-SA 2.5,
6244,4156,0,"In terms of X=min(Y, 1000), that's a good first-cut. I've seen a ""resistance"" when nearing capacity, including something like bounces when the cap is hit, so perhaps something that adjusts the point forecast down as the upper prediction interval exceeds the cap could be a second-cut?",2010-11-04T23:58:40.350,1764,CC BY-SA 2.5,
6245,4212,1,"@chl (""Integral"" is the adjectival form of ""integer"".) With more than two items you can of course still rank them in pairs this way.  If you have intransitivity you can resolve it in some *ad hoc* ways, such as computing the percentage of time one distribution is the ""winner"" among a group of three or more distributions.  As you know, the subject of comparing distributions gets complex because it includes most of the paradoxes of voting theory.  But I like this approach because it forces us to confront such inherent difficulties rather than letting us stay in ignorance of them.",2010-11-05T01:18:08.733,919,CC BY-SA 2.5,
6246,4194,3,"They don't need to be found analytically.  All full conditionals are proportional to the joint distribution, for instance. And that's all that's needed for Metropolis-Hastings.",2010-11-05T02:05:50.703,493,CC BY-SA 2.5,
6247,4217,1,"Hello Srikant, thank you for thinking about this.  Here is a counter example:  Let's say we have done a research (which will be published) on which of a 100 risk factors effects a disease.  In our article we have received 100 P values, 5 of which where around 5% (""significant"").  The Null hypothesis would be different (""risk factor i is not effecting the disease"").  But still if we won't correct for multiplicity, we will get E(x)=5 wrong ""discoveries"".  So unless I'm missing something, relying on the null hypothesis, IMHO, won't be enough.",2010-11-05T04:25:51.417,253,CC BY-SA 2.5,
6249,4194,1,"@Tristan of course. I am, however, talking about gibbs sampling.",2010-11-05T06:22:08.597,1760,CC BY-SA 2.5,
6250,4209,0,"for some reason, I don't find your answer entirely convincing. I do agree that the distribution needs to be ""stretched"", and that it's not correct to say it converges to X before it converges to a normal. That would be a mistake on my part. Still I think there should exist some way to scale the distribution such that only the fourth order and above ""moments"" go towards zero. I need to think a little harder as to what exactly that scaling factor thing would look like, if such a thing were to exist",2010-11-05T06:27:53.097,1760,CC BY-SA 2.5,
6251,4206,0,I think you're right though I'm having trouble understanding your notation - dp for variance looks odd and adding a variance to an estimate is wrong on purely dimensional grounds.,2010-11-05T07:20:15.263,449,CC BY-SA 2.5,
6252,4214,1,"Well, I was rather thinking of meta-analysis as a tool to pool measures of association (or ESs) across several studies, in a *retrospective* way, which means working with at least a common risk factor (most of the time adjusted for potential confounders). So your use of ""meta-analysis"" encompasses a much broader way of combining results from different independent studies (IMO).",2010-11-05T08:36:06.087,930,CC BY-SA 2.5,
6254,4212,0,"(so confused, should have search myself!) Thanks for that.",2010-11-05T08:45:32.907,930,CC BY-SA 2.5,
6255,4228,0,Would you agree that a linear combination of some variables can still be interpreted as reflecting some kind of a weighted contribution of each of them to the factor axis?,2010-11-05T08:48:28.820,930,CC BY-SA 2.5,
6256,4228,0,"Yes, that is exactly it.",2010-11-05T09:39:35.777,1808,CC BY-SA 2.5,
6257,4206,0,"Indeed, it's not variation, is it standard deviation? [apologies, I'm not accustomed to this terminology]. I mean, if p is gaussian, then the probability of having p within [p0-dp,p0+dp] is 66%. Does it make it right then?",2010-11-05T10:07:54.203,1197,CC BY-SA 2.5,
6258,4228,0,"So, why preventing from giving it a name? Variables are just considered as manifest variables, and in some cases it makes sense to consider their weighted combination as reflecting a latent (unobserved) factor.",2010-11-05T10:16:38.080,930,CC BY-SA 2.5,
6259,4233,1,"Does it predict new data with 85% accuracy, or part of the original data set that you held back for validation?",2010-11-05T11:08:07.723,656,CC BY-SA 2.5,
6260,4233,0,@richie..that rings an alarm..I didnt test it for any data. I ran a decision tree and it gave me say x false positives and y true positives than x/x+y is .156 which is misclassification..am i right then in saying that it has 84.6% accuracy ?,2010-11-05T11:24:03.837,1763,CC BY-SA 2.5,
6261,4168,0,"Just to add to what owe said I think the real essence of random variable would only come with such courses. The concepts like expected value, correlation have deep implications in statistics. As some body also said it makes you more mature to deal with statistical processes.",2010-11-05T11:28:14.690,1763,CC BY-SA 2.5,
6262,4234,0,"Yes, you're right in saying that no model of errors is incorporated in PCA, as opposed to FA. I've +1 for that particular point. Note that I said ""it makes sense to consider"", not that principal components extracted from PCA are true LVs. Unless you're interested in assessing scale reliability or measurement models, it makes little difference whether you use PCA or FA, though. Now, data analysis is often concerned with explaining correlation between variables or finding groups of subjects, hence the idea of *interpreting* one or more dimensions of the factorial space. (...)",2010-11-05T12:06:16.677,930,CC BY-SA 2.5,
6263,4234,0,"(...) The [FactoMineR](http://cran.r-project.org/web/packages/FactoMineR/index.html) includes a data set about wines, and many factor methods can be used to play with it (PCA, MFA), and even PLS or CCA as has been done by Michel Tenenhaus.",2010-11-05T12:07:57.503,930,CC BY-SA 2.5,
6264,4233,1,"Which $R^2$ measure are your referring to? If I understand you question correctly, this is logistic regression, and you may find useful information on this related question, http://stats.stackexchange.com/questions/3559/logistic-regression-which-pseudo-r-squared-measure-is-the-one-to-report-cox-s.",2010-11-05T12:14:12.800,930,CC BY-SA 2.5,
6265,4233,1,"For future: when you don't know which tag to use, add for-retag.",2010-11-05T12:16:36.703,,CC BY-SA 2.5,user88
6266,4176,0,"For any mean and variance a Gaussian has a non-zero probability of a negative value, and also a non-zero value of a non-integer value. Both these are not appropriate in this situation (what would it mean for the -2.7th trial to fail?).",2010-11-05T12:22:31.580,247,CC BY-SA 2.5,
6267,4231,1,"*""we select variables which are most statistically significant...""* No we don't! That's really not a good way to do model selection.",2010-11-05T13:06:21.483,449,CC BY-SA 2.5,
6268,4196,0,"@whuber: thanks for the socratic approach :) Right now, I am working on proving that a new process is equivalent to an old one (see another post I made) so (a) is what I'm interested in. Am I correct in thinking that had I been in case (b), I could consider the time series to be independent ? I'm curious to know what (c) is about ? Prediction based on my current measurements ?",2010-11-05T13:13:14.183,1784,CC BY-SA 2.5,
6269,4172,0,chl's additiion of the reliability tag helped me find a similar question here : http://stats.stackexchange.com/questions/859/anova-with-non-independent-observations,2010-11-05T13:26:28.667,1784,CC BY-SA 2.5,
6270,4233,4,Isn't it $R^2$ rather than $r^2$ ?,2010-11-05T13:41:18.783,334,CC BY-SA 2.5,
6271,4225,0,Could you provide a link to the Welch paradox?,2010-11-05T14:05:35.487,,CC BY-SA 2.5,user28
6272,4217,1,"@Tal But, your null hypothesis is ""None of the risk factors impact disease"". This is evident when you talk about wrong discoveries. Thus, you are using the same null hypothesis when you are performing these multiple comparisons and hence the need for control. If on the other hand your null was: ""Age has no impact on disease"" then you would not be interested in a study that did not have age as a covariate and hence your multiple comparisons would be restricted to the studies where age was used as a covariate. Therefore, it seems to me that my original point is still valid in your example.",2010-11-05T14:37:37.020,,CC BY-SA 2.5,user28
6273,4234,0,"@chl,Thanks for the hint as to the package, i'll check that out. On PCA vs FA i agree up to a point. I prefer FA for most applications, as i fund the communalities (the common variance) estimates to be very useful in assessing the worth of a particular factor structure. That may just be a personal preference, however.",2010-11-05T14:54:51.577,656,CC BY-SA 2.5,
6274,4243,1,"(+1) That sounds good. Thanks for sharing your experience with IRT modeling and FA. Apart from the graphics functionalities, the conditional approach in eRm is more in line with the initial thinking of theta by Rasch (as a fixed parameter).",2010-11-05T15:03:27.423,930,CC BY-SA 2.5,
6275,4246,0,"Wow, that would be really useless then :D Can't people see that the means are at the edges already?",2010-11-05T15:03:39.640,1320,CC BY-SA 2.5,
6276,4246,2,"@JCL Perhaps, they can if they see the table. But, what if there is no table? Then a table can be re-constructed to some extent using the text which refers to such things as 'grand means', 'marginal means' etc. I am guessing the phrase 'marginal mean' is a literary convention to signal what we are talking about. In a similar vein you could argue that why use the term 'sample average' when 'average' is sufficient. Qualifying words such as 'sample', 'marginal' add some context.",2010-11-05T15:07:31.960,,CC BY-SA 2.5,user28
6277,4234,0,"You're totally right (I already upvoted your earlier response because it was made very clear). It's just that (unrotated) PCA has its own history in data analysis (esp. the French school), together with CA, MFA, MCA. On the other hand, Paul Kline has two very nice books on the use of FA in Personality research. And the upcoming book of William Revelle should rock for R users :) Well, in any case, I think we agree that these are useful tools to analyse the structure of a correlation matrix.",2010-11-05T15:12:53.823,930,CC BY-SA 2.5,
6278,4246,3,"@Skrikant (+1) I think the term now extends beyond contingency table since we also speak of a marginal (vs. conditional or subject-specific) approach, e.g. in mixed-effects model. So I think, the distinction has to be made between computing a marginal statistic or working with conditional values (this applies for an ANOVA table as for a contingency table).",2010-11-05T15:18:44.163,930,CC BY-SA 2.5,
6279,4221,17,"The NIST is usually authoritative, but here it is technically incorrect (and ungrammatical to boot): having a probability defined at ""an infinite number of points"" does not imply the ""probability at a single point is always zero.""  Of course they're just dodging a distraction about infinite cardinalities, but the reasoning here is misleading.  It would be better for them just to omit the first sentence in the quotation.",2010-11-05T15:20:00.553,919,CC BY-SA 2.5,
6282,2723,8,I learnt this one the hard way. Twice.,2010-11-05T16:38:57.007,449,CC BY-SA 2.5,
6283,4249,0,I agree. (2) is a reason for conditioning on an ancilliary statistic.,2010-11-05T16:50:41.907,449,CC BY-SA 2.5,
6284,4225,0,"What's paradoxical about the Welch example?  There's no contradiction in knowing \theta for sure.  The classical statistician's explanation in this case might go, ""I generated an interval using a method having *at least* a 99% chance of  covering the true parameter (*a priori*).  In this case, we *know* (*ex post facto*) that it indeed covers the parameter.  Be happy!""",2010-11-05T16:56:05.740,919,CC BY-SA 2.5,
6285,3997,0,"yeah, if you notice I already posted the solution below a few days ago.",2010-11-05T17:16:50.320,1716,CC BY-SA 2.5,
6287,4225,0,"@whuber The paradox is about the CI only answering questions about repetitions, and not answering any questions for an observed data set. In the given data set ($X_1=0 and X_2=1$), we know with 100% prob that the $\theta$ lies between them. 

I agree with your comment. I guess I am not doing a great job of explaining, please see this link for what I am trying to say: http://books.google.com/books?id=LhiroD1D72oC&pg=PA37&lpg=PA37&dq=welch's+paradox,+pratt+1961&source=bl&ots=Un-jc8ucEC&sig=EQxlfFL2uh_N61-cSXc61RKrRbU&hl=en&ei=WEbUTLb3McL7lwf3qNGiBQ&sa=X&oi=book_result&ct=result&resnum=2&sqi=2&ve",2010-11-05T18:09:16.280,1307,CC BY-SA 2.5,
6290,4256,0,"Those matrices are not equal, here's the covariance matrix http://yaroslavvb.com/upload/multinomial-covariance-matrix.png",2010-11-05T19:31:36.890,511,CC BY-SA 2.5,
6292,4256,0,"Yes, this is indeed the covariance matrix.  Dropping any ith column and row results in the same normalization term for the Gaussian was my point.  Perhaps I'm missing something obvious?",2010-11-05T19:41:48.793,1835,CC BY-SA 2.5,
6293,4249,0,"@onestop and @whuber I also agree about conditioning on ancillary statistic. But please let me know the fallacy in my line of thinking: A classical statistician will think that a datum (from the sample space) shouldn't trouble her (even though it doesn't look that it's wrong) as the CI is for repetitions and not for that particular datum. 

As whuber points out, a practical statistician would choose better CIs, but the point is more philosophical in my case.",2010-11-05T19:42:41.003,1307,CC BY-SA 2.5,
6295,4249,2,"@suncoolsu It might not look like it, but I did give an answer in a philosophical spirit!  The point is that there is no problem or paradox with someone noting that the confidence actually is 100% *in this particular case* when they used a procedure designed to assure *at least* 95% confidence in *any possible situation.*  In fact, something like this happens all the time with discrete data: in selecting a 95% CI procedure it may happen that the actual confidence is, say, *always* 97% or greater.  A careful statistician would recognize and report that fact.",2010-11-05T19:55:45.717,919,CC BY-SA 2.5,
6296,4254,0,"@Mehper In the last example, which three observations would you remove?  Obviously the 2 and the 121, but what else?  To remain unbiased, it seems you must remove *half* of the 4 and *half* of the 105 for a trimmed mean of (4/2 + 6 + 7 + 11 + 21 + 81 + 90 + 105/2)/7 = 34.64",2010-11-05T19:59:27.507,919,CC BY-SA 2.5,
6297,4256,0,"Ah...didn't notice the determinant sign. Hm...they do seem to be equal on some examples I tried, is there a simple proof of this? Eigenvalues are not equal however. The motivation for question was to find out whether central limit theorem gives you the same approximation error for finite $n$ regardless of which multinomial dist. component you drop",2010-11-05T20:06:16.977,511,CC BY-SA 2.5,
6302,4256,0,Probably the easiest way to convince yourself is that $p_i=1-\sum_{j\ne i}p_j$ and plug that in for $p_i$ in $S$.,2010-11-05T20:33:44.140,1835,CC BY-SA 2.5,
6303,4254,0,"@Mehper: just FYI, you can format maths by writing TeX expression in between $ signs. E.g. `$X_i$`",2010-11-05T20:33:47.623,582,CC BY-SA 2.5,
6304,4256,0,"BTW, I like your application of this idea--hence my interest in responding.",2010-11-05T20:34:47.243,1835,CC BY-SA 2.5,
6305,4254,0,"@whuber: Thanks for your comment, I've added your comment to the answer; @nico: Thanks for letting me know about TeX formatting. I tried to update the answer using TeX format but I couldn't manage it well. Could you please give me a link which explains how to use TeX style in posts? I have no experience in TeX.",2010-11-05T21:14:29.383,69,CC BY-SA 2.5,
6306,4140,0,"It makes sense.  I wonder whether it would help to consider the magnitudes of the local minima along with their values (where the ""magnitude"" is the amount by which its value differs from neighboring local maxima).  Regardless, start by reading about resampling statistics on Wikipedia: http://en.wikipedia.org/wiki/Resampling_%28statistics%29 .",2010-11-05T21:23:38.243,919,CC BY-SA 2.5,
6307,4254,1,"@Mehper: Google ""TeX Manual"" and take your pick.  I like the ""gentle introduction"" because it contains useful, readable tables: http://www.tex.ac.uk/tex-archive/info/gentle/gentle.pdf",2010-11-05T21:25:37.273,919,CC BY-SA 2.5,
6308,4254,1,"@Mepher: sure, here you go! http://www.mathjax.org/help/user/ (note that if you right click on any math formula you'll have a context menu linking to that page). You can also use MathML instead of TeX (if you're very brave :P).",2010-11-05T21:32:29.333,582,CC BY-SA 2.5,
6311,4261,0,"I could suggest ""Randomization, Bootstrap and Monte Carlo Methods in Biology"" by Manly. http://www.amazon.com/Randomization-Bootstrap-Methods-Biology-Statistical/dp/1584885416/ref=sr_1_1?ie=UTF8&qid=1288995733&sr=8-1",2010-11-05T22:29:01.110,582,CC BY-SA 2.5,
6312,4259,4,"Hi Nick - welcome to CV.  Your question is very broad; you might have better luck getting good answers if you broke it up into smaller questions (and once you do, you may find that some of them have already been answered here).  At a minimum, though, you ought to mark your question as ""community wiki"".  That basically means that instead of the usual competing-answers format here, all of the answers as a whole will be considered The Answer.",2010-11-05T22:49:57.433,71,CC BY-SA 2.5,
6313,4252,1,Should this be tagged trimmed instead of truncated?,2010-11-05T23:57:39.017,,CC BY-SA 2.5,user28
6314,4259,1,@Matt The CW check-box no longer appears for a question. A mod will need to mark a question as CW as needed.,2010-11-06T00:06:22.780,,CC BY-SA 2.5,user28
6315,4099,1,"Please clarify the question. In particular, these were some comments from before: By @chl - ""you should consider writing clear questions (they are interesting on their own), with one definitive issue, and reserve comments for additional infos relevant to your original question, not follow-up"". By @shane - ""Regarding this current question: it could also be improved because it's asked many different questions without a clear common thread. Are you interested in multicollinearity in general? Or are you interested in VIF? It would be better to break these out for clarity.""",2010-11-06T00:11:08.707,,CC BY-SA 2.5,user28
6316,4265,0,RRDTool can be found here: http://www.mrtg.org/rrdtool/index.en.html,2010-11-06T00:37:37.940,118,CC BY-SA 2.5,
6317,4266,0,"Technically, the problem is that singular covariance matrix means that some subset of variables is perfectly correlated, so probability density should be exactly 0 in some areas, but that's not possible with a Gaussian. One solution is to instead look at conditional density, conditioned on the fact that random variable lies in a feasible region. This looks like what they are doing in the link. Never heard the term ""G- inverse"", I'm guessing it's Penrose-Moore pseudo-inverse?",2010-11-06T01:42:19.313,511,CC BY-SA 2.5,
6318,4250,0,"A good quote from a UMich document: ""The marginal probability (of A) is obtained by summing all the joint probabilities. Marginal probability can be used whether the events are dependent or independent. If the events are independent then the marginal probability is simplified to simply the probability. The following example will clarify this computation.""",2010-11-06T01:43:38.307,1764,CC BY-SA 2.5,
6319,4266,0,"While it's true that a conventional d-dimensional Gaussian has support on all of $\Re^d$, the singular Gaussian does not.  G-inverse is generalized inverse, and yes, I believe that the Penrose-Moore definition works here.  I think that there's a CLT for singular covariances, stating as expected, convergence in distribution to the singular CLT, although I can't find a ref right now.",2010-11-06T02:30:37.003,1860,CC BY-SA 2.5,
6320,1830,0,"@srikant..the word overfitting as a word suggests me that something is ""overly"" done meaning ..more than required..so whats overly done ? Is it no of variables ? I am sorry I have been hearing the term overfitting many times and would take this oppurtunity to cleare this with you.",2010-11-06T05:35:42.380,1763,CC BY-SA 2.5,
6321,4259,0,"@Nick..I am new as well. I think a general thing and the foremost thing that one needs to keep in thing is how do you want to describe your output variable..is it continious, is it binary ? Because at the end of day you want to observe/model an output variable. Next thing I would think is what are the ways possible to model the required variable..things that would come then is that if the variable is dichotomous the procedure is logit model..The next consideration would be then data, its nitty gritty and the various issues one encounters..Hope this makes sense.",2010-11-06T06:16:17.643,1763,CC BY-SA 2.5,
6323,4250,0,"@JCL i admit i was experimenting with some more slightly advanced (but mathematically correct) TeX. it renders fine for me in Firefox on Windows XP, and in Internet Explorer on Windows XP after a refresh. It doesn't render in IE on Windows Mobile 6, but then no TeX on this site does. I have Ubuntu on a USB drive so i'll be back after a quick reboot...",2010-11-06T08:55:51.780,449,CC BY-SA 2.5,
6325,4250,0,@JCL Renders fine for me in Chrome (v 5.0.375.125) on Ubuntu 10.04 running from a USB drive on my laptop.,2010-11-06T09:06:51.530,449,CC BY-SA 2.5,
6327,4268,2,"I would say ""the"" textbook myself, with Stewart's *Matrix Algorithms* ( [both](http://books.google.com/books?id=XHOQ_HU-85IC) [parts](http://books.google.com/books?id=FtC9rHW7ORMC)) a close second. I would give a list of the pioneering papers myself, but the OP really should explain if he wants the numerics viewpoint or the stats viewpoint (I can help with the former, but not so much the latter).",2010-11-06T09:23:01.200,830,CC BY-SA 2.5,
6328,4231,0,@onestop..can you please elaborate on this ? whats wrong with selecting the variables which are most statistically significant.,2010-11-06T09:34:04.070,1763,CC BY-SA 2.5,
6329,3495,0,-1 because this algorithm has been called out as erroneous but no modification to the response has been made to reflect that.,2010-11-06T15:01:26.813,919,CC BY-SA 2.5,
6330,3434,2,"I'm missing something: if you have to ""maintain the entire vector in memory,"" how does this qualify as an ""online"" algorithm??",2010-11-06T15:03:17.920,919,CC BY-SA 2.5,
6331,3389,0,"That's an interesting idea.  You could supplement it, perhaps, with online detection of outliers and use those to modify the estimate as you go along.",2010-11-06T15:05:07.527,919,CC BY-SA 2.5,
6333,4271,4,"Chen seems to focus on larger datasets, which makes sense because the fourth and sixth and higher moments involved in these tests are going to take some time to settle down to asymptotic levels.  But distributional tests are typically used for datasets smaller than 250 values (the minimum studied in this paper).  In fact, most of them become so powerful with larger amounts of data that they are little more than afterthoughts in such applications.  Or is there more going on here than I am seeing?",2010-11-06T15:18:43.720,919,CC BY-SA 2.5,
6334,4269,0,"This solution ignores the structure of the experiment: it treats all subjects as identical.  If there is any possibility they are not, then at a minimum you should test for differences among their means.",2010-11-06T15:22:09.613,919,CC BY-SA 2.5,
6335,4255,0,rattle requires XML which is not supported for Windows (and unavailable in a Windows binary) :-(.  http://cran.r-project.org/web/packages/XML/index.html,2010-11-06T15:38:37.923,919,CC BY-SA 2.5,
6336,4044,1,"Dunno why, but a Mantel test flashed in front of my eyes when I read your post.",2010-11-06T15:43:45.790,144,CC BY-SA 2.5,
6337,4269,0,By all means suggest an alternative but 1) given the hypothesis I think that it is not unreasonable to expect all subjects to score at chance (the null) and 2) I reread the paper I used as the inspiration and they used a one-sample t-test in this way.,2010-11-06T16:00:37.443,1614,CC BY-SA 2.5,
6338,4047,3,can one calculate the Kullback-Leibler divergence of points without making an assumption of the underlying probability density the points came from ?,2010-11-06T16:22:58.887,961,CC BY-SA 2.5,
6339,4255,0,@whuber: too bad! it's quite a neat package,2010-11-06T17:08:45.367,582,CC BY-SA 2.5,
6340,4231,0,Take a look at http://en.wikipedia.org/wiki/Stepwise_regression#Criticism,2010-11-06T17:24:16.900,449,CC BY-SA 2.5,
6341,4252,0,I'd say either http://en.wikipedia.org/wiki/Truncated_mean will do.,2010-11-06T19:11:35.840,1833,CC BY-SA 2.5,
6342,4268,1,"+1 for Golub and Van Loan. And, yes, the definitive article is appropriate.",2010-11-06T19:42:24.210,795,CC BY-SA 2.5,
6343,4215,1,"as it turns out, for most weighting schemes, only the first few $w_i$ are non-zero, which should also simplify the computation.",2010-11-06T19:54:17.677,795,CC BY-SA 2.5,
6344,4268,2,"I edited my question to clarify that I am focusing on the statistics part.

I agree with everyone that Golub and Van Loan is the standard reference for matrix decompositions. But it's omitting the topic of very large scale decomposition through random projections. A survey paper I would put in my list is ""Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions"" by Halko et al.",2010-11-06T20:16:37.943,30,CC BY-SA 2.5,
6345,4276,1,How frequently is the sensor capturing its readings?  These graphs look like aliasing patterns where the gait frequency and the sensor frequency are only slightly different.,2010-11-06T21:21:18.940,919,CC BY-SA 2.5,
6346,4276,0,Data is being sampled at 10 acceleration readings/second.,2010-11-06T21:30:00.930,1224,CC BY-SA 2.5,
6349,4222,1,"Hey, thanks! I think that point about not needing the norming constant for metropolis-hastings is exactly the information I needed to make sense of all of this.  I think, because the GS in WinBUGS stands for gibbs sampling, I was under the impression that gibbs superseded M-H and that the software was using gibbs exclusively.",2010-11-07T01:54:35.807,1795,CC BY-SA 2.5,
6352,4274,1,"What if I have lots of observations with relatively few variables, but a very low signal-to-noise ratio? So low, in fact, that overfitting is a very real problem. Would regularization be a sensible thing to try and look at to improve predictive accuracy?",2010-11-07T10:50:45.730,439,CC BY-SA 2.5,
6353,1726,6,"This quote seems to be known only in Germany and there is doubt that it is authentic, see the link below where the State Office of Statistics in Baden-W√ºrttemberg show results of their research about this quote (sorry its only available in German). The Times, e.g., said that they never heard about it.
http://www.statistik.baden-wuerttemberg.de/Veroeffentl/Monatshefte/essay.asp?xYear=2004&xMonth=11&eNr=11",2010-11-07T10:58:27.260,1573,CC BY-SA 2.5,
6354,4274,1,"@aix It depends on what you actually call few variables, and what kind of variables you are dealing with. But I think a ridge approach is to be preferred in your case. You can also look at [Boosting Ridge Regression](http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper418.pdf) (Tutz & Binder, 2005). Penalized ML estimation was also proposed as a built-in method to prevent from overfitting; see e.g., Penalized Maximum Likelihood Estimation to predict binary outcomes: Moons KG, Donders AR, Steyerberg EW, Harrell FE. *J. Clin. Epidemiol.* 2004, 57(12): 1262‚Äì70.",2010-11-07T12:03:58.450,930,CC BY-SA 2.5,
6355,4257,0,how do you calculate the Chi square p value for the whole model ? Forgive my ignorance.,2010-11-07T15:25:12.310,1763,CC BY-SA 2.5,
6356,4209,2,"@gabgoh I would like to hear more about which aspect(s) of the answer are weak. As far as scaling goes, you're stuck: you have already used up that possibility in standardizing the elements of the sequence. If (hypothetically) some form of scaling would keep the third moments from going to zero, then you would contradict the CLT because the limiting distribution would not be Normal. There's a related issue with asymptotics of estimators. Often you *can* adjust an estimator to kill higher moments asymptotically (e.g., with bootstrapping): but this still cannot be done by scaling alone.",2010-11-07T16:54:07.813,919,CC BY-SA 2.5,
6358,4235,3,"+1. It sounds like your ""negative probabilities"" are just signed measures, right?",2010-11-07T16:56:34.283,919,CC BY-SA 2.5,
6359,4223,2,"I note that the example given on that wiki page uses probability densities in lieu of actual probabilities for the calculation of posteriors, presumably because the per unit aspect is not necessary for comparative purposes if the units being compared are the same. Extending this, if one doesn't want to assume normality but instead one has empirical data from which density can be estimated, e.g. a kernel density estimate, would it be valid to use a reading at a given value on the x-axis from this kde as input to calculating posteriors in a naive bayes classifier, assuming equal per units?",2010-11-07T17:08:00.227,226,CC BY-SA 2.5,
6360,4223,3,"@babelproofreader I believe the posteriors are Bayesian updates, via the training data, of priors.  It's unclear how a kde could be construed similarly, but I'm no expert in this area.  Your question is interesting enough that you might consider posting it separately.",2010-11-07T17:23:25.873,919,CC BY-SA 2.5,
6361,4289,0,"Although not exactly the answer to my question, I am selecting since it gives pointers to techniques I didn't know and seem relevant. I know Jolliffe's book well, and it has nothing to that effect.",2010-11-07T17:25:21.570,30,CC BY-SA 2.5,
6363,4292,0,"@mbq, thanks I wasn't exactly sure how to title it!",2010-11-07T17:48:53.547,776,CC BY-SA 2.5,
6364,4292,1,"Because this question appears to have nothing to do with R I have removed the ""r"" tag.",2010-11-07T17:50:57.070,919,CC BY-SA 2.5,
6365,4292,1,"Please add to your question the N, and also what you predicted before you obtained these data.  If you predicted that later would always be greater than now that's very different from predicting that there would be 'some effect' of time.",2010-11-07T17:59:29.630,601,CC BY-SA 2.5,
6366,4292,0,"@whuber, I chose the R tag because a solution using R would be incredibly useful.",2010-11-07T17:59:55.827,776,CC BY-SA 2.5,
6367,4293,0,"yes, I did expect this pattern as it represents a well researched natural decline in a product life cycle. In this scenario the largest category represents the company that was first to market. As we would expect a larger and larger proportion of future expenditure is going to competitors with a smaller share going to the first to market company.",2010-11-07T18:04:04.450,776,CC BY-SA 2.5,
6368,4292,0,"@John, yes, it was expected please see the comment to @whuber in his answer below. The sample size has also been added to the description.",2010-11-07T18:06:04.430,776,CC BY-SA 2.5,
6370,4293,0,that understood how would I implement this test using R?,2010-11-07T18:38:31.737,776,CC BY-SA 2.5,
6371,4257,1,"@ayush. I describe the calculation in the third paragraph.  In terms of actually calculating the resulting Chi Square p value, you can use an Excel formula.  If I remember correctly it is either CHIDIST or CHITEST.",2010-11-07T18:59:44.297,1329,CC BY-SA 2.5,
6372,4293,2,"@Brandon I am glad to hear that this is not a post-hoc test.  You don't need R to implement it: the second paragraph of my response contains all the calculations needed!  In effect, this well-researched pattern would have a 1/512 chance of occurring by chance in the case that all nine possibilities (increase/decrease) were independent and equally likely.  Because 1/512 is so small (less than the standard threshold of 1/20), you have a valid claim that it is evidence that the pattern is not due to chance.",2010-11-07T19:08:59.890,919,CC BY-SA 2.5,
6373,4292,2,Sample size informs us about the *power* of any test but it is not needed to conduct a valid test.,2010-11-07T19:09:46.473,919,CC BY-SA 2.5,
6374,4235,2,"Thanks.  Yes, exactly right, mine are.  Those of Khrennikov mentioned in the Haug are entirely different, though, they are limits of relative frequencies in some sort of p-adic topology.  Wild, crazy stuff.",2010-11-07T19:17:21.423,,CC BY-SA 2.5,user1108
6376,4293,0,"I should clarify that saying this movement or change in share of wallet is expected from secondary/tertiary/etc market entrants. As time passes the second/third/etc gain larger share or wallet and also take up some of the first entrants share of wallet. By well-researched, I meant that this phenomenon is well researched not that this specific instance or usage case is well researched beyond what I'm attempting to show right now.",2010-11-07T19:47:46.440,776,CC BY-SA 2.5,
6377,4295,0,"Thanks John, I guess what I'm trying to get at here is if the market leader has reached their potential or saturation point. The other market entrants are continuing to increase their share of wallet whereas the first to market's share of wallet is apparently decreasing or staying the same. To be able to say this with any confidence has substantial marketing and strategic consequences.",2010-11-07T19:52:55.013,776,CC BY-SA 2.5,
6378,4147,0,I'm confused; doesn't this only give the power of the t.test?,2010-11-07T21:35:59.817,196,CC BY-SA 2.5,
6379,4147,0,@drknexus Of course it gives the power of the t-test only. whuber pointed out the impact of grouping data into sets and gave the output for certain specific situations. I simply coded up his simulation in R which demonstrates the point he made vis-a-vis the t-test.,2010-11-07T21:38:33.343,,CC BY-SA 2.5,user28
6380,4299,2,"Right.  Rewriting the x3=1 case as E[y] = B0' + B1'x1 + B2'x2 shows that we're really fitting two models (with a common error variance): one for non-smokers with parameters B0, B1, B2 and another for smokers with parameters B0', B1', B2'.  ""Parallelism"" means B1 = B1' and B2 = B2' (which says, geometrically, that plots of the fits are parallel planes).  In terms of the original variables these hypotheses are B4 = 0 and B5 = 0, exactly as Srikant has proposed.",2010-11-07T21:42:33.850,919,CC BY-SA 2.5,
6382,4299,0,"@Srikant, @whuber - Yeah this is what I was leaning towards (doing the `B1=B1'` and `B2=B2'` but wasn't sure where to go, but now I've got a good idea. Thank you both.",2010-11-07T21:48:11.317,1894,CC BY-SA 2.5,
6383,4287,9,"I'm having trouble understanding the equation; I can't find any way to justify it. Even a basic units analysis identifies problems. Suppose $y$ is measured in parsecs and $x$ in drams, for example, so that $f$ and its estimator (is that what the little dot over the $f$ means?) are also in parsecs. Then the lhs and $\sigma^2$ are squared parsecs; the middle term on the rhs is the square of a difference between a parsec ($f(x)$) and parsecs per dram (due to the division by $x$); and the last term on the rhs is squared parsecs per dram. It's not valid to add any of these terms to one another!",2010-11-07T21:58:10.927,919,CC BY-SA 2.5,
6385,4276,5,"Ah!  So the downward spikes are where a foot hits the ground and the rest is the ""jiggling"" in between.  If that's the case, even a crummy low-pass filter should do the job.  (http://en.wikipedia.org/wiki/Low-pass_filter )  *E.g.*, fdesign.lowpass (http://www.mathworks.com/help/toolbox/filterdesign/ref/fdesign.lowpass.html ).",2010-11-07T22:08:15.143,919,CC BY-SA 2.5,
6386,4287,0,the equation's fine (the little greek letters in the numerator are *not* 'x' but 'kappa'). Try this: begin w/ a formula for SSE that you are comfortable with and just a few steps will get you to the one above.,2010-11-08T00:49:50.667,438,CC BY-SA 2.5,
6387,4147,0,Ah; okay.  It doesn't compare the powers via simulation (as the question asked) then.  But the code you made was easy enough to modify; thanks.,2010-11-08T01:20:52.783,196,CC BY-SA 2.5,
6388,4295,1,"The value didn't go up, which is what you predicted.  The amount it went down is insignificant.  No statistical test is going to allow you to say with more confidence that the value didn't go up than the value actually going down.  Some descriptive statistics, like confidence intervals of the effects would be good.  It would be best to graph all of your values as the difference scores and a confidence interval around those difference scores.",2010-11-08T04:00:24.077,601,CC BY-SA 2.5,
6389,4295,0,(in addition to your current graph),2010-11-08T04:06:02.440,601,CC BY-SA 2.5,
6391,808,0,"Nice quote, though I can't help imagine how it would sound being spoken by Chris Eubank...",2010-11-08T08:57:11.437,449,CC BY-SA 2.5,
6392,4302,3,"Credit: Stephen Senn, *Statistical Issues in Drug Development*, page4.
http://media.wiley.com/product_data/excerpt/71/04700187/0470018771.pdf",2010-11-08T09:03:41.133,449,CC BY-SA 2.5,
6393,4303,0,"From what I can understand, it's not clear what is the response variable and the explanatory variable. Can you please clarify this?",2010-11-08T09:25:32.883,1307,CC BY-SA 2.5,
6394,4305,0,Do you have a three measurements on one individual or three measurements on three individuals?,2010-11-08T12:08:50.470,8,CC BY-SA 2.5,
6395,4309,0,"The size of population is important, for sure. Thus, for the  blood pressure example three individuals may be unrepresentative. My case is different, there is no a problem of sample size. Imagine you have a sample of fixed size (it is not possible to include more individuals) and you want to assess the effects of a drug on blood pressure. Would you realize the experiment only once or several times? I think that it should be replicated at least two (or three) times in order to consider the individual variability of each subject.",2010-11-08T12:56:57.193,221,CC BY-SA 2.5,
6396,4305,0,"I have three measurements in 20 individuals belonging to a small population of about 100 individuals. But my question seeks a more general rule. Csgillespie and you pointed out that the sample size is important when deciding whether or not the experiment should be replicated and how many times. However, it is not always possible to have a sufficiently large sample size, especially if you work with animals.",2010-11-08T13:06:46.073,221,CC BY-SA 2.5,
6397,4256,0,"@Ian Interesting, I hadn't heard of the singular Normal.  Still though, in this example I don't think this distribution is necessary, because as you said (and I implied), one of the events is degenerate.  Hence, you can lop off any row/col.",2010-11-08T13:17:14.657,1835,CC BY-SA 2.5,
6398,4309,3,"@Manuel You are correct; you need to assess individual variability.  The number of repeated measurements needed depends on the size of that variability, how the variability translates to uncertainty in the inferences about the population, the cost of repeating the measurements, practical constraints such as the time needed for replication, and (perhaps) technical issues like the possibility of positive temporal correlation among the replicate measurements.",2010-11-08T13:30:47.913,919,CC BY-SA 2.5,
6399,4295,2,"@Brandon A subtler point revealed by this conversation is that the effects are not independent: they are linked by the size of the market.  Thus, *given* that the market size is practically unchanged, a decrease in the leader's share *necessarily* leads to increases in the shares of at least one competitor.  This calls for a different model and a different test, as suggested by @John's comments: it would suffice to demonstrate that the leader's decrease is unlikely to be due to random market fluctuations or sampling error.  A better model would also account for changes in market size.",2010-11-08T13:38:36.640,919,CC BY-SA 2.5,
6400,4309,0,"So, the idea could be to replicate the experiment two or three times, evaluate the variability of the estimates and based on this variability decide whether more replicates will be necesary.",2010-11-08T13:39:29.790,221,CC BY-SA 2.5,
6401,1996,1,"Google finds 12.3 million hits for this quotation.  After running down the first eight pages of them, I haven't found a single site that actually gives a source--they all seem to be quoting each other rather than Mr. Twain.  Does anyone know where he wrote this?  Or maybe it's apocryphal.",2010-11-08T14:05:51.817,919,CC BY-SA 2.5,
6402,854,0,Now that I think of it: agreed!,2010-11-08T14:38:14.420,506,CC BY-SA 2.5,
6403,4138,0,"@GJ Kerns: l is fixed but arbitrary, question updated",2010-11-08T14:53:39.793,767,CC BY-SA 2.5,
6404,4138,0,@girard: I am most interested in an analytical approach. $\Sigma$ can be arbitrary. Dimension can be small.,2010-11-08T14:57:21.840,767,CC BY-SA 2.5,
6405,4138,0,Without any loss of generality you can take l = 1.,2010-11-08T14:57:35.680,919,CC BY-SA 2.5,
6406,4138,0,"@whuber: true, question updated",2010-11-08T14:58:20.563,767,CC BY-SA 2.5,
6408,4319,0,Okay. That was my first ever attempt at an R function and I failed. I have added some updated information.,2010-11-08T16:17:08.767,1614,CC BY-SA 2.5,
6409,4319,0,"On page 128 of the DAAG book, the authors note that the one sample permutation test (onet.permutation) is an equivalent test to the one-sample t-test. I originally planned to use a one-sample t-test on my data, so I don't see how it is totally irrelevant. I understand that the procedure is a little unusual.",2010-11-08T16:27:16.400,1614,CC BY-SA 2.5,
6410,4321,0,Okay. I will have to go away and have a think about all that. Thanks though. If you want more information about WHY then you can read the question that is linked in the first paragraph.,2010-11-08T16:28:31.810,1614,CC BY-SA 2.5,
6411,4319,0,"You are right in that this is what the book states. I find this misleading: of course, for each generated permutation of the signs of the given empirical difference-variable, a single sample results, and the one-sample t*-statistic can be calculated. But, in order to get the whole permutation distribution of t*, you have to repeatedly permute the signs of the difference variable in the first place. This assumes there are two dependent sample to begin with.",2010-11-08T16:51:39.083,1909,CC BY-SA 2.5,
6412,4320,0,"It sounds like the `Malmig` package is not available on official CRAN package releases, so you're better looking for a source version (that has to be compiled) or another mirror.",2010-11-08T16:53:04.370,930,CC BY-SA 2.5,
6414,4312,0,P.S. The technique being described there is for a two-sample test; the approach you take with a one sample test has to be a bit different.  When the data is continuous you need to use a sign test as indicated by caracal.  I'm not sure a sign test works as expected when many of your values are 0 which is why I suggested the permutation approach I provide below.,2010-11-08T17:12:01.160,196,CC BY-SA 2.5,
6415,4324,0,"That all sounds very reasonable. Again, I will go away and think about it. Many thanks for the consideration and advice.",2010-11-08T17:13:13.270,1614,CC BY-SA 2.5,
6416,4321,0,"This research was an attempt at replicating a published result, claiming that their subjects scored above mu. It didn't claim that only some of them would do well, it claimed that there will be above chance scoring. As such this experiment tests that claim. Though I'm sure there are all manner of claims that could have been tested. Thanks for the very informative response. There is just one thing holding me back. Replication 3 had two-groups (effort vs no-effort). Is there a 2-sample binomial test? or is that another question? Sigh.",2010-11-08T17:31:42.747,1614,CC BY-SA 2.5,
6417,4319,0,"Could it not be compared to mu? If you subtract mu from the scores you are left with some positive and negative scores, and most should be at zero? I believe this is essentially what I was going to do originally with the one-sample t-test",2010-11-08T17:34:30.370,1614,CC BY-SA 2.5,
6418,4258,0,"although not included in the original question, it would also be useful to automatically set the thinning interval as proposed in my answer.",2010-11-08T18:37:48.977,1381,CC BY-SA 2.5,
6419,4295,0,"@Whuber, actually that's not true. There is significant revenue growth in this category. I also have access to the market size changes over the last few years (all positive, and expected to continue as such)",2010-11-08T18:56:26.027,776,CC BY-SA 2.5,
6420,4295,2,"@Brandon Good enough.  But now we are aware that the validity of the analysis may depend on this fact of significant growth, indicating it's an important thing to know and to document.",2010-11-08T19:03:31.197,919,CC BY-SA 2.5,
6421,4329,0,(+1) I prefer your reference (I have it at home :).,2010-11-08T19:26:09.170,930,CC BY-SA 2.5,
6422,4311,0,I am not sure I follow the suggestion properly. How does the idea of a particle filter help here?,2010-11-08T19:50:15.913,,CC BY-SA 2.5,user28
6423,4304,2,"Thanks all, the book  Dynamic Linear Models in R, by Petris et al has high S/N ratio.",2010-11-08T20:15:20.547,1896,CC BY-SA 2.5,
6424,4331,0,"Can you provide us with some context for your data?  At this point, I'm not really sure that the uniform distribution is the right one - but it's hard to tell what a more appropriate distribution would be.",2010-11-08T20:30:37.500,71,CC BY-SA 2.5,
6426,4331,1,"More information is needed.  To check for a *uniform* distribution one would ordinarily check *all values,* not just those near a particular value.  The interval you request doesn't exist because it cannot answer questions about uniformity.  It would be better to state in plain non-statistical terms what data you have obtained and what you're trying to accomplish.",2010-11-08T20:34:26.777,919,CC BY-SA 2.5,
6428,4331,0,"Yes, there are four possible outcomes with equal probability (p=0.25). The data contains of authentication cokes which are examined for their randomness and/or patterns in it. I'm interested to see how often each possibility occurs and how far away this number is distanced from the calculated range (since the assumption already is that it is an uniform distribution with p=0.25).",2010-11-08T21:00:50.740,,CC BY-SA 2.5,Vincent
6429,4331,0,"To clarify; I want to be able to answer the following 2 questions: ""The character `c` is too rare at position `i`. It occurs only `no` times."" and the other one is ""The character `c` is too common at position `i`. It occurs more than `no` times.""",2010-11-08T21:08:36.583,,CC BY-SA 2.5,Vincent
6430,4095,0,"Thank you for all the references, especially the Judea Pearl responses (I will need to buy the book based on all the wonderful info in those short response essays.)",2010-11-08T21:09:29.780,1036,CC BY-SA 2.5,
6431,4331,0,You speak of 4 possibilities. What are those four possibilities? What is a sample for you? (A cookie string or a character in a cookie string?) I would suggest following whuber's suggestion and re-state the question without reference to statistical issues. Some sample data would also help immensely.,2010-11-08T21:34:14.050,,CC BY-SA 2.5,user28
6432,4321,1,"I may be missing something, but IMHO, permbinom() does not perform a permutation of its input (a re-arrangement of the input values), hence permtest() does not perform a permutation test. I think permbinom's random swapping of the input 1s and 0s with probability .5 gives you exactly what just calling rbinom(n, size=1, prob=.5) itself gives you: the probability for a 1 is .5 for each element, same for a 0 - regardless of the input vector. Your solution seems equivalent to the original one - it's a MC simulation with given priors. Sorry if I'm totally off base here, I'm by no means an expert.",2010-11-08T21:39:10.053,1909,CC BY-SA 2.5,
6433,4331,0,"ABCDBCDADBCDA,
BDCAADBCDADBA,
ADCDBDACDBDAD,
CDBDACDBDACDA,

That's some sample data. A sample is one cookie string!
Now I want for each position in that cookie string determine if a character there is too rare or too common at that position. So I count, for all samples, the number of A's on positon 0, the number of B's, C's and D's. Suppose I get a count of 5 A's on position 0 and I would expect a count of roughly 50 A's then the character A is too rare at position 0. 

That's what I want to do for each character position.",2010-11-08T21:47:16.940,,CC BY-SA 2.5,Vincent
6434,4320,0,Malmig is on the CRAN archive: http://cran.r-project.org/src/contrib/Archive/Malmig/. You would need to compile it yourself in order to install it.,2010-11-08T22:30:41.973,159,CC BY-SA 2.5,
6435,4276,0,Thanks for the suggestion. I'll look into the use of a low-pass filter.,2010-11-09T02:44:55.553,1224,CC BY-SA 2.5,
6437,4321,0,"I respectfully disagree with your assessment caracal.  I'll admit a permutation test in a one sample binomial case does look odd. So does a one sample continuous permutation test.  Rather than permute the values between groups the best you can do is change their outcomes to the inverse and see whether you observed something notable (which, as you note in your answer, is how a sign test works).",2010-11-09T04:13:29.830,196,CC BY-SA 2.5,
6438,4321,0,"I admit, I may be wrong, but I think that permtest accomplishes this goal and in doing so it approximates an exact test.  That is, it finds what percentage of ways of picking between these options provides more successes than the observed successes.  Like a sign test, the probability of evidence being in the other direction is .5.",2010-11-09T04:20:12.237,196,CC BY-SA 2.5,
6439,4321,0,"@R Soul:  A permutation test is actually easier to conceive of in the two group case than in the one group case.  Exchangablity in this case is between groups whereas in the one sample case it had to be directly between outcomes.  You can randomly assign the group labels (e.g. with 4 in each group it would be newgroup <- sample(c(rep(1,4),rep(2,4)),replace=FALSE).  Then get the difference between the averages of the two groups over many runs. This builds the empirical distribution you can test against.",2010-11-09T04:26:05.203,196,CC BY-SA 2.5,
6440,4319,0,"Caracal:  I misread your comment earlier; I thought you understood the role of a sign test in a single sample test.  Now, I'm not sure you do - so let me expand: a permutation test of dependent samples is the same as the permutation test of a single sample (just as a paired samples t-test is equivalent to a single sample test of the difference scores).  Thus, you can use a sign test on single sample data even though it doesn't look like the sort of permutation you are used to thinking about.",2010-11-09T04:30:36.700,196,CC BY-SA 2.5,
6441,4321,0,My above comment assumes that Experiment 3 was done between subjects and that the difference between effort and no effort is the issue at hand.,2010-11-09T04:31:56.983,196,CC BY-SA 2.5,
6442,4321,0,"@caracal: Though, I'll yield your point - doing the sign test the way I do above is equivalent to using a .5 prior - it is only in concept that it varies from an MC simulation.",2010-11-09T04:35:38.637,196,CC BY-SA 2.5,
6443,4222,3,"The term Gibbs sampling is often used to imply that you sample one parameter at a time, even if you don't use the original idea of sampling directly from the full conditionals.  All the software samples individual parameters or blocks of parameters in sequence, but the actual step type varies a lot depending on what works best.",2010-11-09T04:49:05.357,493,CC BY-SA 2.5,
6444,4317,2,"Two principles apply: You can never know whether your chain has converged to its stationary distribution.  And any test for convergence you can do manually, you can automate.  So your approach seems sound enough.",2010-11-09T05:07:47.363,493,CC BY-SA 2.5,
6445,3434,0,"@whuber No, not missing something, I guess it isn't an online algorithm.  It requires `O(n)` memory, and in the worst case takes O(n) time for each item added.  In normally distributed data (and probably other distributions) it works quite efficiently though.",2010-11-09T05:29:36.750,179,CC BY-SA 2.5,
6446,3389,0,You could probably use Welford's method for calculating the standard deviation online that I documented in my second answer.,2010-11-09T05:38:19.327,179,CC BY-SA 2.5,
6447,3495,0,"@whuber Yeah, point taken, I've been meaning to revise the algorithm itself but have been short on time.  Updated the quest to flag its problems.",2010-11-09T05:41:24.227,179,CC BY-SA 2.5,
6448,4303,0,@suncoolsu I edited the question on Mengzhen's behalf to attempt to make it clearer,2010-11-09T07:00:03.917,183,CC BY-SA 2.5,
6450,4311,0,@Srikant-vadali: Has my updated answer helped?,2010-11-09T09:34:44.230,8,CC BY-SA 2.5,
6454,4337,0,"How many cinemas are there, roughly?",2010-11-09T11:03:10.540,449,CC BY-SA 2.5,
6455,4340,0,"I'm not clear about using the daily takings, can you elaborate on that please?",2010-11-09T11:05:54.957,1922,CC BY-SA 2.5,
6457,4337,0,"Oops sorry, five, one doesn't have gaming section",2010-11-09T11:08:14.067,1922,CC BY-SA 2.5,
6458,4340,0,@endy_c Does that help?,2010-11-09T11:52:14.637,8,CC BY-SA 2.5,
6460,4287,0,What is 'kappa' in this context?,2010-11-09T12:12:08.173,,CC BY-SA 2.5,user28
6461,4311,0,I am afraid my knowledge of particle filters is close to zero! How does this method of picking a particle with probability 1/M be then used with the new likelihood to walk over the posterior for the parameters?,2010-11-09T12:14:45.460,,CC BY-SA 2.5,user28
6462,4340,0,"OK so instead of trying to eliminate bias from the movie, I can include it in the experiment, got it, thanks!",2010-11-09T12:21:23.497,1922,CC BY-SA 2.5,
6463,4345,1,(+1) Thanks for the hint about `nclass.*`. Why isn't your left-most interval left bounded (should set `include.lowest` to TRUE maybe)?,2010-11-09T13:06:09.803,930,CC BY-SA 2.5,
6466,4345,0,@chl - that's what you get when doing something quickly. Will edit the code to add `include.lowest`,2010-11-09T13:56:29.973,1390,CC BY-SA 2.5,
6469,4345,1,"A good solution is to create an interactive application that dynamically displays a graphic of the binned counts (*e.g.*, a histogram) or appropriate statistics (*e.g.*, chi-squared) in response to the user's manipulation of the cutpoints, so that this issue can be quickly explored.  This is dead easy in Excel, the OP's platform of choice, and also fairly easy in some high-end software like *Mathematica*.",2010-11-09T15:11:00.380,919,CC BY-SA 2.5,
6470,4343,0,Good start!  This solution *assigns* ranges; now you have to group them by following up your calculation with a summarize operation.,2010-11-09T15:13:00.907,919,CC BY-SA 2.5,
6471,4318,5,"+1 A breathtaking harbinger of the follies of the coming century; ""GIGO before its time.""",2010-11-09T15:17:32.290,919,CC BY-SA 2.5,
6472,4335,3,"I don't think that this question really needs to be a CW. It is borderline, but it's not too bad.",2010-11-09T15:22:59.883,8,CC BY-SA 2.5,
6473,4311,0,"Suppose my prior on $\mu$ was N(0,1). A possible proposal would be to just select values from N(0,1) and accept/reject as necessary. In a particle representation, we could generate $M$ RNs from the N(0, 1) and select a particle at random. As M increases, the particle representation becomes as exact.",2010-11-09T15:28:41.227,8,CC BY-SA 2.5,
6474,2329,0,Can you make vary A?,2010-11-09T15:32:27.320,1709,CC BY-SA 2.5,
6476,4311,0,"To be concrete, let: $f(\theta_c|-)$ be the current posterior for which I have MCMC samples. $L(y|\theta)$ be the likelihood corresponding to the new data point. Thus, the new posterior I wish to sample from is: $f(\theta_n |-) \propto f(\theta_c|-) L(y|\theta)$. Does your suggestion amount to implicitly setting the proposal density for the MH algorithm to be $f(\theta_c|-)$ and then use the particle filter approach as an approximation to $f(\theta_c|-)$?",2010-11-09T15:40:23.587,,CC BY-SA 2.5,user28
6480,4345,1,"@whuber I voted up the response although it is certainly not the response expected by the OP, but I like the point about binning and density. Interactive visualization can be made really easy in R too, at least for this kind of viz. app. The Tk-based `rpanel` package provides basic functionalities (see e.g., `rp.slider()` for an illustration with a density plot); `playwith` is a Gtk-based interactive plotting device, and is very handy. Now, I have no doubt that this should be far more easier with Excel (which is the reason why I've also +1 your answer).",2010-11-09T16:33:01.603,930,CC BY-SA 2.5,
6481,4335,0,great information! (the time series package 'xts' has similar this metadata functionality.),2010-11-09T17:38:11.687,438,CC BY-SA 2.5,
6482,4343,1,"@whuber Grouping is something I'm assuming lonut can do if he regularly works with Excel and SQL, and there are many ways to do it. In Excel, for example, he can use a PivotTable or the COUNTIF() function; in SQL he'll probably use COUNT(*) with a GROUP BY CATEGORY clause (where CATEGORY is the computed column).",2010-11-09T17:44:20.330,666,CC BY-SA 2.5,
6484,4345,0,@chl Thanks for the pointers to R interactivity tools.,2010-11-09T17:53:46.993,919,CC BY-SA 2.5,
6485,4319,0,"I think we just disagree about terminology. My narrow idea of a permutation test requires repeated re-assignment of observed units to an experimental condition consistent with the test's null hypothesis (H0) - as justified by assuming exchangeability under H0. Without different experimental conditions (pure 1 sample case), there is nothing to re-assign. You may still find the exact distribution of a certain test statistic in the 1 sample case by tabulating all possible outcomes, I just wouldn't call that ""permutations"" but ""subsets"". This is certainly unimportant. Sorry for the confusion.",2010-11-09T18:02:29.367,1909,CC BY-SA 2.5,
6486,4354,1,"I am not sure if I understand the question correctly. Are you asking if there are any distributions apart from the normal that are completely specified by the mean and the variance? In some sense, variance is a function of the mean as it is a measure of the dispersion around the mean but I guess this is not what you have in mind.",2010-11-09T18:38:02.183,,CC BY-SA 2.5,user28
6487,4354,0,you mean the sample mean $\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$ and sample variance $\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2$ are independant. Good question ! maybe projecting a gaussian random variable will keep independance ?,2010-11-09T18:58:25.730,223,CC BY-SA 2.5,
6488,4354,4,"Srikant is right.  If the question is asking about ""sample mean and variance"" then the answer is ""no"".  If the question is about population mean and variance, then the answer is yes;  David gives good examples below.",2010-11-09T19:15:28.020,,CC BY-SA 2.5,user1108
6489,4356,0,What do you precisely mean by multivariate split? Multivariate test which attribute should be used for split or split based on some function of few attributes?,2010-11-09T19:22:10.623,,CC BY-SA 2.5,user88
6493,4359,4,"@onestop  I guess it is an unfortunate artifact of my age.  It is not an understatement to say that Feller's books revolutionized how probability was done -  worldwide. A large part of our modern notation is due to him. For decades, his books were **the** probability books to study. Maybe they still should be.  BTW: I've added the title for those that haven't heard of his books.",2010-11-09T19:39:28.427,,CC BY-SA 2.5,user1108
6496,4359,1,I have aske the question about other funy characterisation ... http://stats.stackexchange.com/questions/4364/what-is-the-most-surprising-characterization-of-gaussian-distribution,2010-11-09T20:20:10.233,223,CC BY-SA 2.5,
6499,4355,8,In *any* location-scale family the mean and variance will be functionally independent in this fashion!,2010-11-09T20:29:57.317,919,CC BY-SA 2.5,
6502,4354,1,"Just to clarify, what I meant is this. For the normal distribution, the mean $\mu$ and the variance $\sigma^2$ fully characterizes the distribution and $\sigma^2$ is not a function of $\mu$. For many other distributions, this is not so. For example, for the binomial distribution, we have the mean $\pi$ and the variance $n\pi(1-\pi)$, so the variance is a function of the mean. Other examples are the gamma distribution with parameters $\theta$ (scale) and $\kappa$ (shape), where the mean is $\mu = \kappa \theta$ and the variance is $\kappa theta^2$, so the variance is actually $\mu \theta$.",2010-11-09T21:19:30.640,1934,CC BY-SA 2.5,
6503,4309,1,@Manuel: This is a very dangerous strategy and you need to be careful. You can't just carry out a few experiments and stop when you like. Basically you can't change the sample size midway through your experiment unless you are very careful.,2010-11-09T21:25:21.437,8,CC BY-SA 2.5,
6504,4355,1,"David, the double exponential is an excellent example. Thanks! I did not think of that one. The t-distribution is also a good example, but isn't E(X) = 0 and Var(X) = v/(v-2)? Or does Carlin et al. (1996) define a generalized version of the t-distribution that is shifted in its mean and scaled by sigma^2?",2010-11-09T21:30:17.757,1934,CC BY-SA 2.5,
6505,4367,1,@user1566: I formatted your equations using LaTex. Would you double check that I didn't introduce any errors?,2010-11-09T21:31:21.050,8,CC BY-SA 2.5,
6506,4359,2,"Jay, thanks for the reference to the paper by Lukacs, who nicely shows that the sampling distributions of the sample mean and variance are only independent for the normal distribution. As for second central moment, there are some distributions where it is not a function of the first moment (David gave some nice examples).",2010-11-09T21:42:33.717,1934,CC BY-SA 2.5,
6507,4355,0,"You are correct, the t-distribution appears to be frequently characterized with a mean = 0 and variance = 1, but the general pdf for t provided by Carlin and Louis explicitly includes both sigma and mu; the nu parameter accounts for the difference between the normal and the t.",2010-11-09T21:48:37.643,1381,CC BY-SA 2.5,
6508,4370,0,"ok... but I still don't understand why regressing sigma^2.... could you please elaborate on your last sentence ""An estimate of œÉ2 is given by the squared residuals as the mean of the residuals is zero"".",2010-11-09T21:52:12.803,333,CC BY-SA 2.5,
6510,4367,0,"Thanks, the problem is solved, so no longer a biggie, but, yes, everything looks OK.",2010-11-09T22:11:59.353,,CC BY-SA 2.5,user1566
6511,4365,8,That they are sum stable and that they are the unique ones with finite variance are both forced on us by the CLT.  The interesting part of this assertion is that there exist *other* sum-stable distributions!,2010-11-09T23:12:03.263,919,CC BY-SA 2.5,
6512,4370,1,"Regression is used as it tests for one form of heteroscedasticity. In other words, it tests if error variance is a function of the regressors which if true would make the error heteroscedastic. Consider the OLS residuals. An estimate of error variance is: $\sum_i(e_i - \bar{e_i})^2 / n$. But, in OLS, $\bar{e_i} = 0$. Thus, an estimate of error variance from each observation is the square of the error residual.",2010-11-10T00:33:46.250,,CC BY-SA 2.5,user28
6518,4377,1,"Is d a parameter of the logistic regression or a predictor used in logistic regression? It sounds like a predictor but you call it a parameter. If it is a predictor, then its domain is irrelevant.",2010-11-10T02:27:11.183,159,CC BY-SA 2.5,
6520,4377,0,"d is the predictor.   Thanks for correcting me, I've updated the question.",2010-11-10T02:29:26.743,1938,CC BY-SA 2.5,
6522,4372,0,Thank you for your answer. What I like about this model is that the state is directly understandable (mean and return to the mean coefficient of the mean reverting process).,2010-11-10T02:47:45.753,1709,CC BY-SA 2.5,
6525,4383,0,"Is this a validated scale, with known psychometrical properties?",2010-11-10T12:41:33.210,930,CC BY-SA 2.5,
6527,4383,0,"Hi Chris, so you are not reducing the number of items in a likert scale, but rather using less questions/items (who are measured on a likert scale).  In general it sounds like it depends on your measures.  You could check the correlation of the items you intend to take down with the ones you are keeping.  It's actually an interesting how to measure how much to remove - it might be worth to reframe the question that way (if you won't, I might do it later).  Good question :)",2010-11-10T13:13:19.140,253,CC BY-SA 2.5,
6529,4378,0,"BTW, I just took the log of d.  d can include zeros, but the logistic function maps that to zero, which is what I wanted.",2010-11-10T13:30:31.480,1938,CC BY-SA 2.5,
6530,4383,0,"Three additional questions: (1) Is this an unidimensional scale or are there several subscales, (2) What is the No. individuals and the number of items, and (3) Do you work at the level of the items, or a total or mean score?",2010-11-10T13:37:51.280,930,CC BY-SA 2.5,
6531,4384,0,This kinda worked but Excel did sort the ranges as text generating a total mess. I wasn't able to convince Excel (2010) to show the ranges in the logical order.,2010-11-10T14:29:25.027,1901,CC BY-SA 2.5,
6532,4388,0,I suppose the treatment cinema and the control cinema should be as similar as possible for this method? They are spread in 3 different cities and may play different movies.,2010-11-10T15:23:18.887,1922,CC BY-SA 2.5,
6533,4388,0,"@endy I am not sure those differences matter. The suggestion is to use *change in gaming sales* in treatment relative to control and not baseline sales as an indication of treatment effectiveness. Thus, while different cinemas, different movies etc may have different baseline gaming sales, the change in gaming sales would be a function of the presence or absence of the treatment.",2010-11-10T15:29:12.897,,CC BY-SA 2.5,user28
6534,3542,0,@Peter It will be terribly hard to get better references than those provided by @Rob and @ars. Good luck to all of you!,2010-11-10T15:36:27.323,930,CC BY-SA 2.5,
6535,4388,1,"(+1) I was going to suggest something similar. This is sometimes known as a 'difference in differences' estimator 
http://en.wikipedia.org/wiki/Difference_in_differences",2010-11-10T16:56:36.447,449,CC BY-SA 2.5,
6536,4365,1,"@whuber: indeed! this characterization is a bit contorted, and the other sum-stable distributions are perhaps more curious.",2010-11-10T16:58:10.113,795,CC BY-SA 2.5,
6537,4375,1,Why the ratio? Why not just state that the probability of that exact draw is extremely low?,2010-11-10T17:06:13.190,1036,CC BY-SA 2.5,
6543,4395,0,I fixed the original question so that it make sense.,2010-11-10T18:10:00.243,,CC BY-SA 2.5,JWS
6544,4394,2,biostat = application of statistical methodologies to biology ?,2010-11-10T18:16:20.980,223,CC BY-SA 2.5,
6545,4394,0,"Right, but there are applications of statistical methodologies in every discipline.  Why does biostatistics exist (in the US, at least) as a semi-distinct discipline?",2010-11-10T18:24:06.167,71,CC BY-SA 2.5,
6546,4396,0,I'm pretty confused by what you are asking to do. Can you describe the format of your data and why using if statements won't accomplish what you are trying to do?,2010-11-10T18:40:39.657,1036,CC BY-SA 2.5,
6547,4396,0,Hi Andy. I've updated my question to include an example. Hope it's clearer now.,2010-11-10T18:55:04.890,1950,CC BY-SA 2.5,
6548,4363,0,Actually I'm referring to splits using more than one variable.  If you read the Leo Breiman's CART book it refers to this as multi-variate splits as opposed to uni-variate where only one variable is consider.  Thanks for you answer.,2010-11-10T19:10:45.297,1929,CC BY-SA 2.5,
6549,4356,0,"This is a split that uses more than one variable.  Geometrically this means a splits along a hyperplane, rather than a plane perpendicular to one of the axis.",2010-11-10T19:13:12.003,1929,CC BY-SA 2.5,
6550,4403,0,"How do you decide whether the person's answer is 'right', once you have it?",2010-11-10T20:01:23.550,449,CC BY-SA 2.5,
6551,4363,1,"All variables are considered in `rpart`, the binary split is formed by searching over all variables and all possible split locations within each variable. `rpart` also stores information about surrogate splits which can be used when there is missing data in your variables. Does this help?",2010-11-10T20:04:31.173,1390,CC BY-SA 2.5,
6552,4406,2,"I really don't think that biostatistics and biometrics are synonyms. Biometrics includes face recognition, finger print analysis, while biostatistics involved clinical trial design and so on... Similar names only.",2010-11-10T20:14:08.933,1540,CC BY-SA 2.5,
6554,4363,0,"If you mean splits that are some combination of two or more variables, no, `rpart` doesn't handle that case.",2010-11-10T20:25:28.713,1390,CC BY-SA 2.5,
6555,3336,0,"+1 Agree with this interpretation about it looking like DIY Arellano-Bond. NB: I have found Arellano-Bond to be trustworthy only when the number of cross-sectional units is very large---as in, many hundreds.  Arellano hints as much in his articles and textbook by indicating that the consistency is in the number of cross section units, and the convergence rate is not all that fast.",2010-11-10T20:30:43.890,96,CC BY-SA 2.5,
6556,4406,0,That usage of 'biometrics' is an unfortunate neologism. See http://www.tibs.org/interior.aspx?id=290,2010-11-10T20:32:25.197,449,CC BY-SA 2.5,
6558,4406,0,"This isn't really addressing the question, however.  I know what the definition of biostatistics is, but I don't know how it differs from statistics in practice, in education, in philosophy, etc.",2010-11-10T20:52:59.150,71,CC BY-SA 2.5,
6559,4399,0,hmmm.. an auto-regressive model isn't really a good fit for a binary time series is it? Auto-regressive on time between events? I hate the idea of variance of time between events.,2010-11-10T21:09:31.483,1951,CC BY-SA 2.5,
6560,4382,14,"This is a property of the Normal distribution, to be sure, but it does not *characterize* it, because plenty of other distributions also have this property.",2010-11-10T21:18:39.097,919,CC BY-SA 2.5,
6562,4411,1,"(+1) A little bit hard to read, though (especially for someone seeking R applications).",2010-11-10T21:24:29.003,930,CC BY-SA 2.5,
6563,4404,0,"Yes, this would definitely work but you have to admit that the syntax is somewhat unwieldy. dat[ , ""WEIGHT""] is a lot less elegant compared to dat$WEIGHT.
I just wondered why there is no option to turn on the behavior above? This sounds like a very reasonable feature.",2010-11-10T22:14:51.003,1330,CC BY-SA 2.5,
6564,4404,0,"Also, on the heels of my last comment. I wonder if using attaching the data frame and then referencing the variables directly would mitigate this problem...",2010-11-10T22:17:00.863,1330,CC BY-SA 2.5,
6565,4402,0,"Unfortunately, the snippet above would not help. Making sure that all columns (variables) are present in the data frame is one thing and it would never help against accidental misspelling of a variable name in some other corner of the code.",2010-11-10T22:20:04.093,1330,CC BY-SA 2.5,
6567,4404,2,"I would definitely avoid using attach in all circumstances, but I think that's the right idea.  You can use `with()` to attach a data frame for a single command: `with(dat, weighted.mean(x, weight, na.rm = TRUE))`.  Also, many functions like weighted.mean include a `data=` option that essentially internalizes this.",2010-11-10T22:39:35.110,71,CC BY-SA 2.5,
6570,4375,5,"The assertion that a particular probability is low out of context isn't convincing. The probability that I am exactly as tall as my height (whatever that may be) is zero. And, yes, it's problematic to even define height with infinite precision, yada, yada, yada...  My point is that the maelstrom of existence churns with events of infinitesimal probability happening all the time!  10,000 out of 20,000 - out of context - doesn't surprise me at all. Regardless of what its numerical probability may be.",2010-11-11T00:12:19.400,,CC BY-SA 2.5,user1108
6571,4392,0,You should ask another question on the new issue instead of editing the existing one.,2010-11-11T01:24:38.437,,CC BY-SA 2.5,user28
6574,4365,0,"@whuber actually, I do not see how the CLT implies this fact. It only seems to tell us that _asymptotically_, the sum of normals is normal, not that any finite sum is normally distributed. Or do you have to somehow use Slutsky's theorem as well?",2010-11-11T04:20:46.027,795,CC BY-SA 2.5,
6576,4415,0,"Thanks Rob! I do want to note that I added my question comment before your answer so there is no way it was intended as an insult to your generous answer. I have 2 hesitations about variance of time between events (though I may go with it). There's the general ""does this assume too much normality."" But my bigger concern is someone who does laundry twice a month, but misses a few (maybe due to travel) will have a series like ""7,7,14,7,7,7,21"" which would have a var similar to a more frequent but less regular individual. What do you think of multiplying the std dev by the frequency to norm'ze?",2010-11-11T05:24:34.560,1951,CC BY-SA 2.5,
6578,4422,3,What is the aim of your analyses?,2010-11-11T05:38:33.487,183,CC BY-SA 2.5,
6579,4415,0,"Variance does not assume normality. It is a general measure of spread. If you prefer a more robust measure that will allow for the occasional misses, uses the difference of two percentiles. I don't understand why you might want to multiply a measure of spread by the frequency. That would not normalize it. I've updated my answer.",2010-11-11T05:45:06.423,159,CC BY-SA 2.5,
6580,4415,0,"Of course the measurement itself doesn't assume normality, but if you are using it as a comparison of inner n-tile ranges you are at least assuming the distributions are of similar forms; if not normal. But yes, that's not a big deal. If you have 2 sets of diffs {6,6,6,12,18} and {2,2,2,4,6,2,2,2,4,6,2,2,2,4,6} you get standard deviations respectively of 1.66 and 5.37. Even though their regularities are similar. But multiply by washes/week you get 3.62 and 3.91. And these values will converge as you extend your sample period. Thank you for your further update.",2010-11-11T07:16:59.540,1951,CC BY-SA 2.5,
6581,4415,0,"Multiply by the frequency is just another way of saying divide by the mean of the differences, right?",2010-11-11T07:23:45.737,1951,CC BY-SA 2.5,
6582,4415,0,"I think you were using ""frequency"" in the scientific sense (inverse of the period), whereas in time series ""frequency"" is usually a synonym of period. So my suggestion of dividing by $q_{0.5}$ will have the effect you describe.",2010-11-11T08:13:39.153,159,CC BY-SA 2.5,
6583,4422,0,"Yes, I did leave that out, didn't I. The goal is to find a predictive equation for share2 from the variables provided.",2010-11-11T08:16:58.340,776,CC BY-SA 2.5,
6586,4425,0,"Hrmmm, prcomp returns ridiculous results. PC1 accounts for 100% of variance with a SD of 20800! No rotation mind you. Let me try that again",2010-11-11T08:23:21.857,776,CC BY-SA 2.5,
6587,4430,6,"(+1) He's also working on a set of tutorials on [Advanced R development](https://github.com/hadley/devtools/wiki), very handy!",2010-11-11T08:30:10.170,930,CC BY-SA 2.5,
6588,4425,0,Eh? PCA **is** a rotation.,2010-11-11T08:43:32.317,1390,CC BY-SA 2.5,
6589,4425,1,"A problem with PCA is interpretation the final model. Say PC1 and PC2 are significant terms in the model. To explain the relationship between share2 and the original data to gain understanding on it, you need to pick apart the loadings of the variables on PC1 and PC2. This likely won't matter too much is prediction is all Brandon is interested in though.",2010-11-11T08:49:38.923,1390,CC BY-SA 2.5,
6590,4425,0,"Can't edit that comment now, but I meant scale, not rotation.",2010-11-11T08:55:38.180,776,CC BY-SA 2.5,
6591,4405,0,I can only agree with chl's answer. Perhaps it can be helpful for you to have a look at the Psychometric Models Task View at CRAN: http://cran.r-project.org/web/views/Psychometrics.html,2010-11-11T09:10:29.647,442,CC BY-SA 2.5,
6593,4425,0,@Brandon - so that was going to be my other comment on the PCA you did - I think you should standardise (`scale = TRUE`) if the variables are in different units or take different ranges.,2010-11-11T09:21:25.030,1390,CC BY-SA 2.5,
6594,4430,1,"@Jeromy In fact, it seems this is merely a way to draft his future textbook (check HW's past tweets).",2010-11-11T09:26:07.113,930,CC BY-SA 2.5,
6595,4437,1,"What is the question exactly? You want (-0.921,0.389593) as a numeric vector?",2010-11-11T09:32:52.823,930,CC BY-SA 2.5,
6596,4438,0,@mbq Or use the `colClasses` argument in `read.table()`?,2010-11-11T09:36:50.970,930,CC BY-SA 2.5,
6597,4438,1,@chl Possibly; depends on how this data look.,2010-11-11T09:38:21.157,,CC BY-SA 2.5,user88
6598,4438,0,"""This is because you have read the numbers as factors; if you use read.table, try header=T or restructure the data before read. Some sample of the file should be helpful to resolve it."" nop. ""Workaround would be to first convert factors to strings using as.character and then back to numbers with as.numeric."" nop. same result.",2010-11-11T09:40:30.043,603,CC BY-SA 2.5,
6599,4438,2,"@kwak -- remove those ""V1 V2..."" from the file before doing anything. as.numeric won't work having unconvertable elements in vector.",2010-11-11T10:02:49.937,,CC BY-SA 2.5,user88
6600,4438,0,@mbq:> that did the trick. I was expecting something else.,2010-11-11T10:19:27.483,603,CC BY-SA 2.5,
6601,4438,0,@mbq Very elegant! Cannot upvote more :( I initially thought lines with missing values were to be kept as is.,2010-11-11T10:25:44.650,930,CC BY-SA 2.5,
6602,4441,0,Are you interested in studying possible attrition effect or do you only want to assess the overall trend?,2010-11-11T10:47:03.190,930,CC BY-SA 2.5,
6603,4441,0,I am not interested in the attrition effect. Only want to assess the trend,2010-11-11T10:49:35.807,1871,CC BY-SA 2.5,
6604,4439,2,you can use data.matrix() for that http://stat.ethz.ch/R-manual/R-devel/library/base/html/data.matrix.html,2010-11-11T11:43:41.217,603,CC BY-SA 2.5,
6607,4384,0,"Do you mean that Excel doesn't recognize that your data are numerical? If you look at the cell format, is it defined as numbers? Just to make sure I understand your comment.",2010-11-11T13:16:02.163,1945,CC BY-SA 2.5,
6608,4365,3,"Adopting the usual standardization, a sum of two normals is the sum of one normal distribution X_0 plus the limiting distribution of a series X_1, X_2, ..., whence the sum is the limiting distribution of X_0, X_1, ..., which by the Lindeberg-Levy CLT is normal.",2010-11-11T13:28:30.820,919,CC BY-SA 2.5,
6609,4418,0,Maybe this should be tagged as homework?,2010-11-11T13:30:22.300,919,CC BY-SA 2.5,
6610,4443,0,"The preferred approach to answering questions is to provide one answer per question. Ideally, any new material should be integrated with an existing answer. You may want to look at this discussion on the meta site: http://meta.stats.stackexchange.com/q/635/28 FYI- the meta site is the place to go to if you need to know how the site works, have any questions about the site's functioning etc.",2010-11-11T13:56:09.593,,CC BY-SA 2.5,user28
6613,4442,0,"How would you account for the within-person correlation between item responses within such models? (I'm assuming that items are ordered by ""difficulty"")",2010-11-11T14:55:09.610,930,CC BY-SA 2.5,
6618,4454,0,"Are you interested in publishing (online or offline) or editing solutions, finally?",2010-11-11T21:05:35.700,930,CC BY-SA 2.5,
6619,4455,0,Django would be the Rails equivalent in Python; I haven't used but have heard good things about the django-survey module for it: http://code.google.com/p/django-survey/,2010-11-11T21:23:39.063,71,CC BY-SA 2.5,
6620,4455,0,"@Matt Yes, I was thinking of this kind of stuff. Put it as an answer!",2010-11-11T21:29:23.010,930,CC BY-SA 2.5,
6622,4455,0,Sorry for any confusion. I'm more interested in knowing how a survey designer would communicate a survey to a programmer for them to create,2010-11-11T21:35:32.573,305,CC BY-SA 2.5,
6623,4454,0,"I've changed the question wording to ""communicate"" to, hopefully, demonstrate what I mean a little clearer",2010-11-11T21:37:22.850,305,CC BY-SA 2.5,
6624,4454,0,"I guess the ""software"" tag is no longer needed then?",2010-11-11T21:39:07.643,930,CC BY-SA 2.5,
6625,4454,3,"In retrospect (now that I've answered), StackOverflow is probably the ideal place for this question.  Who better to answer than a bunch of programmers?",2010-11-11T22:06:18.027,71,CC BY-SA 2.5,
6626,4461,0,"How it looks visually isn't as important as the content and the rules that should be applied to a question. For instance, if question 1 should skip to question 5 if answers A or B are selected on question 1",2010-11-11T22:20:31.830,305,CC BY-SA 2.5,
6629,4462,6,Can you please work on the style and clarity of your question?,2010-11-11T23:13:46.007,,CC BY-SA 2.5,user88
6631,4465,0,"Roughly what proportion of the individuals died by the end of the experiment? If it's reasonably low there may be little advantage of survival analysis over ignoring the time to death and treating died/survived as a binary outcome, which could be considerably simpler.",2010-11-11T23:46:22.160,449,CC BY-SA 2.5,
6633,4465,0,A more complex option is frailty models - i'm too tired to attempt say any more now but i've added the 'frailty' tag above so you can click that and look at the answers to the other q with this tag.,2010-11-11T23:53:33.010,449,CC BY-SA 2.5,
6634,4459,1,You raise some interesting questions. I've posted a separate question quoting you: http://stats.stackexchange.com/questions/4466/how-to-increase-longer-term-reproducibility-of-research-particularly-using-r-and,2010-11-12T01:06:02.007,183,CC BY-SA 2.5,
6635,4443,0,"Thanks for the reminder Galit, we will keep the big picture in mind.",2010-11-12T01:14:14.843,1922,CC BY-SA 2.5,
6636,4388,0,"Sorry I'm still not clear, the movies are not just different between cinemas, the movies change too. If one cinema starts playing a new interesting movie while the others haven't, this one cinema may have bigger change in gaming sales, no?",2010-11-12T01:50:08.037,1922,CC BY-SA 2.5,
6637,4445,0,Formatting issue now fixed. It was a bug.,2010-11-12T03:27:59.160,159,CC BY-SA 2.5,
6638,4470,0,+1 Thanks for pointing that out. I somehow doubt that the implementation of weights within R is the same as that for SPSS,2010-11-12T04:06:40.907,776,CC BY-SA 2.5,
6639,4470,1,"I know SAS and SPSS are not the same, but that is all I know.",2010-11-12T04:14:23.853,1036,CC BY-SA 2.5,
6640,4471,0,Is $X^2_\alpha$ meant to be a quantile of the chi-squared distribution?,2010-11-12T05:01:00.777,159,CC BY-SA 2.5,
6641,4471,0,@Rob Hyndman yes that is what it is mean to be,2010-11-12T05:06:35.247,,CC BY-SA 2.5,John Brown
6644,4472,0,"+1 Thanks for the tip Jeromy, this would certainly help! The version I have access to is through my school PASW 18. Hopefully it also contains this functionality! Wouldn't that make life easy.",2010-11-12T06:02:45.067,776,CC BY-SA 2.5,
6645,4472,0,And it works like a charm. You're a life saver.,2010-11-12T06:20:00.333,776,CC BY-SA 2.5,
6646,4456,0,Thank you Dave. Will try this one. It seams to be an interesting software. What a pity they have not done a R package instead...,2010-11-12T07:07:03.770,1709,CC BY-SA 2.5,
6647,4474,0,"Thanks Tal! Sometimes the answer stares you in the face and you don't see it. ""Let me put my glasses on, I can't hear you""",2010-11-12T07:54:08.920,776,CC BY-SA 2.5,
6648,4474,0,You're welcome Brandon.  The 'newdata' thing is something that I failed to understand when I only started out using predict.  Cheers :)  Tal,2010-11-12T08:24:10.113,253,CC BY-SA 2.5,
6649,4473,1,"I'm not a specialist of time series and forecasting, but does it make sense to use a model which assumes uncorrelated errors in this case?",2010-11-12T09:53:38.040,930,CC BY-SA 2.5,
6651,4468,1,"(+1) I can only agree with you. About R specifically, it seems very difficult to ensure that (a) some computations will remain reproducible after updating a package (which happens to me recently), and (b) no conflict with dependencies will emerge one day (it was the case e.g., for `lme4`).",2010-11-12T10:03:50.370,930,CC BY-SA 2.5,
6652,4456,0,I recommend the excellent package 'PBSadmb' available on CRAN.,2010-11-12T10:12:30.367,1709,CC BY-SA 2.5,
6653,4465,0,"@onestop- Thanks for contributing some perspective and ideas! I will look into the frailty model option. We had about 20 to 40% mortality in the treatment levels. After some discussion, we are now considering analyzing the replicates, not the individual deaths, since the replicates are independent from one another. Using this perspective, maybe we can arrive with a mean mortality value for each replicate and compare them (accounting for censoring also). Another option could be to compare the chance to survive in each replicate, then compare replicates to each other.",2010-11-12T11:02:12.567,1862,CC BY-SA 2.5,
6654,4465,0,"...maybe Repeated Measures ANOVA could be an option, using replicate level information. We would need to use something that would allow for non-normal data distribution.",2010-11-12T11:02:57.590,1862,CC BY-SA 2.5,
6655,4465,0,Any thoughts on these ideas? (thanks!),2010-11-12T11:03:53.237,1862,CC BY-SA 2.5,
6657,4309,0,"I totally agree with you. We can not decide to stop an experiment because the results are what we wanted or otherwise continue. In my work usually replicate experiments 3 to 5 times depending on the availability of time. I think 3 replicas are sufficient. Also, if the estimates are accompanied by a measure of variability (standard errors and confidence intervals) the reader will have enough information to decide whether these estimates are more or less ""reliable"".",2010-11-12T12:32:30.100,221,CC BY-SA 2.5,
6659,4476,2,But note that the first formula is less prone to round-off error.,2010-11-12T13:25:01.933,449,CC BY-SA 2.5,
6660,4470,2,"Actually, they are the same for SPSS and R. Simple example:
yi <- c(1,3,2,2,3,5,4,6); xi <- c(1,2,2,3,3,4,5,5); wi <- c(.2,.3,.2,.1,.4,.3,.2,.3); summary(lm(yi ~ xi, weights=wi))

gives the exact same results as using the same data in SPSS with:

REGRESSION
/MISSING LISTWISE
/REGWGT=wi
/STATISTICS COEFF OUTS R ANOVA
/CRITERIA=PIN(.05) POUT(.10)
/NOORIGIN
/DEPENDENT yi
/METHOD=ENTER xi  .",2010-11-12T13:34:24.167,1934,CC BY-SA 2.5,
6662,4445,0,"@Rob Thank you, and thanks to all the people who looked at this: I saw your comments before they were deleted and am grateful for your efforts.",2010-11-12T16:25:09.873,919,CC BY-SA 2.5,
6663,4478,1,"Agreed, solid data and metadata first.",2010-11-12T17:04:25.430,1965,CC BY-SA 2.5,
6664,4354,8,"Please consider modifying your question, then, because the response you checked as your preferred answer does *not* answer the question as it stands (and the other one does).  Currently you are using the word ""independent"" in an idiosyncratic way.  Your example with Gamma shows this: one could simply reparameterize Gamma in terms of the mean (mu) and variance (sigma), because we can recover theta = sigma/mu and kappa = mu^2/sigma.  In other words, *functional* ""independence"" of the parameters is usually meaningless (except for single-parameter families).",2010-11-12T18:15:05.757,919,CC BY-SA 2.5,
6665,4488,0,"Hi @Aniko, Thanks for the feedback on this and for the reference. If my goal is to estimate the variance that would be measured on a single day in a single treatment, which term would be approapriate?",2010-11-12T21:44:00.697,1381,CC BY-SA 2.5,
6666,3542,0,Portions of some table design books are free :-); e.g. http://books.google.com/books?id=0sCkxy_1ChMC&printsec=frontcover&dq=related:ISBN1561583464,2010-11-12T23:24:40.063,919,CC BY-SA 2.5,
6667,4436,0,"With 21 subjects and 8 variables, and without any stated question, hypothesis or model .... what question would ridge regression answer?",2010-11-13T00:46:56.177,25,CC BY-SA 2.5,
6670,4494,0,"Thnks! Any way to add a small, filled triangle?",2010-11-13T03:52:53.607,990,CC BY-SA 2.5,
6671,4501,2,"This question seems to me to be straying beyond the bounds of this site. Not only is it subjective, it seems to me a question of page layout graphic design rather than statistics or data visualisation. While the design of figures displaying quantitative information is certainly within our bounds, I'd say the placement and sizing of such figures is straying outside.",2010-11-13T07:26:06.577,449,CC BY-SA 2.5,
6672,4501,4,Try on tex.stackexchange.com.,2010-11-13T09:04:28.320,,CC BY-SA 2.5,user88
6673,4494,0,@Hugo See my response with `pch`.,2010-11-13T09:19:36.997,930,CC BY-SA 2.5,
6674,4436,1,@Harvey - Brandon stated in a comment to his Q that he wanted a model for prediction purposes.,2010-11-13T09:28:49.263,1390,CC BY-SA 2.5,
6676,4511,3,"(+1) [Linear Models with R](http://www.maths.bath.ac.uk/~jjf23/LMR/), by Julian J. Faraway is also a good starting point.",2010-11-13T18:07:39.247,930,CC BY-SA 2.5,
6679,4462,4,"if I understand this correctly, dartdog wants to know whether the median of medians is the can be used to calculate the median of the whole sample, i.e. if $M=M_1 \cup M_2 \cup \ldots \cup M_k$ then median($M$)=median(median$(M_1)$,median$(M_2)$,$\ldots$,median$(M_k)$)...",2010-11-13T19:23:20.857,1573,CC BY-SA 2.5,
6680,4511,1,"Yeah, I guess there's really no escaping me actually digging down and figuring out what exactly I want to compute. I was just hoping there was some sort of standard first approach to use.",2010-11-13T21:14:05.973,1973,CC BY-SA 2.5,
6681,4511,3,Yup. _With great freedom comes great responsibility._ It is easy to just mechanically compute something. It may be much harder to come up something meaningful. Damn _No Free Lunch_ theorem again...,2010-11-13T22:15:30.957,334,CC BY-SA 2.5,
6682,4496,0,"Don't confuse the pretty label locations for the data. The figure region has been set to the range of the data. If you want axes with labels at specific, predefined points, turn off axis drawing in the `plot()` call with `axes = FALSE` and then draw the axes using `axis(side = 2)` for the y axis with suitable `at`, and the x-axis can be drawn using `axis.Date()` as per my answer below.",2010-11-13T22:48:31.523,1390,CC BY-SA 2.5,
6683,4258,1,"I would just like to mention that as someone interested in making generic MCMC algorithms, easily applicable to many problems, I am very interested in this topic.",2010-11-13T23:01:30.183,1146,CC BY-SA 2.5,
6684,4519,0,"If I understand you correctly. In your case, for the tuple (a,b), a and b are both observation (may be missing)?",2010-11-14T05:54:00.287,1307,CC BY-SA 2.5,
6685,4515,0,Is your question related to topic models?,2010-11-14T05:59:26.850,1307,CC BY-SA 2.5,
6686,4502,0,nice link. thx,2010-11-14T06:02:23.663,1307,CC BY-SA 2.5,
6687,4519,3,*Why* did you want to do a regression of one coordinate on the other? The better way to do this depends on what is is you're trying to do. What's the aim? What question are you trying to answer?,2010-11-14T07:39:10.820,449,CC BY-SA 2.5,
6688,4513,0,"Is that the statistical query model of Kearns (1998) http://dx.doi.org/10.1145/293347.293351 ? If so, sorry, i'd never even heard of it before.",2010-11-14T07:54:21.400,449,CC BY-SA 2.5,
6689,4501,1,Okay. I've posted the question on Tex.SE: http://tex.stackexchange.com/questions/5351/aesthetically-pleasing-width-for-figures-in-latex-documents,2010-11-14T08:10:38.220,183,CC BY-SA 2.5,
6691,4506,0,"@Bernd. Thanks for that. With regards to the second question, I was thinking of several scenarios. For example, you conduct two studies and they get written up in a journal article and conference proceedings. Then you do a third study and present a second conference paper using all three studies. Perhaps you can see where I'm going with this. There are connections between the set of studies and the set of publications and presentations. Should they all be in one repository, or would you split them up in some way (e.g., one repository per publication)?",2010-11-14T08:18:24.343,183,CC BY-SA 2.5,
6692,4506,0,"@Bernd; thanks also for the link to the SO question. It suggests having one project per repository. However, for me it still doesn't quite answer the question: ""what is a project?"" In particular, it seems to me that there are a few many-to-many relationships going on between studies and publications that can sometimes complicate things.",2010-11-14T08:21:49.037,183,CC BY-SA 2.5,
6694,4513,0,@onestop yes it is.,2010-11-14T08:35:31.400,1961,CC BY-SA 2.5,
6695,4514,1,"This is the perfect answer, I realize now that I was getting lost with my own proof. :)",2010-11-14T08:42:45.803,1351,CC BY-SA 2.5,
6696,4520,0,"this is a very good idea but for my reasons I would not like to go through articles distance. I want the probability/weight not for the actual tag suggestion which it can be done as you suggested, but as a solid tag score that I will use in my system for many other purposes.",2010-11-14T10:00:04.683,,CC BY-SA 2.5,user1975
6697,4506,0,"(Sorry, just realized that comments are limited to 600 characters)

@Jeromy (part 1/3): You wrote: ""There are connections between the set of studies and the set of publications and presentations. Should they all be in one repository, or would you split them up in some way (e.g., one repository per publication)?"" When you write ""split them up in some way"", do you mean sth like forking? And ""studies"" are datasets? As I wrote before, each publication, each presentation etc. has its own repository.",2010-11-14T11:04:28.490,307,CC BY-SA 2.5,
6698,4506,0,"@Jeromy (part 2/3): 
However, there is one real downside: using the same dataset in different publications means to maintain different versions of ""initialization code"" (define missing values, generate new variables etc.). To overcome this problem, I decided to maintain /one/ study/dataset-related repository which contains the original init-file. For each publication, presentation etc. I am using a copy of the original data-file as well as of the init-file (in R via file.copy()).",2010-11-14T11:05:49.893,307,CC BY-SA 2.5,
6699,4506,0,"@Jeromy (part 3/3): Of course, whenever I create a new variable I need to modify my original init-file and do a file.copy() (which is the most annoying part of my approach).

Regarding your question ""what is a project?"", I would say that project = (primary) study/dataset.",2010-11-14T11:06:34.777,307,CC BY-SA 2.5,
6700,4501,0,@onestop I changed the title to try to make it less subjective,2010-11-14T11:33:03.083,183,CC BY-SA 2.5,
6701,4501,0,I was more worried about the subject matter than the subjectivity to be honest. How about proposing it as a suitable question for http://area51.stackexchange.com/proposals/1924/graphic-design ?,2010-11-14T11:39:41.877,449,CC BY-SA 2.5,
6702,4501,0,"@onestop I can see where you are coming from. I can see how general graphic design, and tex sites are relevant. However, I have a specific interest in what other users of Sweave and R do, and how they justify their decisions. Thus, I thought I'd get more relevant answers here. Perhaps this is an issue other users of Sweave and R encounter.",2010-11-14T11:46:58.687,183,CC BY-SA 2.5,
6703,4518,3,"One might add that fitting the regressions separateley is indeed equivalent to the multivariate formulation with a matrix of dependent variables. In R with package mvtnorm installed (1st: multivariate model, 2nd: separate univariate models):
library(mvtnorm);
X <- rmvnorm(100, c(1, 2), matrix(c(4, 2, 2, 3), ncol=2));
Y <- X %*% matrix(1:4, ncol=2) + rmvnorm(100, c(0, 0), diag(c(20, 30)));
lm(Y ~ X[ , 1] + X[ , 2]);
lm(Y[ , 1] ~ X[ , 1] + X[ , 2]);
lm(Y[ , 2] ~ X[ , 1] + X[ , 2])",2010-11-14T12:26:00.207,1909,CC BY-SA 2.5,
6704,4288,1,"I don't understand what you're trying to do here. To estimate propensity scores you need to know whether each individual received the treatement or not, in which case you can simply count up how many received the treatment. Why do you want to aggregate propensity scores?",2010-11-14T14:04:56.463,449,CC BY-SA 2.5,
6705,4456,0,"If you post some data, I'll run the model for you and see what happens.",2010-11-14T16:45:09.640,1585,CC BY-SA 2.5,
6707,4516,0,"Hi Jon, could you explain how to generate ranks within groups?",2010-11-14T17:32:48.523,1950,CC BY-SA 2.5,
6709,4479,0,Thanks onestop! We are definitely going to use each aquarium as the unit of analysis so we will not have dependent data. We are still formulating the best approach. I will post the final decision once it is made.,2010-11-14T19:45:01.513,1862,CC BY-SA 2.5,
6710,4506,0,@Bernd. Can you please point me to a tutorial (for dummies) which explains the basic use of git+github+ProjectTemplate-like package? Thx for the help.,2010-11-14T19:49:31.350,1307,CC BY-SA 2.5,
6711,4519,0,"Post-edit: Your statements together seems to imply that the number of times *i* attacks *j* is always either 0 or 1. And that if *i* attacks *j* then *j* attacks *i*. So relating the number of attackes that *i* makes on *j* to the number *j* makes on *i* seems trivial. Sorry, I'm obviously not understanding something here.",2010-11-14T19:51:21.973,449,CC BY-SA 2.5,
6713,4527,0,"If you mean the column headed ""Value"" as returned by a call to `summary`, then it is *a priori* a regression coefficient (for your survival model). Its interpretation depends on the fitted distribution.",2010-11-14T20:21:24.817,930,CC BY-SA 2.5,
6714,4519,0,@onestop: Thanks.  Good point.  I was sloppy in my description there.,2010-11-14T20:51:19.613,847,CC BY-SA 2.5,
6716,4521,3,This seems conceptually different than the answer above.  I'm still not clear as to how transforming my variables to PCA coefficients allows me to regress on 2+ dependent variables?,2010-11-14T20:55:10.980,1977,CC BY-SA 2.5,
6717,4522,3,"I could be wrong, but I don't think this is the same thing.  From SEM graphs that I've seen, it looks like SEM uses multiple regression equations to determine the values of latent factors, and then another regression is run on the value of those latent factors to determine a higher-order factor.  Maybe this is wrong, but I've never seen an SEM graph that links several IVs to multiple DVs-- everything is hierarchical.",2010-11-14T20:59:25.597,1977,CC BY-SA 2.5,
6718,4526,0,"Excellent answer, thanks! Since there where no answer for some time, I also asked the question over at [math](http://math.stackexchange.com/questions/10124/), and they have some different approaches there, that might interest you.",2010-11-14T21:36:07.493,977,CC BY-SA 2.5,
6719,4526,0,"Thank you!  @Mike Spivey gives a nice answer there.  However, it's for *choosing without replacement,* which answers a different (and seemingly easier) question.",2010-11-14T21:39:15.187,919,CC BY-SA 2.5,
6722,4506,0,"@ suncoolsu: git: Pro Git (http://progit.org/book), Git References (http://gitref.org/), I also bought O'Reilly's ""Version Control with Git"". Github also offers some learning resources (http://help.github.com/). What do you mean by ""ProjectTemplate-like package""? I know only the original package by JMW.",2010-11-15T10:51:16.090,307,CC BY-SA 2.5,
6723,4541,0,http://www.healthknowledge.org.uk/public-health-textbook looks like a useful resource -- surprised i'd never come across it before. Thanks chl.,2010-11-15T11:55:32.213,449,CC BY-SA 2.5,
6724,4497,1,"Great, they seem to be both available for free on the web!",2010-11-15T14:50:54.277,190,CC BY-SA 2.5,
6725,3671,0,The linked post does provide some good insight into the challenges that readers face when confronted with a table; this can be good information when designing tables or choosing alternatives.,2010-11-15T15:22:26.290,1381,CC BY-SA 2.5,
6726,4551,5,"I'm aware that ""sin"" is possibly inflammatory and that that some aspects of statistical analysis are not black-and-white. My intention is to solicit cases where a given commonly-taught practice is pretty clearly inappropriate.",2010-11-15T18:53:14.270,364,CC BY-SA 2.5,
6727,4544,6,"I understand that you need Type III sum, but this (http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf) article is a good read. It illustrates some interesting points.",2010-11-15T19:03:03.193,1307,CC BY-SA 2.5,
6728,4551,5,You can also add biology/life sciences students to the mix if you like ;),2010-11-15T19:03:17.187,582,CC BY-SA 2.5,
6730,4552,0,"but sometimes... sometimes the potential directions of causation have highly disparate probabilities.  I'm certainly not going to think that a correlation between age and height could be caused by the height... or some intervening variable either.  Also, I think that this is one that behavioural science training is generally quite sensitive to.",2010-11-15T19:21:49.960,601,CC BY-SA 2.5,
6731,4551,1,maybe retitle it life science statistical sins?... or something else more specific...,2010-11-15T19:27:28.377,601,CC BY-SA 2.5,
6736,3214,0,"Sorry about the late comment. I just received the book that Thylacoleo recommended. To answer your questions: Chl - the outcome is survival (i.e. no death) over a period of time. The sample size in each group varies from, say, 5-8. Andy - survival experiments suffer from inherent variabilities. I do indeed examine the Kaplan Meier plots of the groups by a log rank test. But I am more interested in finding out if there is a test that can evaluate inter-experimental variabilities of survival experiments.",2010-11-15T20:47:54.700,1464,CC BY-SA 2.5,
6738,4550,3,"I know Moore is in the car library, but When sample data is provided it is easier for the question asker to understand your response if you use the sample data.",2010-11-15T21:23:59.367,196,CC BY-SA 2.5,
6740,4549,1,"I don't think this answer uses the -3,-1,1,3 contrast I specified nor does it seem to provide a 1 df test of the contrast.",2010-11-15T21:33:58.300,196,CC BY-SA 2.5,
6741,4562,0,"Thanks, sounds reasonable, but is there any other possibility to further constrain the definition of different... For instance, a difference of 8 byte is statistically significant, but it is possibly not in practice. Such a constraint would be useful for hypothesis testing.",2010-11-15T21:34:15.837,1992,CC BY-SA 2.5,
6743,4549,1,"@drknexus Yes, you're right. Wrote too quickly. Something like `Anova(lm(DV ~ C(IV, c(-3,-1,1,3),1), data=sample.data), type=""III"")` should be better. Please let me know if this ok with you.",2010-11-15T22:23:19.763,930,CC BY-SA 2.5,
6744,4549,0,Thanks!  That looks okay I'll validate it against SPSS tomorrow and get back to you.,2010-11-15T22:35:00.313,196,CC BY-SA 2.5,
6745,4553,18,"I don't think this is really a ""sin"" as the results obtained are not wrong. However, it does throw away a lot of useful information so is not good practice.",2010-11-15T22:51:35.173,159,CC BY-SA 2.5,
6746,4562,0,"@stefan-ock Hypothesis testing is only about approximating chance that your result is just a lucky coincidence; here the result is absolutely clear. What you can do is to write, for instance, that the difference is so small that it is not an important factor for comparison; yet judging how many bytes is equally important as 1 ms of computation is purely use-dependent and a bit subjective in its nature.",2010-11-15T23:39:43.497,,CC BY-SA 2.5,user88
6747,4553,2,"Along these lines, using extreme groups designs over-estimates effect sizes whereas the use of a mean or median split under-estimates effect sizes.",2010-11-16T02:47:46.777,196,CC BY-SA 2.5,
6748,4557,3,Would you please explain the consequences of this sin?,2010-11-16T02:50:28.617,196,CC BY-SA 2.5,
6749,4560,6,"""Good idea"" depends on the situation.  When you want to maximize prediction it isn't a horrible idea - though it may lead to over fitting.  There are some rare cases where it is inevitable - where there is no theory to guide the model selection.  I wouldn't count stepwise regression as a ""sin"" but using it when theory is sufficient to drive model selection is.",2010-11-16T02:52:32.267,196,CC BY-SA 2.5,
6750,4563,13,"What's inappropriate about least squares estimation when the data are non-normal or heteroskedastic? It is not fully efficient, but it is still unbiased and consistent.",2010-11-16T03:18:20.200,159,CC BY-SA 2.5,
6751,4560,21,Perhaps the sin is doing statistical tests on a model obtained via stepwise regression.,2010-11-16T03:19:25.860,159,CC BY-SA 2.5,
6753,4573,0,link: http://socialnetworks.mpi-sws.org/,2010-11-16T07:19:07.930,,CC BY-SA 2.5,Kazem
6754,4555,4,"How much does this decrease the power of the analysis? In what conditions is it most problematic? 
In many cases deviations from the assumptions of ANOVA do not substantially affect the outcomes to an important extent.",2010-11-16T07:48:44.663,1679,CC BY-SA 2.5,
6755,4559,1,"Canonical correlation is the correlation between factor scores computed from two-block structures, as with CCA or PLS. This is exactly what I described in my response (PLS regression), although PLS is more appropriate than CCA when the variables play an asymmetrical role, which is likely to be the case here. This is because there's an asymmetric deflation process and we work with the covariance instead (with CCA, we deflate both blocks at the same time, and we seek to maximize the correlation, instead of the covariance).",2010-11-16T07:50:59.637,930,CC BY-SA 2.5,
6757,4579,0,"The tag ""machine learning"" doesn't seem very appropriate there. Maybe ""optimization"" or something like that?",2010-11-16T08:31:25.047,930,CC BY-SA 2.5,
6758,4580,0,"A possible related question, http://stats.stackexchange.com/questions/825/any-suggestions-for-making-r-code-use-multiple-processors.",2010-11-16T08:33:28.243,930,CC BY-SA 2.5,
6760,4579,0,"I agree, but as I am new here I couldn't create that tag.",2010-11-16T08:36:25.650,,CC BY-SA 2.5,another_day
6761,4579,1,they are not the same but if you minimize them with respect to lambda in the whole real line changing lambda in -lambda does not change the solution. Not that there is a mathematic stackoverflow somewhere (This site is more for statistics),2010-11-16T08:54:40.620,223,CC BY-SA 2.5,
6762,4571,0,"Why check against a log odds ratio *z*-test? That's a Wald test, and Wald tests typically perform worse than score tests such as Pearson's chi-squared test. Is this known to be an exception?",2010-11-16T09:11:04.647,449,CC BY-SA 2.5,
6763,4557,1,"If the data are generated by a process with a heteroscedastic noise process, the regression model is likely to give very inacurate out-of-sample predictions.",2010-11-16T09:26:46.770,887,CC BY-SA 2.5,
6764,4582,1,But indeed I am not sure http://stats.stackexchange.com is the best place to ask your question,2010-11-16T09:27:34.477,1709,CC BY-SA 2.5,
6765,4563,3,If the data are heteroscedastic you can end up with very innacurate out of sample predictions because the regression model will try too hard to minimise the error on samples in areas with high variance and not hard enough on samples from areas of low variance.  This means you can end up with a very badly biased model.  It also means that the error bars on the predictions will be wrong.,2010-11-16T09:31:44.380,887,CC BY-SA 2.5,
6767,4582,2,.. and it is not the best place to comment the question ;),2010-11-16T09:54:59.537,223,CC BY-SA 2.5,
6768,4584,0,Thanks for your answer! Do you do the computation for your work/academic research or for own projects on a own PC?,2010-11-16T09:59:35.063,2006,CC BY-SA 2.5,
6769,4586,0,Sorry if I'm being a bit slow.  How do I do the minimization? Is there a formula or an algorithm that I need to use?,2010-11-16T10:15:20.187,847,CC BY-SA 2.5,
6770,4570,4,"Perhaps I don't see why this is such a problem. Hypothesis testing a small sample size using a normal distribution, sure, but using a more conservative/nonparametric test, is this so bad?",2010-11-16T10:18:30.767,1118,CC BY-SA 2.5,
6771,4586,0,You need to use a nonlinear optimization algorithm.,2010-11-16T10:19:26.710,159,CC BY-SA 2.5,
6772,4567,2,@Michael (+1) I added links to abstracts and ungated PDFs. Hope you don't mind.,2010-11-16T10:24:17.297,930,CC BY-SA 2.5,
6774,4584,0,"This is done in a commercial setting. For this task, I am using a single Intel box with 32GB of RAM and RAIDed disks (the main difficulty is lots of data, while the processing itself is not very computationally demanding.)",2010-11-16T11:01:33.627,439,CC BY-SA 2.5,
6775,4582,0,Thus it does not even play a role whether I want to maximize or minimize the function A?,2010-11-16T11:07:32.807,,CC BY-SA 2.5,another_day
6776,4588,1,"@user1806 This is basically what I answered in my comment. Now, the point is that the OP want to know how it reads in plain english, which depends *a priori* of the kind of parametric assumptions that are made in the model.",2010-11-16T12:21:49.337,930,CC BY-SA 2.5,
6777,4584,0,"Alright @aix, how often do you perform these calculations. Is you box running all the day or more idle?",2010-11-16T12:38:19.517,2006,CC BY-SA 2.5,
6778,4563,7,"No, it is unbiased, but the variance is larger than if you used a more efficient method for the reasons you explain. Yes, the prediction intervals are wrong.",2010-11-16T12:39:49.377,159,CC BY-SA 2.5,
6779,4549,1,"BTW, have a look at the ez package (http://cran.r-project.org/web/packages/ez/index.html) for wrapping the Anova code...",2010-11-16T12:45:08.080,253,CC BY-SA 2.5,
6780,4555,0,What is the alternative do the ANOVA procedure?,2010-11-16T12:55:24.730,442,CC BY-SA 2.5,
6781,4587,0,We here in Hamburg always have a problem that the waiting time for the academic data centers are really long. is it the same for you?,2010-11-16T13:16:14.660,2006,CC BY-SA 2.5,
6782,4587,0,"@Heinrich I work for a kind of academic data center, so I don't have such problems (-; Seriously, in Warsaw the scientific CPU time supply is larger than demand, so I believe it is quite easy to get a grant. And I think you should try D-Grid or EGEE, my experience is that grids in general are very underused.",2010-11-16T14:16:36.007,,CC BY-SA 2.5,user88
6784,4555,0,@Michael Lew & Henrik: I just updated this entry to include a link to Dixon (2008),2010-11-16T15:48:33.303,364,CC BY-SA 2.5,
6785,4557,0,"This entry was originally inspired by the observation that some folks estimate non-linear psychometric functions by minimizing least squares. For example, Murd et al (2009, http://www.perceptionweb.com/abstract.cgi?id=p6145 , free pdf available by googling the title) fit a probit function through data by minimizing least-squares.",2010-11-16T16:04:32.697,364,CC BY-SA 2.5,
6786,4587,0,Oh. That is interesting. Dow you know in what kind of businesses R is used in these extends?,2010-11-16T16:22:52.823,2006,CC BY-SA 2.5,
6787,4288,0,"I suppose I worded it poorly; the data I have is all either binary or multinomial and each observation is accompanied by the probability it was observed correctly. So suppose I have 5 persons; 3 of which had a value of 1 for an attribute, 2 of which had a value of 0 for that attribute, each of which had a probability .8,.81,.82.,.83,.84 respectively of being observed correctly. What is the p(having that attribute)?",2010-11-16T17:27:10.687,1893,CC BY-SA 2.5,
6788,4546,0,I have realizations of Turkey and probabilities I observed that correctly; I edited the question to better reflect this. Thanks!,2010-11-16T17:28:23.947,1893,CC BY-SA 2.5,
6789,4596,0,Can you pls tell what is the size of your data set.,2010-11-16T19:28:27.417,1307,CC BY-SA 2.5,
6790,4567,0,@Chi Thanks! I should have done that myself. Next time...,2010-11-16T19:44:01.083,1679,CC BY-SA 2.5,
6791,4598,2,Perhaps moving standard deviations would be more informative.,2010-11-16T22:03:11.017,1679,CC BY-SA 2.5,
6792,4563,4,Yes (I was using biased in a colloquial rather than a statistical sense to mean the model was systematically biased towards observations in high-variance regions of the feature space - mea culpa!) - it would be more accurate to say that the higher variance means there is an increased chance of getting a poor model using a finite dataset.  That seems a reasonable answer to your question. I don't really view unbiasedness as being that much of a comfort - what is important is that the model should give good predictions on the data I actually have and often the variance is more important.,2010-11-16T22:18:23.977,887,CC BY-SA 2.5,
6794,4602,0,"I know many problems not related to econometry which are used as a benchmark, but I want to focus on applications of GA in economics, and I thought that estimation of econometry model is good start.",2010-11-16T22:37:18.667,1643,CC BY-SA 2.5,
6795,4590,7,"The same logic (taking ""absence of evidence in favor H1"" as ""evidence of absence of H1"") essentially underlies all goodness-of-fit tests. The reasoning also often crops up when people state ""the test was non significant, we can therefore conclude there is no effect of factor X / no influence of variable Y"". I guess the sin is less severe if accompanied by reasoning about the test's power (e.g., a-priori estimation of sample size to reach a certain power given a certain relevant effect size).",2010-11-16T23:07:06.833,1909,CC BY-SA 2.5,
6796,4597,0,"What do you mean by needs to implement? Do you mean you simply need to report the ""wiggliness"". Also I would take wiggliness to mean the variance of your series (or intervals within the series.)",2010-11-16T23:33:52.943,1036,CC BY-SA 2.5,
6797,4598,0,"Perhaps I misunderstood the question, I thought he wanted to smooth the data, and was underwelmed by his current results. If he wants to extract the conditional volatility I would use GARCH, or squared logged differences as a first step.",2010-11-17T00:05:17.777,1766,CC BY-SA 2.5,
6798,4597,0,"I need to find the 'wiggly' intervals, as those are text blocks. I'm using this to find blocks of text in a scanned document, and I found that this is the easiest way for the computer to 'see' deformed blocks of text on a sheet of badly-scanned paper.",2010-11-17T00:06:58.243,2002,CC BY-SA 2.5,
6799,4598,0,"How would I go about implementing a moving standard deviation? I'm not familiar with statistics (I'm studying theoretical mathematics at the moment), but I think that's exactly what I need. Thanks a lot!",2010-11-17T00:13:45.410,2002,CC BY-SA 2.5,
6800,4598,0,"@Blender I've never actually calculated a moving stdev, but I expect that you could simply choose a window size (the number of data points to include in the stdev) and calculate a stdev at each point along the dataset. You would have to start half of the window length in from one end of the data, and stop half from the other end. Probably an easy calculation in a spreadsheet.",2010-11-17T00:45:13.093,1679,CC BY-SA 2.5,
6801,4570,0,"I agree that using a more conservative model to fit the data is the best we can do. But in any case you will have to trust this model. It will be a fitting, not a model. A model requires a representative set of data otherwise it may not work in the future.",2010-11-17T02:17:33.240,1709,CC BY-SA 2.5,
6802,4571,0,"thanks for the information! Fisher's test does seem to be a more robust method for questions like these. I don't think the course I am currently taking will address Fisher's test, but I'll definitely keep it in mind for real world applications.",2010-11-17T03:52:29.303,696,CC BY-SA 2.5,
6803,4577,0,"Thanks for the references. I was able to find a ""pre-print"" version of the Campbell paper [here](http://www.iancampbell.co.uk/twobytwo/files/campbell_twobytwo_preprint.pdf) for those who can't access Pub Med.",2010-11-17T03:56:05.740,696,CC BY-SA 2.5,
6805,4608,0,The gradient is a vector since your loss function has real values.,2010-11-17T07:04:51.847,1351,CC BY-SA 2.5,
6806,4605,0,Thanks for your info. What do you mean with course parallelization?,2010-11-17T07:13:13.713,2006,CC BY-SA 2.5,
6807,4586,0,"Okay. I've got as far as using ARIMA(0,1,1).  I am assuming that the code, arima(x, order = c(0, 1, 1)) is how one does that.  (I spent a few hours learning R first so I am completely new to this.) How do I apply the output to my smoothing problem?",2010-11-17T07:22:48.937,847,CC BY-SA 2.5,
6808,4590,0,"If you do not make any concideration about the power, I would say claming $H_0$ is true when it is not rejected is very very bad while claming $H_1$ is true while $H_0$ is rejected is just a little wrong :).",2010-11-17T07:22:53.877,223,CC BY-SA 2.5,
6809,4586,1,"Using R's parameterization, $\alpha = 1+\theta$ where $\theta$ is the moving average parameter.",2010-11-17T07:32:14.803,159,CC BY-SA 2.5,
6810,4600,0,"@Lynn Do you mean that you want to know the % of variance ($R^2$) in your DV explained by all predictors in your final model, or during model selection?",2010-11-17T07:45:39.563,930,CC BY-SA 2.5,
6813,4586,0,"Thanks! Great! I get it now.  My last hurdle is the missing values.  Now I have $\alpha$.  How do handle the missing values? Using the notation on Wikipedia, does this mean I have several $x_0$'s and I just restart the exponential smoothing algorithm after each gap?",2010-11-17T08:03:51.523,847,CC BY-SA 2.5,
6814,4602,0,Surely the travelling salesman problem has huge application within economics. Perhaps you could clarify what you mean by an 'econometry model'.,2010-11-17T08:09:00.483,449,CC BY-SA 2.5,
6818,4602,0,"Sorry it was late, of course I meant econometric model. Example: Y=b1*x1+b2*x2^b3+(b4/x3+b5*x4)^b6 , but I'm not sure if this is hard to estimate using non-linear regression and secondly whether it has any real-world application.",2010-11-17T09:36:23.263,1643,CC BY-SA 2.5,
6819,4524,0,"Your first suggestions worked great. Your second, I wasn't able to achieve what I wanted. Could you give me an example? Thx.",2010-11-17T10:17:22.010,990,CC BY-SA 2.5,
6820,4610,0,can you use R ? (if yes i'll post a code that'll solve this problem).,2010-11-17T11:05:19.737,603,CC BY-SA 2.5,
6821,4626,0,"Thanks, but I am afraid I don't follow. LARS produces a piecewise-linear path (with exactly $p+1$ points for the least angles and possibly more points for the lasso.) Each point has its own set of $\beta$. When we add more observations, all the betas can move (except $\beta^0$, which is always $0_p$.) Please could you expand on your answer? Thanks.",2010-11-17T11:34:29.510,439,CC BY-SA 2.5,
6822,4608,3,your function is not differentiable everywhere.,2010-11-17T11:40:22.203,223,CC BY-SA 2.5,
6824,4621,0,"Thanks! That clears things up for me. Now I just have to get it right in a practical setting. You wouldn't happen to have any idea why the above code doesn't work? It seems to converge in 4 iterations with the loss starting at 1 and going down 0.25 each time and converging at 0. However, the weights it produces seem quite wrong.",2010-11-17T11:56:38.627,2023,CC BY-SA 2.5,
6826,4586,1,arima() will handle the missing values appropriately. There's no need for you to do anything. Just use the forecasts it provides.,2010-11-17T12:37:05.900,159,CC BY-SA 2.5,
6827,4630,2,"+1 Note that the MSE comes with a built-in goodness of fit estimate, so it solves problems #1 and #2 simultaneously.",2010-11-17T13:28:21.080,919,CC BY-SA 2.5,
6829,4567,7,"+1, but I would like to make some critical comments.  Regarding the opening line, one could just as well say that ""almost all"" (in the measure theoretic sense) interpretations of *any* well-defined concept are incorrect, because only one is correct.  Second, to what do you refer when you say ""the conventional usage"" and ""standard approaches""? These vague references sound like a straw man.  They do not accord with what one can find in the literature on statistics education, for example.",2010-11-17T13:43:58.040,919,CC BY-SA 2.5,
6830,4630,0,"@whuber (+1) Thanks, I hadn't noticed that. I admit I've never actually *used* maximum spacing estimation; I'd *heard* of it in connection with estimation for a uniform distribution and just checked Wikipedia.",2010-11-17T14:26:41.310,449,CC BY-SA 2.5,
6831,4613,0,"Thanks for the response. Could you elaborate any more on what these equations actually say? They may well be useful, but I've sort of got to convince myself. :)",2010-11-17T14:45:19.760,1810,CC BY-SA 2.5,
6832,4636,1,"Ouch.  In the real world the incidence of people who can't add correctly to 100 is going to be high.  Even so, in their minds most people can't tell the difference between 90% and 95% so providing them a scale where they can do so is pointless.  I'd suggest you'd be better off giving a Likert scale for each of your major competitors and one marked other.  You can then look at the proportion of your likert scale over the sum of all likert scores (lower bound should have a value of zero) as an indication of market share.",2010-11-17T15:07:42.643,196,CC BY-SA 2.5,
6833,4557,0,"So, should the ""answer"" be amended to ""application of least-squares minimization on heteroscedatic data""?",2010-11-17T15:13:00.360,196,CC BY-SA 2.5,
6834,4613,1,@Noldorin answer updated,2010-11-17T15:13:50.523,223,CC BY-SA 2.5,
6835,4555,2,"But in short, it is most problematic when probabilities observed are low or high as the range of values are constricted and unable to meet Gaussian assumptions.",2010-11-17T15:14:31.483,196,CC BY-SA 2.5,
6836,4632,0,"As it happens, I am using this text in a course that I am TAing. The students hate this book. Stock and Watson is a common intro book, with Wooldridge's Introductory Econometrics a bit more sophisticated, but very clear.",2010-11-17T15:16:27.550,401,CC BY-SA 2.5,
6837,4636,0,@drknexus Adding up to 100 is an issue which is mitigated to some extent if you do online surveys where the software can check if the total adds up to 100. Not being able to tell the difference between 90% and 95% is measurement error and that can be dealt with if necessary. The question form I suggested is one of the standard question types used in market research but has not been used much in the past because of the issue about totals that you mentioned. I feel that the issue of totals is much less pertinent in online surveys.,2010-11-17T15:24:35.387,,CC BY-SA 2.5,user28
6838,4613,0,Thanks for that. Probabilities of probabilities - that may take a little while for me to get my head around! Final question: is the second equation effectively a simplified version of the first (which is always better)?,2010-11-17T15:38:30.503,1810,CC BY-SA 2.5,
6839,4610,0,"Heard of it, but in this case, not relevant - application code is C++, the OpenFOAM CFD package, and runs on a super computer.",2010-11-17T15:44:24.450,,CC BY-SA 2.5,Bluey The Dog
6840,4636,0,"I seldom think being a ""standard"" makes something a defensible practice.  That being said, I suppose even in offline surveys I suppose participants responses can be rescaled to total to 100 if they make some sort of math error.",2010-11-17T15:53:21.347,196,CC BY-SA 2.5,
6841,4631,1,"But what if an individual goes to more than one coffee house?  Or what if Person A goes once per month to five different coffeehouses, and Person B goes five times per month to just one coffeehouse?",2010-11-17T16:03:20.050,71,CC BY-SA 2.5,
6842,4639,9,"the help in R is meant to explain you how to use the function. It's not meant to be a course on statistics. And regarding that, in general I believe the R help pages are among the most complete and handy from all open source packages I know of. And paying packages for that matter. SPSS and SAS give you a lot of mumbo-jumbo with half-truths and complete nonsens as a ""guide for interpretation"".",2010-11-17T16:03:26.087,1124,CC BY-SA 2.5,
6844,4641,5,"I like this sentiment, ""if I don't understand what I'm doing already, I shouldn't try to learn it..."" This is also the approach taken in the R help - it's not helpful unless you already know what's going on. I was hoping this could be the start of something different.",2010-11-17T16:37:55.467,1994,CC BY-SA 2.5,
6845,4641,0,"But I can use this part of your answer: ""Interprete it in exactly that way: it expresses if the model without that term is significantly different from the model with that term.""  To me this means that the Pr(F) values are the significance of each of these terms, and a small value means that this variable is important. So, a good model should include the ""***"" variables and not the ones that have no stars.",2010-11-17T16:41:04.027,1994,CC BY-SA 2.5,
6847,4626,0,@aix: do you want to update the whole lasso path or just the solution ? (i.e. is the sparsity penalty fixed?).,2010-11-17T16:57:53.280,603,CC BY-SA 2.5,
6848,4634,0,That one is a bit too superficial and 'hand-on' imho.,2010-11-17T17:00:36.380,603,CC BY-SA 2.5,
6849,4641,4,"@gakera: You got me wrong. If you don't understand what you're doing, you should definitely try to learn it **before using it**. That means, reading up on statistics and following a course. So, a good model should include the variables that are formulated in the hypothesis. If you base yourself on the ""***"" variables, you need a thorough course on modeling first. You obviously didn't understand my last comment. Sorry for the direct communication, comes with the guy. Nothing personal.",2010-11-17T17:01:14.027,1124,CC BY-SA 2.5,
6850,4580,0,Exact duplicate of https://stat.ethz.ch/pipermail/r-help/2010-November/259921.html,2010-11-17T17:03:45.620,1657,CC BY-SA 2.5,
6851,4641,0,@gakera: I updated my answer to clarify some points that are important. Mainly because you misinterpreted the part you thought you could use.,2010-11-17T17:09:10.180,1124,CC BY-SA 2.5,
6852,4634,2,"@user603 The question doesn't specify anything like that; I also don't see how it's ""superficial"".  It is very high level and shows how to do analysis in R.  I wouldn't recommend it as a textbook for an econometrics course, but it certainly serves a purpose.",2010-11-17T17:18:05.800,5,CC BY-SA 2.5,
6853,4605,0,"Course parallelization would be something like independent runs of a MCMC change., i.e. very large chucks that can be ran in parallel without syncing threads.  An example of fine grain is computing the likelihood where calculations can be performed on the data points independently.",2010-11-17T17:23:46.640,1364,CC BY-SA 2.5,
6855,4626,0,"I was looking to update the entire path. However, if there's a good way to do it for a fixed penalty ($\lambda$ in the formula below), this may be a good start. Is this what you are proposing?

$$ \hat{\beta}^{lasso} = \underset{\beta}{\operatorname{argmin}} \left \{ {1 \over 2} \sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^p x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^p |\beta_j| \right \} $$",2010-11-17T17:48:12.177,439,CC BY-SA 2.5,
6856,4631,0,"I agree with Matt, and this is probably not the best measure per say to assess market share (particularly as people have less loyalty). But this approach is enlightening as to consumers preferences. It will diverge from market share depending on how much people choose multiple vendors and if frequency of coffee buying varies between preferred vendors (i.e. do people who prefer A overall drink more coffee than people who prefer B.)",2010-11-17T18:05:13.360,1036,CC BY-SA 2.5,
6857,4613,0,@Noldorin the first is not always better (take $p=0$).,2010-11-17T18:15:48.640,223,CC BY-SA 2.5,
6858,4641,3,"I'm learning by doing, this is homework after all, nobody is going to die if I don't get this right - the fish are already dead :P    Thanks for the help so far, and don't worry, this is not my first time on the internet :)",2010-11-17T18:16:07.590,1994,CC BY-SA 2.5,
6859,4644,0,"@gakera [Practical Regression and Anova using R](http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf) is a good starting point for understanding linear models, and methods related to variables/model selection. As pointed by @Joris, stepwise regression is rarely the panacea.",2010-11-17T18:33:24.510,930,CC BY-SA 2.5,
6860,4644,0,"hah, thanks for adding the links @chl while maintaining my disclaimer as to why I can't post them. You must agree that I suck :D",2010-11-17T18:36:29.607,1994,CC BY-SA 2.5,
6862,4596,0,Sure. The datasets I am currently working with are ~14 gigs,2010-11-17T18:55:40.557,1451,CC BY-SA 2.5,
6863,4644,1,"@gakera I think you need to have more rep to add more than one link per edit -- I can understand this is not very pleasant when starting on an Q&A website. I was assuming that you would remove your last sentence yourself. On the other hand, I feel you shouldn't expect too much upvotes for providing an answer to your own question, since it is a sort of recap' (useful, though).",2010-11-17T19:02:39.000,930,CC BY-SA 2.5,
6864,4645,0,"I didn't fully digest the ""multiple choice"" aspect of the question previously. I would look further into the Van Westendorp methods if that is what you are limited to...",2010-11-17T19:27:30.327,696,CC BY-SA 2.5,
6865,4644,0,I'm not doing this for upvotes (that's so Reddit :P  ) - useful recap is exactly what I'm going for - mainly for myself but probably useful for others as well.,2010-11-17T20:03:14.960,1994,CC BY-SA 2.5,
6866,4621,1,"You could check what predictions it gives to your training data. If loss goes down to zero, all the instances should be classified perfectly",2010-11-17T20:33:26.607,511,CC BY-SA 2.5,
6867,4642,0,"I wouldn't call this data ""malformed"" - it just has a zero-frequency cell, which in large part is due to the effect being large. From the point of view of the application this is a ""good thing"".",2010-11-17T21:14:34.500,279,CC BY-SA 2.5,
6869,4650,0,"Thanks Raskolnikov dont worry about time or the ""market"" it could be a five sided dice if you prefer. I'm just trying to understand the math of combining 2 (or 20) x 5 sided dice showing the same side.",2010-11-17T21:45:48.937,,CC BY-SA 2.5,Medius
6870,4650,0,"So, you are really asking about what happens when you have a lot of variables that behave in the same way? You want to know how the sum behaves in particular, right? If it is for dice, you might want to look up the multinomial distribution then.",2010-11-17T21:51:06.840,2036,CC BY-SA 2.5,
6871,4650,0,Raskolnikov please ignore my erroneous use of %.  If i have two five sided dice what is the probability of both dice having the same outcome? And what would the math be if I had 3 dice etc,2010-11-17T21:57:20.043,,CC BY-SA 2.5,Medius
6873,4650,0,"Sorry our posts are crossing. ""multinomial distribution"" errr ok",2010-11-17T21:58:28.047,,CC BY-SA 2.5,Medius
6875,4650,0,"Ok if I play a game whith a dice such that when the side with 5 on lands upper most I loose. I can play the game with more dice but if they all show the same side ""5"" I loose. Any other combination I win or at least dont loose.How can I express or calculate the difference?",2010-11-17T22:14:31.630,,CC BY-SA 2.5,Medius
6876,4650,0,"Brilliant so with more dice you simply multiply the possible outcomes.  If you want to express the probability of faces 1,2 and 3 landing face up how would you do so?  This is excellent Raskolnikov I'm learning thank you.",2010-11-17T22:28:05.543,,CC BY-SA 2.5,Medius
6877,4650,0,"Just the product of the probabilities for one die: $1/125$. What makes it different when two or more die come out the same, is that you have to account for the different possible arrangements of these outcomes. Since you're only interested in what comes out and not in what die gives what number, the probabilities are higher, like in the example I gave with two ones and one two.",2010-11-17T22:29:29.380,2036,CC BY-SA 2.5,
6879,4650,0,You are very kind thank you very much. Rigorous understanding. Excellent. Kind regards Medius.,2010-11-17T22:41:44.887,,CC BY-SA 2.5,Medius
6880,4642,0,"@Aniko : I agree, malformed is a wrong word, but I didn't really know how to say it different.",2010-11-17T23:12:13.627,1124,CC BY-SA 2.5,
6881,4646,0,"Thanks for your reply.  I've done (1) and it seems pretty reasonable.  As for dealing with the presence of C - I neglected to mention that the number of flips, or number of heads, of C should be fairly constant, which is why I thought including the intercept term might work.",2010-11-17T23:17:47.937,1777,CC BY-SA 2.5,
6882,4598,0,In the trading world a moving standard deviation is known as Bollinger Bands. A web query on this will generate thousands of hits.,2010-11-17T23:53:00.533,226,CC BY-SA 2.5,
6883,4635,7,"(+1) Worth reading, yes, entertaining, yes. But a textbook? Not exactly... I take the subtitle to mean it's intended as a *companion* to a textbook. Andrew Gelman's review of it is worth reading too: http://www.stat.columbia.edu/~gelman/research/published/angristpischke2.pdf",2010-11-18T00:56:19.357,449,CC BY-SA 2.5,
6886,4649,0,Not clear what you are asking...what do you want to add another market to?,2010-11-18T01:51:40.613,511,CC BY-SA 2.5,
6888,4635,0,"@onestop: That's a fair point; I wouldn't regard this to be a textbook, but worth considering for someone wanting to learn Econometrics, regardless.",2010-11-18T01:58:13.210,5,CC BY-SA 2.5,
6889,4567,5,"@Whuber Have a look at the Goodman paper. It accords pretty well with my experience in the field of pharmacology.
Methods say ""Results where P<0.05 were taken as statistical significant"" and then results are presented with + for p<0.05, ++ for p<0.01 and +++ for p<0.0001. The statement implies the control of error rates a la Neyman and Pearson, but the use of different levels of p suggest Fisher's approach where the p value is an index of the strength of evidence against the null hypothesis. As Goodman points out, you cannot simultaneously control error rates and assess strength of evidence.",2010-11-18T02:41:07.073,1679,CC BY-SA 2.5,
6890,4655,2,"Thanks for the clarification and for encouraging me to read up, think a bit, and revise my question. It led me to the Merkle reference. Additional feedback appreciated.",2010-11-18T03:03:03.407,1381,CC BY-SA 2.5,
6891,4549,0,"@Tal:  I really like ez for most of my ANOVA analyses, but I couldn't see a way to use it for contrast codes; I think I've seen that the author uses this site, perhaps he'll consider adding it to his wrapper.",2010-11-18T03:14:19.860,196,CC BY-SA 2.5,
6895,4625,1,"Thanks Graham, I was looking everywhere for an option to select, but I see you need to double click on the output and a model viewer comes up with the information. Thanks.",2010-11-18T08:41:52.240,2025,CC BY-SA 2.5,
6899,4644,0,"@gakera I'm sure this was not for getting upvotes. Most of the times, we set our own response as Community Wiki (CW), when they don't add further or contradictory information. This is a neutral way to sum up or aggregate others' responses.",2010-11-18T09:30:52.637,930,CC BY-SA 2.5,
6901,4666,6,"With respect, this looks like a direct copy-and-paste from Wikipedia, and doesn't really answer the question.",2010-11-18T09:51:13.590,439,CC BY-SA 2.5,
6902,4639,1,"This question has been downvoted. I didn't intend to give my +1, but it seems to me now that voting it down is not very constructive: (1) the OP makes clear this is homework and uses an R built-in data set for illustration, not his data, (2) a related question with `step()` has been rated +2 at the time of this writing (so why?!), (3) the OP acknowledged the usefulness of @Joris's response.",2010-11-18T09:52:35.903,930,CC BY-SA 2.5,
6903,4644,0,"ah, I didn't notice that option, sorry. I changed this to CW. I assumed everybody could edit, since you could.",2010-11-18T09:53:38.737,1994,CC BY-SA 2.5,
6905,4667,0,Which other languages are you interested in?,2010-11-18T10:00:34.527,930,CC BY-SA 2.5,
6906,4667,0,"Dear chl - any of them.  My real motivation is to find bloggers in other languages, but I would feel fine with other websites from which to find bloggers (see more about my motivation here:  http://www.r-bloggers.com/r-bloggers-now-in-your-language/)",2010-11-18T10:01:50.150,253,CC BY-SA 2.5,
6907,4666,4,"(-1) At the very least, you should acknowledge the quoting from Wikipedia, ¬ß on LASSO method at http://en.wikipedia.org/wiki/Least_squares!!! BTW you forgot to paste the 11th reference.",2010-11-18T10:15:46.470,930,CC BY-SA 2.5,
6908,4644,0,"@gakera No, one needs enough rep to edit others' post, see the [FAQ](http://stats.stackexchange.com/faq). Good to hear you edited your post yourself. Welcome to CrossValidated!",2010-11-18T10:20:37.293,930,CC BY-SA 2.5,
6909,4666,0,"I forgot to put the link, it is true, but anyway I think that is a good reply for this questions. Sorry if I made you think I wrote that",2010-11-18T10:25:34.123,1808,CC BY-SA 2.5,
6912,4639,0,"@chl : seems like I'm not the only one with sensitive toes when it comes to the R help pages :-). But I agree wholeheartedly with you. The question is valid, asked in a clear way and hence there is absolutely no reason whatsoever to downvote it.",2010-11-18T10:31:00.360,1124,CC BY-SA 2.5,
6913,4667,0,The only French blogger I know is writing in... English (and isn't very productive anymore :) http://zoonek.free.fr/,2010-11-18T10:43:00.607,930,CC BY-SA 2.5,
6914,4630,1,"Whuber and onestop thanks for the answers. Now I understand how to check that my marginal distributions X, Y and Z are uniform. However to prove that the joint distribution of (X,Y,Z) is uniform in R^3 we have to prove, that (X,Y,Z) are independent. So the question is: by given sample (X_k, Y_k, Z_k) how can I check whether random variables (X,Y,Z) are independent? Thanks!",2010-11-18T10:43:28.767,,CC BY-SA 2.5,Oleg
6915,4667,0,"Dear chl, I actually got his permission some time ago to add him to R-bloggers.  I do hope he'll go back to writing one day.  Thanks for the pointers so far.",2010-11-18T10:45:25.073,253,CC BY-SA 2.5,
6916,4670,0,Wonderful link to that blogger - thank you aix !,2010-11-18T10:47:09.690,253,CC BY-SA 2.5,
6917,4639,1,"Heh, I'm sorry if I stepped on your toes with my snide at the help, I'm just not very patient when it comes to anything with a command line really. I'm weird that way, I know. You wouldn't be the first ones to call me out on it :) I like this place, people are honest.",2010-11-18T10:49:15.977,1994,CC BY-SA 2.5,
6918,4666,0,"It would be more helpful to refer to [The Lasso Page](http://www-stat.stanford.edu/~tibs/lasso.html) in that case. Now, the question is about pros and cons of LAR and Lasso, not about what Lasso actually does. The LARS algorithm might easily be modified to produce solutions for other estimators, like the Lasso; it works well in the $n\ll p$ case, but it is sensitive to the effects of noise (because it is based upon an iterative refitting of the residuals), as quoted from http://scikit-learn.sourceforge.net/modules/glm.html.",2010-11-18T10:50:03.287,930,CC BY-SA 2.5,
6919,4639,0,"There we go, I edited the question so that it's not as off-putting to advocates of R and the R help :) And reworded the question on AIC to avoid misleading OP only readers.",2010-11-18T10:55:24.253,1994,CC BY-SA 2.5,
6925,4630,0,Good question (i.e. i can't immediately think of an answer).,2010-11-18T12:48:13.253,449,CC BY-SA 2.5,
6926,4673,0,"No, i wish to construct a function of one variable. Something like ""ff<-function(x) 3.23+2.78x-3.45x^4"".",2010-11-18T12:49:55.920,2043,CC BY-SA 2.5,
6928,4673,2,"@Roah Did you try the answer? Because that's exactly what it will give you (you won't see the coefficients, but why do you care about the internals of the function?",2010-11-18T13:54:09.783,279,CC BY-SA 2.5,
6929,4673,0,@Roah - Could you explain in what way the function I suggested does not do what you expect? It certainly accepts scalars so you can call it with a single value. Is the return value not what you are expecting?,2010-11-18T13:55:40.213,439,CC BY-SA 2.5,
6930,4567,8,"@Michael There are alternative, more generous interpretations of that kind of reporting.  For example, the author might be aware that readers might want to apply their own thresholds of significance and therefore do the flagging of p-values to help them out.  Alternatively, the author might be aware of possible multiple-comparisons problems and use the differing levels in a Bonferroni-like adjustment.  Perhaps some portion of the blame for misuse of p-values should be laid at the feet of the reader, not the author.",2010-11-18T14:04:04.577,919,CC BY-SA 2.5,
6931,4454,1,"@Matt this question is not intended to have anything to do with programming. It's about communicating a questionnaire, strictly from a statisticians point of view, to a programmer",2010-11-18T14:31:27.347,305,CC BY-SA 2.5,
6933,4673,0,"Sorry, aix! I was too bothered by the internal structure of the algorithm... And it is very simple! Thanks a lot, because i was going to parse the output...",2010-11-18T15:04:00.280,2043,CC BY-SA 2.5,
6938,4684,0,"I appreciate the effort. I'm going to have to really study this because it's not my ""native tongue"". Also, I'm seeing a lot of dollar signs and formatting stuff. Is there something I don't know about that makes it look like real math?",2010-11-18T17:17:27.467,1270,CC BY-SA 2.5,
6940,1146,1,"+1 ""educate the public"" == *communicate* models. (To whom, how ? papers, pictures / infographics, interactive models ...)",2010-11-18T17:31:52.173,557,CC BY-SA 2.5,
6941,4657,0,"Thanks for your answer, but what I am really interested in knowing is under what cases using medians rather than means provides better estimates the central tendency (median or mean) of the function.",2010-11-18T17:55:23.833,1381,CC BY-SA 2.5,
6942,4684,1,@Mike See http://meta.stats.stackexchange.com/q/218/919 .,2010-11-18T18:12:21.460,919,CC BY-SA 2.5,
6943,4688,0,"(+1) Yes, I'm aware of that project too. But I know that those that are currently developing the `scikit.learn` are serious guys working in neuroimaging, and that the project will evolve and include ever growing features.",2010-11-18T18:45:59.423,930,CC BY-SA 2.5,
6944,4660,3,"It seems to me that the Chi Square test will be particularly *poor* at identifying differences in tails.  If the tails are covered by many bins, then--because they are tails!--there may be few data in any of the bins, invalidating the chi-squared approximation.  If the tails are covered by few bins, then you lose almost all power to discriminate their shapes, and what you do manage to discriminate might not be terribly relevant or useful.  (One problem we're up against here is that ""fatness of tail"" has not been defined, so the question is really too vague to answer well.)",2010-11-18T19:22:27.073,919,CC BY-SA 2.5,
6945,4661,2,Why would this family of distributions be particularly good for this problem and not some other family like the Pearson distributions?,2010-11-18T19:24:55.417,919,CC BY-SA 2.5,
6946,4686,0,Is anything known / can be assumed about the $b_i$'s?,2010-11-18T19:48:40.640,279,CC BY-SA 2.5,
6947,4686,0,"Each $b_i$ is a binary bit -- that is, $b_i \in \{0,1\}$. But we don't assume anything about how the $b_i$s correlate to the costs $c_i$.",2010-11-18T19:50:36.507,,CC BY-SA 2.5,Frank
6948,4688,0,@chi I've decided to go with `scikit.learn` as my reference implementation (and have accepted your answer.) It looks like high-quality code.,2010-11-18T20:27:15.820,439,CC BY-SA 2.5,
6949,4691,1,"Not sure if this helps, but this article claims using Cronbach on ordinal data will underestimate the true value: http://www.amstat.org/meetings/jsm/2007/OnlineProgram/index.cfm?fuseaction=abstract_details&abstractid=308244",2010-11-18T20:57:36.007,1118,CC BY-SA 2.5,
6950,4660,0,"@whuber, I can't say whether I concur with your comment because I don't fully understand one of your points.  What do you mean exactly by ""invalidating the chi-squared approximation""?",2010-11-18T21:04:02.210,1329,CC BY-SA 2.5,
6951,4679,1,"I know those look almost the same, but if I substitute y for n-x, and if I take the Beta pdf and substitute x for a-1 and y for b-1 I get an extra factor of (x+y+1), or n+1. i.e. (x+y+1)!/x!/y!*p^x*q^y. That seems to be enough to throw me off.",2010-11-18T21:10:19.257,1270,CC BY-SA 2.5,
6952,4686,0,are the costs $c_i$ known _a priori_?,2010-11-18T21:12:53.987,795,CC BY-SA 2.5,
6953,4686,0,"Yes, we know the $c_i$.",2010-11-18T21:20:21.137,,CC BY-SA 2.5,Frank
6954,4693,1,"No, that doesn't seem right. Suppose we have many $c_i = \epsilon$, and then some more expensive ones: a greedy sampling procedure would buy -all- of the cheap bits before the expensive ones, but there is no reason to buy all of the cheap bits -- we quickly have high accuracy there.",2010-11-18T21:21:45.317,,CC BY-SA 2.5,Frank
6955,4679,2,"Maybe somebody will chime in with a full response, but in an ""intuitive"" explanation we can always hand-wave away constants (like $n+1$) that do not depend on the variables of interest ($x$ and $p$), but are required to make the pdf add/integrate to 1. Feel free to replace the ""equality"" signs with ""proportional to"" signs.",2010-11-18T21:37:02.647,279,CC BY-SA 2.5,
6956,4679,0,"Good point. I think I'm getting closer to an understanding. I'm still trying to be able to say what x tells you about the p distribution, and why those two cdfs sum to 1.",2010-11-18T21:43:00.413,1270,CC BY-SA 2.5,
6957,4693,1,"You have high accuracy only if $b_i$ are iid. If anything goes, then the only way to make inference is to almost exhaust the population.",2010-11-18T21:45:26.840,279,CC BY-SA 2.5,
6958,4693,0,"The $b_i$ can be set by any arbitrary (adversarial) process, but must be fixed before we begin sampling. At that point, they just form a fixed population, and we can, for example, sample uniformly at random among the individuals with low $c_i$ to obtain high accuracy among one subset of the population.",2010-11-18T21:52:09.380,,CC BY-SA 2.5,Frank
6960,4658,0,"Is the ""fat-tails"" tag really meaningful (for future questions)?",2010-11-18T22:22:31.310,930,CC BY-SA 2.5,
6961,4693,0,"I'm starting to think I do not understand the question. Is it the case that we have observations of $m$ individuals, each with $n$ bits of data associated? And then we can sample one bit of each individual with a known cost? Otherwise, I don't understand the 'subset of the population' part.",2010-11-18T22:23:15.677,795,CC BY-SA 2.5,
6963,4693,0,"@Frank That's right, but if there's any association between the values of the bits and the costs, then your estimate based on sampling only the low-cost bits will be biased and thereby might *never* attain the targeted low error rate (except, as @Aniko points out, when this almost exhausts the population).  The way out is to sample the bits with probabilities that depend on the costs (and, as it turns out, on the value of k) and to use an estimator that accounts for the differing probabilities.",2010-11-18T22:42:20.207,919,CC BY-SA 2.5,
6964,4660,1,The chi-squared test is based on a Normal-theory approximation to the true distribution of the chi-squared statistic.  Typically this approximation gets poor when bin populations drop below 5.,2010-11-18T22:47:14.900,919,CC BY-SA 2.5,
6965,4679,1,"I take a different view of ""intuitive"" explanations.  In some cases we don't care too much about constants, but in this case the *crux* of the matter is to see why an n+1 appears and not an n.  If you don't understand that then your ""intuition"" is incorrect.",2010-11-18T22:49:23.960,919,CC BY-SA 2.5,
6966,4693,0,"@Shabbychef Think of a box containing n envelopes.  On the outside of each envelope is written a cost c_i and inside the envelope is a slip of paper on which is written either a zero or a one: the b_i.  You are allowed to peek into the box where you can see all the c_i, but in order to see any of the b_i you need to pay the value of c_i written on its envelope.  The objective is to obtain an acceptably accurate estimate of the total (B) of all the zeros and ones in the box and to do so by paying as little as possible.",2010-11-18T23:10:05.737,919,CC BY-SA 2.5,
6968,4660,0,"@whuber, thanks for the explanation.  In view of it, I feel like your initial comment's first phrase may not be as nuanced as you may have cared to (""the Chi Square test will be particularly poor at identifying difference in tails"").  Maybe the more appropriate statement would have been ""it depends...""  This test has several merits, including forcing you to define the relevant bins.  And, just as importantly facilitate the construction of a histogram.  Granted if you have fewer than 5 observation in a bin, you will lose accuracy as you well explained.",2010-11-19T00:16:50.433,1329,CC BY-SA 2.5,
6969,4699,0,"I should note that my problem is not 'online': I have all the data at my disposal, and do not need to e.g. identify the first time my process goes haywire.",2010-11-19T01:36:32.127,795,CC BY-SA 2.5,
6970,4693,0,"@whuber: this is what I thought. However, I am still confused by 'subset of the population'.",2010-11-19T01:38:26.837,795,CC BY-SA 2.5,
6971,1036,4,This book it excellent. It's short and practical.,2010-11-19T01:48:48.337,1146,CC BY-SA 2.5,
6974,4635,0,I would say this coupled with Wooldridge's Econometric Analysis of Cross Section and Panel Data would provide the most updated perspective.,2010-11-19T03:02:32.527,96,CC BY-SA 2.5,
6975,4615,3,"Greene's book is encyclopedic and a must-have reference.  But I have two quibbles: (i) I feel it is a bit out of date relative to current econometric practice; (ii) the presentation is very non-linear, but at the same time, in his exposition Greene will use symbols that were defined maybe 100 pages prior in a completely different context, making things horribly confusing at times.  As mentioned below, I would vote for Wooldridge coupled with Angrist and Pischke for an up to date and thorough perspective.",2010-11-19T03:05:59.497,96,CC BY-SA 2.5,
6976,4617,2,"My sense is that this, coupled with Angrist and Pischke, would provide the most up to date perspective, although both are geared more toward micro analysis, rather than the types of time series or time-series cross-section analysis that macro analyses entail.",2010-11-19T03:09:27.527,96,CC BY-SA 2.5,
6977,4567,5,"@Whuber I agree entirely, but only that what you suggest is true in some small fraction of cases (a restricted version of 'entirely'). There are some journals that specify that p values should be reported at one, two or three star levels rather than exact values, so those journals share some responsibility for the outcome. However, both that ill-considered requirement and the apparently naive use of p values might be a result of the lack of a clear explanation of the differences between error rates and evidence in the several introductory statistics texts that are on my shelves.",2010-11-19T03:40:25.297,1679,CC BY-SA 2.5,
6978,4702,6,"+1: very nice link! I guess the definition also varies depending on the field (e.g. #4 is very mathematical/statistical, but #1 and #2 are more ""understandable"" from a life science point of view)",2010-11-19T06:39:42.860,582,CC BY-SA 2.5,
6980,4588,0,@chi Thanks very much for the interpretation. That makes sense.,2010-11-19T07:21:09.993,1862,CC BY-SA 2.5,
6982,4658,0,"@chl You tell me, I am certainly not as experienced as you in statistics. But IMO it is a classic bias to underestimate the importance of tails. Have you read the work of Mandelbrot? Fat tails are very important in applied statistics for finance and the credit crisis of 2008 came for some part from some pricing models that were assuming normality and underestimating the fat tails of some correlation distribution. We can discuss that in another thread :)",2010-11-19T08:23:32.003,1709,CC BY-SA 2.5,
6983,4570,0,"If you use Bayesian regression then the error bars also indicate the uncertainty due to the finite nature of the dataset (given the prior), you only trust the model as far as the error bars suggest you ought to trust it. If you don't have enough data to make a useful inference it will generally be evident in the posterior distributions for the parameters and/or predictions.  The usual frequentist error bars will probably say pretty much the same thing.  At the end of the day, sometimes only a small dataset is available, it just limits the confidence in your conlcusions.",2010-11-19T09:14:11.553,887,CC BY-SA 2.5,
6984,4570,0,"Agreed for the Bayesian regression. Thanks for pointing that out. But if you have two points that form a straight line, how do you calculate the frequentist error bars? And let's say that you have enough points to calculate the frequentist error bars, from how many points can you trust them (should we use the error bars of the error bars?)",2010-11-19T09:26:57.503,1709,CC BY-SA 2.5,
6986,4570,0,"Sorry, I am not that familiar with the limitations of frequentist error bars (I only mentioned them so as not to appear bigoted ;o).  IIRC frequentist error bars for ridge regression are very similar to the Bayesian ones for a fixed value of the ridge parameter (a full Bayesian analysis would marginalise over the ridge parameter as well).",2010-11-19T09:50:49.527,887,CC BY-SA 2.5,
6989,4689,0,"Sorry, I don't understand what you mean by the second sentence. Would you try and reword it?",2010-11-19T10:06:13.320,8,CC BY-SA 2.5,
6991,4606,1,"Here is the link to implementation in R (http://www-etud.iro.umontreal.ca/~botev/kde.R). I understand your question now. Mine is why you want to do that ? I would perform a MC as described to get an estimate of the error of the kde, but not to calculate a kde for NB, which uses maximum likelihood estimates of all variables per definition ! I think that performing the algorithm as described by you and calculating the kde directly from the training data (without any subsampling) will not affect the result of NB.",2010-11-19T10:43:11.947,264,CC BY-SA 2.5,
6992,4685,0,"You may need to explain what you mean by soft and hard EM, or provide a link.",2010-11-19T11:02:03.530,495,CC BY-SA 2.5,
6993,4689,0,"ohu, i'm just joined the world of statistic and machine learning, sorry i didin't find out how to link unsupervised learning with decision theory. but i'm still studing!",2010-11-19T11:23:27.670,2046,CC BY-SA 2.5,
6994,2365,0,"I was recently given a copy of Felsenstein's book.  In chapter 18 he does not say why you can't use an improper flat prior on 0-infinity.  Neither does he mention MaxEnt or transformation groups in his criticism of uniformative priors.  While the rest of the book may be very good; this suggests inadequate scholarship on that particular issue.  Caveat lector - just because something appears in a text book or journal paper, doesn't mean that it is correct.",2010-11-19T13:06:00.873,887,CC BY-SA 2.5,
6995,4557,0,if the noise process is skewed but homoscedastic then it is still inappropriate if you need the error bars.,2010-11-19T13:10:33.167,887,CC BY-SA 2.5,
6996,4570,1,Isn't performing a test implicitly looking at the confidence intervals?  Perhaps it would be that the sin test-wise is ignoring the power of the test?,2010-11-19T13:11:43.460,887,CC BY-SA 2.5,
6997,4711,0,Than you for comment.,2010-11-19T13:17:10.103,,CC BY-SA 2.5,Peter
6998,4711,0,"Then, do two datasets having the same eigenvectors means they are same or similar data sets?",2010-11-19T13:23:03.217,,CC BY-SA 2.5,Peter
6999,2365,0,"@Dikran: entropy maximization without testable information gets only one constraint: probabilities sum up to one. Most often the uniform distribution is taken there. I don't take it as granted, but I do agree with Felsensteins calculations and reasoning. So we disagree, like more people in that field. Felsenstein if far from accepted by everybody, and I'm not accepting everything he says. But on this point, I follow him. Sometimes a Bayesian approach is not superior to another one. And the case he describes is one such a case according to me. YMMV.",2010-11-19T13:32:08.170,1124,CC BY-SA 2.5,
7000,4706,0,I think I'm going to need to see that presentation in person to get the full impact.,2010-11-19T13:36:07.997,1036,CC BY-SA 2.5,
7001,4689,2,"I'm just confused how it fits in with the question. For example, the words ""prediction"", ""decision theory"" or ""unsupervised"" don't appear in the accepted answer",2010-11-19T13:43:02.123,8,CC BY-SA 2.5,
7002,4703,6,"Your not wrong, but your definition of what a fixed effect is is not what I would think of when someone says fixed effect. Here is what I think of when someone says fixed effect http://en.wikipedia.org/wiki/Difference_in_differences , or this http://www.stata.com/support/faqs/stat/xtreg2.html (particularly equation 3 on the Stata page)",2010-11-19T13:44:08.363,1036,CC BY-SA 2.5,
7003,4567,0,@Michael All good points; thank you.  And thank you for sharing the references: they make good reading (even though the papers addressed to medical practitioners belabor their arguments).,2010-11-19T13:49:32.273,919,CC BY-SA 2.5,
7004,4725,1,(+1) Nice that you cite Laura's textbook and add this complement of information about Wilson's CIs.,2010-11-19T13:52:02.693,930,CC BY-SA 2.5,
7005,4660,0,"@Gaetan I do appreciate attention to nuance, but in this case the judgment seems justified.  Compared to the many other methods available for comparing distributions, the Chi Squared test doesn't hold up well.  If you ""define relevant bins"" based on the data themselves, the test is invalid.  Also, a histogram is not usually a useful way to look at tails of distributions.  However, I'm reluctant to propose alternatives because the problem is ill-defined: just what might it mean for two distributions to have the same ""fatness of tails""?  Kurtosis is one possibility, but it's a crude measure.",2010-11-19T13:55:20.570,919,CC BY-SA 2.5,
7006,4658,1,"This question is potentially interesting but some clarification would be welcome.  Are you concerned about one tail or both?  How do you measure ""fatness""?  (Are you willing to shift and rescale the two distributions to make the comparison, for instance?)  How do you measure deviations in ""fatness""?  If you contemplate a hypothesis test, then what will the alternative hypothesis be, precisely?",2010-11-19T13:57:50.223,919,CC BY-SA 2.5,
7008,4690,2,"Nice answer. With respect to comparing canonical examples of discriminative vs generative classifiers (logistic regression and Gaussian naive Bayes respectively), I found this book chapter to be very accessible than the Ng: http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf",2010-11-19T14:25:56.150,1080,CC BY-SA 2.5,
7009,4725,2,Thanks.  I would like to point out that the Wilson interval is discussed in the article that @Joris referenced.,2010-11-19T14:35:02.550,,CC BY-SA 2.5,user1108
7010,4701,0,I appreciate your take on it. All the answers are helping me to think about the question and possibly understand better what I'm asking.,2010-11-19T14:56:34.260,1270,CC BY-SA 2.5,
7011,4697,0,Thanks -- your example is elucidating. You say that methods for solving these kinds of problems are well known -- do you have a reference that discusses a general solution?,2010-11-19T15:09:51.840,,CC BY-SA 2.5,Frank
7012,4693,0,"shabbychef -- in the simplest case, imagine that 90% of the population has cost ""Low"" to sample from, and 10% of the population has cost ""High"" to sample from. We could sample uniformly from the Low individuals to get a high confidence within this segment of the population, and do the same thing among the High individuals, and then combine estimates. But because the ""High"" individuals represent only 10% of the population, we can tolerate higher error in this segment of the population, and so can sample them more sparingly. Our total error will be .9*error(Low) + .1*error(High).",2010-11-19T15:12:39.017,,CC BY-SA 2.5,Frank
7014,4705,0,Thanks a lot. I was dumb not to look at the source code. It mentions this clearly...,2010-11-19T15:52:11.337,1307,CC BY-SA 2.5,
7015,4697,0,"@Frank This is a straightforward, if messy, application of methods of decision theory described in textbooks on the subject by E.S. Lehman (*The Theory of Point Estimation*) and Jack Kiefer (*Introduction to Statistical Inference*), among others.",2010-11-19T16:02:02.910,919,CC BY-SA 2.5,
7016,4693,0,"@Frank My second reply gives some quantitative support to your analysis.  However, the optimal solutions conform to @shabbychef's initial description: they tend to sample all the cheapest bits and then obtain a subset of the more expensive ones, which is exactly what a greedy algorithm would do.",2010-11-19T16:14:39.450,919,CC BY-SA 2.5,
7017,4711,4,"@Peter if the eigenvectors are identical and so are their corresponding eigenvalues, then you can recreate the covariance (or correlation) matrix you used for PCA.  However, such a matrix obviously does not determine the data.  For instance, subtracting a constant from all values won't change the covariances.",2010-11-19T16:18:17.093,919,CC BY-SA 2.5,
7018,4711,2,"re: your last comment.  Take any k datasets you like, each with n values, and combine them into one k-variate dataset of n records after first randomly permuting the order of each dataset.  The random permutations assure the correlation matrix will be close to the identity.  Do the same for another collection of k completely different datasets.  The two PCAs will be essentially the same, but by construction there is *no* relationship between the two collections of data.  What is common is the mutual independence of the coordinates in each k-variate combination, nothing more.",2010-11-19T16:23:04.187,919,CC BY-SA 2.5,
7019,4567,0,@Michael: (+1). The Goodman article was especially insightful in supporting your argument.,2010-11-19T16:36:04.347,1118,CC BY-SA 2.5,
7020,4143,0,"Hi, can you suggest some reference for Bayes Factor that you mentioned? Thanks.",2010-11-19T16:52:57.187,528,CC BY-SA 2.5,
7021,4729,0,"Being not in your area I'm not sure what you mean by ""each treatment should be replicated three times"" or the precise nature of treatment; are they between or within samples?  To me it seems practical to reduce to a treatment and control (after all more samples in a design cell the better), but that is a trade off you should determine based on your needs.  Which is more important, experimental power or evaluating both treatments?",2010-11-19T16:56:43.303,196,CC BY-SA 2.5,
7022,4660,0,"@whuber, your comments are well put.  We just disagree on the value of histograms and actually graphing and visualizing what the tails actually look like.  It counts for a lot, and it is often much more informative than a test that gives you a statistical significance that may often be more a function of n than any real difference in the data set.  Given that, I appreciate your time on this.  And, I will not debate my points any further as I would only repeat myself.",2010-11-19T17:02:33.777,1329,CC BY-SA 2.5,
7023,2365,0,"I am not suggesting a Bayesian approach is any better than a frequentist one - horses for courses.  In this case it is probably transformation groups that hold the key.  It is quite possible that a prior on branch length that is invariant to the units used is equivalent to a flat prior on the probability of a change - in which case Felsensteins criticism is badly misguided.  Uninformative priors are not necessarily flat and it is inappropriate to criticize uninformative priors without mentioning the standard procedures for finding them!  Not that this means Bayesian is better, of course.",2010-11-19T17:32:52.707,887,CC BY-SA 2.5,
7024,4143,1,@Samik. Check this out http://bit.ly/9YXqCy,2010-11-19T17:38:39.710,1307,CC BY-SA 2.5,
7025,4660,1,"@Gaetan Just to clarify a possible misapprehension: I am not advocating not graphing or exploring the data, nor have I ever said anywhere that there is no value in graphing or visualizing data--just the opposite, in fact.  However, histograms often don't do a good job of displaying tails in data.  Probability plots, survival curves, and related approaches do much better.  A histogram is naturally associated with a chi-squared test while a probability plot is naturally associated with KS-like tests and their ilk.",2010-11-19T17:44:22.913,919,CC BY-SA 2.5,
7026,4734,1,"I don't think the preceding question has anything to do with CFA. It was clearly about EFA vs. PCA, with the possible confusion that EFA can actually be based on ""principal components"". But maybe you could reword your question if you have a specific application in mind?",2010-11-19T19:18:52.010,930,CC BY-SA 2.5,
7027,4733,0,"It is hard to imagine data with exponentially distributed ""noise"", as noise is supposed to average to 0. Perhaps you could give more details, and get a more meaningful answer.",2010-11-19T19:28:10.890,279,CC BY-SA 2.5,
7030,4630,0,And is it possible to check whether say (X_k) and (Y_k) are independent?,2010-11-19T20:53:05.280,,CC BY-SA 2.5,Oleg
7031,4733,0,"Do you perhaps mean ""double exponential"" instead of exponential?  http://en.wikipedia.org/wiki/Laplace_distribution .  When errors have a double exponential distribution it is optimal to minimize the sum of absolute values of residuals, leading to median regression.  (Normally one would not model ""errors"" with an exponential distribution because an ""error"" should have zero expectation.)",2010-11-19T21:33:45.040,919,CC BY-SA 2.5,
7032,4740,1,"+1.  Actually, kurtosis and skewness aren't any more of a problem to compute this way: typically, wherever division by N is called for, one divides by the sum of weights.  The median can be obtained by sorting the data, computing the cumulative sum of weights, and picking the value where half the total sum is crossed.  Weighted percentiles are obtained similarly.",2010-11-19T21:52:07.803,919,CC BY-SA 2.5,
7033,4734,4,http://www2.sas.com/proceedings/sugi30/203-30.pdf,2010-11-19T21:54:20.217,919,CC BY-SA 2.5,
7034,4740,2,"I agree that it depends entirely on what sort of weights you have. Decent statistical packages distinguish between several (sampling weights, frequency weights, inverse-variance weights...) and treat them differently.",2010-11-19T22:04:42.683,449,CC BY-SA 2.5,
7035,4720,2,"Did you really mean to say that the logit ""transforms the binomial distribution in a normal distribution""??",2010-11-19T22:06:46.883,919,CC BY-SA 2.5,
7036,4734,1,"You may want to rethink the question.  Whether you put ""Exploratory"" or ""Confirmatory"" in front of it, C(ommon)FA is based on some common portion of the variance presumed to represent latent constructs.  Principal components is based on the total variance and component scores are a unique linear combination that maximizes that total variance explained by the factors.  These are different models.",2010-11-19T22:17:32.110,485,CC BY-SA 2.5,
7040,4720,0,"@whuber: nice catch of the formula, and nice catch of the formulation. Pretty much not. It makes sure the errors in a logistic regression follow the normal distribution. Thx for the correction.",2010-11-20T00:18:56.950,1124,CC BY-SA 2.5,
7041,4743,0,"That my null hypothesis is flat in $x_1$ and $x_2$ doesn't mean that it is flat in some transformed quantities which lead to my measurements being distributed flat in the available parameter space, right? Why do I not need to worry about this? And I am dealing with hundred millions of observations, so histograms are just easier to use for me.",2010-11-20T03:07:22.160,56,CC BY-SA 2.5,
7042,4693,0,"@shabbychef I believe @Frank is thinking in terms of stratifying the population according to costs, in an analogy to importance sampling.  Thus, if we preferentially sample the lowest-cost bits we might consider that we have a good idea of the contribution to B of the ""low cost"" subset of the population but only a poor idea of the contribution from the ""high cost"" subset.  This reflects a suspicion that the frequency of 1's might vary by cost.",2010-11-20T03:46:20.780,919,CC BY-SA 2.5,
7043,4693,0,"@Frank, @whuber I see it now. I had problems imagining why one would be interested in the contribution to $B$ of a sub-population. Apparently this is for good reason because the OP is interested in $B$ for the entire population.",2010-11-20T05:11:53.033,795,CC BY-SA 2.5,
7045,4597,1,"I am not an expert in computer vision, but I would bet there are better and faster algorithms to do that.",2010-11-20T10:23:05.457,582,CC BY-SA 2.5,
7046,4598,1,You can look at the rollapply function in the zoo package.  And use sd as the function.,2010-11-20T11:09:54.873,253,CC BY-SA 2.5,
7056,4701,0,"I revised the question, if you care to take a look. Thanks.",2010-11-20T15:05:48.727,1270,CC BY-SA 2.5,
7057,4759,6,pretty much solved here: http://stackoverflow.com/questions/2200460/what-programming-languages-are-good-for-statistics,2010-11-20T16:06:51.820,22,CC BY-SA 2.5,
7058,4626,0,"@aix. Yes, it all depends on the implementation you use and the facilities you have access to. For example: if you have access to a good lp solver, you can feed it with the past optimal values of $\beta$ and it'll carry the 1-2 step to the new solution very efficiently. You should add these details to your question.",2010-11-20T16:27:51.053,603,CC BY-SA 2.5,
7059,4684,0,"I'm a little slow, but you're getting through to me.
The denominator of the Beta integral is exactly what I came up with.",2010-11-20T17:48:36.443,1270,CC BY-SA 2.5,
7060,4759,2,Made wiki since this is entirely subjective.,2010-11-20T19:17:39.027,5,CC BY-SA 2.5,
7062,4741,1,What do you mean by assigning an uncertainty? Do you want something like a standard error or confidence interval for each bin?,2010-11-20T19:43:54.957,279,CC BY-SA 2.5,
7063,4754,0,"Wow, thanks! This is a lot more efficient than recursively counting minima/maxima within a set interval... I'll have to go and check it out.",2010-11-20T21:58:13.210,2002,CC BY-SA 2.5,
7065,126,34,This is an introductory book for people who have a decent amount of statistical background already.,2010-11-20T23:41:08.573,1146,CC BY-SA 2.5,
7066,4762,3,"Of course now the SPSS way can also be the R way: ""Starting with Version 16, SPSS offers a free plug-in that lets users run R code within SPSS having full access to the active SPSS Statistics data, and writing its output to the SPSS Statistics Viewer"" (http://insideout.spss.com/2009/01/13/spss-statistics-and-r/)",2010-11-21T01:22:33.923,196,CC BY-SA 2.5,
7068,4766,0,"As a first stab, I would guess you should normalize the vectors you are projecting onto, otherwise their magnitude could screw things up.",2010-11-21T04:50:43.183,795,CC BY-SA 2.5,
7069,4768,0,are we to suppose it does each of these with probability $1/4$?,2010-11-21T05:40:42.343,795,CC BY-SA 2.5,
7070,4762,0,"Your link seems like it explains it fairly well, gd. It's a little verbose, but it does the job. If you really feel most comfortable in R and need the output in SPSS, DrNexus' suggestion about the SPSS-R connection is sufficient.",2010-11-21T06:06:00.740,1118,CC BY-SA 2.5,
7071,4768,0,"yes, each outcome has a 1/4 probability.",2010-11-21T06:10:21.813,2079,CC BY-SA 2.5,
7072,4766,0,"Thanks for that.

See ""A randomized algorithm for the decomposition of matrices by Per-Gunnar Martinssona, Vladimir Rokhlinb and Mark Tygert"" for more details",2010-11-21T09:51:39.353,2078,CC BY-SA 2.5,
7073,4701,1,"Regarding your revision: Yes, $F\sim Beta(I+1,N-I+1)$, as long as your sampling intervals are long enough that each observation is independent and identically distributed.

Note that if you want to be Bayesian about it and specify a nonuniform prior distribution for what you expect the actual proportion to be, you can add something else to both parameters.",2010-11-21T11:51:19.177,2456,CC BY-SA 2.5,
7074,4768,19,"from a biological point of view, that chance is 1. The environment is bound to change to a point that no population can survive, given that in x billion years the sun is to explode. But I guess that's not really the answer he was looking for. ;-) The question doesn't make sense either. An amoebe can only divide into 2 or 0. Moral: traders shouldn't ask questions about biology.",2010-11-21T12:34:13.677,1124,CC BY-SA 2.5,
7075,4768,7,Such a question on interview for a such position? Maybe it is something like http://dilbert.com/strips/comic/2003-11-27/ ?,2010-11-21T14:02:48.010,,CC BY-SA 2.5,user88
7076,4773,9,"$\sqrt{2}-1$ is 0.4142, so it is in agreement with Mike's analytical result. And +1, because I like simulations ;-)",2010-11-21T14:04:36.493,,CC BY-SA 2.5,user88
7077,4772,3,"The reason 1 is not a root is easily seen by considering the expected number of Amoeba after $k$ steps, call it $E_k$. One can easily show that $E_k = E_1^k$. Because the probability of each outcome is $1/4,$ we have $E_1 = 3/2$, and so $E_k$ grows without bound in $k$. This clearly does not gibe with $P = 1$.",2010-11-21T18:03:55.047,795,CC BY-SA 2.5,
7078,4778,12,Well the original question was about R though.,2010-11-21T18:42:31.037,334,CC BY-SA 2.5,
7079,4780,0,"Actually it does a surprisingly good job. For the matrix B in my example, you can very accurately approximate the 50 singular values using the algorithm with k=20 p=35",2010-11-21T18:53:17.573,2078,CC BY-SA 2.5,
7081,4772,11,"@shabbychef It's not so obvious to me.  You can have the expectation grow exponentially (or even faster) while the probability of dying out still approaches unity.  (For example, consider a stochastic process in which the population either quadruples in each generation or dies out entirely, each with equal chances.  The expectation at generation n is 2^n but the probability of extinction is 1.)  Thus there is no inherent contradiction; your argument needs something additional.",2010-11-21T21:02:38.383,919,CC BY-SA 2.5,
7082,4659,0,"If ""what you actually wanted to know"" is ""the actual fraction of time that Foo is in execution,"" then you are asking about a Binomial confidence interval or a (Bayesian) Binomial credible interval.",2010-11-21T21:12:33.740,919,CC BY-SA 2.5,
7083,4659,0,"@whuber: Well I've used the random-pause method of performance tuning for over 3 decades, and some other people have discovered it too. I've told people that if some condition is true on 2 or more random-time samples, then removing it would save a good fraction of time. HOW good a fraction is what I've tried to be explicit about, assuming we don't know a Bayesian prior. Here's the general flame: http://stackoverflow.com/questions/375913/what-can-i-use-to-profile-c-code-in-linux/378024#378024 and http://stackoverflow.com/questions/1777556/alternatives-to-gprof/1779343#1779343",2010-11-21T21:54:34.670,1270,CC BY-SA 2.5,
7084,4659,1,"Nice idea.  The statistical assumption is that the interruption is independent of the execution state, which is a reasonable hypothesis.  A **binomial confidence interval** is a good tool to use to represent the uncertainty.  (It can be an eye-opener, too: in your 3/10 situation, a symmetric two-sided 95% CI for the true probability is [6.7%, 65.2%].  In a 2/10 situation the interval is [2.5%, 55.6%].  These are wide ranges! Even with 2/3, the lower limit is still less than 10%.  The lesson here is that something fairly rare can happen twice.)",2010-11-21T22:16:52.733,919,CC BY-SA 2.5,
7085,4766,0,"you might add a link to the related question on stackoverflow : http://stackoverflow.com/questions/4224031/randomized-svd-singular-values/4226055#4226055 . You might also want to refer to the relevant article, as given there in my answer.",2010-11-21T22:26:45.843,1124,CC BY-SA 2.5,
7086,4659,0,"@whuber: Thanks. You're right. Something more useful is the expected value. As far as priors go, I point out that if you only see something once, it doesn't tell you much *unless* you happen to know the program is in an infinite (or exceedingly long) loop.",2010-11-21T22:44:02.680,1270,CC BY-SA 2.5,
7087,4772,1,"@shabbychef -- thanks for the edit.  I didn't realize we could use embedded TeX for math!  @whuber -- shabbychef's statement $E_k = E_1^k$ is just a variation on my statement about the extinction probability, just add expectations instead of multiplying probabilities.  Nice work, shab.",2010-11-21T23:04:10.770,5792,CC BY-SA 2.5,
7088,4782,2,"I must say I have to disagree about R being simpler to learn than SAS. It may be because I learned SAS and SPSS first, but I think SAS, SPSS(PASW now), and Stata are all easier languages to pick up than R. It is a subjective argument though.",2010-11-22T00:01:41.193,1036,CC BY-SA 2.5,
7089,4782,0,I feel like R gives the user so much more in terms of functionality. It goes above and beyond what SAS/SPSS can do.,2010-11-22T00:23:18.063,2078,CC BY-SA 2.5,
7091,4782,3,"I don't disagree with that, but that doesn't make it easier to understand. I think it is pretty transparent what objects I am working with in SAS, SPSS, or Stata and the format/nature of those objects, but it isn't as transparent in R. Although R may be more cutting edge, I rarely have a need for cutting edge statistical techniques in my day to day work.",2010-11-22T01:21:39.127,1036,CC BY-SA 2.5,
7092,4787,0,"I know the various methods (gradient based) that can be applied. I am working on comparing various optimization methods for this model for a comprehensive theoretical evaluation. Therefore, I want to try LP-based methods as well.",2010-11-22T03:43:04.873,2071,CC BY-SA 2.5,
7093,4787,0,"Sounds interesting. In regards to Fisher scoring, you may want to check out ""Meenakshi Devidas and E. Olusegun George, Monotonic Algorithms for Maximum Likelihood Estimation in Generalized Linear Models. Sankhy\-{a}: The Indian Journal of Statistics (Series B), Vol. 61, 382--396, 1999.""",2010-11-22T05:21:15.817,530,CC BY-SA 2.5,
7095,4788,1,"Interior point can be applied to nonlinear problems as well. (wikipedia). This is not an exercise to find the best solution. It is more of an exercise in exploration. May be if I knew as much as you do, I won't even try it. But I don't. And this is how I will. :)",2010-11-22T07:50:50.423,2071,CC BY-SA 2.5,
7096,4788,0,"@euphoria83, sorry I didn't mean to be obnoxious or anything. I've edited my answer.",2010-11-22T08:54:51.373,1540,CC BY-SA 2.5,
7097,4789,3,"Well, R support are places like here, which are often more effective that a paid support. For Googling, there is rseek.org, works very nice. 99% of R-is-slow cases can be solved with some thinking; there are also packages to deal with huge data (it is not straightforward in SAS neither). R is a programming language, SAS is an extended SQL.",2010-11-22T09:30:48.677,,CC BY-SA 2.5,user88
7098,4787,0,@euphoria83: by 'LP based methods' do you just mean interior points or also the 'regular' simplex?,2010-11-22T10:09:30.497,603,CC BY-SA 2.5,
7099,4793,3,"+1 for good advice.  But it's unclear whether the implicit assumptions in your reply match the question: it sounds like the OP is not actually adding any constraints, but only is asking whether *methods* of constrained optimization could be used to solve the *unconstrained* problem.  (Of course my interpretation could be wrong, too...)",2010-11-22T15:02:29.993,919,CC BY-SA 2.5,
7100,4772,1,"That's clear, Mike, but what's your point?  Aren't we talking about how to rule out 1 as a solution?  By the way, it's obvious (by inspection and/or by understanding the problem) that 1 will be a solution.  This reduces it to a quadratic equation which one can easily solve on the spot.  That's not usually the point of an interview question, though.  The asker is probably probing to see what the applicant actively knows about stochastic processes, Brownian motion, the Ito calculus, etc., and how they go about solving problems, not whether they can solve this particular question.",2010-11-22T15:18:59.977,919,CC BY-SA 2.5,
7101,4789,2,"+1 because this answer *is* useful, but I think your points about R's support, speed, and ability to handle large data are out of date or becoming so fairly quickly.",2010-11-22T16:00:46.713,71,CC BY-SA 2.5,
7102,4797,0,Thanks! What kinds of problem can be solved by Gibbs but not or not easily by slice sampling? What by slice but not (easily) by Gibbs sampling?,2010-11-22T16:05:54.157,1005,CC BY-SA 2.5,
7103,4797,0,"If you can use gibbs you can use the slice as well. See the wikipedia link for an example of a slice sampler for the normal(0,1) density. An example where the gibbs is not possible is when you cannot identify the distribution of interest but the density function can be inverted.",2010-11-22T16:24:44.930,,CC BY-SA 2.5,user28
7104,4793,2,"@whuber: It's a bit difficult. He refers to the Interior point method, which is a constrained optimization in the sense that it constrains the search space. But penalty methods are also considered constrained methods, and actually one can say that a binary logistic regression is constrained as well, in the sense that it is built upon a set of assumptions. So I find the question of OP a bit confusing.",2010-11-22T16:51:40.677,1124,CC BY-SA 2.5,
7105,4793,0,Thank you: that perspective helps me appreciate your answer even more.,2010-11-22T17:00:23.963,919,CC BY-SA 2.5,
7106,4770,5,"Thanks. I know both papers. I am not a great fan of Witten [not Whitten] et al., as i think there are more important papers on sparse decompositions. On randomized SVD, I especially like the review paper ""Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions"" (http://arxiv.org/abs/0909.4061) also coauthored by Martinsson.",2010-11-22T17:17:49.607,30,CC BY-SA 2.5,
7107,4787,0,@user one of those. mostly interior point.,2010-11-22T17:48:49.223,2071,CC BY-SA 2.5,
7108,4793,0,"I am not an expert in constrained methods, but from what I have read till now, the constraints in constrained optimization problems are hard inequalities defined by Ax < b, where A is a matrix and b is a vector. Penalized log likelihood (I understand you mean, regularization) does not cut the space, it just modifies the contours of the objective function. The objective function is still defined for all values, even Infinity. But the hard constraints that generally are part of constrained problems will not allow all points in the space to be a valid solution.",2010-11-22T17:56:03.370,2071,CC BY-SA 2.5,
7109,4793,0,"@euphoria83 That's how I understood it as well, which is why I con't consider penalized methods constrained methods. But they tend to end up under the constrained methods in some textbooks, together with Lagrange multipliers and the likes.",2010-11-22T18:25:41.220,1124,CC BY-SA 2.5,
7110,4770,0,i agree. i was just putting out there 2 papers no one had mentioned.,2010-11-22T19:19:42.567,2078,CC BY-SA 2.5,
7112,4803,2,Psst... you might want to ask a question in your question.,2010-11-22T21:45:56.557,71,CC BY-SA 2.5,
7113,4772,5,"@shabbychef: One way to rule out P=1 is to study the evolution of the probability generating function.  The pgf is obtained by starting with t (representing an initial population of 1) and iteratively replacing t by (1+t+t^2+t^3)/4.  For any starting value of t less than 1, a graphic easily shows the iterates converge to Sqrt(2)-1.  In particular, the pgf is staying away from 1, showing it cannot converge to 1 everywhere, which would represent complete extinction.  This is why ""the 1 isn't plausible.""",2010-11-22T21:53:26.423,919,CC BY-SA 2.5,
7114,4805,2,"""The"" ""best-fit"" transformation is the inverse cumulative normal integral of the percentiles (readily computed with almost anything more sophisticated than an adding machine), but I doubt this is what you are referring to.  For this problem to be well-posed, you need to stipulate (a) what family of transformations you have in mind and (b) how to measure the goodness of fit.  You are correct that this problem has been addressed, but as you might guess, it has many different solutions depending on what is assumed in (a) and (b).",2010-11-22T21:57:12.367,919,CC BY-SA 2.5,
7115,4803,0,"Heh, sorry about that.",2010-11-22T22:03:39.053,996,CC BY-SA 2.5,
7116,4789,2,"I'll second @Matt and @mbq's comment about R performance, but I'd like to add that R is pretty good actually for (N)LMEs. I can remember a talk from Doug Bates at the [DSC 2009 conference](http://j.mp/ht1oTG) where he showed how the `lme4` package easily handles a lot of random effects (as encountered e.g., in educational assessment). My own (but limited) experience (SAS `NLMIXED` *vs.* R `lme4`) confirms that point: R is by no way slower than SAS when it comes to apply complex [IRT](http://j.mp/hVEMB5) models, and it handles large data genetic sets as well (thanks to clever C implementation).",2010-11-22T22:25:14.123,930,CC BY-SA 2.5,
7117,4790,0,Can you expand on possible 'ensemble method' in this particular case?,2010-11-22T22:35:25.483,930,CC BY-SA 2.5,
7118,4805,3,"I'm assuming you're doing this so that your data might meet the assumptions of standard least-squares statistics - regression, ANOVA, etc.  Be careful here, though.  Make sure you know what the transformed values actually mean lest your inferences are wrong.  Similarly, there may be meaning in the non-normality of your data.  Perhaps a generalized linear model might be more appropriate for you.",2010-11-22T23:49:25.247,101,CC BY-SA 2.5,
7119,4805,0,By 'SAS EM' do you mean SAS Enterprise Miner or the SAS implementation of the Expectation-Maximization algorithm?,2010-11-22T23:53:02.000,449,CC BY-SA 2.5,
7120,4120,0,Yes.  It does give the means.,2010-11-23T00:15:32.120,101,CC BY-SA 2.5,
7121,4805,0,@onestop By SAS EM I mean Enterprise Miner. @jebyrnes In this case I'm looking at variables to be used in a clustering analysis. But in general I don't like to use techniques that I don't have a basic understanding of.,2010-11-23T05:20:05.097,1951,CC BY-SA 2.5,
7122,4805,0,"@whuber for (b) KS seems natural enough. There's also quantile matching. (a) I would think some linear combination of exponentiation, log, and taking to various powers, with a penalty for complexity. But I do think the details are putting the cart in front of the horse. What I want is a general discussion of the topic that may have its own recommendations for answers to your questions.",2010-11-23T05:25:40.517,1951,CC BY-SA 2.5,
7123,4813,0,Thanks Chris.  This should be a wiki - but I don't have permissions to make it so :),2010-11-23T07:53:26.577,253,CC BY-SA 2.5,
7127,4815,0,"thx for pointing out the obvious. I had my aha-moment when I calculated the df. I should have more coffee before I post a question. :-)  The only thing I fail to see is how the AIC ends up equal, as k is the number of parameters and not the df. Regarding the model specification: collectorID is actually nested in Crownlevel, and as far as I'm concerned one shouldn't omit a main effect when the term is included in an interaction, unless we're talking about purely nested factors. But that's not the case for Season and Crownlevel.",2010-11-23T10:51:55.973,1124,CC BY-SA 2.5,
7128,4815,1,"i'd think you have 16 fixed effects, one for each Season:Crownlevel combination (i.e. df=16) for both models plus two variance parameters (error + ID), so k has to be the same for both (they ARE exactly the same models, just with different parameterizations...).",2010-11-23T11:04:58.443,1979,CC BY-SA 2.5,
7129,4819,0,"Thanks mbq, but that will give us a test on the rank, not on the median - wouldn't it?",2010-11-23T11:07:10.563,253,CC BY-SA 2.5,
7130,4820,0,"Hi psj, your answer gives a good explanation as to why I asked my question :) - exactly because I am not aware of how to generalize the sign test to unpaired data.  However, there might be a solution out there, thus my question :)",2010-11-23T12:05:18.437,253,CC BY-SA 2.5,
7131,4819,0,"@Tal Yes, but as I wrote -- this is all one can get. Yet they both answer the same holistic question ""Is there a significant difference?"", so both are considered nonparametric t-test variants.",2010-11-23T12:40:28.313,,CC BY-SA 2.5,user88
7132,4813,0,I'd vouch for those materials as well. They also have intro tutorials and not just multi level modelling resources.,2010-11-23T13:16:08.913,1036,CC BY-SA 2.5,
7135,4807,3,my son was born by Caesarian. I assure you it was traumatic in the casual sense of the word!,2010-11-23T15:56:49.947,795,CC BY-SA 2.5,
7136,4819,3,"@mbq, @Tal The Wilxocon-Mann-Whitney test is an unpaired test based on ranks.  It has two interpretations (the Wikipedia article has a good discussion).  One of them is that it's a test of equality of *medians* relative to an alternative hypothesis that's essentially a shift of median.  BTW, the Wilcoxon test (not to be confused with the Mann-Whitney test!) is a sign test for paired data.  Is this the ""Sign Test"" referred to in the question?",2010-11-23T16:01:03.087,919,CC BY-SA 2.5,
7137,4819,0,"Hi Whuber, this is the sign test:  http://en.wikipedia.org/wiki/Sign_test",2010-11-23T16:03:53.317,253,CC BY-SA 2.5,
7138,4816,3,"If you define the deltas to be complementary incomplete gamma functions, you obtain exact equalities.  Obviously these are the sharpest possible bounds!  I guess the point of this question is that your calculator doesn't compute incomplete gammas and you're looking for an approximation, but that still omits essential information: how can we answer this question until we know just what your calculator *can* compute?",2010-11-23T16:04:30.073,919,CC BY-SA 2.5,
7139,4827,0,"Thank you Caracal, that was a great explanation/reference of the sign test.  I guess the answer to my question is ""no""...",2010-11-23T16:06:59.897,253,CC BY-SA 2.5,
7140,4819,3,"Perhaps it's useful to distinguish the three Wilcoxon tests: 1. one-sample test: ""Wilcoxon signed rank test"" 2. two dependent samples -> can be reduced to 1. by taking the observation-wise difference and testing for location parameter of 0. 3. two independent samples: ""Wilcoxon rank sum test"". This is equivalent to the Mann-Whitney U-test: both test statistics are conceptually different but numerically identical.",2010-11-23T16:10:49.847,1909,CC BY-SA 2.5,
7141,4826,0,Thanks. I'll look at the package. This is stackexchange isn't it?,2010-11-23T16:23:50.277,261,CC BY-SA 2.5,
7142,4820,3,"You have just described the Mann-Whitney U test!  Also, you use ""every value in the other group [as] a potential counterpart for comparison"" and compute the median of all such paired differences you obtain the Hodges-Lehman estimate of the difference in medians of the two datasets: http://en.wikipedia.org/wiki/Hodges%E2%80%93Lehmann_estimate",2010-11-23T16:31:14.213,919,CC BY-SA 2.5,
7143,4819,3,"@Tal Yep, your ""sign test"" is also known as the Wilcoxon test.  The Mann-Whitney U test is its unpaired counterpart.  The *Mann-Kendall* test is its extension to more than two sets of data.  Although it doesn't seem to be commonly known, the U statistic can be obtained as the t-statistic for the ranks of the *combined* dataset (using midranks for tied groups) and the Mann-Kendall test is really just ANOVA on the ranks of the combined data.  This insight allows you to avoid the complex formulas often cited and use simple software to do the calculations.",2010-11-23T16:35:12.823,919,CC BY-SA 2.5,
7144,4826,1,"I think fabians meant Stackoverflow: http://stackoverflow.com which is for R programming, how do I do something in R questions. This place is more generally about stats. The two can blend a lot, however.",2010-11-23T16:45:58.553,1390,CC BY-SA 2.5,
7145,4815,0,"but isn't k the number of **parameters**, and not the number of df? The different parametrization should give a different number of parameters, no?",2010-11-23T16:48:14.417,1124,CC BY-SA 2.5,
7146,4822,3,"That's right.  The connection can perhaps be seen more clearly by considering Sqrt(M) = (b-c) / Sqrt(b+c).  Approximating the variance of b as b and the variance of c as c (as is usual with counted data), we see that Sqrt(M) looks like an approximately normal variate (b-c) divided by its standard deviation: in other words, it looks like a *standard* normal variate.  In fact, we could conduct an equivalent test by referring Sqrt(M) to a table of the standard normal distribution.  Squaring it effectively makes the test symmetric two-tailed.  Obviously this breaks down if either b or c is small.",2010-11-23T17:11:49.430,919,CC BY-SA 2.5,
7147,4815,2,"@Joris The df is the correct value to use, not the nominal number of parameters.  The former has mathematical and statistical meaning whereas the latter does not.  If you use more parameters than there are df, then one or more of those parameters will not be identifiable.",2010-11-23T17:19:43.593,919,CC BY-SA 2.5,
7148,4816,0,"I am not interested in computing an upper bound, but obtaining something that I can control analytically. The answer that robin has provided is exactly what I was looking for. The question is, are there more precise bounds than those provided by Massart and Laurent?",2010-11-23T17:30:10.510,168,CC BY-SA 2.5,
7149,4821,1,the second inequality seems to be incorrect or am I missing something?,2010-11-23T17:32:43.910,168,CC BY-SA 2.5,
7152,4830,0,Related: http://stats.stackexchange.com/questions/4831/logistic-regression-transforming-variables and; http://stats.stackexchange.com/questions/4832/logistic-regression-classification-tables-a-la-spss-in-r,2010-11-23T17:46:50.293,776,CC BY-SA 2.5,
7153,4821,0,"@mkolar sorry about that, now corrected",2010-11-23T17:47:47.813,223,CC BY-SA 2.5,
7155,4819,0,"Whuber, if I understand correctly, it can be called wilcoxon-sign-test, but not wilcoxon-sign-rank-test.  Other then that - thanks for your references - all are interesting to know.",2010-11-23T17:48:11.477,253,CC BY-SA 2.5,
7156,4832,0,"You should post an example of the SPSS output you want to replicate in R, or explicitly detail what the SPSS output is reporting",2010-11-23T17:48:33.417,1036,CC BY-SA 2.5,
7157,4832,0,"Good idea, output added for clarity. Thanks for the suggestion Andy.",2010-11-23T17:55:29.180,776,CC BY-SA 2.5,
7158,4362,0,Very well explained!  What a wonderful example to the Bayes theorem approach.,2010-11-23T18:04:36.970,253,CC BY-SA 2.5,
7160,4822,0,"Thank you for the intuitive answer Joris.  Still, why is it more common to use this approximation rather then using the normal approximation to the exact binomial test of McNemar?",2010-11-23T18:08:09.840,253,CC BY-SA 2.5,
7161,4834,0,Likert scale is actually a good example of an underlying continuous variable that is collected in discrete fashion.,2010-11-23T19:38:16.297,279,CC BY-SA 2.5,
7162,4839,0,"Are these measurements binary i.e., 1 (for yes), 0 (for no)?",2010-11-23T19:51:57.487,,CC BY-SA 2.5,user28
7163,3129,6,"I agree about the bootstrap approach.  But power is not a function of degrees of freedom.  In many cases, including this one, the Mann-Whitney test often has *greater* power than the t-test.  See http://tbf.coe.wayne.edu/jmasm/sawilowsky_misconceptions.pdf .  In general, the power of a parametric test is good when the parametric assumptions are true but can be lower--sometimes drastically so--when those assumptions are violated, whereas good nonparametric tests maintain their power.",2010-11-23T19:53:08.280,919,CC BY-SA 2.5,
7164,4836,2,"(+1) Agree, i think the assumption of a continuous distribution may affect the standard way of calculating p-values, but simulation as suggested by whuber gets around that (i'd call this a permutation test).",2010-11-23T20:19:26.417,449,CC BY-SA 2.5,
7165,4809,0,"In fitting the nonlinear curves have you modelled the unstransformed response or the log of the response, or something else?",2010-11-23T20:27:57.940,449,CC BY-SA 2.5,
7166,4840,2,"Your question lacks some critical details -- first of all, what should be the protocol of this forecast, i.e. you want to predict the ozone level for next day every day? Predict the mean level in 2020 from 2004-2006? Next, is the ozone levels the only data you have? How it is discretized, one measurement per week/day/hour? Maybe you can pull some other relevant data from other sources? What can be relevant from chemical point of view?",2010-11-23T20:31:48.333,,CC BY-SA 2.5,user88
7167,4774,0,"Thanks for the intricate response. The new tool provides more data and costs less, yes. I ended up doing a lot of what you describe including a correlogramme of the values after matching the data points.",2010-11-23T20:34:30.437,1784,CC BY-SA 2.5,
7168,4836,5,"@onestop That is correct.  Indeed, the Wilcoxon test *is* a permutation test.  All we're discussing is how to compute (or estimate) the distribution of the test statistic.  The distribution can be computed exactly using combinatorial techniques; it can be estimated through simulation; and it can be approximated, usually with a Normal approximation when the amount of data is large enough (m+n of 10 or greater is usually good enough) and the data can be thought of as realizations of a continuous RV.",2010-11-23T20:38:58.477,919,CC BY-SA 2.5,
7169,4842,0,see my answer for further elaboration,2010-11-23T20:40:10.737,1124,CC BY-SA 2.5,
7170,4839,0,It seems feasible to make some clustering; can you state what is the aim of this analysis?,2010-11-23T20:42:56.673,,CC BY-SA 2.5,user88
7171,4816,2,"Gamma integrals can be ""controlled analytically,"" so what distinction are you making?",2010-11-23T20:44:09.220,919,CC BY-SA 2.5,
7172,4842,0,"Hi onestop - Since both are asymptotic, then for smaller N's they might yield somewhat different results.  In such a case, I wonder if the choice of going with chi-square is because it is better then the normal approximation, or because of historical reasons (or maybe, as you suggested - they always yield identical results)",2010-11-23T20:51:43.987,253,CC BY-SA 2.5,
7174,4833,1,"Very helpful description, thanks for the direction and the detail on my subquestion as well.",2010-11-23T20:57:12.563,776,CC BY-SA 2.5,
7175,4830,0,"The raw proportion of defaulters is apparently 1 case out of 4, but it seems you also have a lot of variables. Did you try them all, have you any set of primary variables of interest?",2010-11-23T21:00:47.440,930,CC BY-SA 2.5,
7176,4839,0,"To echo @mbq's comment, are you interested in finding clusters of variables, of individuals, or both? Do you want to build a scoring rule, or to find those computer games that favor ""hard""/""soft"" self-perception wrt. game addiction?",2010-11-23T21:05:16.873,930,CC BY-SA 2.5,
7177,4822,0,@Tal: It's the same. See nonstops answer and my edit.,2010-11-23T21:10:44.987,1124,CC BY-SA 2.5,
7178,4842,0,"@Tal: for smaller N, neither of both hold. And as shown in my edit, they are exactly the same.",2010-11-23T21:12:44.937,1124,CC BY-SA 2.5,
7179,4834,0,"@Aniko Right, but the problem is that we're not sure how well the original interval scale fits into the discretized one, unless making strong assumptions. Optimal scaling might be an option there.",2010-11-23T21:13:35.380,930,CC BY-SA 2.5,
7180,4654,0,"risk differences is indeed the way to go, as that one can be understood by non-statisticians as well. Accepted.",2010-11-23T21:15:20.537,1124,CC BY-SA 2.5,
7181,4830,0,"The original variables are at the front of the file. The rest are transformations of the same identified by x_ (where x = log, ln, inv, sqrt). I've tried a mixture of these. But I'm a little bit miffed as to how to interpret or create residual plots where the predictor is 0,1",2010-11-23T21:16:32.127,776,CC BY-SA 2.5,
7182,4647,1,"+1 for using a binomial mixed effect models. Alas it was rejected as ""not a standard method"". If you can give me some links to papers where this approach is used in a meta-analysis setup, you would help me greatly. Thank you in advance.",2010-11-23T21:16:33.763,1124,CC BY-SA 2.5,
7183,4830,0,"In terms of variables of interest, I've tried all of them primaries and a number of different combinations of the transformed variables as well as mixed models that include interaction effects. Still, nothing higher than 81.7% overall efficiency.",2010-11-23T21:18:03.560,776,CC BY-SA 2.5,
7184,3562,1,"thanks for this, helpful for a project I'm working on currently as well - and totally makes sense.",2010-11-23T21:37:44.150,776,CC BY-SA 2.5,
7185,4815,0,@whuber: see your point. Thx for the clarification,2010-11-23T21:38:29.627,1124,CC BY-SA 2.5,
7186,4844,0,"I guess, even if you drive randomly, temperature will be smooth along time and you won't get independence ...",2010-11-23T21:41:06.253,223,CC BY-SA 2.5,
7187,4844,0,"ok, that's what I thought... so there is no way to derive confidence intervals for the proportions ? In reality, I'm benchmarking two appartuses : is it meaningful to compare the proportions using R's prop.test then ?",2010-11-23T21:43:38.323,1784,CC BY-SA 2.5,
7188,4647,0,I have edited my answer with some references.,2010-11-23T21:43:55.520,279,CC BY-SA 2.5,
7189,2697,20,"Eigenvectors are just the linear combinations of the original variables (in the simple or rotated factor space); they described how variables ""contribute"" to each factor axis. Basically, think of PCA as as way to construct new axes that point to the directions of maximal variance (in the original variable space), as expressed by the eigenvalue, and how variables contributions are weighted or linearly transformed in this new space.",2010-11-23T21:46:15.793,930,CC BY-SA 2.5,
7191,4834,0,"@chl That's why you are using Wilcoxon's test instead of a t-test. Wilcoxon's test does not assume equally spaced intervals, etc, so scaling is not an issue.",2010-11-23T22:11:50.827,279,CC BY-SA 2.5,
7192,4845,0,"I tried it out, presented no substantial increases or decreases in overall efficiency (efficiency being how well it predicted defaulters/non-defaulters in the absence of false-positives,false-negatives)",2010-11-23T22:16:46.020,776,CC BY-SA 2.5,
7193,4848,0,"I certainly appreciate the responses - thanks very much. As one follow-up, how this question came about was the accuracy of the random table above is 0.41 (82/200). But, if we wanted to compare our model against random guess, using only the priors, we'd have an accuracy of 0.38 ((100/200)^2+(60/200)^2+(40/200)^2. Doesn't it seem that this should match the random matrix?",2010-11-23T22:20:48.077,2040,CC BY-SA 2.5,
7194,4835,0,How would I generate Yfac if my variable is already a 1/2 dichotomous variable saved as a factor in R?,2010-11-23T22:27:53.000,776,CC BY-SA 2.5,
7195,4835,0,"Nevermind, worked like a charm, just set Yfac <- df$dichotomous.var",2010-11-23T22:32:03.527,776,CC BY-SA 2.5,
7196,4845,1,@Brandon I've tried few other ideas and it doesn't seem to help. This suggests that this set is just hard enough to make this happen (possibly default is just driven by some unpredictable random factors).,2010-11-23T22:32:52.487,,CC BY-SA 2.5,user88
7197,4845,0,"@mbq, thanks for taking the time! Much appreciated.",2010-11-23T22:34:18.097,776,CC BY-SA 2.5,
7198,4841,0,Thank you! Is that an asymptotic result? How high does the trigger have to be?,2010-11-23T22:45:06.137,996,CC BY-SA 2.5,
7199,4819,0,"@Tal There is a total chaos with these nomenclature; I used Wilcoxon as a keyword, since it is most common. And the whole bundle of such tests is located under `wilcox.test` function in R.",2010-11-23T22:53:03.617,,CC BY-SA 2.5,user88
7200,4836,0,"You're right of course @whuber, i meant ""i'd call that a *Monte-Carlo* permutation test*"". (Sorry was in too much of a hurry for no good reason really)",2010-11-23T22:58:41.913,449,CC BY-SA 2.5,
7201,4836,0,@onestop I didn't mean to sound critical.  I was agreeing with you and felt your meaning was clear.,2010-11-23T23:07:15.167,919,CC BY-SA 2.5,
7202,4851,0,"Further info: the functions represent different measures of proximity to various types of genes. I want to try out different metrics, but they all can be boiled down to ""(some kind of) distance to the nearest gene of type X"". So the values would probably look something like this [graph](http://i.imgur.com/hVyIO.png) repeated over and over again (with different shapes), and a histogram could have pretty much any shape, depending on how far apart the relevant genes are. Using the CDF, I could compute a score for a position and the probability of getting this high a score by chance.",2010-11-23T23:14:22.700,2111,CC BY-SA 2.5,
7203,4851,1,"It sounds like you really need only to store one tail accurately.  That could simplify your precalculations somewhat.  After all, there's little practical difference between a p-value of 50% and 25%, but a huge difference between 5% and 0.001%.",2010-11-23T23:33:31.913,919,CC BY-SA 2.5,
7204,4845,1,"Personally, I think this is live data that my prof is getting paid to model in one of his consulting jobs... but that's another issue entirely",2010-11-23T23:38:57.260,776,CC BY-SA 2.5,
7205,4853,0,I think this will definitely work...in my head I was so attached to the idea of a proper CDF that this didn't even occur to me. Thanks!,2010-11-23T23:46:16.413,2111,CC BY-SA 2.5,
7206,4841,0,"A sum of $k$ exponentials with parameter $\lambda$ gives the erlang with parameters $k$ and $\lambda$. Thus, it is not a asymptotic result but a finitary one.",2010-11-24T00:20:46.100,,CC BY-SA 2.5,user28
7207,4857,0,Where can I find help to display the latex formulae!,2010-11-24T02:12:35.407,1709,CC BY-SA 2.5,
7208,4857,0,Surround latex with `$ .. $`,2010-11-24T02:16:52.837,,CC BY-SA 2.5,user28
7210,4808,0,This combined with Log likelihood seems to work,2010-11-24T03:42:03.013,1951,CC BY-SA 2.5,
7212,4859,0,"The only problem is, I have to be able to reproduce this with SPSS output. :( I'm still going to try it though!",2010-11-24T04:41:15.090,776,CC BY-SA 2.5,
7213,1527,5,"@Neil I can see how the proportion of heads in one flip can be 1.0 or 0.0, but I cannot see how it ever can be 0.50 (except in a Schrodinger Cat experiment, perhaps, but that's a different issue...).",2010-11-24T04:51:35.003,919,CC BY-SA 2.5,
7214,4859,0,"Tried it out, can't seem to get predict.glmnet() to work. Is there any magic that needs to be happening when you set newx?",2010-11-24T05:09:05.783,776,CC BY-SA 2.5,
7215,4856,0,May I ask what kind of data do you want to model and for what usage you want to do it?,2010-11-24T06:35:21.417,1709,CC BY-SA 2.5,
7217,4864,0,Classification and Regression Trees? That is actually a second part to the assignment. After I've maximized the classification I have to classify based on probability deciles.,2010-11-24T06:44:53.813,776,CC BY-SA 2.5,
7218,3816,3,"Question: when citing a package, do you cite the vignette (if one exists) or the package itself?",2010-11-24T06:55:25.207,776,CC BY-SA 2.5,
7219,4864,0,"Actually, someone helped me with producing the classification table in this related question: http://stats.stackexchange.com/questions/4832/logistic-regression-classification-tables-a-la-spss-in-r Thanks for the example with R, much appreciated, I found similar instruction on the quick-r website. Although, for this puprpose I'm forced to apply CHAID in SPSS.",2010-11-24T07:00:28.027,776,CC BY-SA 2.5,
7220,4844,0,"For anyone interested, I found this based on Markov Chains... it uses the weather as an example. http://www.jstor.org/pss/2533067",2010-11-24T07:31:11.013,1784,CC BY-SA 2.5,
7221,4861,2,"(+1) Yes, there exist some penalized versions of logistic regression (or more generally, GLMs), see some references there: http://stats.stackexchange.com/questions/4272/when-to-use-regularization-methods-for-regression/4274#4274.",2010-11-24T08:17:17.883,930,CC BY-SA 2.5,
7224,4870,1,"(+1) Thanks for that. I know about the problem of asymmetry in the distribution of VC, and the difference between ML and REML, but I didn't know you developed that package. I look forward to trying it out.",2010-11-24T09:51:16.240,930,CC BY-SA 2.5,
7225,1527,2,"@Neil : No you can't. It doesn't even make sense in regular english, let alone in statistics.",2010-11-24T10:26:05.837,1124,CC BY-SA 2.5,
7226,1559,0,proportions range from 0 to 1. Or from 0 to 100%. Like probabilities.,2010-11-24T10:28:44.400,1124,CC BY-SA 2.5,
7227,4872,0,This seems off-topic.,2010-11-24T11:06:24.317,930,CC BY-SA 2.5,
7229,4834,0,"@Aniko I agree with you. My comment was not a critic, and reference to optimal scaling was mainly for the case where the assumption of an *a priori* continuous scale of measurement is not tenable--because in practice, we often rely on Likert items without making strong assumption on the underlying construct; I also agree for Mann-Whitney test with ordinal variables.",2010-11-24T11:28:27.800,930,CC BY-SA 2.5,
7231,4845,1,"the solution to a problem in data analysis should never be ""throw away valid data points"" - you could try to use balanced training data sets in order to avoid these effects, but you should still evaluate the predictions on all of the data (i.e. all of the validation set).",2010-11-24T11:46:46.417,1979,CC BY-SA 2.5,
7232,4845,0,@fabians Not always -- sometimes it is obvious that the data is redundant and some algorithm don't have any weights to set. Not to mention this quite often leads to the removal of objects inside the algorithm.,2010-11-24T12:15:16.250,,CC BY-SA 2.5,user88
7233,4864,0,"`predict` method for prediction, `table(originalClasses,predictedClasses)` for table construction. I tried RF (usually it has accuracy as in overfitted CART, but no overfit) and the result was not much better than `glm`.",2010-11-24T12:20:17.777,,CC BY-SA 2.5,user88
7235,4871,0,"I think you have misunderstand the question (I believe it was Can I use CV to just make error approximation instead of the whole model selection?), but +1 for relevant remarks.",2010-11-24T12:46:09.283,,CC BY-SA 2.5,user88
7236,4871,0,"I also had this interpretation in my mind, but I thought that this would be too easy ;), since this is exactly what cv does. Additionally, the whole ""all data""-part confused me.",2010-11-24T13:01:18.677,264,CC BY-SA 2.5,
7237,4872,2,"Seems like a good question to me.  If you only have a Bachelor's degree, you will be lucky to find a position, most likely as a SAS programmer.  Most advertised positions are looking for a Master's or above.",2010-11-24T14:09:49.467,5792,CC BY-SA 2.5,
7238,4876,2,It will help if you can tell us what you are trying to accomplish ultimately beyond the specific visualization difficulties you have at present.,2010-11-24T14:23:50.610,,CC BY-SA 2.5,user28
7239,4879,0,"hmm the table function is a useful one it seems, but it would be better if I could show the percentages of the share rather than the count of each OS. Is there a way to do that?",2010-11-24T14:44:44.407,2101,CC BY-SA 2.5,
7240,4879,0,@sfactor I've updated my response accordingly.,2010-11-24T15:05:28.290,930,CC BY-SA 2.5,
7241,4861,0,"@chl, thank you. I have updated to link to your previous answer.",2010-11-24T15:07:20.127,1036,CC BY-SA 2.5,
7242,4861,0,Thx too. This was just a comment on your already excellent response.,2010-11-24T15:09:09.783,930,CC BY-SA 2.5,
7243,4879,3,"You guys are after the function `prop.table()` which comes with base R, it's essentially a wrapper for what you've got above. With your example, you'd want something like `prop.table(table(x))`",2010-11-24T15:14:27.240,696,CC BY-SA 2.5,
7244,4876,0,"When I look at the system performance metric I want to understand how each subsystem is contributing towards it so I can get an idea of how significant the difference is. Ultimately I'd want to work out the tolerance and raise a red flag when subsystems perform worse by more than that tolerance.  In a healthy system, I'd expect the the subsystems to be delivering approximately the same and therefore the subsystem values should be spread relatively closely around the system value.",2010-11-24T15:15:39.933,2122,CC BY-SA 2.5,
7245,4876,0,"In summary, I want to recognise when a subsystem is performing badly relative to its peers.",2010-11-24T15:20:11.117,2122,CC BY-SA 2.5,
7246,4877,1,"Since you're new to R, I recommend checking out this [post](http://stats.stackexchange.com/questions/138/resources-for-learning-r) to find a good list of resources to help get you up and running doing some analysis.",2010-11-24T15:23:39.677,696,CC BY-SA 2.5,
7247,4879,0,"@Chase (+1) Huh, yes... That's it! I was thinking of even more complex ways with `addmargins()` or `margin.table()`.",2010-11-24T15:33:00.533,930,CC BY-SA 2.5,
7248,4877,0,"@Chase thanks for the link, yah i need to really learn the language, this was just something I need to do quickly.",2010-11-24T15:35:58.523,2101,CC BY-SA 2.5,
7250,4884,0,"What is an AB-test? A link, a brief explanation etc may help.",2010-11-24T16:27:44.840,,CC BY-SA 2.5,user28
7251,4873,0,"Thanks for trying, I played with the threshold as well and it provided little boost to the classification.",2010-11-24T16:28:50.713,776,CC BY-SA 2.5,
7252,4885,0,(+1) especially for the insights in the interpretability of distances vs areas and slope vs height.,2010-11-24T16:35:39.397,264,CC BY-SA 2.5,
7253,4884,0,"@Srikant: Sorry, I added some links to the question.",2010-11-24T16:40:02.187,264,CC BY-SA 2.5,
7254,4871,0,"Thanks for your response, lots of relevant information here. Just to make sure we're on the same page let me state this another way:

In some papers I've noticed that people set aside a certain fraction of the dataset for training purposes. Given a dataset of size 1000 they set aside 300 events for training and use 700 events for classification and never discuss the 300 events used to train the model (i.e. they don't train a separate model and then test those 300 events). Couldn't they just use cross validation technique to classify all of the data?",2010-11-24T16:46:13.817,2113,CC BY-SA 2.5,
7255,4873,1,"Although, it did do a better job of predicting defaulters.",2010-11-24T16:52:06.560,776,CC BY-SA 2.5,
7256,4871,0,"@C.Reed: Not with the same ratios ! Sometimes you want to reduce the computation time and/or control the test-to-training-ratio to study the bias-variance tradeoff. I you have some time to spend, I recommend chapter 3 of Kohavis Phd Thesis discussing this topic extensively (http://robotics.stanford.edu/~ronnyk/teza.pdf). But all in all, the cv (from my point of view) is the favored method for error estimation (as mbq already said)",2010-11-24T16:55:08.647,264,CC BY-SA 2.5,
7257,4888,1,+1 since `aggregate` is an overshoot here and plyr is slow.,2010-11-24T18:15:57.843,,CC BY-SA 2.5,user88
7258,4871,0,"Thanks for the thesis reference, ch. 3 is quite helpful on this matter.",2010-11-24T18:49:35.900,2113,CC BY-SA 2.5,
7259,4891,0,It's still an interesting point.  Thanks for bringing it up.,2010-11-24T19:40:19.393,919,CC BY-SA 2.5,
7260,4836,0,So maybe I'm missing something - is the assumption of continuous distribution only needed for the approximation of the normal distribution of the statistic?,2010-11-24T20:10:33.063,253,CC BY-SA 2.5,
7262,4822,0,"Actually - last question.  So if both are identical (and I think you might also need an ""absolute value"" around the b-c), then why do people go to the chi distribution instead of staying with the normal one?  Where's the advantage?",2010-11-24T20:14:44.297,253,CC BY-SA 2.5,
7263,4836,0,"@Tal I believe that is the case.  In fact, the Wilcoxon (Mann-Whitney) rank sum test is actually required in some US state regulations due to its ability to handle certain kinds of non-continuous distributions resulting from data censoring: http://www.pabulletin.com/secure/data/vol27/27-33/1340c.html",2010-11-24T20:16:36.107,919,CC BY-SA 2.5,
7265,4893,0,"@wok: oops, I haven't seen your last paragraph!",2010-11-24T20:30:25.570,1725,CC BY-SA 2.5,
7267,4894,0,(+1) Thanks and welcome to CrossValidated. No need to post it as a comment (which assumes you have enough rep); it's a response on its own. I always appreciate your input on [r-sig-mixed](https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models). Nice to see you there.,2010-11-24T20:51:47.500,930,CC BY-SA 2.5,
7268,4895,11,"This is correct and appealing.  However, in the end it appears only to rephrase the question without actually answering it: namely, why should we use the Euclidean (L2) distance?",2010-11-24T21:07:08.960,919,CC BY-SA 2.5,
7269,4895,0,"That is indeed an excellent question, left unanswered. I used to feel strongly that the use of L2 is unfounded. After having studied a little statistics, I saw the analytic niceties, and since then have revised my viewpoint into ""if it really matters, you're probably in deep water already, and if not, easy is nice"". I don't know measure theory yet, and worry that analysis rules there too - but I've noticed some new interest in combinatorics, so perhaps new niceties have been/will be found.",2010-11-24T21:39:54.330,2456,CC BY-SA 2.5,
7270,4741,0,"@Aniko: Yes, I am looking for something like a confidence interval for my counts per bin which propagate nicely when I project out parts of my 2D histogram into 1D.",2010-11-24T21:42:12.113,56,CC BY-SA 2.5,
7271,4895,26,"@sesqu Standard deviations did not become commonplace until Gauss in 1809 derived his eponymous deviation using squared error, rather than absolute error, as a starting point.  However, what pushed them over the top (I believe) was Galton's regression theory (at which you hint) and the ability of ANOVA to decompose sums of squares--which amounts to a restatement of the Pythagorean Theorem, a relationship enjoyed only by the L2 norm.  Thus the SD became a natural omnibus measure of spread advocated in Fisher's 1925 ""Statistical Methods for Research Workers"" and here we are, 85 years later.",2010-11-24T21:56:31.867,919,CC BY-SA 2.5,
7272,4822,0,"@Tal: You don't want that absolute value around it. N(0,1) can take negative values as well. Why take the chi2? A matter of taste I guess? chi2 is one-tailed, so tests two-sided by default.",2010-11-24T22:03:02.437,1124,CC BY-SA 2.5,
7273,4836,0,@Tal and @whuber: I was puzzled by the implication (and then by its apparent acceptance) that the test statistic for the Wilcoxon(or M-W) was normally distributed.,2010-11-25T00:18:01.930,2129,CC BY-SA 2.5,
7274,4836,1,"@DWin For a continuous underlying distribution and sufficient sample sizes it is *approximately* normally distributed.  The exact distribution, however, is not normal and departs appreciably from normality when there are less than 10 values (in both datasets combined).  Tables of the distribution have been published to cover those small-data cases, but it is readily computed nowadays by brute force.  (E.g., with 10 values it takes little time to generate all the possible combinations.)",2010-11-25T02:49:38.557,919,CC BY-SA 2.5,
7275,1516,6,I like your answer. The sd is not always the best statistic.,2010-11-25T03:03:17.077,1709,CC BY-SA 2.5,
7276,4895,16,"(+1) Continuing in @whuber's vein, I would bet that had Student published a paper in 1908 entitled, ""Probable Error of the Mean - Hey, Guys, Check Out That MAE in the Denominator!"" then statistics would have an entirely different face by now.  Of course, he didn't publish a paper like that, and of course he couldn't have, because the MAE doesn't boast all the nice properties that S^2 has. One of them (related to Student) is its independence of the mean (in the normal case), which of course is a restatement of orthogonality, which gets us right back to L2 and the inner product.",2010-11-25T03:38:57.890,,CC BY-SA 2.5,user1108
7277,4900,2,Thanks for the references and r function. I still prefer the interpretation of d-based measures (where they apply) over variance-explained-based measures. I find it clearer to think of the effect of an intervention in terms of a difference score.,2010-11-25T04:40:42.377,183,CC BY-SA 2.5,
7278,4822,0,"Dear Joris, you wrote "" chi2 is one-tailed, so tests two-sided by default"", how is that? I thought that once you get to the P value, you can move back and forth from two tails to one tail with simply multiplying or dividing the value with 2.",2010-11-25T06:25:44.537,253,CC BY-SA 2.5,
7279,4888,0,"-1 That can't possibly work unless you make the components of the data frame visible (using say `with()`, or refer to the explicitly via, e.g., `df$Objects` where `df` is the op's data frame.",2010-11-25T07:30:39.897,1390,CC BY-SA 2.5,
7280,4888,0,"@mbq: why is `aggregate()` an ""overshoot here""? It seems very natural as it returns for you a DF with the unique `SessionNo.` plus the sum for each. `tapply()` just gives the sums.",2010-11-25T07:32:46.460,1390,CC BY-SA 2.5,
7281,4896,0,"if you are using `with()`, then go the whole hog and use `within()` here --- gets rid of more `$`. Your first line could be, `tst <- within(tst, tmn <- ave(Objects, SessionNo.))` whilst your second line could be `tst <- within(tst, devmn <- Objects - ave(Objects, SessionNo.))`. Note that the second line contains two modifications as the whole thing can be inside `with()` or `within()`.",2010-11-25T07:39:54.520,1390,CC BY-SA 2.5,
7282,4890,0,"Thank you very much for your response. However, I am not sure if I was able to explain my problem. I've edited the question and added a concrete example. If it is not too much hassle, could you check and (if necessary) revise your response ?",2010-11-25T07:46:43.937,264,CC BY-SA 2.5,
7283,4903,6,"sounds like a remark or is the answer ""think"" ?",2010-11-25T08:15:24.163,223,CC BY-SA 2.5,
7284,4903,3,"@Robin - the latter. I find statistical modelling quite difficult (I'm an ecologist with little formal statistical training, most of what I've learned has been self-taught) but it is a lot easier if I think about the problem first, determine what is plausible, build that model, do my model diagnostics, try interactions where these make scientific sense.",2010-11-25T08:25:34.090,1390,CC BY-SA 2.5,
7285,4903,0,@ucfagls (what is this name related to :) ?) I guess this is what statistician call modelling :)  injecting your apriori  knowledge (Beeing a self-learner here may be an advantage) into a formal description of what you observe. This is necessary in most case (i.e when the number of variables is not very small with respect to the number of observations). This introduces bias.,2010-11-25T08:34:16.503,223,CC BY-SA 2.5,
7286,4822,1,"@Tal: You know R. plot the chi2 with one degree of freedom, you'll see.",2010-11-25T09:02:23.250,1124,CC BY-SA 2.5,
7287,4903,0,"@Robin: Oh, it is my old computer userid from when I was an undergraduate many moons ago. I keep forgetting to change to my real name here - done it on SO already...",2010-11-25T09:24:33.323,1390,CC BY-SA 2.5,
7288,4903,0,"@Robin: There, that's better!",2010-11-25T09:26:14.040,1390,CC BY-SA 2.5,
7289,4903,0,@Gavin Thanks ! very interesting web page !  I'll try to understand what nitrogene cycle is ... I guess makes me understand your answer now :),2010-11-25T10:03:58.313,223,CC BY-SA 2.5,
7290,4901,0,You could tell us a little bit about your data? size (cf my answer) and nature (cf Gavin's answer),2010-11-25T10:09:58.307,223,CC BY-SA 2.5,
7291,4888,0,"@Gavin: You¬¥re right, i¬¥ve edited my post.",2010-11-25T10:36:17.957,1050,CC BY-SA 2.5,
7292,4904,1,"Some example data can be useful, so answerers could test their ideas.",2010-11-25T11:22:57.167,,CC BY-SA 2.5,user88
7293,4872,1,"@Mike It is a good question, but not in the scope of this site.",2010-11-25T11:24:40.753,,CC BY-SA 2.5,user88
7294,4888,0,@EDi +1 following edit,2010-11-25T11:31:34.960,1390,CC BY-SA 2.5,
7295,4901,0,"@Robin: Give him time to get out of bed, Brandon is in Toronto ;-)",2010-11-25T11:41:34.173,1390,CC BY-SA 2.5,
7296,4746,0,"Thank you onestop for your detailed answer.
I ended up in doing the power analysis with the R function power.t.test.

When I do the power test I get somewhat different numbers than you using the following parameters: power.t.test(n=10, power = 0.8, type =c(""two.sample""))
I get delta=1.32 which is higher than your 1.25 SD's. Am I missing here something?

Unfortunately, I can not homogenise the soil before the experiment. I need undisturbed soil columns. 

I will take baseline measurements before I start the treatment. Any hints on ANCOVA are appreciated.",2010-11-25T11:49:14.860,2063,CC BY-SA 2.5,
7297,4904,0,"Will append example data later. Right now you can assume that if everybody plays fair, then the difference of v to the average v is roughly 0.7 times standard deviation. If somebody cheats (cheater must be excluded from calculating the stddev), his deviation from the fairplayer average v is noticably > 1 times stddev, somewhere in the range between 2 and 3 times stddev normally, but this can go up to 8 times stddev. Right now I loop through the array of all players and calculate stddev, excluding the i-th player each time)",2010-11-25T11:51:22.900,2132,CC BY-SA 2.5,
7298,4888,0,I would have opted for by().,2010-11-25T11:58:34.007,144,CC BY-SA 2.5,
7299,4888,0,"@Gavin Subjective thing; I like the simplicity of `tapply`, yet of course `subset` will be the way to go to make it the fastest possible in plain R.",2010-11-25T12:08:48.690,,CC BY-SA 2.5,user88
7300,4888,0,"@Roman Add an answer, it will be a nice pool.",2010-11-25T12:09:31.043,,CC BY-SA 2.5,user88
7301,4910,0,"For discrete distributions, there is no point in defining the ecdf at all in $\mathbb R$. It makes no sense to ask ""what proportion of families have at most 1.5 children"", for example. As for sampling from the sample - I have never thought of that as a particularly good idea to begin with, either.",2010-11-25T15:02:20.870,2456,CC BY-SA 2.5,
7303,4906,0,"Is there a difference if i give the argument ""simplify = FALSE"" to tapply()?",2010-11-25T15:22:15.700,1050,CC BY-SA 2.5,
7304,4901,1,"@Robin, I would prefer to keep it more general. If, in your response you're providing a method that requires an assumption about the size or nature of data, please state it. The problem I'm having spans a number of different modelling tasks, all with different data. So, in this case, I'm looking for general recommendation on identifying interaction effects.",2010-11-25T15:24:26.920,776,CC BY-SA 2.5,
7305,4362,1,"@Srikant: that prior cannot be formally defined. In any case, Prob(No of heads = X | experimenter is not faking) is always around zero when N=20000, no matter the value of X and no matter your prior. So your posterior for any number is also always close to 0. I don't see what this has to do with Bayes theorem.",2010-11-25T15:29:47.337,1124,CC BY-SA 2.5,
7306,4903,0,"@Gavin, I agree thinking is important, but sometimes significant interaction effects exist that are not immediately explainable by what you know about the data. What I'm looking for here is a graphical or statistical way of identifying potential interactions. Some may agree that it is not always the case that interactions are easy to ""explain in words"".",2010-11-25T15:50:25.867,776,CC BY-SA 2.5,
7307,4902,1,"thanks for the direction to Sobol indices. Again, I'd like to specify that I'm looking for a general rather than a specific answer here. I'm not asking about a specific set of data but rather trying to explain a problem I've been having with a number of different sets.",2010-11-25T15:52:32.450,776,CC BY-SA 2.5,
7308,4907,0,Very simple and very useful. Thanks for the reference to Crawley's text as well!,2010-11-25T16:21:14.343,776,CC BY-SA 2.5,
7309,4362,0,"All of this from a guy that was holed up trying to prove god existed. Elegant, really.",2010-11-25T16:29:59.843,776,CC BY-SA 2.5,
7310,4822,0,Thanks Joris - I just got it (once you square it - it gives you the two sided test),2010-11-25T16:30:16.737,253,CC BY-SA 2.5,
7311,4907,1,Be careful - you can't easily fit those kinds of interactions in say a linear model. The interactions occur only in one branch of the tree (or part of). You need a **lot** of data to use these sorts of tools in real world data.,2010-11-25T17:38:44.573,1390,CC BY-SA 2.5,
7312,4903,3,"@Brandon: If there is a missing interaction, there will be patterns in the residuals conditional upon values of the covariates. Plotting residuals against the covariates may help determine where an interaction might be appropriate.",2010-11-25T17:41:17.440,1390,CC BY-SA 2.5,
7313,4913,0,"Thanks! That's very good to get me started, I'll see how it goes and then mark it as correct answer when I'm capable to judge this. :)  _Note_ I also edited the question a little.",2010-11-25T17:46:08.013,1785,CC BY-SA 2.5,
7314,4903,0,"@Gavin, Is there a document, text, or article that describes the shape of these patterns?",2010-11-25T18:56:47.033,776,CC BY-SA 2.5,
7315,4912,1,"X seems irrelevant to your question of whether A is more often positive than B. Unless perhaps you might wish to stratify by X, i.e. ask (1) whether A is more often positive than B when X is positive, then (2) whether A is more often positive than B when X is negative ??",2010-11-25T19:52:03.957,449,CC BY-SA 2.5,
7316,4913,1,I think it's now clear that McNemar's test answers your original question. Fisher's exact test is not relevant here.,2010-11-25T19:53:49.447,449,CC BY-SA 2.5,
7318,4746,0,"No, you're not missing anything. I was doing the power calc in Stata using its -sampsi- command that uses the normal distribution rather than the *t*-distribution, so R's power.t.test() function is more accurate, especially when the sample size is very small as here. Power calculations are always approximate, so the difference is rarely important.",2010-11-25T20:16:42.767,449,CC BY-SA 2.5,
7319,4913,0,"I can use both tests to test the relationship between A and B, Fisher's test tells me whether these are independent indeed, McNemar's seems to be more appropriate when comparing the same measure before and after an event. What I intended to test is more whether A or B have a higher probability, e.g. if I toss two coins 350 times and one turns up heads 80% of the time and the other 20%, how can I calculate the significance of this difference. Sorry for my unclear question.",2010-11-25T20:19:30.267,1785,CC BY-SA 2.5,
7320,4912,0,"X seemed irrelevant to my question, but maybe it's not. Still, the question is more along the line of tossing two coins. If one turns up heads 80% of the time and the other 20% of the time, how can I asses this difference. With the coin I have an assumed probability of 0.5, so maybe this is solved differently, I don't have an assumed probability for my example, unless I deduce one from all samples, whether or not **X** is positive.",2010-11-25T20:24:22.697,1785,CC BY-SA 2.5,
7321,4746,0,"I guessed you wouldn't want to homogenise your soil columns. Nevertheless, if would help if you can take more soil columns and use baseline measures to select fairly homogeneous ones to use in the experiment, or find group of 2 or more soil columns that are similar on your three outcome measures and randomly allocate equal numbers within each group to each treatment, i.e. a form of blocking.",2010-11-25T20:24:49.313,449,CC BY-SA 2.5,
7323,4362,0,@Joris I am talking about the OP's *subjective* beliefs about Prob(No of heads = 10k | experimenter is not faking) and not about the actual probabilities assuming a fair dice. I would imagine that the OP's subjective beliefs require that Prob(No of heads = 10k | experimenter is faking) be close to 1. Assuming prior probabilities vis-a-vis an experimenter being a liar or not (say Prob(fake experimenter) = 0.5) gives the desired posterior. Does that help clarify?,2010-11-25T22:16:07.317,,CC BY-SA 2.5,user28
7324,4890,0,"@steffen If you wish to deploy A if it is effective on a per day basis then you do aggregate to the day level. If on the other hand you do not care if it is inferior on some days then you aggregate it across days. Of course, if you aggregate it across days then as you said you will have one data point which means that you need to run the test many times across weeks/months to assess 'statistical effectiveness'. You need to decide first the answer to the qn: Under what conditions will you conclude that A is effective? and test if those conditions are met via experiments.",2010-11-25T22:26:33.933,,CC BY-SA 2.5,user28
7326,4840,0,"thanks for your effort mbq
Yes, i want the mean level in 2006- 2007 or more if that is possible.Regarding data, i guess that is the only data that i got as of now,the measurement is once in a month because the data is available in monthly intervals.if more relevant info is needed then i will try to inquire them after two days.Do you mean the focusing can't be done without other data/info.
but you mean the focusing can't be done without other data/info.",2010-11-25T22:43:27.503,2108,CC BY-SA 2.5,
7327,4362,0,"@Srikant: Point is that getting _exactly_ 10000 out of 20000 is about just as likely as getting _exactly_ 10093 or 9781, being about 0.5^20000. So if you calculate it, P(cheating|10) appx P(not cheating|10) appx 0. In R both are 0 if you use P(10k|cheating) = 0.995, using Bayes theorem. This means by the way that cheaters almost always chose 10k, which is not the assumption OP makes. And on a more philosophical point, you could say that if you believe chance is big he's cheating when heads=10k, chance is bigger he's cheating. Bayes never meant his theorem to be used that way.",2010-11-25T22:45:40.313,1124,CC BY-SA 2.5,
7328,4915,0,"Thanks, but what is AIC? I just realized I asked a long winded question when the purpose is quite simple. I just need to get a ""normalized"" standard deviation. Someone suggested a t-student, but this gets quite cumbersome in excel. Am sure statisticians have found a way to eliminate differences in sample sizes and amplitudes to be able to compare standard-deviation-like measures? Thanks again",2010-11-25T23:02:14.140,2137,CC BY-SA 2.5,
7329,4362,0,"@Joris No the probabilities are not *exactly* equal. You need to also account for the combinatorial coefficients. In any case, we may be talking past each other as you seem be talking about probabilities with a throw of fair dice whereas I am talking about subjective probabilities.",2010-11-25T23:02:35.137,,CC BY-SA 2.5,user28
7330,4914,0,"just realized I asked a long winded question when the purpose is quite simple. I just need to get a ""normalized"" standard deviation. Someone suggested a t-student, but this gets quite cumbersome in excel. Am sure statisticians have found a way to eliminate differences in sample sizes and amplitudes to be able to compare standard-deviation-like measures?",2010-11-25T23:02:36.120,2137,CC BY-SA 2.5,
7331,4362,0,"@Srikant : appx is not exactly. I am not talking about probabilities with fair dice, I'm calculating posteriors using bayes theorem and your subjective probabilities. Which still makes the posteriors to be essentially almost zero and even a bit smaller. But both posteriors are lower than the chance to win the lottery twice. It's not a philosophical point of view, it's basic math.",2010-11-25T23:25:00.857,1124,CC BY-SA 2.5,
7332,4856,0,Turbulent velocity components (3D) collected by an HR Acoustic Doppler Current Profiler (HRCP) in an estuary.,2010-11-26T00:41:06.087,1637,CC BY-SA 2.5,
7333,4857,0,"Thanks fRed, first part is quite useful, but as this calculation is a part of much larger code, I cannot use dlm unless it can be linked with Mathematica Kernel.",2010-11-26T00:43:30.033,1637,CC BY-SA 2.5,
7334,4918,1,Calculate in what sense?,2010-11-26T01:02:13.797,,CC BY-SA 2.5,user88
7335,4921,1,Well D itself is a another equation that is calculated by *other* variables: $D = \frac{n_1}{2} * \frac{N_2}{n_2}$. Does this still apply?,2010-11-26T01:25:52.083,1894,CC BY-SA 2.5,
7336,4918,0,"because I want to sample those numbers based on how big they are, so I have to find a threshold, I want give the top 10% numbers a higher chance to be selected.",2010-11-26T01:28:36.583,2141,CC BY-SA 2.5,
7337,4857,0,"You can use http://www.scienceops.com/Rlink2.asp
But too bad you have to pay to use R!",2010-11-26T02:21:28.510,1709,CC BY-SA 2.5,
7338,4915,0,'what is AIC' is answered here: http://stats.stackexchange.com/questions/834/what-is-aic-looking-for-a-formal-but-intuitive-answer,2010-11-26T02:28:34.507,1381,CC BY-SA 2.5,
7339,4915,0,this question should also be useful  http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other,2010-11-26T02:38:57.170,1381,CC BY-SA 2.5,
7340,4914,1,I would reccomend that you consider software designed for statisics rather than calculating your own statistics in Excel or using the opaque calculations in the analysis tool pack. Also see: http://stats.stackexchange.com/questions/3392/excel-as-a-statistics-workbench,2010-11-26T02:49:02.903,1381,CC BY-SA 2.5,
7341,4917,0,"Thanks. the Scott B. Morris' article is just what I was looking for. And yes, I agree that I should provide an explanation of the calculation (perhaps I call it something like $\hat{d}$).",2010-11-26T03:01:59.617,183,CC BY-SA 2.5,
7342,4921,0,"If you discard B or E and treat them as equivalent then you are implicitly asserting that V is all that really matters.  If that is the case, you would be better off retaining B in the model as its interpretation is clear.  Further, if you retain E, but D actually has limited variance, the validity of the interpretation of your results would even more suspect (than usual) for different values of D.",2010-11-26T03:44:12.047,196,CC BY-SA 2.5,
7343,4558,0,Is there a standard article that can be cited to dissuade reviewers from demanding this all-too-common practice?,2010-11-26T03:56:18.223,196,CC BY-SA 2.5,
7344,4919,4,"Feel free to provide links to Wikipedia, but at least provide a sentence or two of explanation in your answer.",2010-11-26T05:00:09.763,159,CC BY-SA 2.5,
7345,4890,0,"@Srikant Thanks again for your response, especially for the question (I was too focused on statistical properties and hence lost the sight of the aim). First I was afraid that I am totally wrong, because it seems that hardly anyone is discussing this issue (at least on the web). Now I am afraid that most AB-Testers do not ask even this question.",2010-11-26T07:45:50.260,264,CC BY-SA 2.5,
7346,4926,1,(+1) for the explanation of PCA.,2010-11-26T08:16:29.437,264,CC BY-SA 2.5,
7347,4918,3,"Please reformulate the header of the question (more specific, less generic) and complete the last sentence to increase the long-term-value of this question for the site/audience. Thank you :)",2010-11-26T08:19:25.020,264,CC BY-SA 2.5,
7348,4906,0,"Try it, I've provided a full working example. :) You will see the difference if you use str(df.out) on both instances.",2010-11-26T08:55:55.973,144,CC BY-SA 2.5,
7349,4928,0,"Sorry now that you have reformulated your question, my answer is not really appropriate",2010-11-26T09:40:03.347,1709,CC BY-SA 2.5,
7350,4928,0,"That's ok! The first formulation was not clear enough, my bad.",2010-11-26T11:22:13.900,1314,CC BY-SA 2.5,
7351,4935,0,"Thanks for the references. Incidentally, the paper by Friedman et al partially answers a related question I've just posted, about the relative performance of coordinate descent vs LARS.",2010-11-26T11:33:31.097,439,CC BY-SA 2.5,
7352,4926,1,"Thanks, this was a great explanation. I've heard and read about PCA, but this is for a final project for a ""regression"" graduate course I'm taking, and the professor just wants us to use LR. Regardless, I really appreciate the explanation of PCA and will probably use it myself for fun.",2010-11-26T11:38:40.750,1894,CC BY-SA 2.5,
7354,4935,1,"""A brief history of coordinate descent for the lasso"" on pp15-17 of the UseR! talk is a gem :-)",2010-11-26T13:14:57.703,439,CC BY-SA 2.5,
7355,4938,0,"(+1) @ogrisel Thanks a lot! Since I'll probably end up having to code this up myself (need it in Java, and haven't seen any open-source Java implementations yet), which algorithm would you say is easier to implement?",2010-11-26T13:24:17.617,439,CC BY-SA 2.5,
7356,4938,1,both coordinate descent and SGD are easy to implement (check Leon Bottou's webpage for a nice intro to SGD). LARS is probably trickier to get right.,2010-11-26T13:25:36.717,2150,CC BY-SA 2.5,
7357,4938,0,"Superb, thanks! I'll check out L√©on Bottou's site.",2010-11-26T13:28:46.070,439,CC BY-SA 2.5,
7358,4938,0,@ogrisel (+1) Nice to see you there.,2010-11-26T13:31:41.333,930,CC BY-SA 2.5,
7359,4558,0,"The only critique I know is Blouin & Riopelle (2005) but they don't get to the heart of the matter.  I generally don't insist on not showing them but doing something correct as in the effect graphs of Masson & Loftus (2003, see figure 4, right panel... if they were removed from the left one you'd have done it right).",2010-11-26T13:41:01.367,601,CC BY-SA 2.5,
7360,4558,0,"Just to be clear, the problem with those CI's is that they're purely used for inferential reasons with respect to differences among conditions and therefore are worse even than PLSD... in fact I prefer them.  At least they're honest.",2010-11-26T13:42:01.457,601,CC BY-SA 2.5,
7361,4938,0,@ogrisel BTW `scikit-learn` looks very impressive indeed.,2010-11-26T13:46:08.923,439,CC BY-SA 2.5,
7362,4931,0,"hi chi, thanks for the answer. When you want you can also answer the identical question at SO and I would accept it there as well. => http://stackoverflow.com/questions/4280371/how-do-you-test-a-implementation-of-k-means",2010-11-26T13:56:34.980,2147,CC BY-SA 2.5,
7363,4939,1,"If anyone wants me to, I can produce a similar graph as the one I was given and link to it here. This question came from a practice IEEE Certified Software Development Associate exam - the correct answer is apparently ""out of control caused by assignable causes"". Unfortunately, I don't know why that's the answer - I said ""out of control caused by chance causes"" since there aren't a series of points out of control.",2010-11-26T14:14:12.487,110,CC BY-SA 2.5,
7364,4908,0,How many contexts do you have and how many data points per context?,2010-11-26T15:19:05.767,,CC BY-SA 2.5,user28
7365,4942,1,$v_0=(M_1+M_2)/2$ seems to be a relatively good candidate (at least by symetry)?,2010-11-26T15:44:21.690,223,CC BY-SA 2.5,
7366,4942,0,"@robin: although $v_0=(M_1+M_2)/2$ or $v_0=P_1M_1+P_2M_2$ (where $P_1$ and $P_2$ are a priori probabilities of classes) are good heuristic solutions, i need the optimum $v_0$. Is it possible to determine it?",2010-11-26T15:51:56.570,2148,CC BY-SA 2.5,
7367,4910,5,"@sesqu Hmm...  You ought to rethink that last statement, because it seems you have rejected out of hand some significant advances in statistical concepts and methodology.  The first statement seems to misunderstand what the ecdf is; it perhaps is related to an unusual conception of what a distribution function is.  Are you using a standard definition such as that given at http://mathworld.wolfram.com/DistributionFunction.html ?",2010-11-26T16:01:00.947,919,CC BY-SA 2.5,
7368,4943,0,"I've only taken one course in Engineering Stats, but I seem to remember that you don't stop the process until you have 3 (maybe 2) points that are out of control. However, your second argument makes sense, where a process truly in control wouldn't have 4/15 observations outside of +/- 3 std dev. Unfortunately, I don't have my EngStats book at home to verify that. At least it's plausible. +1 for now, until I can research this some more. But at least it's a starting point.",2010-11-26T16:07:35.603,110,CC BY-SA 2.5,
7369,4943,0,"(+1) Good answer.  Alternatively, assuming the standard deviation was previously estimated from a very long series of data, one might wonder about the normality of the distribution.  Furthermore, these 15 points were not likely a random selection: they must have been chosen as a short sequence in which an unusual number of OOC measurements appeared.  The former suggests the chance of a single OOC may be quite a bit greater than 0.01 while the latter indicates that the binomial calculation is misleading. After all, it is virtually *certain* that such a sequence will eventually occur by chance!",2010-11-26T16:11:47.410,919,CC BY-SA 2.5,
7370,4939,0,"Yes, the graph would be useful. As stated in my answer, the look of the chart is important as well, not only which points are outside the control limits.",2010-11-26T16:12:23.353,666,CC BY-SA 2.5,
7371,4932,0,"if it's really about programming in R, there are quite some plot people on http://stackoverflow.com that can help you out as well.",2010-11-26T16:20:35.110,1124,CC BY-SA 2.5,
7372,4939,0,"I just added a picture of the question, graph included. I marked the correct answer as well.",2010-11-26T16:27:07.287,110,CC BY-SA 2.5,
7373,4943,0,I added a picture that includes the original question and graph.,2010-11-26T16:28:00.813,110,CC BY-SA 2.5,
7374,4944,0,I added a picture that includes the original question and graph.,2010-11-26T16:28:56.913,110,CC BY-SA 2.5,
7375,4945,5,"I think you'll get more useful help if you take a few steps back, and ask us what scientific question  you are trying to answer. Why are you looking for the smallest normalized SD?",2010-11-26T16:42:55.523,25,CC BY-SA 2.5,
7376,4942,1,@Isaac What is missing from the explanation given at http://en.wikipedia.org/wiki/Linear_discriminant_analysis#Fisher.27s_linear_discriminant ?,2010-11-26T16:51:35.613,919,CC BY-SA 2.5,
7377,4945,0,I agree with Harvey.,2010-11-26T17:04:57.817,439,CC BY-SA 2.5,
7378,4919,1,"please try to provide context to answers, not just links.",2010-11-26T17:19:29.893,1390,CC BY-SA 2.5,
7379,4942,0,@whuber: then which one is better threshold? $v_0 = V^{T}(M_2+M_1)/2$ or $v_0 = V^{T}(P_1M_1+P_2M_2)$ ? (although generally none of them are optimum),2010-11-26T17:21:34.763,2148,CC BY-SA 2.5,
7380,4942,1,"@Isaac The point of that article is ""there is no general rule for the threshold.""  That is, the answer to your question is you cannot find an optimum $v_0$ analytically because there is nothing to be optimized in the general problem.  This situation invites you to examine your data more closely via ""analysis of the one-dimensional distribution.""  In other words, *you need more information* in order to find a demonstrably best value for$v_0$.",2010-11-26T17:27:32.853,919,CC BY-SA 2.5,
7381,4750,3,"I changed ""monotonous"" (= boring) to ""monotonic"" (= strictly increasing or strictly decreasing).  However, the second statement is not true: there are nonlinear f's that preserve the mean.  For example, when the RV is symmetric, then *any* f for which f(x - mu) = -f(mu - x) (for all x) will preserve the mean.",2010-11-26T17:34:09.803,919,CC BY-SA 2.5,
7382,4943,3,"@Thomas It still looks like a bad question to me.  It attempts to measure two concepts (how to read a control chart and the distinction between ""assignable"" and ""chance"" causes), which is one mistake, and it punishes the thoughtful test-taker who knows that much more information is necessary to interpret the OOC points than is given here, which is the more egregious mistake.",2010-11-26T17:40:26.973,919,CC BY-SA 2.5,
7383,4926,4,"In certain circumstances the recommendations in this answer would not work.  For example, what if the true relationship is Y = B + E = V/3000 + V*D?  Then the variables happen to have high correlation due to the ranges of V and D in the dataset--which is (or can be) pure accident--while throwing away either one of B or E will result in the wrong model.  In short, ""dependence"" is not in general a valid reason for removing some variables from a model; including strongly dependent variables does not necessarily ""weaken"" a model; PCA is not always the way out.",2010-11-26T17:49:19.910,919,CC BY-SA 2.5,
7384,4938,2,@aix I have edited my answer to add some benchmarks on the current implementations in scikit-learn. Also checkout the [java version of liblinear](http://www.bwaldvogel.de/liblinear-java/) before implementing your own coordinate descent as it might be good enough for you (though you cannot have both L1 and L2 reg at the same time).,2010-11-26T17:49:32.500,2150,CC BY-SA 2.5,
7385,4923,4,"+1: Not only is this suggestion a good approach to the problem in question, it shows that throwing away variables is not always the right (or even a good) approach to solving collinearity problems.",2010-11-26T17:52:15.750,919,CC BY-SA 2.5,
7387,4938,0,"@ogrisel Many thanks for both the pointer to `liblinear` (I'll check it out right away!) and the graphs. On the graphs, what is the significance of `alpha=0.01`? I take it `alpha` is the penalty parameter. Are we computing the entire regularization path, or only a part of it?",2010-11-26T17:58:28.283,439,CC BY-SA 2.5,
7388,4938,0,"This benchmark is computing the LARS path down to a fixed alpha (usually called lambda, but lambda is a reserverd keyword in python). Here is the [source code of the benchmark](https://github.com/scikit-learn/scikit-learn/blob/544258d58bea1c8d1fe8a57105b206fcd4f23965/benchmarks/bench_lasso.py). In case of the Coordinate descent, alpha is fixed although it is possible to compute an approximate path using a logscaled grid of aphas and using warm restarts.",2010-11-26T18:05:57.407,2150,CC BY-SA 2.5,
7389,4942,0,"@whuber: For normal distributions, all we should know is the mean and variance. Then, if we assume the distribution of two classes are normal, it should be possible to determine the $v_0$. but how?",2010-11-26T18:23:43.500,2148,CC BY-SA 2.5,
7390,4926,0,"@whuber, I am not sure I agree with your comments.  I would think ""dependence"" is in general a pretty valid reason for removing some variables from a regression model.  Otherwise, your regression coefficients can't be reliable.  In the example you use that would be problematic for regression, one simple solution is to use the entire expression (V/3000 + V*D) as a single variable.",2010-11-26T18:35:27.067,1329,CC BY-SA 2.5,
7391,4908,0,The problem is that there are a lot of contexts and only one data point per context in common case.,2010-11-26T18:37:47.883,1314,CC BY-SA 2.5,
7392,4926,5,"More generally, if the model is beta1*(V/3000) + beta2*(V*D) you can't do this: in other words, your suggestion presumes you know a linear constraint among the coefficients.  It is true that the regression coefficients can have *relatively* large VIFs or standard errors, but with sufficient amounts of data--or with well chosen observations--the estimates will be *reliable enough.*  So, we agree there is a problem and indeed I agree with your solution *as one of several alternatives to consider*.  I disagree that it is as general and necessary as you make it out to be.",2010-11-26T19:00:32.323,919,CC BY-SA 2.5,
7393,4942,2,"@Isaac If I have understood the context correctly it seems we ""know"" nothing: all parameters are estimates from data.  Thus, at a minimum, we would want some information about the standard errors of these estimates.  If I'm wrong and you know the means and covariance matrix for certain (or have highly accurate estimates), then the problem is one-dimensional: it comes down to separating two normal distributions on the line, which is straightforward to do (this is the basis for power calculations when testing equivalence of means, for instance).  Would that be what you're looking for?",2010-11-26T19:05:32.273,919,CC BY-SA 2.5,
7394,4942,0,"@whuber: we know underlying distribution parameters for certain. I think I got the idea. But what do you meant by _this is the basis for power calculations when testing equivalence of means, for instance_?",2010-11-26T19:33:14.260,2148,CC BY-SA 2.5,
7395,4947,1,"Good point, and interesting links.",2010-11-26T19:37:25.663,1381,CC BY-SA 2.5,
7396,4945,4,"It's less that the standard deviation is *sensitive* to scale, more that the standard deviation is a *measure of* the scale.",2010-11-26T19:37:45.100,449,CC BY-SA 2.5,
7397,4946,0,(+1) Your comments on the possible ways to generate relevant synthetic data are very welcome.,2010-11-26T19:53:34.150,930,CC BY-SA 2.5,
7398,4931,0,(+1) The first paragraph quickly gets to the heart of the matter.,2010-11-26T20:07:56.853,919,CC BY-SA 2.5,
7399,4949,1,Is this question the same as http://stats.stackexchange.com/q/4942/919 ?,2010-11-26T20:40:10.320,919,CC BY-SA 2.5,
7400,4907,3,"As @Gavin said, one of the potential pitfalls is that decision trees need a large sample size and are quite unstable (which is one of the reason bagging and random forests were proposed as viable alternatives). Another problem is that it is not clear whether we seek for second- or higher-order interaction effects. In the former case, CARTs are not a solution. In any case, I will find very doubtful any interpretation of an interaction between 6 variables in any kind of study (observational or controlled).",2010-11-26T20:43:39.283,930,CC BY-SA 2.5,
7401,4949,0,@whuber Your answer suggests it is the case indeed.,2010-11-26T20:47:29.560,930,CC BY-SA 2.5,
7402,4949,0,@whuber: Yes. i don't know this question suited to which one. I am waiting for a response for one to remove the other one. Is it against the rules?,2010-11-26T20:49:03.353,2148,CC BY-SA 2.5,
7403,4949,0,"It might be easier, and surely would be cleaner, to edit the original question.  However, sometimes a question is restarted as a new one when the earlier version collects too many comments that are made irrelevant by the edits, so it's a judgment call.  In any event it's helpful to place cross-references between closely related questions to help people connect them easily.",2010-11-26T20:52:35.363,919,CC BY-SA 2.5,
7406,4950,1,"OK, thanks for the clarification: it resolves your original question. ""Assignable"" seems to mean ""not attributable to chance,"" which is consistent with the dichotomy in the question. What I am struggling with is the assumption that OOC events can *not* be due to chance. This is clearly mistaken, as @HairyBeast has noted. Another striking aspect of the study document is how informal, unquantitative, and *ad hoc* it seems, as in ""a number of points"" (how many?) and ""systematically"" (meaning what?). It seems to refer to CUSUM or runs charts without providing appropriate guidelines for their use.",2010-11-26T21:15:34.003,919,CC BY-SA 2.5,
7407,4950,2,"@whuber I agree fully. Considering this is published and maintained by the IEEE, I expected a whole lot better. I'm just wondering if they are handwaving a bunch of stuff because it's a software engineering certification, and they don't want to get in too much depth in other things. But that's no excuse for some of this confusion.",2010-11-26T21:30:04.567,110,CC BY-SA 2.5,
7408,4827,0,Update - I found that there is a relevant non parametric test - I added it as an answer.,2010-11-27T08:01:52.390,253,CC BY-SA 2.5,
7410,4903,3,"@Brandon: This is standard model diagnostics and exploratory plotting skills. I would plot the residuals against one of the covariates I think might be a candidate for an interation, conditioned (in the ggplot2 or lattice way) on the values of the covariate I think is involved in the interaction. Stick a loess smoother through each panel to see if there are patterns. Depends on what type of variables your covariates are.",2010-11-27T09:20:04.863,1390,CC BY-SA 2.5,
7412,4918,2,"I've edited your title, to make it fit with other questions. As you may have noticed, questions are usually comprised of things like (a) a brief description of the problem at hand, (b) what are the available data actually, and (c) ... a motivated and understandable question. All three ingredients seem lacking there, which is the reason why you get downvoted. Would you pls make a little effort toward describing what your data set looks like, and take the time to finish your last sentence (nobody will ever complete it for you)?",2010-11-27T11:56:43.783,930,CC BY-SA 2.5,
7413,4919,0,"I've +1 because you addressed the above remarks from @Rob and @Gavin. The term 'software' seems, however, misleading here because the links you referred to have nothing to do with a possible instantiation in any statistical software (as [@Chase](http://stats.stackexchange.com/questions/4918/how-to-get-the-required-data/4925#4925) did), but this is no more than a remark.",2010-11-27T12:04:17.800,930,CC BY-SA 2.5,
7415,4960,0,Thanks. A very neat way of answering the question (leading to the same answer) was also provided on math.stackexchange (link above in the question) a few minutes ago.,2010-11-27T17:24:13.733,1934,CC BY-SA 2.5,
7416,4961,8,"An example is ridge regression, which is OLS with a bound on the sum of the squared coefficients. This will introduce bias into the model, but will reduce the variance of the coefficients, sometimes substantially. LASSO is another related method, but puts an L1 constraint on the size of the coefficients. It has the advantage of dropping coefficients. This is useful for p>>n situations Regularizing, in a way, means ""shrinking"" the model to avoid over-fitting (and to reduce coefficient variance), which usually improves the model's predictive performance.",2010-11-27T18:30:09.193,2144,CC BY-SA 2.5,
7417,4958,0,"I derived the posterior expression as the product of Normal likelihood function and Wishart prior. I sampled precision matrix from this posterior expression, but not getting the correct output. I am generating the samples from wishrnd command in matlab, by using the degree of freedom and the scale matrix of the posterior.",2010-11-27T19:13:35.950,2157,CC BY-SA 2.5,
7418,4965,6,"(+1) If you have the time and inclination you might strengthen this answer by expanding on your claim that including X1*X2 makes the effect of X1 on Y vary with X2.  Specifically, a model Y = b0 + b1*X1 + b2*X2 + b3*(X1*X2) + error can also be viewed as having the form Y = b0 + (b1 + b3*X2)*X1 + b2*X2 + error, showing precisely how the coefficient of X1--which equals b1 + b3*X2--varies with X2 (and, symmetrically, the coefficient of X2 varies with X1).  That's a simple, natural form of ""interaction.""",2010-11-27T21:13:37.667,919,CC BY-SA 2.5,
7419,4960,2,"The math answer computes the integrals using linearity of expectation.  In some ways it's simpler.  But I like your solution because it exploits *statistical* knowledge: because you know a sum of independent Exponential variables has a Gamma distribution, you're done.",2010-11-27T21:19:29.357,919,CC BY-SA 2.5,
7420,4966,1,"In the earlier question, he pointed out that the goal was to compare the SD of raw data with the SD of smoothed data with the SD of highly smoothed data. I don't think the F test (or Bartlett's or Levene's) tests will do any more than complete the circle. The whole point of smoothing is to reduce the variation, so tests that compare variation will tell you smoothed data are smoother. I don't see how this circular logic will get you anywhere.",2010-11-27T21:22:33.277,25,CC BY-SA 2.5,
7421,4965,1,@chl - Thanks for the response. The problem that I have is that I have a large `n` (11K) and am using MiniTab to do an Interactions Plot and it takes *forever* to calculate but doesn't show anything. I'm just not sure how I see *if* there is interaction with this dataset.,2010-11-27T21:33:33.543,1894,CC BY-SA 2.5,
7422,118,0,"Related question: http://stats.stackexchange.com/q/354/919 (""Bias towards natural numbers in the case of least squares."")",2010-11-27T21:53:19.177,919,CC BY-SA 2.5,
7423,2207,1,"+1 for introducing the idea of loss.  (But aren't ""exponential"", etc., *distributions*, not loss functions?) Historically linear loss was the first approach formally developed, in 1750, and there was a straightforward *geometric* solution available for it.  I believe Laplace established the relationship between this and the double-exponential distribution in an 1809 publication (for which the MLE will minimize absolute error, not squared error).  Thus squared loss is not uniquely distinguished by the criteria of having an MLE and being mathematically easy.",2010-11-27T21:58:23.167,919,CC BY-SA 2.5,
7424,4965,4,"@TheCloudlessSky: One approach is to slice the data into bins according to values of X1.  Plot Y versus X2 bin by bin, looking for changes in slope as the bins vary.  Do the same with the roles of X1 and X2 reversed.",2010-11-27T22:05:28.900,919,CC BY-SA 2.5,
7425,4958,0,"It would be useful if you could update your question with the model setup, the assumptions about priors and the corresponding posteriors.",2010-11-27T22:34:00.463,,CC BY-SA 2.5,user28
7426,1916,0,I would agree with whuber though that the visualization of the variogram cloud would be really helpful in determining if you choice of model seems appropriate.,2010-11-28T07:51:48.753,787,CC BY-SA 2.5,
7427,4951,0,"After trying to find $v_0$ for this problem with a assumption of $\pi_1 = \pi_2 = 1/2$, i realized that $v_0 = -(\mu_1 + \mu_2)/2$ cannot give me the $v_0$ because the $\mu_i$'s are unknown (although $M_1$ and $M_2$ are known) and they directly depend on $v_0$ value. How i can determine $\mu_1$ and $\mu_2$ ?",2010-11-28T09:09:12.043,2148,CC BY-SA 2.5,
7429,4973,0,"(+1) Nice points, especially the latest.",2010-11-28T11:57:18.307,930,CC BY-SA 2.5,
7430,4961,2,"@HairyBeast You should put your nice comment as an answer. If possible, try to add an illustrative example so that the OP can figure out how it translates to the problem at hand.",2010-11-28T12:02:37.223,930,CC BY-SA 2.5,
7431,4967,1,Could you elaborate on how you propose to do a binary search in a sorted doubly-linked list?,2010-11-28T12:14:36.850,439,CC BY-SA 2.5,
7432,4972,2,[This question](http://stackoverflow.com/questions/4290081/fitting-data-to-distributions/4290196#4290196) seems directly relevant. See my post for data vis tips to compare your data to other known distributions in base R.,2010-11-28T13:18:20.207,696,CC BY-SA 2.5,
7433,4948,0,"In the denominator, isn't it $s^2/n$ instead? Also, do you need to assume independence of A and B for this test?",2010-11-28T14:35:34.680,1909,CC BY-SA 2.5,
7434,4975,0,"You are right about the QQ plot being applicable to any distribution. I read the question to fast and (for whatever reason) assumed it was a QQ norm plot. One minor note: be careful about concluding a distribution from QQ plots. For example, data that has a t distribution with 20 df will still give you nice QQ norm plots.",2010-11-28T15:25:23.290,2144,CC BY-SA 2.5,
7435,4766,0,I am not really familiar with R but it seems that there is a problem in your implementation: you should only keep the first (k + p) lines (or columns?) of q to actually truncate the svd and make it faster.,2010-11-28T16:15:22.510,2150,CC BY-SA 2.5,
7436,4362,1,"Putting this in a more general perspective, the point, with which I agree, is that Bayes theorem is at work here.  Specifically there is are alternative likelihoods (corresponding to different generative processes) for cheating and for honest experimenters.  Establishing the cheating is posterior inference with respect to the intuitive and therefore woefully underspecified cheater process.",2010-11-28T16:27:58.177,1739,CC BY-SA 2.5,
7437,4362,0,"Happily one can use just this sort of problem to elicit what people really think is 'random' looks like.  There is often, it seems, a rather structured model available for things like a 'random coin flip sequence' (it's got negative auto-correlation, fewer runs, etc.)  Some fun research modelling both the structure of these intuitions and inferences based on them is here: http://cocosci.berkeley.edu/tom/papers/random.pdf and in later papers.",2010-11-28T16:33:03.643,1739,CC BY-SA 2.5,
7438,4967,0,"one 'link' allows you to traverse the list in sorted order; the other allows you to traverse in the order in which the elements appear. It is not clear how you would do this with pointers, though, as @aix questions.",2010-11-28T17:02:03.763,795,CC BY-SA 2.5,
7439,358,0,"So, if the errors are not normally distributed, but for instance according to another L√©vy-stable distribution, it might pay off to use an exponent different from 2?",2010-11-28T18:28:22.097,2036,CC BY-SA 2.5,
7440,4974,0,"Ripley's K is a global measure as well. But I agree with your first point, and one may be interested in simulating local autocorrelation as well as global autocorrelation. The ambiguity of the OP's question makes it difficult to give a more thorough or directly pertinent answer.",2010-11-28T18:29:24.397,1036,CC BY-SA 2.5,
7441,4766,0,i will take a look into that.,2010-11-28T19:01:13.553,2078,CC BY-SA 2.5,
7442,4967,2,"@aix I think your intimation is correct; I would need an indexable skip list, not just a sorted doubly-linked list.  The idea is to have a data structure that permits the insertion of one element, the deletion of one element, and finding the median in expected O(log(n)) time (or better).",2010-11-28T19:17:32.337,919,CC BY-SA 2.5,
7443,4948,0,"@caracal Yes, thanks for catching that error.  I have fixed it.  (The calculations in the example were nevertheless correct.)  Independence of A and B is asserted by the OP in the second sentence of the question.",2010-11-28T19:19:15.797,919,CC BY-SA 2.5,
7444,4965,3,"@chl The trellis display is a nice illustration.  Slicing one variable at equal-interval quantiles is attractive.  There are other approaches.  E.g., Tukey recommended slicing by halving the tails: that is, slice the X2 values into halves at the median, then slice those halves by *their* medians, then slice the *lower* half of the lowest group at its median and the *upper* half of the highest group at its median, and so on, continuing for as long as the new groups have enough data.",2010-11-28T19:25:41.377,919,CC BY-SA 2.5,
7445,4951,0,"@Isaac When asked, you specifically said you ""know [the] underlying distribution parameters for certain""!  When you don't know them, you'll have to guess.  One approach is to estimate v0, use that to estimate mu1 and mu2 from M1 and M2, then compute the ""optimal"" v0.  Iterate until the value of v0 does not change.",2010-11-28T19:29:52.667,919,CC BY-SA 2.5,
7446,4958,0,"There was a bug in my code, its working now.",2010-11-28T19:31:12.043,2157,CC BY-SA 2.5,
7447,4951,0,"yes, i know the underlying distribution parameters that are $M_1$, $M_2$,$\Sigma_1$, and $\Sigma_2$. But $\mu_1$ $\mu_2$ are not parameters of data distribution, they are parameters of the distribution of $h(x)$ that is a mapping from data to 1-D space. And they depend on $v_0$ value. anyway, I believe a good solution is what you said: _iteration until finding a good $v_0$_. Then is there really no way to find best threshold value for Fisher criterion analytically?",2010-11-28T20:20:33.157,2148,CC BY-SA 2.5,
7448,4972,0,@Chase: +1 Actually yes :) I think I missed that one. I'll do that rightaway. Thanks a lot.,2010-11-28T20:31:06.897,2164,CC BY-SA 2.5,
7449,4973,0,"+1 for the suggestions. I updated my post with a negative binomial distribution as well. It looks like it will serve its purpose except that the third bar is not as expected. As for your final point, I heard that if the data does not come from any known distributions, I can use something like a kernel density estimation. Would you suggest this? If so, can you kindly give me a very short example on how to do this for discrete data using R? Would I still be looking at QQ-plots to verify my model?",2010-11-28T20:33:07.660,2164,CC BY-SA 2.5,
7450,4975,0,"+1 for the explanation as to why Cauchy does not make sense in this case. That would have been my next question if it were right :) If you get some time, could you kindly take a look at my comment above? In short, because my data need not come from a specific distribution, my readings yesterday revealed that a kernel density estimation technique can be used but am not really sure if this is the right approach and if it is, how one goes about doing it.",2010-11-28T20:35:13.667,2164,CC BY-SA 2.5,
7451,4972,0,@Chase: So I tried out your approach there and obtained a bunch of plots and can confirm that the data does not fall into any of those three distributions that but I'll try the other ones as well and get back. I was just wondering if this is the defacto approach. Do I go through every possible distribution and look at the QQ-plots to figure out if my data fits into that distribution?,2010-11-28T20:45:05.657,2164,CC BY-SA 2.5,
7453,4972,2,"@Legend You can also try a [rootogram](http://rss.acs.unt.edu/Rdoc/library/vcd/html/rootogram.html) (don't know if it overlaps with @Chase's response on SO). Now, I don't understand why you want to try and fit every discrete distribution to your data. Either you have *a priori* knowledge or hypothesis about the law of your outcomes, or you don't. In the former case, you might want to explain why the observed data don't fit the model. In the latter case, you're left with exploratory data analysis (and, potentially, non-parametric density estimates, mixture models, etc.)",2010-11-28T21:12:47.680,930,CC BY-SA 2.5,
7454,4972,0,"@chl: Thank you for the pointers. I will look into it now. I think my problem fits more into an exploratory domain because what I want to do is to see if I can get some model to fit this data so that I can use it in a discrete simulation. So for example, I want to evaluate a few scenarios for the case when the population is not really 100 but say 500. In that case, I am trying to find a model that would let me generate the numbers that I could get from the survey data. Please correct me if I am taking the wrong path though.",2010-11-28T21:22:06.623,2164,CC BY-SA 2.5,
7455,4972,1,"@Legend 'Scenario' means that you already have some hypothesis, no? It's difficult to answer your question because you seek to fit the 'best' model (in the sense of goodness-of-fit) to your data, but it is not necessary the 'correct' model. After all, your data may be subjected to measurement error or any other sources of error. Finally, you can still work with your observed sample and use bootstrap to simulate new samples.",2010-11-28T21:31:32.930,930,CC BY-SA 2.5,
7456,4972,0,"@chl: Oh I think I am just using the wrong terminology here. I will try to correct myself as I am still reading some material. But for now, in my opinion, a scenario for me is to simulate the communication patterns inside a small city with a set number of people. I am trying to evaluate how much time it takes for some arbitrary piece of information to propagate for a population of size `X`. As this depends on the communication patterns of people, what I was trying to do was to build a model that could use the information from the 100 people I surveyed to give me patterns for the `X` number.",2010-11-28T21:43:27.720,2164,CC BY-SA 2.5,
7457,4972,0,"@chl: As an additional note, would you mind explaining a little more about bootstrapping to simulate new samples? Are you suggesting that I go ahead with the 'not-so-perfect' model as a starting point or are you suggesting something different? Thank You once again.",2010-11-28T21:44:41.807,2164,CC BY-SA 2.5,
7458,4951,0,"@Isaac Sorry, I completely misremembered what is going on here.  I should reread my own answers!  I defined mu1 and mu2 in the first sentence: if you know M1, M2, and the covariances, that determines V which determines mu1 and mu2.",2010-11-28T22:22:33.830,919,CC BY-SA 2.5,
7459,4979,4,(+1) That 2002 thread is a hell of an exchange of ideas.,2010-11-28T22:50:49.563,919,CC BY-SA 2.5,
7460,4362,0,"@Conjugate Prior: Yes, bayes theorem is at work here. But with Bayes theorem, as I calculated for you, both **posterior** probabilities mentioned are close to zero (smaller than R can cope with actually). So the posterior conclusion, based on Bayes theorem, when you have exactly 10k, is that the researcher is not cheating. Why talking philosophy if you have numbers?",2010-11-29T01:14:41.683,1124,CC BY-SA 2.5,
7461,4362,0,"@Conjugate Prior. Plus, the paper you refer to mentions that people are likely to introduce less runs when they consider a ""random"" process (p=0.6 instead of 0.5). So it rejects intuition rather than confirm it. The paper also talks about sequences and patterns, but that is not what is discussed here. HTTHHT is more likely than HHHTTT according to that paper, but both have a p of exactly 0.5",2010-11-29T01:20:21.883,1124,CC BY-SA 2.5,
7462,4987,1,Please do *not* cross-post simultaneously on SO and here.,2010-11-29T01:53:11.730,334,CC BY-SA 2.5,
7463,4987,1,"@Dirk Eddelbuettel: Deleted my other post. I wasn't sure if this comes under programming or pure statistics.. Anycase, thanks for pointing it out.",2010-11-29T03:14:03.983,2164,CC BY-SA 2.5,
7464,4908,0,"I am not sure that you can do statistics on the player's strategy without more data. Either you have enough data and you can train a machine learning algorithm, or you don't and you have to do assumptions on the underlying model you want to use.",2010-11-29T03:17:40.347,1709,CC BY-SA 2.5,
7465,4908,0,Well you can try but I would not be very confident with the results. Do you want to invest real money into this strategy?,2010-11-29T03:19:09.300,1709,CC BY-SA 2.5,
7466,4766,0,the dimension of Q is ( N ) X ( K + P ),2010-11-29T04:32:58.047,2078,CC BY-SA 2.5,
7467,4972,1,"@Legend Bootstrap is useful to estimate, based on an observed sample, the variability of an estimator when you don't know (or don't want to assume) its law. But in your case, given the context you added, I would suggest to update your question so that people can have a better idea of what you really intend to do with (which is beyond simple distribution fitting, apparently).",2010-11-29T06:01:19.877,930,CC BY-SA 2.5,
7468,4908,0,"@fRed I have machine learning algorithm, but I'm not sure about its reliability. No real money, this is an academic research.",2010-11-29T06:07:14.863,1314,CC BY-SA 2.5,
7469,4990,0,Awesome! Thanks a lot for your time and explanation. I have accepted this as an answer but can you kindly explain how to interpret my original graph?,2010-11-29T06:52:43.410,2164,CC BY-SA 2.5,
7470,4972,0,@chl: Thank You for the suggestions. I updated my question to reflect the latest discussion.,2010-11-29T06:58:32.767,2164,CC BY-SA 2.5,
7473,4990,0,"@Legend. As I pointed out, your graph plots the sorted data points (y in our case) vs the corresponding values of negative binomial random variable which correspond to the value of probabilities `ppoints(data)` in your case. Therefore, the last point in your graph is (175, 170) which is below the `abline(0,1)`.",2010-11-29T07:34:08.003,1307,CC BY-SA 2.5,
7475,4991,0,My question is a bit similar to  http://stats.stackexchange.com/questions/2149/what-is-the-difference-between-a-particle-filter-sequential-monte-carlo-and-a-k. I have re opened a question on purpose as the situation is a bit different and I would like different opinions. (The answer by gd047 was mainly focusing on unscented Kalman filter (UKF) ),2010-11-29T07:54:44.913,1709,CC BY-SA 2.5,
7476,4988,0,This made me discover there is some ML underground on TCS... Neh.,2010-11-29T15:22:18.673,,CC BY-SA 2.5,user88
7478,4988,0,"I am curious: why did you specify the ""graphical-model"" tag for this question?",2010-11-29T16:18:05.830,919,CC BY-SA 2.5,
7481,4984,1,"Thanks. I didn't think that ""The logic of science"" talked about this stuff too, i'm definitely going to read that book.",2010-11-29T17:15:08.090,2171,CC BY-SA 2.5,
7485,4960,1,I enjoyed it quite a bit and I am by no means a statistician or a mathematician.,2010-11-29T18:04:00.000,1673,CC BY-SA 2.5,
7488,4988,0,added clarification,2010-11-29T18:31:09.497,511,CC BY-SA 2.5,
7489,5001,0,"When I initially asked this question I was planning to add this answer myself, besides alternatives I didn't know. Thanks for answering, I will accept this answer as the accepted as it is a 'standard' in the ASR world!",2010-11-29T19:12:35.613,190,CC BY-SA 2.5,
7490,5004,4,Doesn't http://en.wikipedia.org/wiki/Kalman_filter#Unscented_Kalman_filter answer this question?,2010-11-29T19:26:17.420,919,CC BY-SA 2.5,
7491,5003,0,"My understanding is that lme4, by default uses REML where glm uses ML.  They might be comparable if you made lmer use ML by setting REML = FALSE.",2010-11-29T20:46:48.177,196,CC BY-SA 2.5,
7492,5003,0,"In addition to your Gavin's comment, it also depends what do you want to do with the model. Is the model for prediction or Thomas is looking for parsimony ? (I think)",2010-11-29T20:52:14.553,1307,CC BY-SA 2.5,
7493,5003,3,@drnexus: I don't think that is sufficient; you have to be sure that the same normalising constant is being applied in the log likelihood calculation.,2010-11-29T20:52:20.247,1390,CC BY-SA 2.5,
7494,5007,2,"@drknexus - if you can share your data, that would be helpful, or point to a similar dataset within R that has the same shape as your data.",2010-11-29T21:09:50.747,696,CC BY-SA 2.5,
7496,5009,0,"I think what I'm missing is a function like scale_color_discrete that ties into the ""group"" or ""lty"" specification in ggplot(data=ex.daata, aes(x=V4, y=DV, group=V3, lty=V3))",2010-11-29T21:31:59.447,196,CC BY-SA 2.5,
7498,5009,1,"Ah I found it: scale_linetype_discrete(name=""bob"")",2010-11-29T21:35:34.700,196,CC BY-SA 2.5,
7499,5009,0,@Chase: Edit with the scale_linetype_discrete bit and I'll accept your answer.,2010-11-29T21:36:10.673,196,CC BY-SA 2.5,
7500,5009,1,"@drknexus - the issue with that is in my working example, the appropriate command is `scale_colour_discrete()` and the code you are working with is obviously a bit different. I don't know what that code is...",2010-11-29T22:09:51.647,696,CC BY-SA 2.5,
7501,5003,0,"@suncoolsu: I'm not trying to use this model for prediction.

@Gavin: How would I know about normalising constants? I confess my understanding of what R is doing behind the scenes is minimal.",2010-11-29T22:17:59.363,2182,CC BY-SA 2.5,
7502,5003,2,"@Thomas: for that you'd need to look at the code or speak to the person who wrote it to be sure. In general, assume the likelihoods aren't comparable across different software/packages/functions.",2010-11-29T22:46:26.327,1390,CC BY-SA 2.5,
7503,4996,0,"I think you are right, I may be asking the wrong question",2010-11-29T22:47:55.340,511,CC BY-SA 2.5,
7507,4986,0,I only have an implementation of the Bayesian LASSO for linear regression and the LASSO for logistic regression currently uploaded. Appologies.,2010-11-29T23:12:38.550,530,CC BY-SA 2.5,
7508,5009,3,"@Chase Re: ""why we have to treat V3 as a factor"" `scale_linetype_discrete` expects a variable with discrete values (factor or character, from `plyr::is.discrete`), check out the src code on Github, http://j.mp/ejaRRT. Nice response (+1).",2010-11-29T23:29:35.103,930,CC BY-SA 2.5,
7509,5003,0,"Is AIC the only thing that you have? Try using, may be, BIC. But again, going back to Gavin's comment, the comparison of likelihoods across package may not be ""consistent"".",2010-11-30T01:40:08.627,1307,CC BY-SA 2.5,
7511,5017,0,"Thanks for the answer. These are significances of large sets of genome regions; typically a hypergeometric or binomial test is used in cases like these, and p-values on the order of 10^-50 to 100 are not at all uncommon (if you pass in a set of regions that turn out to all affect limb development, you get a pretty small p-value for a ""limb"" test). I initially dismissed Monte Carlo as too slow, but I don't care too much about speed so I will definitely experiment some with that.",2010-11-30T06:02:35.643,2111,CC BY-SA 2.5,
7512,4560,3,"It's fine if you use cross-validation, and don't extrapolate. Don't publish the p-values though, as they are meaningless.",2010-11-30T06:16:54.457,74,CC BY-SA 2.5,
7513,5014,2,It's a little nitpicky but the CLT doesn't tell us that the 'sample' itself will be normally distributed.,2010-11-30T06:16:55.343,1028,CC BY-SA 2.5,
7516,5017,0,"@bnaul: No idea how people work in your field, but contemplating differences on a 10^-50 or 10^-100 level?? Even with n=1000 I image that would be hard to back up with actual data. Hope you people can sleep well ;)",2010-11-30T06:41:14.363,56,CC BY-SA 2.5,
7517,5015,0,"This in indeed a very interesting question, but maybe (only maybe) more appropriate for http://math.stackexchange.com/ ?",2010-11-30T07:08:17.360,264,CC BY-SA 2.5,
7518,5012,0,"Why (2k+1)/n? I think it's k/n. For example, if my range is of length k=10 and my given point is 50, the range must fall on [41,50] or [42,51] or [43,52] or ... [50,60] in order to cover this point (10 options). The simplest case - the range is of length k=1 - it has only one option to cover the point (fall exactly on it).",2010-11-30T09:07:29.843,634,CC BY-SA 2.5,
7519,5012,0,"And in the general case, when we look at the probability a randomly mapped range of length k covers a given range of length m on a circumference of length g, we get p=(k-m+1)/n. When m=1, as in the original post, this is indeed k/n.",2010-11-30T09:12:10.213,634,CC BY-SA 2.5,
7521,5023,1,"'Non-independent' is a bit vague; there are many causes and models for dependence (clustered data, hierarchical/multilevel models, time series, ...). Could you be a bit more specific on your area of interest?",2010-11-30T10:31:42.940,449,CC BY-SA 2.5,
7524,5014,0,"@Chase In python, the AD test for normal, exponential, or Gumbel dist. is available in the [scipy](http://www.scipy.org/) toolkit (see `anderson()` in `morestats.py`).",2010-11-30T11:33:05.310,930,CC BY-SA 2.5,
7528,5003,0,@suncoolsu: BIC is available (as is anything that's been written for R). But obviously I don't want to use the numbers it spits out unless I'm confident they're really comparable.,2010-11-30T11:55:51.270,2182,CC BY-SA 2.5,
7529,5023,0,"Yes, I'm mostly interested in dependencies between organisms. More specifically, a virus that would have evolved over time by means of mutations would lead to several different strains, which are all related in some way because they were derived from the same sequence.",2010-11-30T12:20:01.320,2197,CC BY-SA 2.5,
7530,4987,1,I came across [this article](http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf) regarding QQ plots and various distributions and thought you may appreciate reading through it.,2010-11-30T12:58:42.083,696,CC BY-SA 2.5,
7533,4987,0,@Chase: Awesome! Looks like it discusses a number of things. I'll read it right away. +1 Thank you very much.,2010-11-30T14:33:35.590,2164,CC BY-SA 2.5,
7534,5023,0,"The mutations are still random and independent. It's the selection process that is not random and introduces dependencies (although which selection process is acting could be random). Or maybe you have more complex selection processes in mind where the specific proportion of genotypes can act as a selector as well, which would be an explicit dependence?",2010-11-30T14:44:50.293,2036,CC BY-SA 2.5,
7535,5014,0,"@Dason - nice catch and good point. I modified my answer to more accurately reflect that subtlety. @chl - thanks for the reference. I don't have any experience with python, I'm still trying to grok R in my spare time!",2010-11-30T14:53:28.027,696,CC BY-SA 2.5,
7537,5023,0,"No, what you describe seems to fit the situation, but an idea of what has been used so far to model that selection could still be helpful.",2010-11-30T15:17:59.347,2197,CC BY-SA 2.5,
7539,5023,0,"What do you know about the computational side of phylogenetic trees? There's a whole mathematical field flourishing around that type of problems. I'm no expert, but I know of a few papers using that approach. I'll try to dig them up and post them as a reply.",2010-11-30T15:51:36.373,2036,CC BY-SA 2.5,
7540,5012,0,"@David You're right, thank you.  I was (confusedly) equating ""length"" with ""radius.""  But this does not render the explanation any less valid.  The question is whether the discrepancies you are seeing are as predicted (by the corrected formulas).",2010-11-30T15:58:01.030,919,CC BY-SA 2.5,
7543,5015,8,I think this is a perfectly reasonable question on computation statistics.,2010-11-30T16:36:24.897,8,CC BY-SA 2.5,
7544,5016,0,"Where do the empirical estimates come from and in what form are they?  (In a brief test Mathematica computed the convolution of 1000 discrete RVs, each specified by ten parameters, in just a few seconds, so I don't understand why computing the exact distribution is so slow.)",2010-11-30T16:42:26.920,919,CC BY-SA 2.5,
7545,4960,0,very elegant answer.,2010-11-30T16:44:37.633,96,CC BY-SA 2.5,
7546,5014,0,"@Chase where is the modification you promised to @Dason?  Your answer still asserts that all ""samples"" of n >= 30 will be ""normal,"" which is plainly wrong.  That's no ""nitpick"": it's fundamental.",2010-11-30T17:10:39.423,919,CC BY-SA 2.5,
7549,5005,0,"Whether the Poisson distribution is a ""good"" approximation depends on your purpose.  In this situation, a general rule of thumb sheds no light.  You have noted a discrepancy; one obvious place to look for an explanation lies in using the Poisson as an approximation to the correct distribution, which is Binomial (that's not in question, I hope).  Another place, indicated in your update, lies in using ranges of ""different sizes"": now all bets are off.  You have a mixture of Binomials, which should create some overdispersion: http://en.wikipedia.org/wiki/Overdispersion",2010-11-30T17:26:47.693,919,CC BY-SA 2.5,
7550,5014,0,"@whuber - Feel free to modify my answer as you see fit, you almost certainly have more expertise in formal statistics than I do. My answer above states that a sampled population will be ""approximately"" normal as `n` increases with `n == 30` as the generally accepted rule of thumb for minimum `n`. That language comes directly out of my stats 101 text book and class notes. I understand ""approximately"" is a bit of a vague term though unfortunately I don't have the experience or expertise to offer any further insight into defining ""approximately"".",2010-11-30T17:26:56.787,696,CC BY-SA 2.5,
7551,5014,0,"@Chase The potential for confusion here is that your stats textbook is referring to the distribution of the *sample mean,* not the sample data.  As you draw more and more data from *any* non-normal distribution, it will more and more clearly show itself to be non-normal! -- but, under mild assumptions, the *means* of many independent such samples will be almost normally distributed.",2010-11-30T17:30:00.263,919,CC BY-SA 2.5,
7552,5014,0,"It's not the *sample* itself that starts to resemble a normal distribution, but the distribution of certain *statistics* (like the mean) that will be approximately normally distributed.",2010-11-30T17:31:50.787,1934,CC BY-SA 2.5,
7553,5013,1,"Out of curiosity, is the misspelling of ""theoretical"" in the plot built into the matplotlib and scipy.stats packages?",2010-11-30T17:31:55.287,919,CC BY-SA 2.5,
7554,5014,0,"@whuber Got it, I understand the difference now - thanks for the clarification. If that's the case, then the CLT doesn't seem terribly applicable after all and I'm just babbling on confirming my idiocy in front of all of the interwebs...right?",2010-11-30T17:40:02.563,696,CC BY-SA 2.5,
7555,5009,0,"@chl (+1) for the link to Github, very informative.",2010-11-30T17:43:23.793,696,CC BY-SA 2.5,
7557,5003,0,"@Thomas. It's great that you want to understand the numbers compared to just using it. AIC is known to overfit, but if the model is meant to be used for prediction, this is fine. However, if you think the true model lies in your present ""model space"", BIC may give you better or parsimonious answers.",2010-11-30T17:48:25.230,1307,CC BY-SA 2.5,
7558,5003,0,"@suncoolsu: Thanks :). This is whole-organism biology, so I think the 'true model' is probably not achievable.",2010-11-30T17:51:38.253,2182,CC BY-SA 2.5,
7560,5014,1,"@Wolfgang Very nicely put!  In two lines you said what I could not express in 10.  @Chase Look at it this way: for the price of writing a few comments online and suffering the criticism of fools like me you have today learned something important that was somehow overlooked in an entire semester's course.  That's an efficient way to learn (and the price is right, too).  You will come across as quite knowledgeable once you fix that initial response :-).",2010-11-30T18:14:03.113,919,CC BY-SA 2.5,
7561,5016,0,"@whuber They're stored as histograms, i.e. lists of frequencies. My problem is that I have to ~20000 of these computations in a reasonable time, so a few seconds is unfortunately not fast enough.",2010-11-30T18:40:07.007,2111,CC BY-SA 2.5,
7562,5005,0,"@whuber Indeed, I have a mixture of Binomials, and it seems sometimes the Poisson approximation is not very good. My question is what are my options for getting a p-value for an actual observation. One way would be to run many simulations and just get an empirical p-value. This is the simplest way I know, but it uses nothing of the info I have -- I do now it's a mixture of Binomials and I know all their p's -- isn't there a way to use this valuable information?",2010-11-30T19:13:44.810,634,CC BY-SA 2.5,
7563,5016,1,Summarize each histogram with a low-order truncation of its cumulant generating function and add those functions to obtain an Edgeworth-type expansion of the sum (http://en.wikipedia.org/wiki/Edgeworth_series ).  The computational effort is proportional to the total number of bars in all those histograms.,2010-11-30T19:21:55.847,919,CC BY-SA 2.5,
7564,5032,1,"Thanks for your answer. It was very useful. Actually, I wasn't very precise in the formulation of the second part of the question. Imagine, that x-s and y-s are positive numbers, measured in the same units. The z-s (observed outcome) somehow measure the ""interaction"" in that sense that if there is no interaction the z-s should be (x+y)/2 (expected outcome). So from my point of view it was the same to use regression with the null hypothesis a=b=1/2 or to compare goodness of fit using Pearson's chi^2 statistics. Does this make any sense? Thanks!",2010-11-30T20:28:06.243,1215,CC BY-SA 2.5,
7566,5032,1,"@Lan I think Wolfgang's answer nicely illustrates how to make the test you are proposing.  It is an example of what meant by testing a hypothesis ""in the usual way.""",2010-11-30T20:56:56.467,919,CC BY-SA 2.5,
7567,4893,0,I was mistaken by the term 1. This is a linear separating rule in $\mathbb{R^{5}}$. :'(,2010-11-30T22:11:37.123,1351,CC BY-SA 2.5,
7569,4889,0,"I think the colnames line above needs to be rewritten as `colnames(df)[1:2] <- c(""variable"",""value"")`?",2010-11-30T22:53:43.337,696,CC BY-SA 2.5,
7570,5013,0,Good eye for details =),2010-11-30T23:24:41.097,1934,CC BY-SA 2.5,
7571,5013,0,@whuber: That would by my speling of the title `;-)`,2010-11-30T23:26:24.557,2191,CC BY-SA 2.5,
7572,5003,0,"@suncoolsu: OK in whole-ogranism biology I am sure BIC will give terrible answers. (I guess: a la ""big p small n"")",2010-11-30T23:28:41.393,1307,CC BY-SA 2.5,
7573,5039,2,"(+1) Good response. I would add that these are often particular contrasts that are interesting in DoE as analyzed through ANOVAs, and that the default ones (as returned by `summary.aov(lm(...))` or `anova(lm(...))` are not of particular interest.",2010-12-01T00:17:44.793,930,CC BY-SA 2.5,
7574,5039,0,"@chl: It's a good point.  Although, I think there is something to be said for actually constructing the contrasts for the hypotheses that you are interested in testing.  There is no fundamental distinction between ANOVA and regression.  Sometimes, ANOVA routines hide the hypotheses that are being tested.  At the same time, ANOVA procedures sometimes package things in exactly the way that you want to see the results.",2010-12-01T01:47:21.767,485,CC BY-SA 2.5,
7575,546,14,Bananas are always tasty.,2010-12-01T04:00:58.017,1577,CC BY-SA 2.5,
7577,5038,1,This is a near-duplicate of http://stats.stackexchange.com/questions/555/why-is-anova-taught-used-as-if-it-is-a-different-research-methodology-compared,2010-12-01T06:48:25.580,449,CC BY-SA 2.5,
7578,5039,0,"Thank you sir :) So I would first look at the anova, if the factor made any difference at all, before I looked at the lm()-output to see _which_ factors made the difference? But doesn't the lm()-output already tell me if the different factors are significant (with the 1-3 stars behind them)?",2010-12-01T07:38:43.523,2091,CC BY-SA 2.5,
7580,5039,1,"Not quite. The output from lm() above tells you whether there is a significant difference between level A (the reference level) and levels B and C. It does not tell you whether levels B and C are significantly different from each other. Of course, you could make level B (or C) your reference (with the relevel() function), but suppose you have a factor with many levels. Checking all pairwise contrasts between levels will become tedious. Plus, you are conducting lots of tests. The alternative is to first inspect the omnibus test to check whether there is anything going on to begin with.",2010-12-01T09:25:25.797,1934,CC BY-SA 2.5,
7581,5039,0,Ha! Thanks a lot. It seems like I didn't understand that lm() only tested the factors against the reference level.,2010-12-01T10:23:52.460,2091,CC BY-SA 2.5,
7582,5015,0,@csgillespie: All I know about splines and interpolation I learned in numeric/math lectures. Hence I may be a little biased ;).,2010-12-01T11:10:05.563,264,CC BY-SA 2.5,
7584,5049,0,"I think you made a slight mistake in the second paragraph: $|X|/|A| \approx m/n$. Otherwise, what you are doing is basically reinventing Monte Carlo integration, well, the subset version I have not encountered yet, but I'd not be surprised if it's done already.",2010-12-01T12:12:02.703,2036,CC BY-SA 2.5,
7585,432,11,Definitively my favorite cartoon about Data Mining,2010-12-01T12:24:46.090,264,CC BY-SA 2.5,
7586,5049,0,"Thanks, yes that was a mistake (in fact, there was a similar one later on also).",2010-12-01T12:26:10.457,386,CC BY-SA 2.5,
7587,4629,2,"Take a look at the sin I have already proposed about testing. If you do the test and do not reject normality, it does not mean that you have a normal sample... it only means that you cannot say the sample is not normal. sin !",2010-12-01T16:04:56.537,223,CC BY-SA 2.5,
7588,5054,0,Maybe you should ask [Johan Bollen](http://informatics.indiana.edu/jbollen/Home.html). It's his speciality.,2010-12-01T16:31:33.113,2036,CC BY-SA 2.5,
7589,5055,0,"great, I guess this is the answer ! I did not know Hansen-Hurwitz...",2010-12-01T16:43:54.273,223,CC BY-SA 2.5,
7591,5054,2,"Isn't that what AR and MA are for (in ARMA)?  You might also look up ""long memory process"", such as ARFIMA.",2010-12-01T18:32:32.113,5,CC BY-SA 2.5,
7593,4552,0,"indeed, inferring something from `A and B are correlated` usually only see `A causes B` but not `B causes A`...(and forget about `C` which causes `A` and `B`)",2010-12-01T20:13:59.237,961,CC BY-SA 2.5,
7594,4574,0,"sorry to be ignorant, but what's wrong with a Neyman-Pearson construction for the analysis of (the outcome of) scientific experiments ?",2010-12-01T20:15:43.133,961,CC BY-SA 2.5,
7595,5016,0,Have you considered calculating the convolution by using (fast) Fourier transform (and its inverse) ?,2010-12-01T20:36:58.537,961,CC BY-SA 2.5,
7596,5058,0,Might you be able to remember *where* you 'read a few days back'?,2010-12-01T20:48:02.440,449,CC BY-SA 2.5,
7598,5058,0,@onestop: Sorry. I read that in some research paper but I found a quick link on wiki `http://en.wikipedia.org/wiki/Generalized_gamma_distribution` for a generalized gamma distribution. Please feel free to correct me though.,2010-12-01T21:04:41.400,2164,CC BY-SA 2.5,
7600,5058,1,"This probability plot strongly suggests a mixture of two distributions, one located between 5 and 60 and the other between 50 and 150+.  This casts doubt on any simpler characterization, like negative binomial.",2010-12-01T21:27:36.137,919,CC BY-SA 2.5,
7601,5060,0,"A pre-post, experimental-control group design is extremely common in psychology. I agree that few people seem to be aware of the comparatively complicated and strict assumptions. Mixed models often seem to be beyond the statistical horizon.",2010-12-01T21:36:20.573,1909,CC BY-SA 2.5,
7602,5061,6,"Machine learning and pattern recognition are not the same thing, machine learning is also interested in things like regression and causal inference etc.  Pattern recognition is only one of the problems of interest in machine learning.  Most of the machine learning people I know are in computer science departments.",2010-12-02T00:04:02.240,887,CC BY-SA 2.5,
7603,5058,0,@whuber: That is interesting. Would you have any suggestions on how to proceed? Do I start looking for ways to separate the distributions?,2010-12-02T01:42:01.663,2164,CC BY-SA 2.5,
7604,5067,0,"Does anyone know how LaTeX works here?  I tried the faq here and also Google searching, but that of course leads to questions on tex.stackexchange.com/",2010-12-02T03:41:48.030,1815,CC BY-SA 2.5,
7605,5067,0,Okay I sort of got it working...but it's absurd how I have to keep doing the captchas due to my editing.  Can I get around this short of not editing?,2010-12-02T03:51:36.400,1815,CC BY-SA 2.5,
7606,5067,0,"Wow.  I am your biggest fan.  That is so awesome.  I got CVX, and moreover now have a couple hours of new homework to learn what the hell you did there.  : )",2010-12-02T04:28:01.283,2224,CC BY-SA 2.5,
7607,5071,0,"Right, you already know the bias. Would this strategy also work for a dice where one face was heavier (again, you are aware that it is)?",2010-12-02T06:08:29.177,2226,CC BY-SA 2.5,
7608,5072,0,I'm not certain what 50:50 odds really means. Does the Kelly criterion require the bets to be dependent on the outcome of the previous bet?,2010-12-02T06:11:10.250,2226,CC BY-SA 2.5,
7609,5071,0,"Yes, why would you think that it is different? you always play where the probability to win is the highest. Then you can also think about the sizing of your bet, but as you want to ""maximize our probability of winning"", that's it I guess",2010-12-02T06:20:40.297,1709,CC BY-SA 2.5,
7611,5074,0,thanks about that but dont see how i can apply it before i draw the chart first,2010-12-02T07:09:20.973,18462,CC BY-SA 2.5,
7612,5074,0,"something like that? This is simple data visualization, I suggest you read a few tutorials",2010-12-02T07:34:02.107,1709,CC BY-SA 2.5,
7614,5075,0,+1 for the use of `qplot()` but maybe it would be interesting to also show the other way (stacked) and the use of color.,2010-12-02T07:39:18.050,930,CC BY-SA 2.5,
7615,5061,2,"@Dikran Agree but ML and PR are often aliased and presented under similar topics of data analysis. My preferred book is indeed *Pattern Recognition And Machine Learning*, from Christophe M Bishop. Here is a review by John MainDonald in the JSS, http://j.mp/etg3w1.",2010-12-02T07:46:36.230,930,CC BY-SA 2.5,
7617,5076,0,"Cleveland plot, of course!",2010-12-02T07:51:15.837,1709,CC BY-SA 2.5,
7618,5076,2,"I always seem to forget about dotcharts, even though everywhere you look Tufte talks about their superior data-ink ratio... I think this suggestion combined with the reordering of the data makes for an informative and easy to digest graph. For completeness sake, to change from bars to points in `ggplot2` simply needs the  `geom = ""point""`: `qplot(x = names, y = freq, data = df, geom = ""point"") + coord_flip()`",2010-12-02T08:09:44.393,696,CC BY-SA 2.5,
7619,5063,0,"The errors I have are not that far away from the true circle, however they are shaped as a circle with greater radius. Therefore they have a big influence on the solution. The errors does not have a normal distribution.",2010-12-02T08:46:00.960,2223,CC BY-SA 2.5,
7620,5063,0,The image processing approach is really a great idea. I will definately try it. Thanks a lot!,2010-12-02T08:48:47.807,2223,CC BY-SA 2.5,
7621,5076,0,"@Chase Thanks for this (I also appreciated that you promptly update your response following my comment). Reordering would make sense iif there's no natural grouping among items (which is often the case in strutured questionnaire), but I use it too because it is very convenient to display potential ceiling/floor effects. Now, the point is that a stacked barchart rarely conveys effective information about binary items (it seems more appropriate for ordered response categories, like Likert-type items) whereas dotchart can cope with both type of items. Here, the limiting factor is the No. items.",2010-12-02T09:26:14.267,930,CC BY-SA 2.5,
7622,5056,0,"w = sum over i (ai ti xi). you will have to minimize the lagrangian to find the values of the multipliers, ai. i wonder how u got the support vectors ? the same process should also give u the value of ai's.",2010-12-02T10:15:26.647,2071,CC BY-SA 2.5,
7623,5072,0,"The Kelly criterion tells you how much to bet in order to maximize winnings without dipping to the negatives, depending on 1) payout 2) odds 3) current bankroll. In this case 50:50 payout meant that you're being paid as if the coin were fair, and you can take advantage of that.",2010-12-02T10:20:02.313,2456,CC BY-SA 2.5,
7624,5077,1,"Never heard of this algorithm before, but it sounds pretty interesting. Could you provide a link to a explanatory source ? Thank you !",2010-12-02T10:35:07.253,264,CC BY-SA 2.5,
7625,5063,0,I discovered that this is really the Hough Circle Transform and have tested it now. It works really well!,2010-12-02T10:41:48.197,2223,CC BY-SA 2.5,
7627,5077,0,@steffen Here's a link to the [original paper](http://ioe.engin.umich.edu/techrprt/pdf/TR81-01.pdf) - well actually the technical report. I've never used this procedure either.,2010-12-02T11:20:51.397,8,CC BY-SA 2.5,
7628,5082,0,"It seems that a disadvantage of this implementation is, that one cannot define a bn-architecture on his own.",2010-12-02T11:59:01.083,264,CC BY-SA 2.5,
7630,5078,0,"Thanks chl, the version number was a type (zeros were supposed to be nines), but your tip helped. plotLMER.fnc works again after the update. So, my immediate issues are solved. However, I think I'll leave the question here anyway (unless a mod thinks otherwise?) if anybody wants to discuss other ways of getting intervals outside of the plotLMER function.",2010-12-02T12:28:49.593,2228,CC BY-SA 2.5,
7631,5085,0,Thanks! I tried the example on the FAQ and it provided a nice-looking plot.,2010-12-02T12:41:29.453,2228,CC BY-SA 2.5,
7632,5063,0,@Bj√∂rn Thank you for reminding me of the name: that provides a useful way for readers to learn more about the procedure.,2010-12-02T13:37:25.007,919,CC BY-SA 2.5,
7633,5069,1,"This probability plot does not exhibit any outliers at all, but it does indicate a region of sparse probability density around 40-60.  This will not be detected by most GoF tests, which will be invalid anyway.  Distribution fitting is *exploratory*, not confirmatory, and so requires more flexibility and creativity than is afforded by parameter estimation and rote application of hypothesis tests.  If a mixture is suggested by theory--and it might be here--and if capturing that in a model might be important, then it is worthwhile considering despite the complications that ensue.",2010-12-02T13:44:55.343,919,CC BY-SA 2.5,
7634,5069,0,"(A GoF test, to be valid, must quantitatively account for how many possible distributions one has attempted to fit to the data.)",2010-12-02T13:46:43.953,919,CC BY-SA 2.5,
7635,5058,0,"Not necessarily.  It depends on why you're doing this fitting.  At this stage it seems you are *exploring* the data.  Let the data behavior guide you.  Can you find any possible cause or explanation for a mixture?  Would there be a meaningful difference between values in the 0-60 range versus values in the 40-150+ range?  In short, what can you *learn* about your social experiment from this?",2010-12-02T13:50:16.057,919,CC BY-SA 2.5,
7636,5084,0,"This is very helpful, thank you. I wonder could I ask though, I'm guessing statistical significance isn't that useful here, because one would expect there to be some differences in ranks- after all, they are not randomly drawn. So would it be best to look at Kendall's w value, do you think? With my data I get .474, which seems low-moderate when compared with other reliability co-efficients.",2010-12-02T14:03:05.060,199,CC BY-SA 2.5,
7637,5078,0,"Your question makes perfect sense to me, if we make abstraction of the R issues. Someone may have a good idea about how to derive relevant CIs in your case without resorting to any software consideration.",2010-12-02T14:08:12.813,930,CC BY-SA 2.5,
7638,5084,2,"@Chris On the contrary, statistical significance is extremely useful here: it has *exactly* the same interpretation it would have for a huge dataset.  What you might be worried about is *power*: the ability of the data to discriminate between a null hypothesis and the alternative.  But you have no control over that.  The most you can hope for is to find a significant association, which would suggest that what you see in your scatterplots is not due to chance alone.  (Student, in his original paper of 1908, simulated datasets of just *four* values to demonstrate the t-test.)",2010-12-02T14:14:42.340,919,CC BY-SA 2.5,
7639,5072,0,"Yes, sorry, I meant 1:1 odds, not 50:50 odds...I've changed the answer to reflect this",2010-12-02T14:21:36.233,2168,CC BY-SA 2.5,
7640,5072,3,"The Kelly criterion is not about Gambler's Ruin.  Simply observe that after many (N) plays, you expect to win about pN of them, multiplying your wealth by (1 + bx)^(pN), and lose about (1-p)N of them, multiplying your wealth by (1 - x)^((1-p)N).  To maximize this w.r.t. x, take logs.  Factoring out N yields the Kelly criterion.  In short, *if* you follow a system of investing a fixed proportion (x) of your wealth each time, and *arbitrarily small bets can be made,* and you can play for an indefinite time, then the Kelly Criterion gives the optimal proportion to choose.",2010-12-02T14:31:32.183,919,CC BY-SA 2.5,
7641,5072,0,"@user2168 1:1 odds and 50:50 odds are ways to express exactly the same thing; namely, 1/(1+1) = 50/(50+50) = 50% chance of winning.",2010-12-02T14:32:29.767,919,CC BY-SA 2.5,
7642,5072,0,"@whuber, x:y is usually written with no common denominator and was confusing as it was written, that's why I changed it.  Thank you for the explanation, that makes a lot of sense to me.  i.e. your winnings, $W_N$ at time $N$ will be $ W_N = (1 + b x)^{pN} (1 - p)^{(1-p)N} W_0 $ (with $W_0$ your initial savings), then just maximize.  The (easy to understand) assumption, which you've stated, is that you want to reinvest a constant proportion of your savings (assuming independence of trials).",2010-12-02T14:43:38.743,2168,CC BY-SA 2.5,
7643,5072,3,"@user2168 A subtlety is that your winnings are unlikely to be *exactly* this formula.  You really have to take the expectation.  But, as the Central Limit Theorem shows, when N is sufficiently large, the number of wins is highly likely to be within O(Sqrt(N)) of pN.  This chance of a relatively small deviation from the expected number of wins does not alter the optimal fraction.",2010-12-02T15:36:31.863,919,CC BY-SA 2.5,
7644,5088,0,"it's a start, but somehow I can't get the combined spline using these coefficients or a combination. Care to guide me on how I can go from the basis functions constructed with smoothCon to the final smoother, using only coefficient multiplications (basically doing it by hand)?",2010-12-02T16:29:55.373,1124,CC BY-SA 2.5,
7645,5088,0,"Thx for the update, but indeed, they're not the same. Actually, some basis functions are turned upside down even. Thin plate regression splines are indeed a penalized analysis, and the mixed in gamm isn't making things easier either. I'm afraid I'll have to get back to the math as well.",2010-12-02T16:50:20.177,1124,CC BY-SA 2.5,
7647,5091,0,"Thank you for your explanation of ACF, I am looking more for the justification to do it on the wavelet coefficients, and the signification of it...",2010-12-03T01:26:24.493,1709,CC BY-SA 2.5,
7649,5014,0,"@whuber - you are certainly on track with the price being right! I have thick skin and appreciate the time and thoughtful answers that others on this site provide. I hope that I can provide some useful knowledge back to this site as well. I'm quite confident my instructors did a very sufficient job at teaching these basic concepts and the issue is my inability to *apply* that information in an appropriate context. I was a little over zealous here and grasping at straws trying to fit a concept I vaguely understood to a question. As they say, when all you've got is a hammer...",2010-12-03T03:50:03.087,696,CC BY-SA 2.5,
7650,5095,0,"Thanks Paul for your detailed answer. I will up-vote it once I have enough points :) ! Features in HF is around 100, but NF is around 500. I have tried feature selection using information gain method.",2010-12-03T05:06:55.010,529,CC BY-SA 2.5,
7651,5094,0,"Mbq: thanks for sharing your thoughts. I have tried a k-fold cross-validation and the results are average of the k-runs. HF is a subset of NF, but several features are merged as  one single feature.",2010-12-03T05:11:08.767,529,CC BY-SA 2.5,
7652,5086,0,I'll try. Which R-package do you recommend ?,2010-12-03T07:31:28.643,264,CC BY-SA 2.5,
7653,4552,12,google makes $65B a year not caring about the difference...,2010-12-03T07:41:55.810,74,CC BY-SA 2.5,
7654,4991,0,Weird that my bounty doesn't help... Is my question badly formulated.... No one has a piece of answer? Or a question on my question?,2010-12-03T07:52:32.200,1709,CC BY-SA 2.5,
7655,4552,7,I agree with your points and they all are valid. But does Google's profit imply: correlation => causation?,2010-12-03T07:53:58.967,1307,CC BY-SA 2.5,
7656,5095,1,@Khader RF importance is more reliable than IG.,2010-12-03T09:25:47.407,,CC BY-SA 2.5,user88
7657,5095,0,@Khader And a moderator tip: you can (and should) accept the best (in your view) answer by clicking the tick mark below voting arrows. This will give you 2 points and 15 extra points to the answerer. The accepted answer will be sticked to the top so that each new reader of a question would notice it first.,2010-12-03T09:45:54.800,,CC BY-SA 2.5,user88
7658,5088,1,"In relation to Joris' 2nd comment above, my Update 2, shows that the difference is due to `um` in Joris' code not including the identifiability constraints in the basis functions. If you include these constraints into the basis function, `smoothCon()` returns the same thing as `Xp`",2010-12-03T11:08:57.223,1390,CC BY-SA 2.5,
7659,823,0,"-1, see what @walkytalky and @L√®se majest√© said.",2010-12-03T12:00:52.483,1979,CC BY-SA 2.5,
7661,5100,0,"What do you expect? If you want a typical Brownian motion, just generate one. If you want the mean behaviour, generate a lot and take the mean, like you did. Maybe you can add the variance as extra information. An idea would be to combine the three. Plot one realization against the mean and variances of many realization. You'll get 4 curves: the one from 1 realization, the mean curve, the mean+standard deviation and the mean-standard deviation.",2010-12-03T13:24:20.927,2036,CC BY-SA 2.5,
7662,5102,0,"I will look into the reference range. Best means approaching the values distribution as bes as possible. What I get are flat and boring curves, almost straight lines. The expectation is to get more wild curves with expononential growth/decline, sine wave like or something in between. Some amount of random noise has also to be thrown in for good measure.",2010-12-03T13:54:54.483,2218,CC BY-SA 2.5,
7663,5100,0,"The process could indeed have in theory some component of Brownian motion. The idea of the 4 curves sounds good to me. Although it seems that then you have to assume a normal distribution, which is certainly not the case. I was also thinking of using geometric mean, harmonic mean, median absolute deviation or something more exotic?",2010-12-03T13:59:53.747,2218,CC BY-SA 2.5,
7664,2269,0,"@Andy W., with all due respect, your immediate dismissal of this question is what I think is inappropriate. Numerous questions on this forum have to do with specific R package issues and no one closes them. SAS is used widely in business settings (much much more than R; I use both, and Python) and it seems useful to allow software related questions, given that software and analysis are so integral these days. If we are going to have such obvious double standards then the forum should be renamed ""Statistical Analysis Using Open Source Software""",2010-12-03T14:05:48.660,1080,CC BY-SA 2.5,
7665,5100,0,"Sure, when you said random walk, I assumed Brownian motion because the term is sometimes used synonymously, although strictly speaking, you are right random can be refer to any type of randomness a priori. You could still use moving averages and variances over a certain time window if the process is more exotic. But you can avoid that by transforming to the proper scale sometimes. Say, you have a geometrical Brownian motion, just plot the log of it.",2010-12-03T14:26:51.450,2036,CC BY-SA 2.5,
7666,5103,0,"These plots look really spectacular. Looking at the first plot, I realize that what I want is to show the most probable paths, because there are certain paths that are just too improbable. The problem is how choose the most probable paths in an elegant manner and summarize them. Eventually I want to display a reference ""real"" curve of the measured observed data, next to the simulated ones.",2010-12-03T14:44:49.840,2218,CC BY-SA 2.5,
7667,2269,0,"@Josh Hemann, this topic was discussed on meta  in the question Shane and Srikant linked to. If you have comments it would be useful to bring them up there. This question was re-opened so if you have an answer feel free to contribute.",2010-12-03T15:10:19.550,1036,CC BY-SA 2.5,
7668,5103,0,Perhabs you could make a histogramm after the 100 steps?,2010-12-03T15:52:01.990,1050,CC BY-SA 2.5,
7669,5095,0,@mbq: Thanks for your tip. I will wait for a day or two before chosing the final answer.,2010-12-03T16:14:32.990,529,CC BY-SA 2.5,
7670,5099,0,Thanks for the insightful answer. I will try with SD of accuracy for further analysis.,2010-12-03T16:16:43.863,529,CC BY-SA 2.5,
7671,5103,0,"@Navi: by definition, the most probably path is a horizontal line around zero. These random walks are just cumulative sums of Gaussian noise with mean 0 and unit variance",2010-12-03T17:13:01.100,1390,CC BY-SA 2.5,
7673,2623,0,"If the time series is not stationary, often the 1st difference of the series will be stationary (for example, financial time series).",2010-12-03T22:18:19.247,1146,CC BY-SA 2.5,
7675,813,18,"I don't know, you've seen many efficient citizens lately?",2010-12-03T23:26:37.490,2036,CC BY-SA 2.5,
7676,878,13,"Besides, Tzippy is misquoting Sagan, since Sagan never believed that. He in fact listed it among the [fallacies in his baloney detecion kit](http://en.wikiquote.org/wiki/Carl_Sagan#The_Demon-Haunted_World:_Science_as_a_Candle_in_the_Dark_.281995.29).",2010-12-03T23:31:58.870,2036,CC BY-SA 2.5,
7678,5113,0,"Thanks! I agree Simulated Annealing might better handling such situation, but this is an assignment that requires to use Metropolitan-Hastings algorithms. So I am afraid I have not other choice.",2010-12-04T00:05:07.413,1005,CC BY-SA 2.5,
7679,5112,0,"Thanks! I agree Simulated Annealing might better handling such situation, but this is an assignment that requires to use Metropolitan-Hastings algorithms. So I am afraid I have not other choice.",2010-12-04T00:05:30.033,1005,CC BY-SA 2.5,
7680,5113,1,"@Tim As I wrote, MH is only a way of accepting steps -- SA is also MH. Yet you can hold the temperature constant to make it just 'plain MH'.",2010-12-04T00:08:26.740,,CC BY-SA 2.5,user88
7681,5113,2,Simulated annealing is an adaption of the Metropolis-Hastings algorithm (as mentioned in the Wikipedia link).  You could always ask whoever set the assignment whether SA was what he/she had in mind.,2010-12-04T00:08:33.837,887,CC BY-SA 2.5,
7682,5115,1,Converted to community wiki.,2010-12-04T00:14:08.750,,CC BY-SA 2.5,user88
7683,5115,0,what is community wiki?,2010-12-04T00:53:59.723,1808,CC BY-SA 2.5,
7685,5120,0,"Thank you for your reply.
In the original paper, one of the parameters that is estimated is $\sigma_{a}$, the variance of the technology shock. Unfortunately, in Dynare, while you are able to estimate transforms of parameters, you can't do so for the standard error of shocks, such as the technology shock in the model equation. Instead, you must assign a prior to the standard error of the shock directly. Given that the prior of the variance is inverse gamma I assumed that a suitable prior for the stderr would be the ""square root"" of this invgamma distribution. Does that make sense?",2010-12-04T03:10:24.333,2251,CC BY-SA 2.5,
7687,5115,0,@Mariana: http://www.sharepointoverflow.com/questions/432/what-is-community-wiki,2010-12-04T04:32:47.227,1118,CC BY-SA 2.5,
7688,5117,4,It's worth noting that Fisher is equally famous for his work as a biologist (evolutionary biology and agricultural science) as he is for his statistical work.,2010-12-04T06:49:04.617,1679,CC BY-SA 2.5,
7689,5117,14,"*Even today, I occasionally meet geneticists who ask me whether it is true that the great geneticist R. A. Fisher was also an important statistician*
-- Leonard Savage 
(Annals of Statistics, 1976 http://www.jstor.org/stable/2958221).",2010-12-04T08:06:48.953,449,CC BY-SA 2.5,
7691,5098,0,"Right, power analysis. I think that perhaps I do indeed care about relative frequency, however. I'll try to read around that also.

Without a clearly defined number of tests to run, I have been running 2% of the tests, selected uniformly randomly, on each of the datasets. 2% is arbitrary, but also tractable on the larger datasets. It means my sample size increases with respect to the population of tests on a dataset, which may lead to more tests than I _need_ on the larger datasets...",2010-12-04T10:54:26.267,2246,CC BY-SA 2.5,
7692,5136,1,"That seems rather a general question - would you like to take a look at http://en.wikipedia.org/wiki/Latent_variable and see if that helps, or if not, which bits you don't follow?",2010-12-04T12:17:29.563,449,CC BY-SA 2.5,
7693,5135,0,"residuals are not so badly deviating from normality, why do you think so?",2010-12-04T13:14:18.493,582,CC BY-SA 2.5,
7694,5135,0,@nico: I think @Alexx Hardt was speaking hypothetically. I.e. once *could* use the five number summary to see if residuals were deviating from normal,2010-12-04T13:39:36.373,1390,CC BY-SA 2.5,
7695,5138,0,"This will take some time and playing around with R to understand. Big thanks for now, I might follow up with some questions at some point :)",2010-12-04T13:42:01.740,2091,CC BY-SA 2.5,
7696,5138,6,@Gavin (+1) Great response with nice illustrations!,2010-12-04T14:04:02.000,930,CC BY-SA 2.5,
7698,5135,0,"@Gavin Simpson: you're right, I misread the sentence. Disregard my previous comment.",2010-12-04T14:34:14.040,582,CC BY-SA 2.5,
7699,5138,2,"Nice job. One thing you might clarifiy, with regard to calculating t values: sqrt(diag(vcov(mod))) produces the SE of the estimates. These are the same SEs that are output in the model summary. Easier and clearer just to say that t = Estimate/SEestimate.  In that sense it is no different that any other t value.",2010-12-04T14:49:48.403,485,CC BY-SA 2.5,
7700,5138,2,(+1) This is great.  The only thing I'd add is that the $F$ value is the same as $t^2$ for the slope (which is why the p values are the same).  This - of course - isn't true with multiple explanatory variables.,2010-12-04T15:05:39.607,,CC BY-SA 2.5,user1108
7702,5138,2,@Jay; thanks. I thought about mentioning that equivalence too. Wasn't sure if it was too much detail or not? I'll ad something on this in a mo.,2010-12-04T15:43:08.297,1390,CC BY-SA 2.5,
7703,5136,0,"Can you fix this question? It gets answers because of the topic, but the content seems a pure word clutter to me.",2010-12-04T15:48:47.357,,CC BY-SA 2.5,user88
7704,5138,1,@Brett; thanks. I've tried to clarify this a bit above as per your comment.,2010-12-04T15:54:07.770,1390,CC BY-SA 2.5,
7705,5115,3,@Mariana The idea is that pools and list-ofs are converted to a form in which they can be easily managed (due to lower rep req to edit) and voted up/down without hurting participants' reputation (votes on CW posts does not give/take reputation).,2010-12-04T16:01:50.720,,CC BY-SA 2.5,user88
7706,5136,0,i fix questions with examples...,2010-12-04T18:54:23.407,1154,CC BY-SA 2.5,
7707,5139,0,Thanks i see what you mean... But my goal is to have statistics rules. The complexity you talk about can be seen in econometrics problems which use classical approach.,2010-12-04T18:56:29.413,1154,CC BY-SA 2.5,
7708,5144,0,"It sounds like it is indeed the topic of design of experiments. However, your description lacks some information; e.g., what's the sample size, are all variables crossed, is this a repeated-measures design (i.e., data are collected on the same statistical units); how many levels have your variables, and most importantly, are you interested in a predictive or an explicative model (from what you described, your use of the term 'predict' is not obvious)?",2010-12-04T19:58:04.760,930,CC BY-SA 2.5,
7709,5047,0,Do you know any good Matlab packages that can get the job done?,2010-12-04T20:00:35.780,1224,CC BY-SA 2.5,
7710,5044,0,Do you know any good Matlab packages that can get the job done?,2010-12-04T20:01:15.590,1224,CC BY-SA 2.5,
7711,5144,0,"@chl I'm honestly a bit lost with this, but I've updated the question with an example of what I'm trying to achieve. If that's not enough I'll try to expand it a bit more.",2010-12-04T20:15:42.677,2261,CC BY-SA 2.5,
7712,5144,0,"So, do you already assume a linear relationship between your variables (like the function you cooked up for $t$) or is that also part of what you want to determine?",2010-12-04T20:43:22.003,2036,CC BY-SA 2.5,
7714,5144,0,That is also part of what I want to determine. However I don't necessarily need extremely precise results - a linear approximation would probably be okay...,2010-12-04T20:48:10.030,2261,CC BY-SA 2.5,
7715,5146,0,"Thanks for the detailed answer. I've heard that ANOVA's aren't practical for more then 3 independent variables. Is this true? If so, why?",2010-12-04T21:26:21.123,2261,CC BY-SA 2.5,
7716,5146,2,"@honitom It really depends on the context of your experiment and your sample size. Keep in mind that the reliability of your estimates (the regression coefficients summarizing the effect of your factors on your outcome) depends on the available sample size and associated variance. In DoE, interaction terms can be aliased with main effects, depending on the sample size; still we can estimate the main effects. Any decent textbook should cover this topic much well than me in a comment, so I'd suggest to have a look at ANOVAs and DoE.",2010-12-04T21:34:05.670,930,CC BY-SA 2.5,
7717,4991,0,"The way it's posed, this seems like a degenerate problem -- the errors could equally be attributed to the observation noise or the process noise. Are there more constraints? Is the state one dimensional?",2010-12-04T21:48:28.393,2077,CC BY-SA 2.5,
7718,5148,0,"If (1) is mentioned in the lecture notes, it suggests that simulated annealing is the answer to your previous question on minimization of a function by Metropolis-Hastings algorithm as (1) would only be the mode for simulated annealing.",2010-12-04T23:30:01.730,887,CC BY-SA 2.5,
7719,5148,0,"The purpose of only considering the tail subsequence of the original samples is because there is time needed to wait for the instrumental markov chain to have its distribution converge to its stationary distribution, whose density has been designed to be the target function.",2010-12-04T23:32:34.820,1005,CC BY-SA 2.5,
7720,5148,0,"In the class note, the first choice was not just mentioned for simulated annealing, but also for Metropolitan-Hastings algorithms.",2010-12-04T23:33:44.523,1005,CC BY-SA 2.5,
7721,5148,2,"After the chain has reached its stationary distribution, it will still be sampling from that distribution, so all you can say about the last sample is that it will be a sample from that distribution.  Of course the mode is the most probable value, but that doesn't even imply that the last sample is probably the mode!",2010-12-04T23:39:18.073,887,CC BY-SA 2.5,
7722,5148,0,"Thanks! If I understand correctly, your opinion is to evaluate the function on every sample after the MC has been seen as reaching its stationary distribution.",2010-12-04T23:44:56.937,1005,CC BY-SA 2.5,
7723,5148,0,"I would have thought that any sample, even before the burn in period is complete would be the mode (as it is only the maximum of f(x) that you seek).  However I could be wrong, but samples taken after reaching the stationary distribution would be fine, provided you had a sufficient number to be confident of adequately exploring the distribution. I suggest you speak to your instructor about the last sample issue as it seems there is some uncertainty about it.",2010-12-04T23:52:16.877,887,CC BY-SA 2.5,
7724,5148,0,Thanks! I think I will ask him. I now see your point to consider the samples before reaching stationary distribution.,2010-12-05T00:05:38.767,1005,CC BY-SA 2.5,
7725,5148,0,"@Tim Please, Metropolis; this has nothing to do with cities, guy had just strange surname. Back to the topic -- if you degenerate MH to simple accept-only-if-better (very small T), with some luck at the end you'll get $x$ just sitting in the global minimum.",2010-12-05T00:12:09.903,,CC BY-SA 2.5,user88
7726,5145,6,"Maybe not the most famous statistician, but when you put it like that, definitely the most important! ;o)",2010-12-05T00:13:14.643,887,CC BY-SA 2.5,
7727,5117,2,"@Michael Lew: He certainly is. He managed to get over the supposed disconnection between Mendelian genetics and Darwinian evolution, among other accomplishments. http://digital.library.adelaide.edu.au/coll/special//fisher/9.pdf",2010-12-05T00:18:50.350,1118,CC BY-SA 2.5,
7728,5148,0,"@mbq: my mistake. By ""degenerate MH to simple accept-only-if-better"", do you mean at each iteration evaluate the function immediately when a new sample is generated and compare to the previous best? Also why choose ""very small T"", I assume T is fixed at some value, and if T is small, chance of getting stuck in a local minimum will be great, is it?",2010-12-05T00:34:01.140,1005,CC BY-SA 2.5,
7729,5148,2,"@Tim Yes -- this is sadly quite common. About getting stuck -- there is one more issue. Here we talk nothing about the next step generation, yet obviously making big steps can be a way to escape local minima (tunneling in physical termin.). Those are of course pathologies, but sometimes people use them ""because they work"".",2010-12-05T00:44:56.017,,CC BY-SA 2.5,user88
7731,5152,0,How old are the subjects at the start of the study?,2010-12-05T10:48:03.980,449,CC BY-SA 2.5,
7733,5162,0,"Shouldn't ""optimality"" take into account the cost of false positives versus false negatives? I can imagine a scenario where a false negative is far more costly than a false positive, which should call for a criterion biased towards capturing as much as the true distribution as possible. Also, I believe you mean ""sensitivity"" and not ""sensibility"" in your answer.",2010-12-05T13:22:29.230,364,CC BY-SA 2.5,
7734,5162,0,"@Mike Yes, your comment is really welcome and this was what I tried to point to in my last sentence (optimize FN/FP rates depending on the context). Thanks for noticing the typo (which was already addressed in a preceding post...)!",2010-12-05T13:31:45.803,930,CC BY-SA 2.5,
7735,4991,0,@lanS. All objects have indeed here only one dimension. Can you develop a bit more on the fact that the errors that can be either observation or noise. It is exactly what I would like to achieve. I would like to get a rolling estimation of the signal to noise ratio by estimating the sd of the 2 time varying noises....,2010-12-05T15:34:36.663,1709,CC BY-SA 2.5,
7736,4991,0,Maybe should I start by fixing the sd of the process noise for a start and see how the sd of the observation noise reacts?,2010-12-05T15:36:28.847,1709,CC BY-SA 2.5,
7737,3090,0,We have also a dedicated Wiki page at http://grass.osgeo.org/wiki/R_statistics,2010-12-05T16:47:39.887,2267,CC BY-SA 2.5,
7739,5162,0,"So just to clarify, I have my data in the range of [0.6,1.2], then I calculate the corresponding ROC curve for a bunch of thresholds in that range. I find the cut-off points in each of those ROC curves and I try to pin-point the one that has the maximum Sensitivity/Specificity combination. Is that correct or have I completely misinterpreted you?",2010-12-05T19:48:32.487,1224,CC BY-SA 2.5,
7740,5162,0,"@rohanbk Yes, that's basically the idea; e.g., compute Se/Sp and the four cells of your two-way table (TP, FN, etc.) in the range [0.6,1.2] (say with an increment of .05 or less -- it depends on the precision you can/want to attain in the future). Choose the compromise Se/(1-Sp) that best reflects your classification objective. Some references were provided to you in earlier threads. I don't really know if there are good MATLAB functions for ROC analysis, but I just found this page, http://j.mp/ekzg3m. HTH",2010-12-05T20:23:00.687,930,CC BY-SA 2.5,
7741,5163,0,Am I correct in assuming that when you talked of a parameter in place of one's location on a given latent trait you are indeed saying that subjects abilities are modeled without assuming any prior distribution (as is done in the marginal likelihood approach for estimating IRT model from the Rasch family)?,2010-12-05T20:27:49.200,930,CC BY-SA 2.5,
7742,5158,15,You don't correct the mean!,2010-12-05T21:42:41.210,919,CC BY-SA 2.5,
7743,5172,0,do you know the sample sizes... the range of them?  Small sample sizes wreak havoc on the mean... moreso than the median.,2010-12-06T00:40:56.477,601,CC BY-SA 2.5,
7744,5168,0,"Thank you. Actually I just needed pointers for the R package. I appreciate your helpfulness! By the way, the book is awesome.",2010-12-06T01:09:55.757,1307,CC BY-SA 2.5,
7745,5172,0,the sample sizes range from 1-100 million and the population size is upwards to 1 billion. But I have no knowledge of how disjoint the samples are from the total population.,2010-12-06T03:33:15.570,2271,CC BY-SA 2.5,
7746,5178,0,"ok, what I'm really interested is if it's statistically sound to include the median row in the summary table at http://en.wikipedia.org/wiki/Usage_share_of_web_browsers I can go into more details if you want.",2010-12-06T04:52:09.327,2271,CC BY-SA 2.5,
7747,5171,0,"Is each row the count of a different structure for the same conversation, or the count of the same structure for different periods of time?",2010-12-06T05:07:13.743,1709,CC BY-SA 2.5,
7748,5077,1,What is the advantage of the hit and run MCMC over other methods? Speed of convergence?,2010-12-06T05:14:53.223,1709,CC BY-SA 2.5,
7749,5178,0,"In that case you should be okay. You can consider each of the samples A,B,C to be experimental units, and each of the categories a,b,c to be some measurement taken on each sample (with the restriction the measurements sum to 1.) This could be helpful when you have a lot of samples, not so much when you have 3 or 5.",2010-12-06T05:40:02.133,2144,CC BY-SA 2.5,
7750,5168,0,"However, I will wait for someone to point me to an R package or tutorial that shows how to do HDP based modeling in R.",2010-12-06T05:49:23.457,1307,CC BY-SA 2.5,
7751,5044,0,"@rohanbk Sorry, no. It is a long time since I have used matlab.",2010-12-06T07:42:58.960,264,CC BY-SA 2.5,
7752,5168,0,"@suncoolsu I never tried those packages; so my contribution is just some googling (because I'm interested in using HDP in the future, but have no time for the moment -- this is the reason why I made my answer CW). I guess someone more used to those methods will provide insightful remarks.",2010-12-06T07:48:01.620,930,CC BY-SA 2.5,
7753,5169,0,"(+1) I like this answer. It took some time to me, but it seems I have been missing something in the question (*200 random samples* of size 25 each).",2010-12-06T07:57:24.543,930,CC BY-SA 2.5,
7754,794,0,I love this one. It is great !,2010-12-06T09:00:59.907,264,CC BY-SA 2.5,
7757,5135,9,"Minor quibble: You cannot say anything about normality or non-normality based on those 5 quantiles alone. All you can say based on that summary is whether the estimated residuals are approximately symmetric around zero.  You could divide the reported quantiles by the estimated residual standard error and compare these values to the respective quantiles of the N(0,1), but looking at a QQ-plot probably makes more sense.",2010-12-06T09:29:00.867,1979,CC BY-SA 2.5,
7758,2031,9,"... and yet it is commonly done.  One thing to point out, and yes this is a little pedantic, if you are using a single Likert-type item that isn't a Likert scale.  The difference is meaningful (though the question asker is talking about a Likert item and the ordinality is an issue). A Likert scale is a consequence of summing or averaging several Likert items.  This approach was developed specifically to offset the extent to which the ordinal data was actually ordinal and make it more reasonable to be treated as being on the interval scale.",2010-12-06T09:31:45.500,196,CC BY-SA 2.5,
7759,5179,1,"Both assume continuous variables, so that the probability of ties within a sample is $0$. With the given data, there will be a lot of ties.",2010-12-06T10:37:56.110,1909,CC BY-SA 2.5,
7761,5167,0,1) Do you mean Wishart or Inverse Wishart?,2010-12-06T11:07:30.470,495,CC BY-SA 2.5,
7762,5167,0,"2) What do you mean by ""maximum occurrence""? Are you trying to find the mode (the point of highest density)?",2010-12-06T11:10:14.240,495,CC BY-SA 2.5,
7764,5161,0,**This response seems essentially wrong** as I forgot about the sampling issue.,2010-12-06T12:15:32.990,930,CC BY-SA 2.5,
7766,5181,0,"About using priors, I would rather suggest to look if there are some data available for country with small $n$ at the population level (e.g., through census or national surveys).",2010-12-06T12:41:56.800,930,CC BY-SA 2.5,
7767,5163,0,"@chl Yes and no.  Yes, although they probably do come from some population we have chosen to ignore that in the model. If we make a 'prior' assumption about the population then we can ask for a posterior over the individual's location but not otherwise.  It's the panel model fixed effects/random effects distinction all over again.  But no, it's not like Marginal Maximum Likelihood (as in Bock and Aitkin) for IRT models.  This *does* assume a prior; that's what the quadrature machinery is busy integrating over.  *Joint* ML is all parameters, but is inconsistent as you add subjects (Anderson 70)",2010-12-06T14:01:47.573,1739,CC BY-SA 2.5,
7768,5176,0,"Thanks Rob.  2 quick things though.  I converted by zoo object to ts using as.ts.  That didn't seem to do the trick as I got the same error.  I then backed all the way up created a new object from the raw CSV, converted to ts using ts(object).  Functions ets and auto.arima both found model, but I got an error when trying to plot the fitted auto.arima object (Error in xy.coords(x, y, xlabel, ylabel, log) : 
  'x' and 'y' lengths differ).  Thanks again for your help!",2010-12-06T14:08:25.780,569,CC BY-SA 2.5,
7769,5163,0,"(+1) Thanks for clarifying that point. I was unsure about (a) the use of 'parameters' because we often speak of person (resp. item) parameters when estimating IRT models (I don't speak of person-specific covariates, just locations on the latent trait), and (b) if there was any suggestion about differences between the marginal approach (where we assume that the $\theta_i$'s are random, with $\mathcal{N}(0;1)$ for prior distribution) and the conditional approach (where subject and item parameters are estimated separately, in line with Rasch's *specific objectivity*).",2010-12-06T14:16:26.530,930,CC BY-SA 2.5,
7770,5176,0,"UPDATE.  Sorry, for the multiple comments. From the comment above, I didn't add the frequency portion, but that seemed to work for the auto.arima (and subsequently generated a great looking forecast) using the plot as found in your help.  You can probably tell I am new to R, but can you tell me why you add 4 in the start for April and not 5?",2010-12-06T14:33:22.363,569,CC BY-SA 2.5,
7771,5184,0,"Never heard of Underwood's textbook, but this article seems to discuss the pros and cons of pooling: [Pragmatics of pooling in ANOVA tables](http://www.jstor.org/pss/2684424) (Hines, Am. Stat. 1996). Now, I seem to remember that Sokal & Rohlf (1995) also recommend to consider very conservative values ($p\geq .25$); I need to check before posting an answer, unless better references come up.",2010-12-06T14:35:35.517,930,CC BY-SA 2.5,
7773,5186,1,"Before the experiment, it was believed that the factor would be significant. However, it was swamped by the other two effects. I removed the factor because keeping it in didn't change the conclusions and only made the explanation harder.",2010-12-06T15:03:05.850,8,CC BY-SA 2.5,
7774,5184,4,"Just a comment.  A guideline based on $p \geq something$ smells like a misuse of a $p$-value, in that a non-significant $p$ value is not a measure of non-evidence. Since $p$-values are uniformly distributed under the null hypothesis, why not just flip a (biased) coin? The end result is the same, and at least it's honest about being dopey.  (OK, dopey is a bit strong, but you get the idea.)",2010-12-06T15:18:38.883,,CC BY-SA 2.5,user1108
7775,5184,7,"That would be an interesting response to a referee: ""We thank the referee for their comments, but think they're a bit dopey"" ;) Good comment though.",2010-12-06T15:28:27.310,8,CC BY-SA 2.5,
7776,5185,4,"+1 Nice answer: clear, to the point, insightful, and authoritative.",2010-12-06T15:33:17.447,919,CC BY-SA 2.5,
7778,5156,0,"The Pr(X > 3) you provide is different than the question askers 0.0015, you might want to revise it.",2010-12-06T16:33:50.740,196,CC BY-SA 2.5,
7779,5186,2,"Hmm, in that case i think i'd keep it in. I can't see why it makes the explanation harder, and as you've discovered it may be harder to explain why you dropped it than why you kept it in!",2010-12-06T17:12:14.180,449,CC BY-SA 2.5,
7780,5187,0,You mean something like http://www.people.fas.harvard.edu/~plam/teaching/methods/convergence/convergence_print.pdf ?,2010-12-06T17:32:58.093,919,CC BY-SA 2.5,
7781,5188,0,"Thanks! That is just what I need.  I have ""Bayesian Data Analysis"" and I will take a closer look for the worked examples.",2010-12-06T18:29:52.187,,CC BY-SA 2.5,Will
7782,5187,0,"I looked through that exact link last night actually, which was what prompted me to convert my bugs output to CODA, but from what I can see, they aren't calling WinBUGS, they are using a built in function in R's MCMC package.",2010-12-06T18:43:23.420,,CC BY-SA 2.5,Will
7783,5077,0,"@fRed: That's why I offered the bounty. I've read about Hit and run, but don't know under what circumstances it would be useful.",2010-12-06T18:46:49.900,8,CC BY-SA 2.5,
7784,5188,0,(+1) E.g. [Example of computation in R and Bugs](http://www.stat.columbia.edu/~gelman/bugsR/software.pdf) (Online Appendix C of BDA).,2010-12-06T18:52:13.237,930,CC BY-SA 2.5,
7785,5189,0,"You are familiar with the tutorial at http://www.stat.pitt.edu/stoffer/tsa2/R_time_series_quick_fix.htm? Admittedly, it accompanies another package / book, but there may be some potential for carry over.",2010-12-06T19:44:01.410,1909,CC BY-SA 2.5,
7786,5007,1,"Everybody here seems very knowledgeable but I know for ggplot related questions I find their google group to be incredibly helpful.


http://groups.google.com/group/ggplot2",2010-12-06T19:48:47.210,1028,CC BY-SA 2.5,
7788,5176,0,"Weekly data is trouble anyhow, in my experience. Monthly data can have length-of-month issues and trading-day issues, but at least there are always 12 months in the year. And there are quite a few tools that aren't set up to handle weekly data, so you narrow your options considerably. I switched to monthly and saved myself a lot of grief.",2010-12-06T20:20:08.557,1764,CC BY-SA 2.5,
7789,2209,7,"Like Sivia, this is very nice if you have some physics background and can be rough if not.  Not a good guide to any kind of applied social statistics (for that use Gelman and Hill, or Gelman et al. above) but really *great* for prompting you to really think about the core issues.",2010-12-06T21:12:25.640,1739,CC BY-SA 2.5,
7791,5080,4,"Beautiful, exactly what I was looking for.

The two lines:

w <- colSums(coef(svp)[[1]] * x[unlist(alphaindex(svp)),])
b <- b(svp)

were a godsend.  Thank you!",2010-12-06T21:54:00.193,2221,CC BY-SA 2.5,
7792,5191,0,Could you supply a reference for this definition of order of a kernel?,2010-12-06T22:01:57.360,919,CC BY-SA 2.5,
7793,5176,0,"You need to drop the frequency component in the zoo object. So as.ts() won't work. There is no plot method for Arima objects. For the start argument, Jan 1 is 2009, so Jan 2 is 2009+1/365, etc.",2010-12-06T22:08:27.793,159,CC BY-SA 2.5,
7794,5191,0,"for example Tsybakov  ""Introduction to nonparametric estimation "", definition 1.3",2010-12-06T22:15:51.587,2189,CC BY-SA 2.5,
7795,5189,0,"Thanks for the help.  I did see this and I think it is fantastic.  I did edit my question to just focus on how to read in a univariate time series.  Mainly, if I have the dates for my daily data, how can I construct a univariate time series in R dynamically?  I have tried a few methods but they seem to include the date as another variable when I would only like to use it as a reference for start/end dates.  Sorry, new to R and I am just trying to wrap my head around this.",2010-12-06T22:16:03.373,569,CC BY-SA 2.5,
7796,5191,0,http://books.google.com/books?id=mwB8rUBsbqoC&printsec=frontcover&dq=tsybakov+++book+introduction+to+nonparametric+estimation&source=bl&ots=Z4wHSaMrEH&sig=3VC9bW3JoOYOhwvZpVYiPgr4nmk&hl=en&ei=61_9TI7eK8-28QPPyojPCw&sa=X&oi=book_result&ct=result&resnum=1&ved=0CBcQ6AEwAA#v=onepage&q&f=false,2010-12-06T22:16:21.557,2189,CC BY-SA 2.5,
7797,5177,1,"auto.arima is in the forecast package, isn't it?",2010-12-06T22:19:06.690,1764,CC BY-SA 2.5,
7798,5117,6,"If you want to ask a tough question, ask who the second most famous statistician is. There's no doubt that Fisher is # 1. You may not like the man or some of his ideas, but he is undoubtedly the creator of Statistics as we know it today.",2010-12-06T22:41:13.103,666,CC BY-SA 2.5,
7799,5178,0,"It's amusing that that site calculates the median (and goes to the trouble of labeling it as such and suggesting that they know the difference between the mean and the median) on just two points of data which results, actually, in the mean of the two values!",2010-12-06T23:33:47.537,485,CC BY-SA 2.5,
7800,5177,0,@Wayne. Yes. Sorry I omitted that information.,2010-12-06T23:36:56.750,159,CC BY-SA 2.5,
7801,5169,0,"@chl Thank you. This one's open to some interpretation. In light of a related question, the OP might be fishing for the use of a finite population correction term, which I didn't mention at all (http://stats.stackexchange.com/q/5158/919). What interests me most is the effect of rounding; everything else is routine. In some cases (not this one!) rounding has a pronounced effect (both as a bias and a variance inflator), so it's nice to be able to present a simple example of how one can evaluate that effect. Actually I'm amazed the effect is so small in this case.",2010-12-07T00:08:31.050,919,CC BY-SA 2.5,
7802,5195,2,"I've got a few ideas about how to do and implement this, but would appreciate having some data to play with. Any ideas on that?",2010-12-07T01:53:05.997,696,CC BY-SA 2.5,
7803,5195,1,"Yes, ggplot can easily draw a plot that is made up of points and lines ;) geom_smooth will get you 95% of the way - if you want more advice you'll need to provide more details.",2010-12-07T02:05:40.573,46,CC BY-SA 2.5,
7804,5171,0,"Each row is the total number of usages of the same structure by two different people talking to each other. So, for instance, if I were counting passive sentences, then the number in column A would be the number of passive sentences used by speaker A, and the number in column B the number of passive sentences used by speaker B. So each row is a different conversation. Each conversation has exactly two participants.",2010-12-07T04:13:09.730,52,CC BY-SA 2.5,
7805,5171,0,"I also have the same data calculated relative to number of sentences spoken by each speaker in each conversation, if that makes a difference.",2010-12-07T04:15:42.287,52,CC BY-SA 2.5,
7806,5179,0,"I did try this. But with so many 0s I am not sure what to make of it. I tried excluding any conversations in which either speaker used none of these constructions (i.e., the data point would fall along either axis), but the resulting Spearman correlation was not significant (and the coefficient was very small)..",2010-12-07T04:20:25.560,52,CC BY-SA 2.5,
7807,5167,0,"Sorry for being vague, I have modified the question.",2010-12-07T04:26:49.983,2157,CC BY-SA 2.5,
7808,5180,0,"You got it correctly, it is very easy to sample a variance from the distribution (by taking out the mode value). But I am not sure how to do that in multivariate case. I am using an Inverse Wishart distribution for sampling Covariance matrix.",2010-12-07T04:31:15.410,2157,CC BY-SA 2.5,
7809,5197,2,"I may be completely wrong here, but all the example outputs seems like simple counting operations (additionally the word ""assumptions"" confuses me). Do you want to extract statistical measures or do you want to e.g. extract the combination of variable values which ""cause"" certain injuries and hence allow prediction?",2010-12-07T07:55:48.057,264,CC BY-SA 2.5,
7811,5181,0,"chl: Actually I'm not *really* studying data on people's height, I just took that as a simplified example. My actual data has much more dimensions.",2010-12-07T10:12:21.137,1737,CC BY-SA 2.5,
7812,5186,1,"I take your point, although I don't 100% agree with it. I could easily see another referee suggesting that you should remove the factor (that's what the bio-statisticians recommend that I've spoken too). As you mentioned, when it's a grey area an arbitrary rule is not the way to go. If we wanted to mislead we would never mention that the other factor was ever involved! Completely unethical, but I suspect it happens.",2010-12-07T10:49:42.930,8,CC BY-SA 2.5,
7813,5207,0,"In the [Singh-Maddala distribution](http://en.wikipedia.org/wiki/Singh-Maddala_distribution) link, the pdf has two parameters - {c, k}, yet the R function ``dsinmad`` takes three parameters or am I missing something.",2010-12-07T11:10:11.260,8,CC BY-SA 2.5,
7815,5207,0,"Sorry, the wikipedia link cites the wrong formula, it looked ok at first glance, when I was composing the question. I did not find a ready link, so I just put the formula in the question.",2010-12-07T13:05:25.380,2116,CC BY-SA 2.5,
7818,5212,2,"Another reference for the comparison of binomial CIs is Brown LD, Cai TT, & DasGupta, A. (2001). Interval Estimation for a Binomial Proportion. Statistical Science, 16(2), 101-133. http://projecteuclid.org/euclid.ss/1009213286 (open access). R's `binom` package also has the Agresti-Coull CI.",2010-12-07T15:48:48.367,1909,CC BY-SA 2.5,
7819,5203,0,Thank you very much. I picked up Durbin-Koopman today and it looks great.,2010-12-07T15:55:09.080,2251,CC BY-SA 2.5,
7821,5216,1,"(+1) I located and installed this package less than two months ago.  For more sophisticated work, consider using censored regression models.",2010-12-07T17:10:35.940,919,CC BY-SA 2.5,
7822,5195,2,"This is not a funnel plot.  Instead, the lines evidently are constructed from estimates of standard errors based on the number of admissions.  They seem intended to enclose a specified *proportion* of data, which would make them *tolerance limits.*  They are likely of the form y = baseline + constant / Sqrt(# admissions * f(baseline)).  You could modify the code in the existing responses to graph the lines, but you likely would need to supply your own formula to compute them: the examples I have seen plot *confidence intervals* for the *fitted line itself*.  That's why they look so different.",2010-12-07T17:32:19.790,919,CC BY-SA 2.5,
7823,5216,0,@whuber: I've done MCMC modelling on the data but the target audience is chemists so some simpler stats are useful to warm them up.  My worry is that NADA was withdrawn because it gave the wrong answers.  (I know it's calculations of medians was dodgy.),2010-12-07T17:35:20.177,478,CC BY-SA 2.5,
7824,5077,0,Would you tell us why you want to use this instead of one of the more established MCMC-algorithms?,2010-12-07T18:09:03.210,1979,CC BY-SA 2.5,
7825,5216,0,"@Richie That is extremely interesting to hear.  I didn't test NADA that far: I tried it out, quickly found it lacking in capabilities, and went immediately to survival analysis and censored regression models.",2010-12-07T18:48:10.867,919,CC BY-SA 2.5,
7826,5201,0,thanks for the response! I was first using SAS and could not get a different response variable. I fit the following in R as an example and see that a warning is issued about not being an integer reponse variable but the result did fit a different model.,2010-12-07T20:30:51.977,2040,CC BY-SA 2.5,
7827,5201,0,"data(ToothGrowth)
attach(ToothGrowth)

#1/0 coding
dep <-ifelse(supp == ""VC"", 1,0)
OneZeroModel <-glm(dep~len,family=binomial)
OneZeroModel 
predict(OneZeroModel)


#Platt coding
dep2 <-ifelse(supp == ""VC"", 31/32 ,1/32)
plattCodeModel <-glm(dep2~len,family=binomial)
plattCodeModel
predict(plattCodeModel)

compare<-cbind(predict(OneZeroModel),predict(plattCodeModel))
plot(predict(OneZeroModel),predict(plattCodeModel))",2010-12-07T20:32:15.673,2040,CC BY-SA 2.5,
7828,5195,0,"@whuber (+1) That's a very good point, indeed. I hope this might provide a good starting point anyway (even if my R code isn't that optimized).",2010-12-07T20:34:30.570,930,CC BY-SA 2.5,
7829,5171,0,"Sorry if I'm a bit slow, but I still have some difficulty to understand your data structure. Are there only 2 speakers, with 420 repeated measurements collected on each (I mean one row=one type of conversation, but the same outcome is recorded, e.g. No. passive sentences)? I ask this because you have some kind of matching or pairing (between subjects A and B), but in this latter case, you would also have to deal with repeated measurements, and this renders marginal models for matched pairs less relevant.",2010-12-07T20:45:14.503,930,CC BY-SA 2.5,
7830,5171,0,"No, these are all different speakers as well. This data was taken from a corpus of recorded telephone conversations. So speaker A in conversation 1 is not the same person as speaker A in conversation 2.",2010-12-07T22:53:25.853,52,CC BY-SA 2.5,
7831,5197,0,I want to find where most incidents are in a particular facility if they can be grouped by attributes I have and I am asking about statistical apparatus which can help me to do it. I want to 'see' with statistics that people 'Fall' MOST OFTEN than they 'Burn' or have 'Wrong Medication' or I want to 'see' that there is nothing suspicious. Sorry for using non statistical language. It is not simple counting for sure.,2010-12-07T23:41:47.297,,CC BY-SA 2.5,Igor
7832,5228,0,"BTW, since I am a beginner in R, I would appreciate any comments regarding my code, however brief, both on points of style and substance. In particular, I was hoping there might be a more elegant way to phrase the for loop.",2010-12-08T03:05:39.730,795,CC BY-SA 2.5,
7833,5178,0,"The table with 2 data sources, is one of the things I'm trying to correct. Some of the editors of the page are handling the data in a way that's statistically unsound, or at least I'm trying to prove that.",2010-12-08T03:10:22.477,2271,CC BY-SA 2.5,
7834,5226,0,This is neither a quadratic nor a cubic form: it is *quartic*.  (Section 6.2.3 of the Matrix cookbook does not appear to be applicable.),2010-12-08T04:03:06.330,919,CC BY-SA 2.5,
7835,5234,0,"Hah, good point. Well, let's say I could break this down into about 400 different groups?",2010-12-08T05:47:08.907,52,CC BY-SA 2.5,
7836,5234,2,"Sure, that might work.  What you're doing then is tantamount to using the color value of a graphic to represent word density.  Color values are difficult to interpret accurately; graphical representations that map the variable of interest to *length* or *distance* usually work best.  That's exactly what a kernel density plot will do for you.  By varying the half-width you can range from extreme detail to a gross overview.",2010-12-08T05:55:30.023,919,CC BY-SA 2.5,
7837,5235,4,"What is a ""criterion variable""??",2010-12-08T05:59:48.523,919,CC BY-SA 2.5,
7838,5234,0,"haha nice one, whuber",2010-12-08T06:02:03.937,1307,CC BY-SA 2.5,
7839,5231,0,nice one!,2010-12-08T06:12:43.257,1307,CC BY-SA 2.5,
7840,5240,0,"@ Matti Pastell: I have this book. It is very good indeed. My question is about the difference between the particule filter (which from what I understand is a sequential version of MCMC), and a MCMC on a rolling window (in the latter, we re run the optimisation process on a rolling window). 
Which method should be prefered, and why?",2010-12-08T07:53:49.610,1709,CC BY-SA 2.5,
7841,5240,0,"Also, I don't really find easy to build this time varying model with dlm. Honestly the package is very easy to use for non time varying models, but it starts to be more tricky for everything else. Edit: By more tricky I mean that there is no function to solve the problem. You need to code yourself the script.",2010-12-08T07:56:24.460,1709,CC BY-SA 2.5,
7842,5240,1,"OK, I also have the book but I haven't had time to read it yet. Sorry that it doesn't help with your problem.",2010-12-08T08:01:39.637,214,CC BY-SA 2.5,
7844,5201,0,@user2040: 1. Yes ... is there a question or is this just a remark :) ? 2. why don't you add the code to the question ? No formatting is horrible.,2010-12-08T08:21:12.520,264,CC BY-SA 2.5,
7846,5228,1,"on the R style, you don't need `;` on the ends of your statements. You did right to pre-allocate, but no need to fill with `NA`, `kvals <- numeric(length = n_trial)` would have sufficed. With `rep`, you only need arguments 1 and 3 from your call (e.g. `rep(NA, 10)`). In the `for` loop set-up, `1:n_trial` can be dangerous if programming; better is `seq_along(kvals)` or `seq_len(n_trial)` in this case. Finally, if you don't need to force printing, drop the `print()` round `summary()` - you'd only need it if you weren't working interactively with R. HTH.",2010-12-08T08:28:44.730,1390,CC BY-SA 2.5,
7847,5239,0,"With regards to reproducible research, if the file format is CSV, then the CSV file could easily be put under version control with commits when changes are made. Would this suffice for reproducible research? With regards to data entry, I often do or get others to do raw data entry in fix width format in order to maximise efficiency; however I prefer to store data in CSV or TSV (I find it more transparent); with regards to formatting a column as numeric. this information would presumably be lost on saving.",2010-12-08T08:45:14.223,183,CC BY-SA 2.5,
7848,5239,0,"with regards 12/3, I had an example of a multiple choice test where this represented the 12 divided by 3. However, I use it more as an illustrative example. When I have a large CSV file, I don't want even one inappropriate conversion. With regards to warnings, If I'm opening CSV files 10 times a day the warnings get tedious. They also make it less clear whether I have actually saved the CSV file or not. Sorry for the rant.",2010-12-08T08:48:59.133,183,CC BY-SA 2.5,
7849,5239,0,"@Jeromy yes, it would, if you were strict about making a single change (or single data processing step) that you immediately save and commit. I mean't to include a note about doing the processing in R (as it is my preferred language) using a script. That is how I work with data colleagues send me; I might have a quick look in a spreadsheet, then read the csv into R and write a script that includes all the data processing steps I need to apply. I then comment that script so I have a record of what I did and why I did it and I haven't changed the original data file at all.",2010-12-08T08:50:23.133,1390,CC BY-SA 2.5,
7850,5239,0,@Jeromy: fair point on the warnings. I've long since filtered them out as noise so they don't bother me. I just tested Gnumeric and OpenOffice.org and they convert a csv with 12/3 to dates automagically - that is rubbish! So I see what you mean. The only way to stop that would be to store those data as text and force that data type on load/import.,2010-12-08T08:58:33.440,1390,CC BY-SA 2.5,
7851,5239,0,"@Jeromy: re: losing numeric formatting - yes, it will, unless you specify the column type as ""numeric"" (Gnumeric) or ""text"" in (OpenOffice.org) upon import. Better might be to store it as text (see my edited answer) to avoid the conversion - you'll still need to specify the data type upon import...",2010-12-08T09:05:28.973,1390,CC BY-SA 2.5,
7852,5239,0,"@Gavin I agree about having a script that processes a supplied data file. I guess I'm thinking about some relatively ad hoc files that I create (for example, I might create a little csv file that lists the variable names and corresponding labels that I'll use when preparing an R table).",2010-12-08T09:05:32.217,183,CC BY-SA 2.5,
7853,5243,1,"following this question, I did a hunt: there is a Windows version http://projects.gnome.org/gnumeric/downloads.shtml",2010-12-08T09:11:07.450,183,CC BY-SA 2.5,
7855,5243,1,"it does warn about only saving the current sheet though (which was one of Jeromy's bug-bears), and it has the annoying feature of converting properly saved 12/3 numerics to dates unless you manually specify they are numeric on import (Data > Get External Data > Import Text File...) not load.",2010-12-08T09:31:36.547,1390,CC BY-SA 2.5,
7856,5221,0,"Yes, mixture models are exactly what I need.",2010-12-08T09:31:39.843,2116,CC BY-SA 2.5,
7857,5225,1,"For now flexmix is working very nicely, thank you very much for the hint",2010-12-08T09:32:09.390,2116,CC BY-SA 2.5,
7858,4904,0,be aware of the fact that 95% of your **fair** players are situated within 2 standard deviations from the mean as well. You need more than that to find cheaters.,2010-12-08T09:49:44.020,1124,CC BY-SA 2.5,
7859,5240,1,"Thank you anyway, it is a good book, it deserves to be cited here",2010-12-08T10:03:41.730,1709,CC BY-SA 2.5,
7861,5239,0,@Gavin Thanks for the suggestion about Data Interchange Format.  I've asked a separate question about this: http://stats.stackexchange.com/questions/5249/simple-reliable-open-and-interoperable-plain-text-format-for-storing-data,2010-12-08T10:44:32.513,183,CC BY-SA 2.5,
7862,5249,2,"I should add, R doesn't have a `write.DIF()` so it is a bit of a one-way street I am afraid.",2010-12-08T11:01:00.837,1390,CC BY-SA 2.5,
7863,5249,1,"I do not understand the issue of csv and reliability. Do you mean that csv is not strict enough ? Strict means that if it the regulations for csv were strict enough, every tool following these definitions could load a file without the need of extra parameters.",2010-12-08T12:20:51.963,264,CC BY-SA 2.5,
7864,5249,0,"@steffen I mean things like: loading and saving a csv file in some programs changes the csv file; loading csv files can result in inappropriate conversion unless you are careful; csv files sometimes break when strange combinations of characters are added without proper escaping. Perhaps I'm confusing use of csv with the format itself, although I have heard people comment about the lack of an official standard. Of course, I realise that in many cases it works just fine.",2010-12-08T12:36:18.053,183,CC BY-SA 2.5,
7865,5222,1,"Wilson's macros are definitely the way to go if you want to use SPSS. You may also consider picking up a copy of ""Practical Meta-Analysis"" by Lipsey & Wilson.",2010-12-08T12:37:02.197,1934,CC BY-SA 2.5,
7866,5253,3,"I also recommend taking a look at `str()` as it provides other useful details about your object. Can often explain why a column isn't behaving as it should (factor instead of numeric, etc).",2010-12-08T13:45:39.647,696,CC BY-SA 2.5,
7867,5249,5,@steffen: CSV doesn't store any information about the format or data-types of the data stored in the file. You can well open a CSV file in two different apps and have it interpret the data in the file in two different ways.,2010-12-08T13:49:30.860,1390,CC BY-SA 2.5,
7868,5249,1,"@JeromyAnglim, I think that changing of  the csv file depends on your software, not the csv format per se.",2010-12-08T13:52:49.500,144,CC BY-SA 2.5,
7869,5226,0,@whuber: you are right. The applicable formula is on *8.2.4 Mean of Quartic Forms* of MatrixCookbook. And not surprisingly it gives $3\sum_{i}\lambda_i^2 + \sum_i \sum_j \lambda_i \lambda_j$,2010-12-08T14:27:46.980,2148,CC BY-SA 2.5,
7870,5253,3,"Please read the R guide of Owen first (http://cran.r-project.org/doc/contrib/Owen-TheRGuide.pdf), and if possible, Introduction to R (http://cran.r-project.org/doc/manuals/R-intro.pdf). Both are on the official website of R. You're incredibly lucky you actually get an answer. On the r-help list one would redirect you to the manual in less elegant terms. No offense meant.",2010-12-08T15:19:06.880,1124,CC BY-SA 2.5,
7872,5249,0,"@Jeromy,Gavin: I see. This is indeed an attribute I have seen only in tool-dependent formats (so far). All in all a very interesting question.",2010-12-08T15:24:32.087,264,CC BY-SA 2.5,
7873,5226,0,I believe the second (double) sum is restricted to i != j.,2010-12-08T15:31:40.660,919,CC BY-SA 2.5,
7875,5253,13,"@Joris - Point taken (without offence), but it was my impression that SE sites were designed to foster problem/solution learning in a way not afforded by manuals. Additionally, this question will now be available for other beginners. Thanks for the links though.",2010-12-08T15:42:15.557,1950,CC BY-SA 2.5,
7876,5244,0,"I tried installing it.  After a quick try it seemed like rubbish for data entry; maybe if I gave it a longer try I'd be able to figure out how to use it effectively, but I suspect not.",2010-12-08T16:06:54.077,196,CC BY-SA 2.5,
7877,5239,0,"I glanced at Wikipedia's page, and yes DIF files are plain text... but they also look more computer readable than human readable and seem like they'd also be difficult to do data entry with.",2010-12-08T16:11:40.063,196,CC BY-SA 2.5,
7878,5233,0,"Seems a little silly to post as an answer, but [wordle](http://www.wordle.net/) may give you something useful, or at least interesting to ponder...",2010-12-08T16:51:31.060,696,CC BY-SA 2.5,
7879,5226,0,@whuber: Yes you are completely right! ;),2010-12-08T17:03:22.290,2148,CC BY-SA 2.5,
7880,5253,1,"If you're looking for pure code solutions, stackoverflow might be more appropriate. Although, all the R gurus present @ SO are also here (not counting myself). :)",2010-12-08T17:39:27.177,144,CC BY-SA 2.5,
7881,5228,0,"thanks! definitely what I was looking for. I was running this from a file, so I needed the `print`. The args to `rep` were certainly erroneous.",2010-12-08T17:46:21.007,795,CC BY-SA 2.5,
7882,5242,0,"+1 for `set.seed`, which would help in examples like this. Is there a substantial reason for encapsulating in a function (_e.g._ the R interpreter will pre-compile functions, thus there is some speedup), or is stylistic (_e.g._ encapsulating functions, like broccoli, is good for you), or somewhere in between (_e.g._ there are, say, lots of operators in R which act on functions, so one should get used to functional programming)?",2010-12-08T17:49:21.407,795,CC BY-SA 2.5,
7883,5245,0,Thank you very much. Your explanation is very helpful to me.,2010-12-08T17:50:04.013,400,CC BY-SA 2.5,
7884,5232,2,"The G2 estimator from `e1071` gives the 'standard' bias correction; see http://cran.r-project.org/web/packages/e1071/e1071.pdf . Using this estimator instead of g2, implemented by the `moments` package, had little effect, as I noted in the Q. The dependence on the eighth moment would indeed imply the sample size is too small here.",2010-12-08T17:55:50.540,795,CC BY-SA 2.5,
7885,5242,0,"@shabbychef: I think the main one is effort. You could run your code with the loop etc repeatedly and it would be fine, but you have to keep running all the lines of your code. By encapsulating it, you run 1 line of R code for each simulation you want to run. There is no speed up as R doesn't compile anything ahead of time IIRC.",2010-12-08T18:07:35.020,1390,CC BY-SA 2.5,
7886,5242,1,"thanks for the clarification. Since this is all in a small file, it is one line anyway: `source('foo.r')` ;)",2010-12-08T18:10:02.193,795,CC BY-SA 2.5,
7888,5272,1,"@Daniel: if applying this, turn off the x-axis in the `plot()` call: `plot(x, y, type = ""l"", xaxt = ""n"")` and change the two `x` s so you don't overwrite the data you created to draw the plot.",2010-12-08T19:12:45.340,1390,CC BY-SA 2.5,
7889,5272,0,"This worked, except it plotted the new axis over the old one. How would I address this issue?",2010-12-08T19:16:12.437,1973,CC BY-SA 2.5,
7891,5238,1,"What kind of operations do you actually do on the files? In my mind, that dramatically affects the range of acceptable options. Also, it seems to me that you could edit TSV data in a word processor and line up tabs to preserve columns -- as long as your word processor can be set to not do ""smart"" conversions and can save as plaintext, which many can.",2010-12-08T19:30:35.090,1764,CC BY-SA 2.5,
7892,5273,0,"exactly. The error is in the first line of the question, where the OP defines $\hat{m}$ as the sum of the $X_i$ not the mean, as you do.",2010-12-08T19:43:21.327,795,CC BY-SA 2.5,
7894,5273,3,"The quantity $(N(0,\frac{\sigma^2}{n}))^2$ is a scaled Chi-square. As a check, the variance of a Chi-square with $k$ dof is $2k$, which gives the same result you get.",2010-12-08T20:14:04.613,795,CC BY-SA 2.5,
7895,5253,2,"I disagree with your assertion that this question will be helpful for other beginners, *especially* if they don't skim the manual.  They will just create a duplicate question.",2010-12-08T21:01:34.220,1657,CC BY-SA 2.5,
7897,5223,1,"If this answer is not satisfactory, could you explain why?",2010-12-08T22:11:15.703,1146,CC BY-SA 2.5,
7898,5248,0,"Thank you! Just to add, I found in the sampleSelection R package, under the help page of the inverse mills function, the ability to extract inverse mill ratios for bivariate probit models which can then be used in the final outcome equation. This doesn't resolve the panel data issue, but it does address the multiple selection one. FWIW.",2010-12-08T22:35:18.340,2050,CC BY-SA 2.5,
7899,5260,2,"The variance of the sample mean (""and I know that..."") scales as 1/N, not 1/N^2.  That's the only change you need.",2010-12-08T22:55:59.210,919,CC BY-SA 2.5,
7901,5279,2,"RData certainly works well with R. It might be problematic with regards to version control. I suppose the R function `dput()` provides a plain text alternative that would work with version control. However, one of the appeals of csv/tsv is that when I share a repository with data (say for a journal article), people could take the data and reanalyse it easily using any software they like.",2010-12-09T01:45:06.677,183,CC BY-SA 2.5,
7902,5279,1,"Yes, it is a hugely complicated matter. I think people have discussed this since the dawn of computing.  I had two more thoughts (and I could expand my answer): ProtocolBuffers are good for *efficiently* sharing with Python, Java, C++, ... and a host of other languages; Romain and I cover R.  The the new-ish site http://mldata.org covers this for research in Machine Learning -- they even have tools they make available to convert. That may be worth a look.",2010-12-09T01:48:23.523,334,CC BY-SA 2.5,
7903,5262,0,Thanks. It sounds like using a delimited file format with appropriate care may be the best option.,2010-12-09T01:48:34.927,183,CC BY-SA 2.5,
7905,5238,0,"@Wayne Good point. Because I've grown up with spreadsheet programs, there are many shortcuts that I'm familiar with (functions, copy and paste, adding an extra column, and much more). Note, I'm not talking here about data analysis, but just creating a simple tabular data file (e.g., some meta information needed to process my R code). While I could do all these basic table manipulations in R, it's not quite as intuitive for me. Probably over time, opening a csv in R, making a few little changes and saving it again will become my preferred option.",2010-12-09T02:01:15.447,183,CC BY-SA 2.5,
7907,5279,1,"Actually, SVN takes binary blobs such as pdf files etc without problem. I'd suspect git does too.",2010-12-09T03:08:17.417,334,CC BY-SA 2.5,
7908,5279,0,That's good to know about binary blobs. It would still be nice to be able to run diff on text files and get meaningful information about changes. Thanks also for link to mldata.org. That looks interesting.,2010-12-09T03:27:35.123,183,CC BY-SA 2.5,
7909,5279,0,"Pleasure. Sister site mloss.org is simply great, if hope they get traction for mldata.org.  The time is right for that.",2010-12-09T03:32:35.970,334,CC BY-SA 2.5,
7910,5274,1,"You're on your way towards re-inventing Ripley's K function (http://www.stat.iastate.edu/preprint/articles/2001-18.pdf ).  Note, however, that much information is lost in this approach.  Suppose, for example, that 15 of the distances are zero and the other 15 equal 10,000.  Is that 15 clusters of two words or one cluster of 16 words and 14 isolated words?",2010-12-09T03:35:59.313,919,CC BY-SA 2.5,
7911,5241,0,"Yep, and it turns out I already have `csv-mode.el` installed thanks to the `emacs-goodies-el` package in Debian/Ubuntu -- neat.",2010-12-09T03:50:15.587,334,CC BY-SA 2.5,
7912,5284,0,"Thanks, whuber! Could you recommend some books/tutorials that have some good cover on antithetic methods?",2010-12-09T05:02:07.487,1005,CC BY-SA 2.5,
7913,5286,0,(+1) Any thought about its performance compared to [NetCDF](http://www.unidata.ucar.edu/software/netcdf/)?,2010-12-09T06:39:50.913,930,CC BY-SA 2.5,
7916,5223,0,"I'm not sure about @fred, but when I offered the bounty I suppose I was wanting a bit more insight into hit-and-run MCMC. For example, what types of problems would it be best suited for. Of course, if there are no other answers, then this question would win the bounty.",2010-12-09T09:49:17.370,8,CC BY-SA 2.5,
7917,5260,0,@whuber. yes you are right again :D,2010-12-09T10:19:03.773,2148,CC BY-SA 2.5,
7918,5289,3,"I'd add that the mean, sd, max and min provide some evidence regarding the issue of whether assuming the underlying distribution is symmetric is a reasonable assumption; if the distribution is symmetric then the population mean and median will be the same.",2010-12-09T10:54:42.763,183,CC BY-SA 2.5,
7919,5290,2,Your question is unclear at the moment - perhaps you could edit it to make it clearer?,2010-12-09T11:18:27.857,226,CC BY-SA 2.5,
7920,5223,3,@csgillespie: I edited my answer to better accomodate your interest. Let it not be said that I was undeserving of the bounty. ;),2010-12-09T11:50:48.890,1979,CC BY-SA 2.5,
7921,3790,0,"If mean is unique minimizer, shouldn't it be 
$E|X-\mu|\le E|X-m|$?",2010-12-09T13:09:09.680,2116,CC BY-SA 2.5,
7922,5241,1,"+1, For point #1 all stat software I have ever worked (besides R, Stata, SPSS, and SAS) with provides this type of functionality. So your advice generalizes to whatever software someone wants to work with.",2010-12-09T13:45:13.737,1036,CC BY-SA 2.5,
7923,5289,0,"Thank you. Unfortunately my data does not conform to any particular distribution, so I will be unable to use these methods. It is still useful information to know, so thank you anyway.",2010-12-09T14:50:59.457,261,CC BY-SA 2.5,
7924,5289,0,"@robintw: If you guess the data does not fit to any particular distribution, the Normal distribution may be the best choice, especially if the values are mixture of several other values (Look at http://en.wikipedia.org/wiki/Central_limit_theorem )",2010-12-09T15:02:39.520,2148,CC BY-SA 2.5,
7925,5292,17,"I use Emacs + ESS so I guess I would not be of much help here. However, for your information, there will be a [special issue](http://www.jstatsoft.org/misc/CFP-JSS_SV-GUIs_in_R.pdf) on R GUIs (expected for mid 2011).",2010-12-09T15:08:41.043,930,CC BY-SA 2.5,
7926,5299,3,"[The Art of Data Augmentation](http://j.mp/hiZ5ED), by van Dyk and Meng may be a good start. I also quickly found some R examples on Bayesian statistics on [Peter M. Lee](http://j.mp/fslE35) and [Brian Neelon](http://j.mp/h1yJyQ)'s websites. But I guess you can find other good tutorials with Google, or a more experienced user will have some good recommendations to offer.",2010-12-09T15:25:07.840,930,CC BY-SA 2.5,
7927,5305,0,Could you provide an example?,2010-12-09T16:23:33.723,1657,CC BY-SA 2.5,
7928,5305,2,"If there ever was a list of FAQ to xts, this would score highly. Please look around here, search for `[r] xts` and peruse the r-sig-finance archives.",2010-12-09T16:24:21.273,334,CC BY-SA 2.5,
7929,5299,0,Great thanks.  Anyone know of a good book/textbook reference?,2010-12-09T16:35:00.393,2310,CC BY-SA 2.5,
7930,5274,0,"If you hadn't let me know, I might've named it Wayne's Method, written a paper on it, and gotten lots of references... or maybe that only works in medical literature... ;-) My moment of glory gone. On a more serious note, I guess for this exploratory application, the usual worrying about appropriate bandwidths isn't really an issue?",2010-12-09T16:58:10.887,1764,CC BY-SA 2.5,
7931,5286,0,IIRC that is also the internal format chosen at http://mldata.org -- with a suite of tools which convert.  The converters may be worth a look.  I always had the feeling that R support for HDF5 was less that perfect.,2010-12-09T17:07:32.407,334,CC BY-SA 2.5,
7932,5302,1,"+1.  Given that this is truly ""quick and dirty,"" one might be inclined to emphasize that $\beta_4$ is the *average* marginal change in Y.  It could differ appreciably from the actual marginal changes.  This average will be meaningful only if the relationships among the independent variables and the dependent variable are reasonably linear.",2010-12-09T17:13:08.987,919,CC BY-SA 2.5,
7933,5286,0,"@chl I had vaguely thought that NetCDF used HDF5 internally, but that seems to be not quite accurate.",2010-12-09T17:13:35.010,795,CC BY-SA 2.5,
7934,5311,0,"+1 I agree completely.  Either command line or standard windows gui.  Learn the language, and leave all the bells and whistles for later.",2010-12-09T17:14:11.327,5,CC BY-SA 2.5,
7935,5274,0,"(myself): just got back from lunch and realized that the usual bandwidth concerns are because you are using the kernel density to attempt to visualize an underlying population distribution from which you have sampled, while in this case the data is the entire population. Right?",2010-12-09T17:19:37.593,1764,CC BY-SA 2.5,
7936,5307,6,"Ahh.  That answers the question.  However, it leads on to the next one - which one is preferred?",2010-12-09T17:37:12.800,1991,CC BY-SA 2.5,
7937,5223,2,"Many thanks for the link. One of the reasons I placed the bounty was that my google searches turned up a few mathematical discussions of the method, but little in the way of practical applications. Please don't take it as a slight if I wait another 48 hours before awarding the bounty (it is a particularly generous bounty!)",2010-12-09T18:03:07.540,8,CC BY-SA 2.5,
7938,5310,0,"There is only a small issue - if I have ..., 14:59, 15:00, ..., it will retrieve 14:59 and not 15:00 as I'd like. Is there a way to make it return 15:00? I tried something like `""seconds"", 3601`, but it doesn't work",2010-12-09T18:39:45.307,749,CC BY-SA 2.5,
7939,5310,0,Since the timestamps are irregular one can only make guarantees on the dt between data points. The actual points depend on the starting point of the series. If you fix your starting point to a round hour it will behave as you like... (assuming you have points every N minutes),2010-12-09T18:46:12.197,300,CC BY-SA 2.5,
7940,5316,0,"The analysis is being done on means of response times and accuracies that are collected into 8 bins based on an individual's ordered response times for each trial type. For this type of analysis, will nlme() work on means rather than the data points that compose the mean?",2010-12-09T18:59:31.887,2322,CC BY-SA 2.5,
7941,5310,0,This is exactly what I don't want - being dependent on the first point,2010-12-09T19:04:29.967,749,CC BY-SA 2.5,
7942,5319,0,"Consider any time series - outside temperature. I want to know what's the temperature at 11:00 wall clock, at 12:00 wall clock, .... I'll check your example later to see if it does this",2010-12-09T19:05:35.707,749,CC BY-SA 2.5,
7943,5316,1,"Get the raw data.  The analysis works on the raw data the went into generating the means.  Otherwise, you'll get a result that looks suspiciously like your original ANOVA.",2010-12-09T20:01:45.300,601,CC BY-SA 2.5,
7945,5289,2,"@Isaac The CLT is irrelevant and inapplicable: we seek the median of the *parent distribution*, not the median of the *sampling distribution of the mean*.  However, it (coincidentally) turns out that the maximum entropy solution (given the mean, sd, and upper and lower bounds for all values) is a doubly truncated Gaussian distribution.  One would numerically solve for its parameters to match the observed mean and sd and then compute its median (which will usually *not* be the same as its mean because the truncation can be asymmetric).",2010-12-09T20:52:36.177,919,CC BY-SA 2.5,
7946,5289,0,"@Jeromy The only statistic that can be formed of those four values that provides any information about symmetry is the ratio (max - mean):(mean - min).  That's a (highly) non-robust statistic, though: I would entirely discount its use in a practical problem.  However, when data are *constrained* to lie between two limits, those limits (relative to the mean) do add useful information.",2010-12-09T20:56:17.343,919,CC BY-SA 2.5,
7947,4450,2,"@Srikant Thank you.  I was thinking along similar lines when I posed the question and have since tested out a GLM (Poisson distribution with *linear* link) as well as some other models.  Unfortunately, it now looks like any model based solely on land cover type and proportion will not work well: a sample of these data suggests that population patterns depend on a larger spatial context.  At a minimum, then, we would need to include spatially lagged covariates in a linear model.",2010-12-09T21:03:55.730,919,CC BY-SA 2.5,
7950,4449,2,Thank you.  That work provides a pointer into a web of recent research on dasymetric mapping.,2010-12-09T21:55:46.667,919,CC BY-SA 2.5,
7953,5299,0,Maybe you could add some info about its intended use and the target language .,2010-12-09T22:12:47.010,930,CC BY-SA 2.5,
7955,5301,3,"+1 for Rcmdr. It's nice for when one is just learning R, and offers more of a gateway to a good statistics package than SPSS or Minitab, but the code it writes is needlessly verbose, often. A scatterplot produced in Rcmdr: scatterplot(tab~pct, reg.line=FALSE, smooth=FALSE, spread=FALSE, 
  boxplots=FALSE, span=0.5, data=senate.race), when most of those parameters were defaults to begin with. It writes some very wordy code.",2010-12-09T23:00:56.893,1118,CC BY-SA 2.5,
7956,5301,0,"@Christopher I agree with you; sometimes the code is wordy.  It comes from Tcl/Tk and how Rcmdr is currently written: it would take a substantial rewrite to put in checks for whether the default is selected (and return an abbrev. version in that case).  But, yes, it would be an improvement.  A redeeming feature is that it's *good* code, in that it encourages good practice (FALSE instead of F, etc.).  This is important for beginners.",2010-12-09T23:23:09.183,,CC BY-SA 2.5,user1108
7957,5296,6,"Well done! The purpose of a GUI is not to learn the language, but to make learning the language unnecessary. (edit:) I didn't know that some GUI's had the features described by G. Kay Kerns. I suppose that would help you in the early stages of learning the language.",2010-12-10T00:04:31.113,666,CC BY-SA 2.5,
7958,5311,6,"Honestly, I don't agree. Scripting is much more powerful.",2010-12-10T02:47:57.947,1709,CC BY-SA 2.5,
7959,5301,1,"This is true. Another advantage to this is allowing beginners to see what parameters a function takes without using the ? command. Using FALSE instead of F is a great point. I once saw a program give an incorrect output because the programmer had earlier set the result of an ANOVA to a variable called ""F"".",2010-12-10T03:34:51.577,1118,CC BY-SA 2.5,
7960,5289,0,@whuber Would the SD have some relevance to variables like income and reaction time which are constrained just at the lower end?,2010-12-10T05:19:37.613,183,CC BY-SA 2.5,
7961,5295,0,"(1) I would like to be able to open and close the data file as easily as I can open an Rdata, Excel, or SPSS data file. Spending time walking through a wizard works, but it's not quite the simple and reliable workflow that I'd ideally like. (2) Yep, I agree about using an irregular delimiter. In general Tab is sufficient for me most of the time; (3) I don't have huge problems with CSV/TSV. I have occasional problems that are easily resolved. However, I'd like not to have to think about the issues of delimiters and format conversion.",2010-12-10T05:29:17.650,183,CC BY-SA 2.5,
7962,5311,0,@fRed Scripting in included in my operational definition of command line interface (-;,2010-12-10T07:09:04.110,,CC BY-SA 2.5,user88
7963,5337,0,"Thank you, a nice point, the problem with minimum distance in my case is that it is I have thousands of nodes.",2010-12-10T07:55:22.090,2325,CC BY-SA 2.5,
7964,5337,0,I want to give you +1 but I need at least 15 points of reputation:(,2010-12-10T07:56:19.157,2325,CC BY-SA 2.5,
7965,5333,0,it would be helpful to see <code>summary(r.only.lmer)</code>,2010-12-10T08:11:29.907,2116,CC BY-SA 2.5,
7966,5338,0,"Thanks, but even if the data is not independent, when you want to calculate the EMA return from t-m to t, you would ideally not want to have the EMA at t include information before t-m.",2010-12-10T09:01:34.867,2316,CC BY-SA 2.5,
7967,5280,0,Is the first illustration produced using R?,2010-12-10T09:35:34.937,339,CC BY-SA 2.5,
7968,5235,2,@whuber I'm guessing Jfly is referring to the response/outcome/dependent/etc. variable. http://davidmlane.com/hyperstat/A101702.html It's interesting to see the many names such variables go by: http://en.wikipedia.org/wiki/Dependent_and_independent_variables#Alternative_terminology_in_statistics,2010-12-10T10:13:14.997,183,CC BY-SA 2.5,
7969,5280,0,@gd047 Yes. Should have the ugly source code somewhere if you want. The second one is done in Metapost.,2010-12-10T10:29:15.637,930,CC BY-SA 2.5,
7970,5333,0,"Is it related to the question here, http://stats.stackexchange.com/questions/5171/testing-paired-frequencies-for-independence/?",2010-12-10T10:45:21.303,930,CC BY-SA 2.5,
7971,5268,0,"in addition to Galit: Do you want primarily to find correlations and hence clustering is just the way, not the goal, or do you want to find clusters of users to discover some knowledge in the data (with respect to correlation) ?",2010-12-10T10:53:53.980,264,CC BY-SA 2.5,
7972,5343,0,"What do you mean by ""n some cases I can observe ..."". Does that man that neither y1 nor y2 can be observed all the time (i.e. for all the x)?",2010-12-10T11:02:14.550,264,CC BY-SA 2.5,
7977,5345,6,"@ 2) more precisely, in `lme4` you can either specify a diagonal covariance structure (i.e., independent random effects) or unstructured covariance matrices (i.e. all correlations have to be estimated) or partially diagonal, partially unstructured covariance matrices for the random effects. I'd also add a third difference in capabilities that may be more relevant for many longitudinal data situations:  `nlme` let's you specify variance-covariance structures for the residuals (i.e. spatial or temporal autocorrelation or heteroskedasticity),  `lme4` doesn't.",2010-12-10T11:55:51.977,1979,CC BY-SA 2.5,
7979,5345,0,"@fabians (+1) Ah, thanks! Didn't realize `lme4` allows to choose different VC structures. It would be better that you add it in your own response, together with other ideas you may have. I will upvote. BTW, I also realized that `lmList()` is available in `lme4` too. I seem to remember some discussion about that on R-sig-ME.",2010-12-10T12:09:26.517,930,CC BY-SA 2.5,
7982,5295,0,"@Jeromy Anglim, for point #1, I would guess you normally only have to do this once (unless you migrate between two different environments frequently that can not read or output the others files). For point #3, fixed text files fix that problem. I have never come across a situation where SPSS formatted a different file type incorrectly. If you don't need to disseminate the files this whole question is mute, if you can get the file to save correctly in whatever environment you will be working in there is no more need for conversion/storage.",2010-12-10T13:40:35.893,1036,CC BY-SA 2.5,
7983,5280,3,"@gd047 All right, as usual it's always when we look for old code that we cannot found it (despite my best effort with grep/find), so I rewrote a quick (still ugly) [R script](https://gist.github.com/736212) for that. I've also put an example of the [MP code](https://gist.github.com/736216).",2010-12-10T13:54:21.740,930,CC BY-SA 2.5,
7986,5327,2,"You should not be using truncated regression just because you have skewed or bounded data. It is specifically for situations when values below a threshold (eg negative values) are possible, but would not be observed for some reason. Is that the situation you have?",2010-12-10T15:28:13.053,279,CC BY-SA 2.5,
7988,5349,3,"This is not true when the p_i approach zero as i increases.  Otherwise, you have just proven that the Poisson distribution is Normal!",2010-12-10T16:14:38.633,919,CC BY-SA 2.5,
7990,5327,0,"@Aniko, negative values of the dependent variable don't really make sense (it would mean getting paid to receive a service), but I'd heard that Wooldridge (in *Econometric Analysis of Cross Section and Panel Data*, 2002) had recommended truncated or censored regression models instead of OLS when $P(Y=0)>0$ yet $Y$ is a continuous random variable over positive values.",2010-12-10T16:27:34.857,1583,CC BY-SA 2.5,
7991,5347,0,Are your p_i fixed throughout the modeling exercise or can they change from one calculation to the next?,2010-12-10T16:30:42.473,919,CC BY-SA 2.5,
7992,5235,1,"@Jeromy Thanks!  I had guessed that was the meaning but wasn't sure.  That's a new term to me--and to Wikipedia, evidently.",2010-12-10T16:32:44.810,919,CC BY-SA 2.5,
7993,5252,12,"+1 This is exactly the right analysis.  But why don't you finish the job and answer the question?  The OP asks whether this correlation is ""high"" and what it might *mean*.",2010-12-10T16:33:54.123,919,CC BY-SA 2.5,
7994,5289,0,"@Jeromy Yes.  But note that we're talking about the *constraints*, not the observed min and max.  In fact, the maxent solution for a lower constraint, given the mean and SD, is a shifted exponential distribution: exactly the example @Isaac uses!",2010-12-10T16:36:07.523,919,CC BY-SA 2.5,
7995,5343,0,"@James And what do you mean when you call y2 a ""prediction""?  Isn't it functioning here as a *predictor*?",2010-12-10T16:39:20.093,919,CC BY-SA 2.5,
7996,5340,3,"Please vote up *any* and *all* reasonable answers: let's encourage people to contribute their ideas!  Remember, you have the additional ability to award an additional 15 points to the answer you eventually think is best.  Overall you will gain far more by *awarding* points than by *withholding* points.",2010-12-10T16:42:12.727,919,CC BY-SA 2.5,
7997,5340,0,"@whuber, I so agree with you on all counts.  That is why I actually expressed the concept within my question.  I feel that many participants in this forum do not quite get this concept as clearly as you do.  Thanks for your comment.",2010-12-10T16:56:29.157,1329,CC BY-SA 2.5,
8001,5354,1,"not sure I deserve the ""accept"". @Conjugate Prior's answer explained what was wrong with your model. I thought it worth explaining the warning you mentioned in terms of the algorithm.",2010-12-10T17:11:51.610,1390,CC BY-SA 2.5,
8002,5342,1,"+1 This is correct.  Just for fun, how does Excel's answer differ from R's when you use 5000 as an argument instead of 5?",2010-12-10T17:13:03.917,919,CC BY-SA 2.5,
8003,5340,0,Check out @chl's voting: *there* is someone who gets it!,2010-12-10T17:14:15.913,919,CC BY-SA 2.5,
8004,5354,5,"If you have the actual delay times, you are likely to get better information by modeling them, rather than reducing them to a binary variable.",2010-12-10T17:18:46.050,919,CC BY-SA 2.5,
8006,5333,0,"@chl: Yes, it's the same data and basically the same hypothesis.",2010-12-10T17:19:53.950,52,CC BY-SA 2.5,
8007,5343,0,"@steffen - that's correct, sometimes I have estimates of x independent of any y's.",2010-12-10T17:24:46.507,2328,CC BY-SA 2.5,
8008,5337,0,"Thanks a lot sirus anyway, I am glad you found it helpfull",2010-12-10T17:55:22.430,1808,CC BY-SA 2.5,
8009,5349,1,"That is why it must be $B_n\to\infty$. If $p_i$ approach zero at rate faster than $1/i$, $\lim B_n<\infty$.",2010-12-10T17:58:11.103,2116,CC BY-SA 2.5,
8010,5353,0,"I don't think I have the funding to purchase anything from SSI.  Just in case I find $250 under the couch, did you have a particular product in mind?",2010-12-10T17:59:05.820,196,CC BY-SA 2.5,
8011,5353,0,Thanks for the pointers.  I guess JAGS MCMC it is; thankfully I can be very confident in the priors so hopefully something shakes out.  I'm a little concerned because though I have lots of observations on the items I have relatively few subjects.  I'll start throwing models at it and see how it shapes out.,2010-12-10T18:00:43.540,196,CC BY-SA 2.5,
8012,5258,0,"Wow, this looks really useful. I will acquire a copy of this book as soon as I can, since everyone seems to be citing it. In the meantime, just a naive question: can these models deal with an arbitrary number of random effects? I think I need 3 in my model.",2010-12-10T18:03:00.793,52,CC BY-SA 2.5,
8013,5349,0,"@mpiktas is right.  The analogy to the Poisson distribution doesn't quite fit, here.",2010-12-10T18:12:53.410,,CC BY-SA 2.5,user1108
8014,5333,0,@mpiktas: I have edited the question so that it includes the random effects only model.,2010-12-10T18:16:12.287,52,CC BY-SA 2.5,
8016,5349,0,"By the way, I didn't actually check that monstrous condition in the second paragraph.",2010-12-10T18:22:31.443,,CC BY-SA 2.5,user1108
8020,5349,0,"@G. Jay Kerns I agree that the analogy to the Poisson is imperfect, but I think it gives good guidance.  Imagine a sequence of p's, p_i = 10^{-j}, where j is the order of magnitude of i (equal to 1 for i <= 10, to 2 for i <= 100, etc.).  When n = 10^k, 90% of the p's equal 10^{-k} and their sum looks Poisson with expectation 0.9.  Another 9% equal 10^{1-k} and *their* sum looks Poisson (with the same expectation).  Thus the distribution looks approximately like a sum of k Poisson variates.  It's obviously nowhere near Normal.  Whence the need for the ""monstrous condition.""",2010-12-10T18:53:44.430,919,CC BY-SA 2.5,
8021,5358,1,"+1 Not a bad idea.  It's likely that a small mixture of Poissons would do a good job, depending on how the question is clarified.",2010-12-10T18:55:10.937,919,CC BY-SA 2.5,
8023,5358,1,"I did think about suggesting a negative binomial distribution, which arises as a Gamma-Poisson mixture, but that has a variance larger than its mean, while this problem has a variance smaller than its mean. Based on that, i'm not sure if any mixture of Poissons will work, as surely any such mixture will have variance larger than its mean??",2010-12-10T19:25:29.770,449,CC BY-SA 2.5,
8024,5342,0,"Both give 1. Though I agree with your answer below, I would not trust Excel with anything.",2010-12-10T19:57:33.423,2116,CC BY-SA 2.5,
8025,5342,0,"Hmmm... I get 1 - 2.81997E-14 in Excel.  I'm using an older version, Excel 2002 SP3.  The correct value is 1 - 2.79823E-14.  Regardless, 1 = 1-0 is *way* off.  (Remember, the quantity of interest is usually the *tail* value, not the cumulative value.)",2010-12-10T20:25:32.993,919,CC BY-SA 2.5,
8026,5258,0,"@Alan Be aware that we (@caracal and me) are not citing the same book. CDA is the more complete one, but Laura Thompson's textbook already includes about 10 pages of summary for each chapter. Look at her textbook first. ICDA is available on Google books I think.",2010-12-10T20:38:56.483,930,CC BY-SA 2.5,
8027,5342,0,"If I subtract the result with 5000 from 1, in Excel I get 2.79776E-14 and in R: 2.797762e-14. I use R 2.11.1 on Mac OS X 10.6.5",2010-12-10T20:55:12.393,2116,CC BY-SA 2.5,
8029,5361,2,"+1 for highlighting the importance of model interpretation. I won't add anything about the uninformed ML approach (or ensemble methods) with more complex cross-validation schemes, because I feel you already said what's really matter here: (1) feature selection through resampling is hardly interpretable in isolation (i.e., by comparing one result after the other), and (2) it all depends if we are seeking for a predictive or an explanatory model.",2010-12-10T21:03:26.437,930,CC BY-SA 2.5,
8030,5349,0,"@whuber, well said.",2010-12-10T21:05:17.183,,CC BY-SA 2.5,user1108
8031,5350,1,"With the possible exception (as you pointed out) of being able to incorporate temporal autocorrelation in nlme but not lme4.  If the data set is big enough, and if the data have this sort of structure, that could be a big advantage of nlme.",2010-12-10T21:10:50.873,2126,CC BY-SA 2.5,
8032,5362,2,"(+1) I always think that the takeaway messages when teaching ANOVA is: (1) we have an F-ratio which reflects the relative importance of the variance accounted for by our factor of interest wrt. total variance (or MSB/MSW where MSW=MST-MSB), (2) the differences between the group means is a variance, and (3) we explicitly test $H_0:~\mu_1=\mu_2=\ldots=\mu_k$ *vs.* $H_1:~\exists\ i,j~|~\mu_i\neq\mu_j$ ($H_1\equiv\neg~H_0$). If you're able to convey those ideas into a graphical display--which seems to be the case here--, then I think you're almost done.",2010-12-10T21:21:14.077,930,CC BY-SA 2.5,
8033,5343,0,Do you have observed values of x at least sometimes matched with observed values of y1 and/or y2?,2010-12-10T21:28:04.020,1583,CC BY-SA 2.5,
8034,5361,0,"Thanks for your insight.  I have done some pre-screening to narrow my search space and simply want to find the best model for prediction with the fewest variables.  I am only throwing 7 predictors into the model, which as I understand it, should be ok.  I understand the idea of sticking with a sample, but on the flip side, my model was fundamentally different and shows that the results are entirely sample-dependent, which made me pause.",2010-12-10T21:30:30.120,569,CC BY-SA 2.5,
8036,5367,3,"Thanks, Alex. I see a problem with this though: what if over the loops the solutions generated never satisfy the constraint? That could happen if k means were set to run with no cluster size constraint.  I'd love a solution that avoids this. (The nature of the application is such that I really need to ensure clusters are of a minimum size.)",2010-12-10T21:50:26.300,96,CC BY-SA 2.5,
8037,5326,0,"Note that `FORMAT x y 17.12` will show both `x` and `y` as 17 characters long (including the decimal and, when present, the negative sign), and 12 of those characters will be digits after the decimal point. Numbers with more characters or greater decimal precision may not display correctly.",2010-12-10T21:57:37.027,1583,CC BY-SA 2.5,
8038,5361,0,"@Btibert3 Right: when the results vary among random subsets of your data, you can take that as evidence that the independent variables are not strong or consistent predictors of the independent variable.",2010-12-10T22:03:15.237,919,CC BY-SA 2.5,
8039,5365,2,"I'm not familiar with Bishop's notation, but IIRC, the loading $\lambda_{ij}$ in matrix $W$ equals the correlation of observed variable $i$ with latent factor $j$ only in the orthogonal-factor model. In a model with correlated factors, matrix $W \cdot C_f$ contains the variable-factor correlations if $C_f$ is the correlation matrix of the factors (not equal to the identity matrix).",2010-12-10T22:08:41.067,1909,CC BY-SA 2.5,
8040,5342,0,@mpiktas Interesting; it looks like some improvements were made in more recent versions of Excel.  It's also interesting that R and Mathematica differ.  I suspect R is using a high-precision approximation to the integral (there are many good ones) whereas Mathematica is numerically integrating to whatever precision is requested (I used 6 to 20 sig figs).  That doesn't mean R is bad; it only means we need to pay attention to numerical precision and accuracy when computing extreme probabilities with *any* software.,2010-12-10T22:08:42.907,919,CC BY-SA 2.5,
8041,5365,1,"@caracal (+1) Good point. Yes, Bishop considers an orthogonal solution.",2010-12-10T22:20:06.547,930,CC BY-SA 2.5,
8042,5322,0,Thanks! What is a good approach if it's not linear in the logit scale?,2010-12-10T23:52:55.953,1991,CC BY-SA 2.5,
8043,5343,0,"@Firefeather - Yes I do. I'd like to know what's strictly right versus what's pragmatic to do - maybe modeling these as Gaussians isn't right, so adding or multiplying is the theoretical right answer but not practical.",2010-12-11T00:54:41.767,2328,CC BY-SA 2.5,
8044,5358,0,@onestop Where was it said that the variance is less than the mean?  I missed that statement.,2010-12-11T07:02:49.883,919,CC BY-SA 2.5,
8045,5362,0,"Hi chl, thank you for the positive feedback (and for your previous detailed answer)! I think some of the biggest take home massages I got from preparing the material for this class are:  1) How to describe the transformation of the original data so to get te MSB and MSW variances measures.  2) How the test statistic of the MSB/MSW is actually a one sided (not a two sided) test where the H0 is that MSB<=MSW. Lastly, I just thought to note that it is true that SSW=SST-SSB (but I don't see how it's true for MSW=MST-MSB).",2010-12-11T08:25:11.763,253,CC BY-SA 2.5,
8046,5369,2,"Well, you can do FA on the factors determined by an FA method, its called hierarchical factor analysis and is done all the time in psychology. So in that edge case, it is useful. On the original question, i really dont see why you would even want to do that.",2010-12-11T09:33:52.250,656,CC BY-SA 2.5,
8047,5358,0,"Sorry whuber, that was a bit cryptic but these comments don't allow that much elaboration. mpiktas's $B_n = \sum p_i(1-p_i)$ is the variance, which is less than the mean, $\sum p_i$. Only slightly less if the $p_i$'s are on average very small though, so standard Poisson might be a good enough approx. Maybe I should expand my answer above.. but then the conversational thread gets confusing.",2010-12-11T10:40:33.597,449,CC BY-SA 2.5,
8050,5362,1,"Yep, sorry I wrote quickly. I meant: Consider the model $y_{ij}=\mu + \alpha_i + \varepsilon_{ij}$ or $y_{ij}=\mu_i + \varepsilon_{ij}$ and highlight graphically the following decomposition: $y_{ij}=\bar{y_i}+\varepsilon_{ij}=\bar{y}+(\bar{y}_i-\bar{y})+(y_{ij}-\bar{y}_i)$, that is obs. are expressed as deviations from the grand mean + group mean + fluctuations around group mean. Then, we have $(y_{ij}-\bar{y})=(\bar{y}_i-\bar{y})+(y_{ij}-\bar{y}_i)$ or total variation = between-group variation + within-group variation (which is basically your first picture).",2010-12-11T12:30:00.023,930,CC BY-SA 2.5,
8051,5369,3,"@richiemorrisroe: higher-order factor analysis is done on the basis of **correlated** lower-order factors. As a sidenote, I wouldn't count the fact that some method is routinely used in some psychological disciplines as evidence for its usefulness.",2010-12-11T12:39:12.960,1909,CC BY-SA 2.5,
8052,5363,0,"NN is rather an obsolete technology, so I'm afraid you won't get an answer because no one here is using them...",2010-12-11T12:55:40.160,,CC BY-SA 2.5,user88
8053,5347,1,The `p_i`s are fixed.,2010-12-11T13:18:02.770,634,CC BY-SA 2.5,
8055,5358,0,What do you mean by $\sum X_i$? How do I get $X_i$ values?,2010-12-11T13:24:43.190,634,CC BY-SA 2.5,
8056,5358,0,Good point! That should be $\sum p_i$. I'll change it.,2010-12-11T13:56:41.137,449,CC BY-SA 2.5,
8057,5375,1,(+1) I remind me of `plot.design()` (but yours in an enhanced version :-),2010-12-11T14:41:44.953,930,CC BY-SA 2.5,
8058,5352,0,"As for a general tutorial, I always like to refer to [A visual guide to item response theory](http://www.metheval.uni-jena.de/irt/VisualIRT.pdf) (it does not cover the MCMC approach).",2010-12-11T14:43:56.373,930,CC BY-SA 2.5,
8059,5353,0,(+1) A recent tutorial with BUGS/R can be found here: [The Use of R and WinBUGS in Fitting Item Response Theory Models](http://idecide.mskcc.org/stats/multi/irt_winbugs.php).,2010-12-11T14:45:06.573,930,CC BY-SA 2.5,
8061,5378,2,"(+1) I gave a link to Robert Pruzek's article, but I didn't know it was available in R.",2010-12-11T18:25:00.263,930,CC BY-SA 2.5,
8063,5380,1,"+1 for the pointer to regularized logistic regression.  It's unclear how one could formally ""look at patterns"" when resampling the ""data many times,"" though.  That sounds a lot like data snooping and therefore seems likely to lead to frustration and error.",2010-12-11T19:07:17.033,919,CC BY-SA 2.5,
8064,5379,0,"+1 I worry that lots of resampling will only be confusing and misleading.  Resampling in a controlled way, via cross validation or a hold-out sample for verification, obviously is not problematic.",2010-12-11T19:08:34.550,919,CC BY-SA 2.5,
8065,5374,1,"You're welcome! BTW, once you're happy that each of your questions has been resolved to your satisfaction you can indicate this by 'accepting' the best answer. This helps to indicate that the question has been resolved in the listing on the front page and elsewhere.",2010-12-11T19:53:30.987,449,CC BY-SA 2.5,
8066,5322,0,"You can try transformations (e.g. using user2040's suggestions), or splines.",2010-12-11T20:02:28.763,279,CC BY-SA 2.5,
8068,5382,1,"What exactly do you mean by ""effective rating""?  If it reflects expertise, why would this have anything to do with an amount of work allotted, which has little or nothing to do with expertise?  What would it mean to rate a user ""relative to the group""?",2010-12-11T20:32:38.830,919,CC BY-SA 2.5,
8069,5380,5,"Feature selection when the selection is unstable will always be a recipe for frustration and error. Using only one sample cuts down on the frustration, but increases the likelihood of error as it encourages you to draw inferences about the relevant features for the problem based on what works best on the particular sample you look at - which is a form of over-fitting.  The re-sampling gives you an idea of the uncertainty in the feature selection - which is often just as important.  In this case we shouldn't draw any strong conclusions about the relevant features as there isn't enough data.",2010-12-11T20:42:30.493,887,CC BY-SA 2.5,
8071,5382,0,"I was with you up to, but not including, the sentence starting ""Now, we want to..."". Could you try explaining what you want to achieve again?",2010-12-11T20:54:56.330,449,CC BY-SA 2.5,
8072,5377,0,"Why do you say ""the code isn't really appropriate if your $p$s are too small""? Seems to work ok to me, e.g. with shape1=1, shape2=999, giving a mean $p$ of 0.001.",2010-12-11T22:07:32.453,449,CC BY-SA 2.5,
8076,5380,0,"Good point; I hate when people only count mean from resampling, it's such a waste.",2010-12-11T23:38:55.780,,CC BY-SA 2.5,user88
8077,5388,0,"Are you saing it's fine to treat ordinal data as interval data in the model? If so, how can I intepret the coefficient if I don't know the ""distance"" between the levels?",2010-12-11T23:49:02.697,82,CC BY-SA 2.5,
8078,5358,0,I wish I could vote up this answer again in light of the edit: that's great information.  I'll post a response to elaborate on why.,2010-12-11T23:52:20.790,919,CC BY-SA 2.5,
8079,5347,0,"In light of the current responses, could you share estimates of (a) the sum of the p's and (b) the sum of their squares?  These values determine your options.",2010-12-11T23:53:49.183,919,CC BY-SA 2.5,
8081,5377,0,"@onestop what I meant was the specific choice of (1,10) written above doesn't give values of $p$ that are very small, to the point that the normal approximation looks pretty good.  If a person wanted the Poisson to come out then they would need to try something else;  it sounds like your choice of (1,999) does a good job, yes?  I had also thought to make $\alpha < 1$, say, 0.25, but I haven't tried that.",2010-12-12T02:55:02.737,,CC BY-SA 2.5,user1108
8082,5393,2,"To some purists this may not be Bayesian. This is actually empirical Bayesian, but it is a quick way to simulate your probabilities in R, without resorting to hyper prior mumbo jumbo.",2010-12-12T03:41:05.133,1307,CC BY-SA 2.5,
8083,5382,0,"Ok, I'll try to explain. Users think they are expert to some level in every component. They can rate themselves pretty good numbers. But not everyone has actually the same level of knowledge. This is what we have to find out with the help of amount of actual work done data. The ranking of a user should be higher if he has done some work in the component than the one who has not - even though the latter might have rated himself higher. This is a comparison of expertise of users based on their perception of possession of skill and actual work done. Is this clear now? Please write back if not.",2010-12-12T04:01:13.650,2344,CC BY-SA 2.5,
8084,5358,0,"...and @onestop for the win.  That's really, really cool.",2010-12-12T04:19:54.180,,CC BY-SA 2.5,user1108
8086,5388,3,"Yes. The coefficient reflects the change in log odds for each increment of change in the ordinal predictor. This (very common) model specification assumes the the predictor has a linear impact across its increments. To test the assumption, you can compare a model in which you use the ordinal variable as a single predictor to one in which you discretize the responses and treat them as multiple predictors (as you would if the variable were nominal); if the latter model doesn't result in a significantly better fit, then treating each increment as having a linear effect is reasonable.",2010-12-12T05:21:16.283,11954,CC BY-SA 2.5,
8087,5358,0,+1 Great! Is there an elaboration somewhere on what `pi`s are considered reasonably small enough?,2010-12-12T08:33:30.483,634,CC BY-SA 2.5,
8088,5392,0,"Don't you mean $Var(w_1 A + w_2 B)$, since you weigh them?",2010-12-12T09:20:46.730,2036,CC BY-SA 2.5,
8089,5392,0,@Raskolnikov: Thank you for pointing that out. I have edited it.,2010-12-12T10:41:41.237,1636,CC BY-SA 2.5,
8091,5363,0,"@mbq: I do not doubt your words, but how do you come to the conclusion that NN are ""obsolete technology""?",2010-12-12T11:15:47.833,264,CC BY-SA 2.5,
8092,5388,0,"I give you +1 because this response sounds fine, and I guess @fgregg would have do the same (with enough rep).",2010-12-12T11:30:14.553,930,CC BY-SA 2.5,
8097,5400,0,"If the MATLAB looks wonky, I apologize; I haven't touched MATLAB in years... XD",2010-12-12T13:08:48.600,830,CC BY-SA 2.5,
8098,5357,1,"I read your answer in full, but I'd vote it up even if all you wrote was the first paragraph: ""I mistrust all but the lowest-level functions in Excel..."" :D",2010-12-12T13:12:01.967,830,CC BY-SA 2.5,
8099,5342,0,"@whuber, @mpiktas: Is there anything in the R documentation that discusses what's under the hood? My hazy memory of *Mathematica* 's implementation when I managed to take a peek (don't ask how) is that the error function routine uses an optimized minimax rational approximant for machine precision, and series (power or asymptotic) and continued fractions for arbitrary precision (definitely not numerical quadrature! ;) ), at least in the real case. That may be one reason for the accuracy difference.",2010-12-12T13:17:51.120,830,CC BY-SA 2.5,
8100,5342,0,"Also, there is also the possibility that the environment did not provide for both $\mathrm{erf}$ and $\mathrm{erfc}$ ; trying to compute one in terms of the other at extreme probabilities is bound to end disastrously.",2010-12-12T13:20:00.197,830,CC BY-SA 2.5,
8101,5337,0,"An other question, do the data need to be positive or normalized, because the representation of no-path can be many values, like ""inf"" or a negative value.",2010-12-12T13:55:00.277,2325,CC BY-SA 2.5,
8103,5399,1,"@asd123 partly, yes, because then $x_1$ and $x_2$ will be independent if and only if they are uncorrelated (look at the variance/covariance matrix of $x^{\mathrm{T}}=(x_{1}^{\mathrm{T}},x_{2}^{\mathrm{T}})$.  If the off-diagonal block matrices are zero then they are uncorrelated).  If they are independent then you can just write out the dot product above, take expected values and you will be all set.  If they aren't independent then do you know anything about the off-diagonal block entries?",2010-12-12T14:42:49.500,,CC BY-SA 2.5,user1108
8105,5399,0,"Sorry, I intended to transpose the other variable. So the result is a matrix. I.e. (Mx1) x (1xM) = (MxM)",2010-12-12T15:03:46.263,,CC BY-SA 2.5,asd123
8106,5399,0,"Also, I don't have the covariance matrix of $x_1$ and $x_2$, though I do know that one of them is has diagonal covariance.",2010-12-12T15:22:58.703,,CC BY-SA 2.5,asd123
8107,5399,0,It's a special case of random Gram(ian) matrices.,2010-12-12T15:55:55.300,2036,CC BY-SA 2.5,
8108,5370,0,"Thanks, Marianna.  I would prefer a solution that relies less heavily on (typically, unjustifiable) parametric models, but will definitely look into it.",2010-12-12T15:59:17.907,96,CC BY-SA 2.5,
8109,5371,0,"Thanks, Manuel.  This actually sounds like a very intriguing possibility.  I need to think about whether the hierarchical partitioning would impose certain constraints that would prevent the algorithm from achieving the optimal cluster partitioning directly under the size constraint.   But intuitively, I can see that this might work.",2010-12-12T16:01:20.307,96,CC BY-SA 2.5,
8111,5358,0,@David B: I've added a bit more to my answer (EDIT 2) partly in response to that.,2010-12-12T17:21:26.937,449,CC BY-SA 2.5,
8112,4362,0,"@Joris Meys Your calculations are off, *way* off.  The chance of getting *exactly* 10,000 heads is 0.564%.  The chance of getting *exactly* 10,093 heads is 0.238%.  (These assume *any* reasonably diffuse and uninformative prior.)  Both are astronomically larger than 2^(-20000) and neither is so close to zero as to be entirely surprising *even if it were predicted in advance* (concerning which see a comment by @G. Jay Kerns following his answer).",2010-12-12T18:12:44.570,919,CC BY-SA 2.5,
8114,4916,0,"+1, But I'm not convinced.  What's special about 10,000 is that it exactly equals the expected number of heads under the hypothesis that the coin is fair.  This fact is independent of any psychology or system of number representation.  The analysis in this response might provide some insight into a situation where, say, 20,005 coins were flipped and 10,000 heads (and therefore 10,005 tails) were noted and somebody's ""intuition"" suggested fakery took place.",2010-12-12T18:18:42.723,919,CC BY-SA 2.5,
8115,5402,1,Where did you hear this statement?,2010-12-12T18:20:24.360,919,CC BY-SA 2.5,
8116,5114,1,In your question you have $P(X = 2) = 0$ and $P(X = 8) = 0$.  Do you mean: $P(X < 2) = 0$ and $P(X > 8) = 0$??,2010-12-12T18:32:26.010,1499,CC BY-SA 2.5,
8118,5395,0,"I thought of using Bayesian methods, but I'm very new to such techniques and still trying to figure out factors and variables. But your suggestions conforms that Bayesian methods may hold the answer. I'm trying :-/",2010-12-12T18:56:54.313,2344,CC BY-SA 2.5,
8119,5401,0,"About self-rating, they can not be completely ignored. A person might not get much work, but has great knowledge of component will lose the ranks significantly if self-rating is not considered.",2010-12-12T19:01:42.967,2344,CC BY-SA 2.5,
8121,5398,3,"(+1) very thorough answer. I'd only add that it is possible to obtain tests that assume only monotonicity of the dose-response relationship without assigning scores to the ordinal variable using isotonic regression., although simulation or permutation methods are usually needed to obtain the distribution of the test statistic under the null. See Salanti & Ulm 2003 http://dx.doi.org/10.1002/bimj.200390012 .",2010-12-12T19:16:46.370,449,CC BY-SA 2.5,
8122,5401,0,"@Nayan: I'm afraid you're in a corner then. You can't exclude the bad variance from the self-rating and keep just the good variance.  In absence of actual performance information you have no source of data other than their self-rating and you say you don't trust the self-rating. The method I outline above doesn't penalize those who don't get much work, but it does somewhat penalize those who get very low amounts of work or no work at all.  One thing you might do is just acknowledge you don't have sufficient evidence to judge some of the people.",2010-12-12T20:33:22.617,196,CC BY-SA 2.5,
8123,5398,0,@onestop Thanks for this reference. I'll look at it.,2010-12-12T21:06:34.210,930,CC BY-SA 2.5,
8124,5020,6,Would you break these out into separate options?,2010-12-12T21:27:42.730,196,CC BY-SA 2.5,
8125,4362,1,"@Srikant @whuber: the combinatorials... you're right. I started from an uniform probability, which is off course nonsense in this case. My bad",2010-12-12T21:34:30.403,1124,CC BY-SA 2.5,
8126,5410,1,"+1 Good finds.  However, most of the differential geometry applications to statistics do not use symplectic geometry.  I think our difficulty in finding references is testimony to how obscure and rare are the applications of symplectic geometry to statistics.",2010-12-12T21:45:08.240,919,CC BY-SA 2.5,
8127,5349,0,"@mpiktas The convergence to normality can be slow.  For example, with p_i = 1/(i+1) (a sequence for which B_n diverges) and n = 2^16 = about 33000, the skewness is still 0.26 and a normal approximation assigns 0.1% probability to negative totals, an impossibility.  Clearly we're starting to approach normality but we're not there yet.",2010-12-12T22:04:14.043,919,CC BY-SA 2.5,
8128,5114,0,"@M. Tibbits: yes, sorry for the error.",2010-12-12T22:39:46.030,1381,CC BY-SA 2.5,
8129,5393,1,Why do you need priors when the p_i are given?,2010-12-12T22:55:38.990,919,CC BY-SA 2.5,
8132,4592,7,+1  I made sure to read this book shortly after it came out.  I get plenty of opportunities to make statistical mistakes so I'm always grateful to have them pointed out *before* I make them!,2010-12-12T23:02:01.107,919,CC BY-SA 2.5,
8133,4574,0,@Andre I think this remark may be closely related to another one offered by @Michael Lew elsewhere in this thread (http://stats.stackexchange.com/questions/4551/what-are-common-statistical-sins/4567#4567 ).,2010-12-12T23:04:48.967,919,CC BY-SA 2.5,
8138,4924,0,"I think this is a very good answer, but I disagree with your second paragraph. I don't think Srikant's original conditional probability is counter-intuitive, and simply because it is a hard question to answer is not an argument against it. I also don't think your uniform probability of lying within 9900 to 10100 makes any sense at all, although it is useful for demonstration purposes.",2010-12-12T23:47:37.370,1036,CC BY-SA 2.5,
8140,5393,0,"@whuber. Thanks, you are right. I missed the fixed part. I thought David is just using the value to be $p_i$ as (q-x)/c and is not fixed. I will edit my answer.",2010-12-12T23:58:43.960,1307,CC BY-SA 2.5,
8142,5351,1,"Question is not clear. What do you mean by ""standard error for a vector""? Typically we want the standard error of a parameter estimate. What parameter are you estimating?  The code you provided produces the Newey West estimate of the squared standard error of the mean. Is that what you want?",2010-12-13T00:23:41.270,96,CC BY-SA 2.5,
8143,152,0,"I was considering asking ""How can I do an MCMC model of the output on lmer for models with random slopes?"" but I'm wondering whether that question is redundant with this one.  That is, is the ""label switching issue"" when using MCMC to estimate mixture models the same sort of problem take makes it so that pvals.fnc() in languageR is able to MCMC intercept models but not models with slopes?  If not, please let me know and I'll go back to asking my initial question.",2010-12-13T01:14:44.917,196,CC BY-SA 2.5,
8144,5419,2,Thanks. Why do you recommend it?,2010-12-13T01:48:45.673,183,CC BY-SA 2.5,
8145,5421,0,"Great points. I was planning to explore myself. And of course, exploration is half the fun. And I agree different packages  teach different things. I agree that many packages would be good starting points. I'll update my question to reflect the idea that multiple packages would satisfy the criteria. But having some concrete suggestions might save myself and others new to R packages some time. I also think that established developers of R packages are in a better position to comment about the merits of a given package for the purposes of learning.",2010-12-13T03:14:27.330,183,CC BY-SA 2.5,
8146,5401,0,"@drknexus: Hmm.. you are right. Still we can experiment with the different models until we reach the one which suits best. Frankly, since I'm not into statistics and maths, I find it bit hard to formulate your idea. Sorry, but can you help by showing the formula or pointing to some system that I can study? Thx!",2010-12-13T03:18:00.247,2344,CC BY-SA 2.5,
8147,5401,0,@Nayan: I need to determine where we can start in terms of common tools/knowledge.  Do you use SPSS or R?  Do you know how to do regression?  Do you know how to calculate a Z-score?,2010-12-13T05:22:12.677,196,CC BY-SA 2.5,
8148,5363,1,"@steffen By observation; I mean it is obvious that no one significant from NN community will come out and say ""Hey guys, let's drop our life work and play with something better!"", but we have tools that achieve same or better accuracy without all this ambivalence and never-ending training. And people do drop NN in favor of them.",2010-12-13T09:18:04.653,,CC BY-SA 2.5,user88
8149,5351,0,"@Cyrus -- By ""vector"" I mean not an `lm` object. I frequently have a vector (let's say a series of stock returns) that I don't want to involve in any regressions (because I don't care about it's projection, other than on 1), but for which I still want the HAC standard error. In this case the parameter estimate is the stock return. The answer above does that, but requires calculating the `lm` object, which I really don't need. So I'm wondering if there's a routine in R that does this without creating an `lm` object.",2010-12-13T11:07:52.410,1445,CC BY-SA 2.5,
8150,5428,0,Thanks! There may not be a more efficient way for me to code this than forming the `lm` object.,2010-12-13T11:11:11.973,1445,CC BY-SA 2.5,
8152,5383,0,"I planned to estimate lambda for each store/SKU separately.   (1) What did you have in mind regarding mixed models, and what are mixed models :)? I must mention that I'm dealing with very different stores (from 50 sq m to 1500 sq m) with very different turnovers. this is reason why I looked each store separately. (2) I planned to adress this problem with modelling sales per hour. You asked me about my statistical background Well, I have some basic knowledge about mathematical statistics (common distributions, statistical tests, etc..).",2010-12-13T11:30:42.877,2341,CC BY-SA 2.5,
8153,5351,0,"Sorry, still not clear: ""In this case the parameter estimate is the stock return."" By that, do you mean the ""average of the stock returns in the series""? If yes, then what you've got is perfectly fine.",2010-12-13T12:59:30.083,96,CC BY-SA 2.5,
8154,5430,0,"@Theodor This doesn't look like a gaussian at all. Not only do you need to make abstraction of values < 4m, but also of the truncation at zero, the long right tail, and the bimodal shape (at approx. 3 and 7m).",2010-12-13T13:38:39.017,930,CC BY-SA 2.5,
8155,5430,0,"Is the peak at around 7m the *only* object? What about the peak around 4m? Is that an artefact you wish to filter out, or can you just ignore it?",2010-12-13T13:46:04.577,449,CC BY-SA 2.5,
8157,5430,0,"@onestop - Yes, the peak aroun 7m is the only object. The peak around 4 m is noise. Yes it's noisy.",2010-12-13T14:14:40.920,1411,CC BY-SA 2.5,
8158,5430,0,@chl - I'll insert an image with the log-normal fit.,2010-12-13T14:15:20.623,1411,CC BY-SA 2.5,
8159,5430,0,"Are there known harmonics for your equipment?  That is, is there a reason to expect that when a signal occurs at 7m that you'd get a sort of echo at 4m?",2010-12-13T14:29:51.310,196,CC BY-SA 2.5,
8160,5430,0,"@drknexus - no, the noise curve looks the same without the object present.",2010-12-13T14:53:45.647,1411,CC BY-SA 2.5,
8161,5351,0,"@Cyrus -- I know that what I have works, but I was hoping that there is a way to calculate the SEs without passing through the `lm` object for the case of a single vector. I guess not. Thanks for helping me clarify my question!",2010-12-13T15:54:28.437,1445,CC BY-SA 2.5,
8162,4916,0,"I fully agree that - as you point out in your answer - it all depends on the a-priori definition of the hypothesis: if you define in advance that by ""faking the experiment"" you mean ""achieving a result for number of heads that is close to the expected value"", then that's a basis for a statistical test with ""number of heads"" as a test statistic. However, without such a-priori clarification, the meaning of ""faking"" and ""special value for number of heads"" remain cloudy, and it's not clear what they have to do with each other.",2010-12-13T16:06:17.147,1909,CC BY-SA 2.5,
8163,5383,0,"If you've got enough data, then estimating lambda for each store/SKU separately should be OK. Mixed models (multilevel, hierarchical) are models that assume that the lambdas for SKUs are themselves random variables, drawn from a distribution. See e.g. Gelman and Hill (2009? I forget the title etc -- something about multilevel regression modeling)",2010-12-13T16:12:24.740,2126,CC BY-SA 2.5,
8164,5430,4,@Theodor; couldn't you just subtract the curve when the object is not present from the one when it is present and start from there?,2010-12-13T17:16:21.237,1390,CC BY-SA 2.5,
8165,5436,0,"I've got plenty of data (almost 300k data points), but I am not convinced there are significant interactions between each predictor. I wanted to explore this possibility, but I guess I bit off more than I could chew.",2010-12-13T17:39:20.587,1973,CC BY-SA 2.5,
8166,5436,0,"@Daniel by all means then give it a go, but you will need to supply some starting points for the algorithm",2010-12-13T17:47:19.697,1390,CC BY-SA 2.5,
8167,5437,3,"slightly simpler would be: `Response ~ .^2 -1, data = DF`, but excellent point!",2010-12-13T18:00:01.120,1390,CC BY-SA 2.5,
8168,5123,0,"was he a statistician? I mean, did he wrote things about estimation?",2010-12-13T18:26:37.463,223,CC BY-SA 2.5,
8169,5431,0,You should give some details about the test you want to do. This would give more adapted and focused answers.,2010-12-13T18:48:44.887,223,CC BY-SA 2.5,
8171,5437,0,"Ok, ran the code based on this suggestion and now I have the error ""Warning while fitting theta: alternation limit reached"". What is the alternation limit and how do I address this?",2010-12-13T20:55:32.380,1973,CC BY-SA 2.5,
8172,5437,0,"As described on the `?glm.nb` help page: ""An alternating iteration process is used. For given theta the GLM is fitted using the same process as used by glm(). For fixed means the theta parameter is estimated using score and information iterations. The two are alternated until convergence of both."" Your model does not converge (you reached the maximum of the theta iterations). You could try setting the `trace` parameter to see what's going on. Or you could use a different package - I believe `VGAM` might use a different algorithm.",2010-12-13T21:04:07.443,279,CC BY-SA 2.5,
8173,5445,0,thanks for your answer - this is exactly what I was looking for. I have fixed the error in my code (rescaled by $\alpha/\beta$. I could rescale the distribution except that to do so would require adding a special case to code that is generalized to use any distribution that is supported by JAGS.,2010-12-13T21:10:05.210,1381,CC BY-SA 2.5,
8174,5430,0,"@Gavin - Well, the curve is a histogram, accumulated data from >50k samples. Unfortunately the sample rate is way too low to do the subtraction you mention. I think a probabilistic approach might be the only way to go.",2010-12-13T21:30:50.580,1411,CC BY-SA 2.5,
8175,5192,6,"While looking for other names in Wikipedia, I stumbled on the fact that Box was a son-in-law of Fisher's.",2010-12-13T22:05:22.627,1764,CC BY-SA 2.5,
8176,4589,6,Not to mention not labeling them at all http://xkcd.com/833/,2010-12-13T22:18:00.607,22,CC BY-SA 2.5,
8177,5446,2,This story is identical to one told about John Milnor.  In Milnor's case there's at least one paper (co-authored with his professor when Milnor was still an undergraduate) to give the story credence.  Have you ever found a reference to the paper(s) Dantzig wrote giving his solutions?,2010-12-13T23:17:07.367,919,CC BY-SA 2.5,
8178,5445,0,is it appropriate to assume that X*a/b also has an F distribution?,2010-12-13T23:49:47.520,1381,CC BY-SA 2.5,
8179,5450,7,"you could probably get a better answer if you provided more details about your experimental design, research question, and statistical model.",2010-12-13T23:52:15.460,1381,CC BY-SA 2.5,
8180,5450,0,"I have survey data, v1 and v2 predict the outcome, as I expected; however, the interaction between v1 (dichotomous) and v2 (5 groups) is not significant -- and (my question) it makes my v1 and v2 direct effects non-significant too. I can't find an example on reporting this in the literature.",2010-12-14T00:03:33.020,2367,CC BY-SA 2.5,
8181,5450,0,"If the v1:v2 interaction is not significant, do you need to have it included in the model?",2010-12-14T01:40:08.247,1118,CC BY-SA 2.5,
8183,5454,0,"Thanks. The devtools wiki, in particular, has lots of great ideas.",2010-12-14T02:29:11.977,183,CC BY-SA 2.5,
8184,5454,0,@Jeromy - Having read your recent posts on reproducible research etc (uber uber helpful so thank you!) means that you'll take to roxygen very quickly. I should have also mentioned that Eclipse supports roxygen syntax and made it a pretty easy transition. The most difficult task I've been overcoming is writing vignettes that are useful and productive. I imagine you're starting from a much higher jumping off point than I am in that regard.,2010-12-14T02:30:30.527,696,CC BY-SA 2.5,
8185,5401,0,I'm so afraid that I do not know much. But I'm willing to learn. Any guidance will be highly appreciated. I've just started studying R.,2010-12-14T02:52:09.460,2344,CC BY-SA 2.5,
8186,5430,1,"@Theodor Does the noise come from the environement, or the radar? Is there a rational (theory based) to have such distribution of noise? I think that you should first dig in that direction before removing data.",2010-12-14T02:52:22.410,1709,CC BY-SA 2.5,
8188,5450,0,Maybe this question is relevant?  http://stats.stackexchange.com/questions/5184/removing-factors-from-a-3-way-anova-table,2010-12-14T03:00:55.870,2310,CC BY-SA 2.5,
8189,5457,0,I threw on the careers tag. I think that fits somewhat. Also thinking this should be CW.,2010-12-14T03:48:47.607,1118,CC BY-SA 2.5,
8190,5446,1,"According to Wikipedia, the first paper was with/by Neyman and the second result was enough to get Dantzig a co-authorship with Abraham Wald. No actual references given, though.",2010-12-14T04:09:51.950,1764,CC BY-SA 2.5,
8191,152,0,"@drknexus I do not know R to comment in your question. Perhaps, you should just post your question with a comment that your qn may be lined to this one.",2010-12-14T04:10:28.170,,CC BY-SA 2.5,user28
8195,5343,0,"@James, why wouldn't a regression model work, then?",2010-12-14T04:38:03.280,1583,CC BY-SA 2.5,
8196,5453,0,"How do you get that $Dr_t=\frac{1}{2}+V$? It is clear that $Er_t=\sin t$, since $\varepsilon\sim N(0,V)$, then $Dr_t=E(r_t-Er_t)^2=E(r_t-\sin t)=E\varepsilon^2=D\varepsilon=V$.",2010-12-14T05:00:51.790,2116,CC BY-SA 2.5,
8198,5453,0,"I believe 'unconditional' here means that $t$ is uniform on $[0,2\pi]$ and independent of $\epsilon$.",2010-12-14T06:16:35.800,795,CC BY-SA 2.5,
8200,5459,8,Your last sentence - brilliant!,2010-12-14T07:02:06.323,253,CC BY-SA 2.5,
8201,5453,0,"correct, E(r) = 0 when taken over t",2010-12-14T07:07:55.863,2379,CC BY-SA 2.5,
8202,5347,0,@whuber: these greatly vary between cases. It's not a one-time module I'm creating (unfortunately).,2010-12-14T07:09:04.663,634,CC BY-SA 2.5,
8203,5458,0,"i don't follow on how p(v|r) = 0, wont the epsilon noise term in r mean that there is uncertainty in v even when you know r and m? even more specifically wont p(v|r) be exactly and only the noise term in r and thus p(v|r) ~ N(0,V)?",2010-12-14T07:13:04.390,2379,CC BY-SA 2.5,
8204,5443,1,"If x1 < .5 AND x2 < .1, then y=""b"". Is that the intended behaviour? (i.e., is the order of lines 7 and 8 significant?).",2010-12-14T08:27:41.083,892,CC BY-SA 2.5,
8205,5457,3,"""Statisticians are people who try to come up with meaningful conclusions from messy data."" - unknown",2010-12-14T08:54:18.220,830,CC BY-SA 2.5,
8206,5430,1,@fRed - The noise is from the environment. I'm sorry that I cannot disclose more details about the experimental setup.,2010-12-14T09:06:37.947,1411,CC BY-SA 2.5,
8207,5457,2,"I think it is two different questions. The first is the one you asked, the second is what to tell your friends. It depends on friends really. I think saying that you will do consulting work, where you consult firms on solving problems with their data, will be enough for the majority of your friends. At least for me it works :)",2010-12-14T09:19:58.290,2116,CC BY-SA 2.5,
8208,5383,0,"Thanks for directions. Just one more thing, does using this mixed models have sense when dealing with high differences between demand, or should one first group stores and then use it on these groups?",2010-12-14T09:23:11.857,2341,CC BY-SA 2.5,
8209,5452,0,"You can write in LaTeX on this site, just put the code in `$ $`.",2010-12-14T09:38:44.987,,CC BY-SA 2.5,user88
8210,5453,1,"If $t$ is uniform, then $p(v|r)=p(\varepsilon)$ is Gaussian, but $p(r)$ is definitely not. Maybe that is your problem.",2010-12-14T09:51:17.627,2116,CC BY-SA 2.5,
8211,5470,8,"(+1): I cannot count the times I thought I've understood something, but then I failed to explain it to someone else in easy words. Example: p-value ;)",2010-12-14T10:21:38.893,264,CC BY-SA 2.5,
8212,5465,0,One example: http://stats.stackexchange.com/questions/4768/amoeba-interview-question,2010-12-14T12:01:50.907,22,CC BY-SA 2.5,
8213,5443,0,"Hi F Tusell - it is significant since this order creates a behavior were a is dependent on x1 and x2, while b only depends on x2.",2010-12-14T12:21:03.913,253,CC BY-SA 2.5,
8214,5430,2,"@Theodor: ""I'm sorry that I cannot disclose more details about the experimental setup."" - then that would make your problem insoluble as it stands. I would suggest that your firm hire and pay for a professional statistician.",2010-12-14T13:07:20.217,830,CC BY-SA 2.5,
8215,5383,0,"With some programs (such as lme4) you can fit *crossed effects*, i.e. fitting a parameter for each store (deviation from average demand), for each SKU (deviation from average demand), and for each store:SKU combination (deviation from expectation based on store and SKU).",2010-12-14T13:07:20.597,2126,CC BY-SA 2.5,
8216,5470,6,"""If you can't explain it to a six-year-old, then you probably don't understand it yourself"" - Albert Einstein. Maybe not that extreme, but you get the point... :)",2010-12-14T13:09:37.643,830,CC BY-SA 2.5,
8217,5474,0,"Yes, I forgot the log on the left hand side, or rather I added unnecessary log on the right hand side. Motivation for $p_i =1/(1+\exp(f(x_i)))$s comes from the fact, that $f(x_i)$ is unbounded, but $p_i$ is constrained to interval $(0,1)$, so we need to transform $f(x_i)$ in order to succesfully model $p_i$. This transformation does exactly that. Look for more explanation in wikipedia under logit function.",2010-12-14T13:24:34.817,2116,CC BY-SA 2.5,
8218,5467,5,"This statement is only true if you assume that the other methods of survey recruitment contain less bias than an all internet based sampling approach. The fact is that all sampling methods contain some amount of bias, and it is incumbent upon the researcher to be cognizant of what those biases may be and to account for them to the extent possible. Where accounting for that bias is not possible, documenting the limitations of your findings is critical. There is research that shows internet based sampling methods induce less or equal bias than other ""traditional"" methods for certain surveys.",2010-12-14T14:01:03.337,696,CC BY-SA 2.5,
8219,5479,1,"As a side note, I'll say that the DLM approach to time-series modeling is nice, but it reminds me of C programming: you can do anything, but you can cut your hand off easily as well. The dlm package makes it easy to assemble your matrices, but you've got to figure out what each resulting column means and if you change something, you can easily get crazy results because the columns changed.",2010-12-14T14:08:16.820,1764,CC BY-SA 2.5,
8220,5476,0,This would probably be subsumed under Chris's answer.,2010-12-14T14:13:43.700,830,CC BY-SA 2.5,
8221,5479,0,Could you please add what are you trying to model?,2010-12-14T14:35:30.327,2116,CC BY-SA 2.5,
8222,5347,0,"@David But can't you give some guidance, such as typical ranges?  For instance, if the sum of the p's ranges between 1 and 100 that's useful information and suggests some efficient solutions, but if it can get up to 10,000 that could exclude some approaches.",2010-12-14T14:48:47.757,919,CC BY-SA 2.5,
8223,5445,0,"@David That's what you stipulated!  The notation you use (and your sample code) imply alpha and beta are constants.  If they are not, then you need to tell us what their distributions are.",2010-12-14T14:51:13.013,919,CC BY-SA 2.5,
8224,5453,0,"@Jesse ""This product will itself be a Gaussian with total variance T = 1/(1/V + 1/(0.5+V)).""  Where does this come from?  If V and 0.5+V are variances, then T should be their *sum*.  If V and 0.5+V are precisions, then this formula gives the *precision,"" which--of course--goes down.",2010-12-14T15:00:10.273,919,CC BY-SA 2.5,
8225,5383,0,thx Ben. you have been of great help.. I think that i will post some questions about mixed modelling when i study it a bit :),2010-12-14T15:25:22.900,2341,CC BY-SA 2.5,
8226,5481,0,"Hmmm. This all seems plausible but more complex than my solution (the comments on the original question suggest that the predictors are both categorical). But as usual, the answer is ""look at the data"" (or the residuals).",2010-12-14T15:34:31.483,2126,CC BY-SA 2.5,
8227,5476,4,Does the correct answer to this one include knowing that there is a controversy about whether fixed marginals make sense and having an informed opinion on the subject?,2010-12-14T15:37:05.537,2126,CC BY-SA 2.5,
8228,3816,7,"@Brandon: if the package author cares enough to guide you, then they have given the answer in a form that will be picked up by citation(""some_package"")",2010-12-14T15:38:46.690,2126,CC BY-SA 2.5,
8229,5479,0,"@mpiktas: If you mean what the data is, it's the weekly average attendance (by month) at a theater. If you mean what I want to learn, it's the overall trend of attendance and the seasonal effects, to help the management understand those dynamics and in preparation for forecasting. Hope that helps.",2010-12-14T15:44:04.463,1764,CC BY-SA 2.5,
8230,5481,1,"@Ben I agree but I don't understand where the perception of ""more complex"" comes from, because analysis of univariate distributions and post-hoc analysis of residuals are essential in any regression exercise.  The only extra work required here is to think about what these analyses mean.",2010-12-14T15:53:13.067,919,CC BY-SA 2.5,
8231,5445,0,"sorry, my previous question was poorly written. Maybe I should have asked: if X~F() is kX~F()? I was wondering if there are general rules about transforming continuous random variables by a constant; it seems that a variable X transformed by k would maintain its shape if its pdf is scaled by k>0, but that this would not hold for k=0 or k<0. (and is this somewhere in my intro probability text by S. Ross? - I cant find it).",2010-12-14T16:11:56.073,1381,CC BY-SA 2.5,
8232,5481,1,"Perhaps by ""more complex"" I just mean ""In my experience, I have seen the issues I referred to in my answer (contrast coding) arise more frequently than those you referred to (non-additivity)"" -- but this is really a statement about the kinds of data/people I work with rather than about the world.",2010-12-14T16:23:41.157,2126,CC BY-SA 2.5,
8233,5458,0,"Sorry, got my notation messed up. Fixed now.",2010-12-14T16:28:17.267,2067,CC BY-SA 2.5,
8234,5349,0,@mpiktas +1 Thank you for quantifying the approximation error.  It's good to see this kind of analysis.,2010-12-14T16:54:46.353,919,CC BY-SA 2.5,
8236,3084,1,"An old topic, but a key point for clarification in the OP: when they say that they have ""rain or not"" data against which to compare the prediction, do they mean ""at my house"", or do they mean ""within the prediction area""?",2010-12-14T17:34:24.573,1764,CC BY-SA 2.5,
8237,5478,1,"This is a good point. I have found in the past that it is very difficult to tell whether a given candidate will work out, unless you have worked with them in the past.",2010-12-14T17:34:37.497,795,CC BY-SA 2.5,
8238,5470,2,"I like ""Explain a p-value"", with or without the ""to a novice"" part.",2010-12-14T17:36:06.637,795,CC BY-SA 2.5,
8239,5485,0,How do I calculate this?,2010-12-14T17:47:58.290,2380,CC BY-SA 2.5,
8240,5485,0,"This looks like a homework question.  What are the rules about homework questions on SE?  I don't see anything in the FAQ -- but I would probably be more willing to contribute if you could convince me this is **not** homework. PS.  If you want to know how to calculate bounds using Chebyshev's inequality, just follow the link posted!  There is an example in the middle of that page ...",2010-12-14T18:12:16.577,2126,CC BY-SA 2.5,
8243,5485,0,@Ben: visit http://meta.stats.stackexchange.com/search?q=homework .  Note that most homework questions are both artificial and specific.  The vagueness of this one suggests to me it originates from a personal interest only.,2010-12-14T18:17:50.447,919,CC BY-SA 2.5,
8244,5461,0,What do you want to know?  I don't see a clear question here.,2010-12-14T18:21:29.367,919,CC BY-SA 2.5,
8245,5485,0,"Believe me, it is purely interest, not homework. But believe what you want.",2010-12-14T18:25:52.530,2380,CC BY-SA 2.5,
8246,5461,0,"I'm trying to figure out which of my sample groups displays different statistical properties (in this case by taking slightly longer) from the rest. There is exactly one group for which this is known to be the case, all other groups come from the same distribution.",2010-12-14T18:28:42.033,1750,CC BY-SA 2.5,
8247,5453,0,"@mpiktas You should post your comment as a solution.  (There are other problems with the calculation in question, but this one alone demonstrates it is invalid.)",2010-12-14T18:32:26.037,919,CC BY-SA 2.5,
8248,5487,1,Surely one of the applets at http://statpages.org/#Power will solve your problem.,2010-12-14T18:35:19.217,919,CC BY-SA 2.5,
8249,5461,0,"@Paul ""Statistical properties"" in what sense?  What role does the timing play: is it an auxiliary variable or is it the variable of interest?  I would find it *much* easier to think about your question if you were to reformulate it without the extensive speculation about types of tests and simply describe as clearly as possible what your experiment consists of and what you want to learn from it.",2010-12-14T18:57:11.460,919,CC BY-SA 2.5,
8250,5349,0,You're welcome :) I lifted the approximation from Petrov's book: http://www.amazon.co.uk/Limit-Theorems-Probability-Theory-Independent/dp/019853499X/ref=sr_1_1?ie=UTF8&s=books&qid=1292354039&sr=8-1,2010-12-14T19:14:19.370,2116,CC BY-SA 2.5,
8252,5461,0,"I have a process which takes a certain amount of time to run depending on the input value. I have 16 potential inputs. Of these, one will make the process run for a slightly longer period of time than the others. I need to identify this input. I am attempting to do so by running many tests, and recording the time my process takes for each test.",2010-12-14T19:44:34.543,1750,CC BY-SA 2.5,
8253,5461,0,"Unfortunately, my data is noisy. This noise manifests itself (in one of many ways) by periodically causing all samples (regardless of input) to take longer to process. Since I am sampling in a round-robin fashion, it seems likely that I may be able to correct for this.",2010-12-14T19:49:45.877,1750,CC BY-SA 2.5,
8254,5468,2,"Its also worth noting that as soon as you changed the model because of the modification indices, you were no longer doing CFA, rather you have moved back into exploratory mode.",2010-12-14T19:50:29.043,656,CC BY-SA 2.5,
8256,5461,0,"Part of the reason that the question is long is that I wanted to show that I've done my homework. It seems rude to just present the problem and ask ""which test should I use?""",2010-12-14T19:53:45.780,1750,CC BY-SA 2.5,
8257,5483,1,"The most information usually hides in the data itself, not the description of it (-;",2010-12-14T19:59:12.550,,CC BY-SA 2.5,user88
8260,5458,0,"luispedro, I believe your answer is correct. my misunderstanding was in thinking that p(v|r) was a Gaussian, but it is not, as you point out, and so the rest of my ""solution"" is invalid. Thank you very much for the help.",2010-12-14T20:22:35.907,2379,CC BY-SA 2.5,
8262,5492,0,Thanks for your quick reply. Yes I know this straight forward computation. But I am not sure is this the lower bound of the entropy or upper bound.,2010-12-14T20:44:42.030,2384,CC BY-SA 2.5,
8263,5492,0,"Bound in what sense?  You need a context here.  What set of values are you trying to bound and how is it defined?  Read the Wikipedia segment I linked to: it has some good examples.  For instance, if you model the image as one realization of a set of possible images, then its entropy equals -1*log(1) = 0.  You can't do better than that!",2010-12-14T20:47:55.777,919,CC BY-SA 2.5,
8264,5493,3,"(+1), excellent program. To spare the reader an extra click, you could link to the new version (G*Power3) which has a GUI for Windows and MacOS X: http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/",2010-12-14T20:50:07.290,1909,CC BY-SA 2.5,
8265,5492,0,"Let us take an example, we have a sensor filed of M*N dimension and sensors are uniformly distributed in grid architecture. let say they are sensing the temperature in the agriculture field. and it is obvious they have spatial correlated data. Assume the they are sensing 35,40,45 and 50 degree C. Now I want to calculate the minimum entropy, is this follow the same formula as you said earlier in your post?",2010-12-14T20:50:57.267,2384,CC BY-SA 2.5,
8266,5492,0,Nope: This is a different probability model.  You need to specify the nature of the spatial correlation and estimate its parameters.,2010-12-14T21:13:07.410,919,CC BY-SA 2.5,
8267,5495,0,"Dave, wouldn't glmmADMB work for this problem (as a start)?",2010-12-14T22:42:11.840,2126,CC BY-SA 2.5,
8268,5471,0,Thanks! I did not get the connection that f(x) was the logit transform. Did you happen to have insight into the terminal node -- should that be self explanatory to me? It looks like GBM uses a regression tree for classification (versus the classification algorithm shown in Elements of Statistical Learning. Is that your understanding as well? Thanks again!,2010-12-14T23:14:31.983,2040,CC BY-SA 2.5,
8270,1370,1,How about multiplying only $X$ by a constant $\alpha$? And what happens if $\alpha=0$ ?,2010-12-15T01:44:13.520,2148,CC BY-SA 2.5,
8271,5493,0,"Oops - thanks for that, caracal.  I just grabbed the link without noticing the content of the page.",2010-12-15T02:34:12.453,71,CC BY-SA 2.5,
8272,5491,1,Thank you for your answer. Where can I learn how to do SMC and which R package would you recommend?,2010-12-15T03:05:43.873,1709,CC BY-SA 2.5,
8273,5467,0,OK. interesting. I guess also that it depends on the type of survey you want to do.,2010-12-15T03:07:12.950,1709,CC BY-SA 2.5,
8274,5463,0,If you want to open an SPSS file without paying for software check out: PSPP: http://www.gnu.org/software/pspp/ or read.spss in R http://stat.ethz.ch/R-manual/R-devel/library/foreign/html/read.spss.html,2010-12-15T04:23:12.637,183,CC BY-SA 2.5,
8275,5503,0,I am asking this here because of its use in Reproducible Research and if I remember correctly this quote was in a presentation related to reproducible research. Apologies in advance if it wasn't meant to be here.,2010-12-15T04:40:50.250,1307,CC BY-SA 2.5,
8276,5463,0,"@Jeromy: I had trouble opening an SPSS sav file in the past, but was unable to solve them via PSPP. Here's my blog post about it: http://blog.bzst.com/2008/09/data-conversion-and-open-source.html",2010-12-15T04:59:51.080,1945,CC BY-SA 2.5,
8277,5463,0,"@Galit Your post is from September 2008. It might be worth trying again. Alternatively, use the read.spss() function in R.",2010-12-15T05:07:52.443,183,CC BY-SA 2.5,
8278,5463,0,"@jeromy  Ah yes, i completely forgot! I haven't used PSPP, but I use read.dta several times a day. thanks for the reminder.",2010-12-15T05:31:07.483,2262,CC BY-SA 2.5,
8279,5471,0,"I am not very familiar with machine learning. For what I gathered from the vignette of GBM, yes GBM uses regression tree for classification, but regression trees are just one of classification algorithms. The terminal node formula is used to calculated the addition to regression function, during the learning process. It is used everytime after classification. I suggest to formulate separate question to get a better answer.",2010-12-15T07:16:09.787,2116,CC BY-SA 2.5,
8281,5117,3,"@Carlos ""undoubtedly the creator"" are you a member of FA (Fisher's Adorators) ? Anyway Gauss introduced least square almost a hundred year before Fisher was born, and Fisher as well as Gauss were inspired by a lot of other very inspired people... it is a long and laborious story and unfortunatly for those who like THX surround movies I don't really think it has its Guru.",2010-12-15T07:30:19.340,223,CC BY-SA 2.5,
8284,5503,1,"I don't really see the point made by this quote. Is there a computational philosophy ? computational poetry ? ... It seems that it would be more correct to say : ""For every field ""x"" that has a ""computational x"" there is a ""computational x"". :)",2010-12-15T08:04:13.200,223,CC BY-SA 2.5,
8285,5504,1,The same question here http://math.stackexchange.com/questions/14387/normal-distribution-probability,2010-12-15T08:13:24.473,2116,CC BY-SA 2.5,
8286,5508,0,"Really 21st century 2000 and onward? Not 20th, 1900 and onward? Could you please cite these references. I think this is a misquote.",2010-12-15T08:24:20.767,2116,CC BY-SA 2.5,
8288,5123,2,"he was a mathematician that was very important to statistics. Half of the men on here lived before statistics was an ""official"" field of study .",2010-12-15T08:48:24.790,74,CC BY-SA 2.5,
8289,5507,4,"Note that you don't need to evaluate the probabilities to answer the question, you only need that ‚Äì3/4 < ‚Äì2/3.",2010-12-15T08:54:45.483,449,CC BY-SA 2.5,
8291,4734,0,"@chl, i don't see how the related question is about EFA at all. the accepted answer is about confirming hypothesized factors, not finding them.",2010-12-15T09:12:42.327,74,CC BY-SA 2.5,
8292,5122,3,"+1 for Francis Galton played a very important role in giving importance to the concept of correlation. However, I found a bit strong the formulation ""creating correlation"". I would quote Galton itself: << ""Co-relation or correlation of structure"" is a phrase much used in biology, and not least in that branch of it which refers to heridity, and the idea is even more frequently present than the phrase; but I am not aware of any attempt to define it clearly >> In : ""Co-relation and their Measurment"" (see here http://galton.org/galton/index.html)",2010-12-15T09:19:36.523,223,CC BY-SA 2.5,
8293,5509,0,"@chl. Thanks. You are a great resource. Did you google this? If yes, can you please enlighten me with your google-fu skills? And yes, I did check these two articles but didn't find the quotation.",2010-12-15T09:21:42.013,1307,CC BY-SA 2.5,
8294,5503,0,"@Robin. I agree with you, but it sounds good for a presentation on reproducible-research and I am about to give one.",2010-12-15T09:23:24.757,1307,CC BY-SA 2.5,
8295,5508,0,"@mpiktas 1900 and onward seems unlikely as the standard probit model dates from 1934/5. What's computationally tractable depends on the available algorithms as well as the available computing power, so 2000 onward seems believable to me.",2010-12-15T09:35:23.250,449,CC BY-SA 2.5,
8297,4734,2,"Ok, maybe I missed something and in this case I'll let @Jeromy add support to his claims, but from what I've read it's mainly about PCA vs. EFA, where in the latter we *assume* a 'reflective' measurement model; I agree that when he said ""if you assume or wish to test  (...)"" we can easily switch to any CFA framework. But in this case, usual PCA has strictly nothing to do with CFA, so the question becomes ill-posed. Do you know this paper from Denny Borsboom, [The Theoretical Status of Latent Variables](http://j.mp/h0KhJo)? Its focus is on LV models, not really PCA, but it is a good paper",2010-12-15T10:09:43.330,930,CC BY-SA 2.5,
8298,5509,0,"@suncoolsu Yes, I was lucky enough with `reproducible research de leeuw ""for every field""` in Google, but as I know you have some background in genetics, I was initially expecting to get a hit from Bioconductor tutorials instead...",2010-12-15T10:13:32.157,930,CC BY-SA 2.5,
8300,5492,0,Entropy calculations for fully speciÔ¨Åed data have been used to get a theoretical bound on how much that data can be compressed. My specified filed is the data from sensor filed. And I want to calculate the minimum entropy (assuming lossless compression) for entire sensor field not for individual nodes.,2010-12-15T13:02:20.707,2384,CC BY-SA 2.5,
8301,5495,0,It might require some tweaking.  glmmADMB is intended to fit a Negative Binomial mixed model. It would be fairly simple to modify it for just a Negative Binomial regression.,2010-12-15T14:16:50.560,1585,CC BY-SA 2.5,
8302,2293,2,"I don't think NIPS is a really a data mining conference, it is a machine learning and (decreasingly) computational neuroscience conference.",2010-12-15T14:42:44.327,887,CC BY-SA 2.5,
8303,5401,0,To Z score a variable use the scale function.  If you scale G1 G2 etc... you'll get a sense of how people rate their expertise compared to others.  If you also scale WD then both stated expertise and work done will be put on the same scale and you can simply sum the scores to get some combined metric.,2010-12-15T16:11:19.477,196,CC BY-SA 2.5,
8304,5401,0,"My approach is something like: scale(resid(lm(WD.G1 ~ W.G1)))+scale(G1).  You'll (of course) have to restructure the data some to make that all work.  Good luck.  If you want clarification on any of these techniques, consider asking a new question.",2010-12-15T16:13:07.510,196,CC BY-SA 2.5,
8305,5503,1,Computational poetry (PDF): http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.4337&rep=rep1&type=pdf,2010-12-15T16:15:23.357,71,CC BY-SA 2.5,
8306,5517,3,"(1) I'm surprised that pvals.fnc still works at all; I thought mcmcsamp (which pvals.fnc relies on) had been non-functional in lme4 for quite a while. What version of lme4 are you using? (2) There is no conceptual reason why having random slopes should break whatever one is doing to get a significance test (3) Combining significance testing with MCMC is a little bit incoherent, statistically, although I understand the urge to do so (getting confidence intervals is more supportable) (4) the only relationship between this Q & the other is 'MCMC' (i.e. none, practically)",2010-12-15T16:42:26.313,2126,CC BY-SA 2.5,
8307,5418,2,"Another way to look at this is to look for particular package *authors*; certain authors are very good at following best practices and writing clear code, in which case you can study all of their materials.",2010-12-15T16:52:47.603,5,CC BY-SA 2.5,
8308,5114,1,"@David After reading this question over many, many times I still don't understand it.  Are you using the two observations and the scientist's opinion to estimate a prior for a Bayesian analysis?  Is your prior going to be based on the scientist's opinion only and then you want to update it with the observations?  What is the distinction between ""unsurprising"" and having zero probability?",2010-12-15T17:24:27.373,919,CC BY-SA 2.5,
8309,5114,0,"@whuber I was originally trying to state the prior using the expert opinion and two data points by summarizing the two data points as a central tendency. But the idea of setting a prior with the expert opinion and updating it with may be a better idea since this approach does not rely on my own choice of a measure of central tendency. ""Unsurprising"" can be interpreted as ""small but nonzero probability"", e.g. $P(X>5)<0.05$ and $P(X>6)<0.01$",2010-12-15T17:32:24.967,1381,CC BY-SA 2.5,
8310,5114,0,"@David Why not just use a ""maximally uninformative"" prior or even a uniform prior then?  (Both are scaled beta distributions.)  You could even fit a maximal entropy solution to the expert opinion information (although I would be disinclined to do that, given that experts notoriously mis-estimate probabilities).",2010-12-15T17:34:50.097,919,CC BY-SA 2.5,
8311,5438,0,Why the downvote?,2010-12-15T17:36:52.963,919,CC BY-SA 2.5,
8312,5446,2,Thanks.  It is interesting how such a story can recur and how it might actually be true in both cases!,2010-12-15T17:39:48.860,919,CC BY-SA 2.5,
8313,5114,0,"@whuber I'd like to stay away from the uninformed priors, and my question to the scientist attempted to take into account the difficulty of estimating probabilities, for example by following ""what is your 95%CI"" by ""What values of X are inconceivable?"" and ""What values of X are conceivable but unlikely"" as well as ""Is X=6 at all possible, even if you would not expect to see it?"". I tried to summarize this information in the question. I want to be conservative but not exclude available information.",2010-12-15T17:40:39.720,1381,CC BY-SA 2.5,
8314,5509,0,"@chl, suncoolsu: The brute-force obvious search on 'For every field ""x"" there exists a field ""computational x""' turns up this thread as hit #1 and Peng's tutorial as hit #2.",2010-12-15T17:44:19.713,919,CC BY-SA 2.5,
8315,5114,1,@David I appreciate your care in eliciting the information.  But assigning specific values of 0.05 and 0.01 to those probabilities is questionable.  That's not your fault; it's just how things are.  We can't expect people to pin down probabilities that well (that's what data are for).  Maybe you would like to represent those with hyperpriors :-)?,2010-12-15T17:54:23.557,919,CC BY-SA 2.5,
8316,5123,0,"good point ! I guess I meant that even before it was official, some mathematicians (like Gauss) concidered statistical problems while I don't really know if Markov did concider statistical problem himself.",2010-12-15T18:01:36.157,223,CC BY-SA 2.5,
8318,5358,0,"@onestop, @David B: I compared the Poisson approximation to the exact results for the sequence p_i = 1/(i+1), i = 1, ..., 2^15. The Poisson parameter is the mean of the p_i, 9.97447. The approximation of individual probabilities is within 5% for sums between 6 and 15 inclusive: that is, within one to two SDs of the mean.  The relative errors increase exponentially outside this interval. The *absolute* errors are all between -0.0026 and +0.0043. Errors in the CDF range from -0.0090 to +0.0074: just under 1%. By any of these measures, simulation with 10,000 trials would be at least as accurate.",2010-12-15T18:18:11.277,919,CC BY-SA 2.5,
8321,5114,0,"@whuber thanks for the suggestion, I can't justify hyperpriors, but it would be sufficient to inflate the variance and overestimate the probabilities in the tails; isn't this done by giving an upper bound rather than an exact value, e.g., to $P(X=5) < 0.05$ ?",2010-12-15T18:39:24.370,1381,CC BY-SA 2.5,
8323,5114,0,"@David I don't see how an upper bound allows you to determine the distribution unless you specify an objective function to be optimized subject to that upper bound as a constraint.  For example, you might want to maximize the variance or the entropy subject to these constraints.  (The latter has more justification.)  Until you provide such a criterion, there seems to be no basis to choose among the many distributional forms suggested by existing responses.",2010-12-15T18:55:04.707,919,CC BY-SA 2.5,
8324,5122,0,"@robin: You could say that Galton did not create the idea of correlation, but did create the first statistical representation of it. Google ""francis galton's account of the invention of correlation""",2010-12-15T19:03:39.697,74,CC BY-SA 2.5,
8325,5118,0,I'm really surprised that Tukey is beating Pearson right now! :),2010-12-15T19:04:14.767,74,CC BY-SA 2.5,
8326,5472,15,"In more academic contexts one may also ask: 'have a look at this model output in this paper *that you (co-)authored*.  Tell me what it means.'  Underwhelming answers are then fatal because there are no unfamiliarity excuses available, yet dismayingly common.",2010-12-15T19:27:49.403,1739,CC BY-SA 2.5,
8327,5358,0,"That's interesting, whuber. I assume you mean the *sum* of the p_i is 9.97447 ? I suspect the relatively few large probabilities affect things - is the approx much better if you start at i=10 or 100 instead of i=1? David B hasn't told us how large his largest $p_i$ is but i suspect it's considerably smaller than 0.5. Starting at i=1 the sum of the squares of the $p_i$s is 0.645 so Le Cam's inequality says the sum of the absolute differences is no more than 1.29. If you start at i=10 that would be 0.19, and for i=100 it would be 0.02.",2010-12-15T19:28:49.937,449,CC BY-SA 2.5,
8328,5114,1,"@whuber Thank you for following up. Now I understand the concept of the maximum entropy solution that you previously suggested, although it would require some learning on my end before I would want to apply it. Your alternative, maximizing variance, sounds sufficiently consistent with my objective and straightforward for me to implement. Thanks again.",2010-12-15T19:37:00.303,1381,CC BY-SA 2.5,
8329,5522,0,@Conjugate (+1) Nice catch!,2010-12-15T19:38:59.297,930,CC BY-SA 2.5,
8330,5521,0,+1 but what would be the rationale for considering a rotation when the GRM actually assumes an unidimensional scale?,2010-12-15T20:00:37.850,930,CC BY-SA 2.5,
8332,5470,1,"this is why cross-validated is great. lots of ""layman"" questions and answers.",2010-12-15T20:17:04.217,74,CC BY-SA 2.5,
8334,592,1,only 3 reviews on Amazon...sketchy,2010-12-15T20:32:33.867,74,CC BY-SA 2.5,
8335,5487,0,"@Roland How could you compute a sample size in a power study from a non-inferiority trial if you don't know the expected difference (or equivalently, the % of patients responding to the new treatment compared to the actual one)?",2010-12-15T20:35:09.287,930,CC BY-SA 2.5,
8336,5114,1,"@David Be aware that a problem with maximizing variance is that the solution will be a discrete distribution.  I expect it to concentrate 0.01 probability at X=8, 0.04 at X=6, 157/300 at X=2, and the rest (32/75) at X=5.  (This variance equals 2.59.)",2010-12-15T20:38:41.280,919,CC BY-SA 2.5,
8337,5494,3,Should we add Hadley Wickham for making GoG possible in R?,2010-12-15T20:41:09.000,930,CC BY-SA 2.5,
8338,5507,0,"Haha, yes of course, noted.",2010-12-15T21:19:16.573,2387,CC BY-SA 2.5,
8339,5526,0,Thank you. In the meanwhile I realized that I also need the number of observations (n). One way is this: dim(model.matrix(model))[1] Can you suggest another way ?,2010-12-15T21:28:32.217,632,CC BY-SA 2.5,
8340,5531,1,Doesn't multiple regression with $N$ observations require an $N$-dimensional TinkerToy set though?,2010-12-15T21:31:05.197,449,CC BY-SA 2.5,
8343,5522,0,Wooo! Hats off to your googling skills :-( Me sux.,2010-12-15T22:21:21.003,1307,CC BY-SA 2.5,
8344,5522,0,@Conjugate and Chl. You guys have put me in a dilemma: whose answer should I accept?,2010-12-15T22:22:01.893,1307,CC BY-SA 2.5,
8345,5521,0,@chl The thought was that some people rotate to simple structure to be able to say things like 'indicators 1-4 measure one thing and indicators 5-11 measure something else' on the basis of the rotated loadings.  The related but not quite identical thought with the IRT approach would be to say things like: 'this Mokken procedure tells me there's a scale underlying indicators 1-4 and another one underneath 5-11 so I'll apply my graded response model to each subset separately'.  Hope that makes better sense.,2010-12-15T22:27:37.213,1739,CC BY-SA 2.5,
8346,5522,2,"@suncoolsu No googling involved - I look through de Leeuw's mammoth preprint list once every 6 months to see what goodies have newly arrived.  If you can bear the fierce matrix algebra and super terse presentation, there's always something interesting to read.",2010-12-15T22:30:25.760,1739,CC BY-SA 2.5,
8347,5509,0,@whuber I was adding an extra Jan de Leeuw to your search and I guess it messed up the results.,2010-12-15T22:35:06.873,1307,CC BY-SA 2.5,
8348,5505,1,"(Sorry I could not include graphs,  I don't have permission to share the data.) So there can be a Kalman filter settling time. That makes total sense, looking at the Big Bash (moving seasonality) effect: the first year has movement in the ""baseline"": in the one case it wanders a bit, and in the second case it has that horrible spike. After that, the baseline (Big Bash indicator 0) runs nearly flat around 0.95, while the spikes (Big Bash indicator 1) go up to 1.10-1.25 (multiplicative effect), which is what I'd expect. The 0.95 baseline I think due to me not centering the 0/1 Big Bash series.",2010-12-15T22:46:50.467,1764,CC BY-SA 2.5,
8349,5498,0,"Yes, point well-taken. I was doing things the hard way to learn DLM's, mainly, since one of the touted benefits of the SS/DLM approach is that you model each element explicitly so your results are more open to understanding/explanation than a more black-box approach like ARIMA. In some ways true, and in some ways not. Especially if a combination of tools (`stl` and `arima`) can give you similar results with less pain and on more familiar territory.",2010-12-15T22:55:31.707,1764,CC BY-SA 2.5,
8350,5505,0,"In my previous note I used ""spike"" in two different ways: 1) the Bad Spike that appeared evidently due to filter settling, and 2) the Good Spikes which correspond to the once-a-year Big Bash indicator being 1. Sorry for any confusion.",2010-12-15T22:57:29.740,1764,CC BY-SA 2.5,
8351,5498,0,"(Not to mention that the `dlm` package won't forecast if you include a `dlmModReg` element, in the current version anyhow. As a follow-on exercise, I am going to see if I can modify their `dlmForecast` function (which is completely in R, thankfully) to handle this specific case.)",2010-12-15T23:00:53.840,1764,CC BY-SA 2.5,
8352,5505,0,"@F Tusell: several good hints in one reply, thanks!",2010-12-15T23:05:03.087,1764,CC BY-SA 2.5,
8353,5522,0,"@suncoolsu I like Jan's work (I was at this UseR also!), and actually I think you really should accept this answer.",2010-12-15T23:44:19.053,930,CC BY-SA 2.5,
8354,5509,1,"@suncoolsu In terms of ""Google fu"", a good approach seems to be to start with broad searches and then narrow them in response to the hits you are getting.  Starting with an extremely narrow search risks overlooking good stuff.  In this case even my search perhaps was too aggressive.  When you consider, though, that every word in that quote can have many meanings unrelated to your intended topic, using the entire quotation seemed to be a reasonable starting point.",2010-12-15T23:46:56.233,919,CC BY-SA 2.5,
8355,5521,0,"Yes, indeed. Thanks for clarifying what I've extrapolated from your last sentence. Still we have no way of linking each latent trait if it happens they are truly correlated (unless looking at MIRT).",2010-12-15T23:48:44.990,930,CC BY-SA 2.5,
8356,5526,1,"@Brani Try `nrow(na.omit(cbind(y,x1,x2)))` (following your notation) or `length(lm.fit$model$y)`, to get the number of observations used when estimating model parameters. BTW, you're better off making a data.frame, e.g. `df <- data.frame(y,x1,x2)`, and then use `lm(y ~ ., data=df)`.",2010-12-15T23:55:17.307,930,CC BY-SA 2.5,
8357,5541,1,+1 It's a straightforward sampling problem and this is a straightforward solution.,2010-12-16T02:36:52.477,96,CC BY-SA 2.5,
8358,5418,1,@Shane Good point. Any suggestions on which package authors might be good to study when first learning to write packages?,2010-12-16T03:20:48.183,183,CC BY-SA 2.5,
8360,5517,0,"The version of lme4 I use depends on the computer I am sitting at. This console has lme4_0.999375-32, but I seldom use this one for analysis. I noticed several months ago that pvals.fnc() was ripping apart the lme4 results after analysis - I built a work around for it at the time, but it doesn't seem to be an issue anymore.  I'll have to ask another question on your 3rd point in the near future.",2010-12-16T04:12:01.880,196,CC BY-SA 2.5,
8364,5552,0,What does NIPS stand for?,2010-12-16T08:23:14.067,449,CC BY-SA 2.5,
8365,5552,0,@onestop link added. NIPS=Neural Information Processing Systems . It is a community (not a system :) ) . But pisk is talking about the conference NIPS 2010.,2010-12-16T08:40:29.463,223,CC BY-SA 2.5,
8366,5516,0,"Wow, I didn't even think about the question from this perspective.  Thanks!",2010-12-16T09:03:09.557,2385,CC BY-SA 2.5,
8369,5554,0,"Ok but i know that the Bonferroni is very conservative. As an exemple, if i'm doing 10 comparaisons, does that mean that for a p < .05 (for a single comparaison) i'll need to find a p < .05/10 on the 10 comparaisons?",2010-12-16T09:22:23.770,2402,CC BY-SA 2.5,
8370,5554,4,Yes. You may try this http://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method. It is less conservative and at least in regression setting for me it gave reasonable results.,2010-12-16T09:32:15.317,2116,CC BY-SA 2.5,
8372,5554,6,"@Richard There's generally a trade-off between FPs and FNs in MCP, the Bonferroni's method being prone to increase FNs, while step-down methods (like the one suggested to you by @mpiktas) will perform better in most cases. As an alternative, you might want to look at permutation-based methods which offer a good compromise for the simultaneous control of FP and FN rates. @mpiktas (+1) In R, relevant methods are found in the base function `p.adjust()`, and the [multcomp](http://j.mp/eVflBe) and [multtest](http://j.mp/hirQrK) packages have great vignettes describing all of this.",2010-12-16T10:13:09.383,930,CC BY-SA 2.5,
8376,5559,3,"Could you provide additional context about this? E.g., do you work with ordinal data such as Likert-type item or discrete scale scores that are are highly skewed, where you want to find some kind of meaningful quantiles on the underlying scale? What's the purpose of this discretization?",2010-12-16T13:25:43.063,930,CC BY-SA 2.5,
8378,5559,0,"Thanks for the reply, Let me re-work the question...I am not sure of the correct way to communicate the question and the correct vocab to use...",2010-12-16T14:29:58.810,2405,CC BY-SA 2.5,
8379,5561,0,+1. Did not know about this Formula feature. Thanks,2010-12-16T15:16:30.320,2116,CC BY-SA 2.5,
8380,5563,4,I would suggest using a symbolic computation package such as Maple or Mathematica.,2010-12-16T15:24:38.713,229,CC BY-SA 2.5,
8381,5563,0,"What would be the use of such a function, i.e. do you want to make symbolic calculus in R?",2010-12-16T15:24:51.747,930,CC BY-SA 2.5,
8382,5446,2,"@whuber Yes, it is! My skept-o-meter says that the details of multiple stories may not be quite right, though. Perhaps both of them solved open problems, but maybe one of them knew the problem was open and the other mistook it for a homework problem. The stories would match in terms of ""naive young student solves open problem, not having realized how hard it really was"", but when the story became legend and someone told it as ""some student ..."", someone else chimed in, ""oh, that was Dantzig"", or ""oh, that sounds like Milnor"" and the stories converged.",2010-12-16T15:44:21.533,1764,CC BY-SA 2.5,
8383,5520,0,"For two balls with probabilities p and 1-p and two draws the expectation equals 4p(1-p), not 1. With n > 2 draws the expectation is np(1-p)(p^(n-2) + (1-p)^(n-2)) because you cannot have two unique balls and the only way to have one unique ball is to draw n-1 of the other color.",2010-12-16T16:21:20.147,919,CC BY-SA 2.5,
8384,5566,3,"Haven't read the Wikipedia entry (which one?), but: this problem is not fully specified. In the absence of more information I would say you should calculate P(H>=14 OR H<=6), i.e. a two-tailed test (the probability, under the null hypothesis (H=10) that the value deviates by as much or more than observed **in either direction** from the expected value under the null).",2010-12-16T16:31:22.490,2126,CC BY-SA 2.5,
8386,5559,1,"The question as it stands is clear enough (although I am wondering how you can characterize merely ""ordinal"" data as ""skewed"").  What we need is a statement of your objectives and how to determine how well any particular binning will meet them.  That is, *why* are you binning?  *How* can that influence the decision or action you ultimately will take based on the data?  It can also help us to understand something about the data generation process: are these data a random sample of something?  A convenience sample?  An entire population?",2010-12-16T16:48:50.887,919,CC BY-SA 2.5,
8387,5545,1,"The answer depends (*strongly*) on whether these repeated correlations are independent.  In a common case you have a set of *m* related variables and you test all m(m-1)/2 mutual correlations, which obviously are not independent.",2010-12-16T16:53:01.147,919,CC BY-SA 2.5,
8388,5564,0,"Great answer - thanks. This will save me a lot of brain power, time, and - most importantly - from potential errors.",2010-12-16T16:54:37.150,1381,CC BY-SA 2.5,
8389,5342,0,"@J. M. I don't think Excel or R are *quite* that bad!  (A standard initial move in any algorithm for erf or erfc is to reduce the problem to a tail probability, subtracting it from 1 afterwards if need be.)  BTW, by ""numerically integrating"" I didn't mean ""numerical quadrature""; I merely meant any method that is a numerical approximation, however that is arrived at.",2010-12-16T17:02:23.230,919,CC BY-SA 2.5,
8390,5568,0,Perfect - I wasn't sure if something so simple existed or not.,2010-12-16T17:30:08.677,2395,CC BY-SA 2.5,
8391,5551,0,"Good analysis, but it leads to computationally intractable algorithms even for moderate numbers of colors and balls.  In some cases that's ok when the approach suggests approximations or asymptotic formulas, but that does not seem to be the case here.  It turns out the essence of this problem concerns the *additivity* of *f*: f(R,G,B,Y) = f(R,G,B) + some function of Y.  If *f* were not additive but could still be computed recursively as f(R,G,B,Y) = g(f(R,G,B), Y) then there might be a dynamic programming solution of complexity O(n*N), which is still pretty efficient.",2010-12-16T17:40:19.407,919,CC BY-SA 2.5,
8392,5548,1,"nice answer. I can see where you are going with this and I think that it is the 'correct' answer. However, I have chosen not to do this (yet) because of the time it would require since I have developed the model (itself heirarchical) to be very flexible and accept a variety of types of data already, and would need to build this generality in to the extended model as well. So your answer is correct although I would really like to know how to calculate a prior as in a discrete step.",2010-12-16T18:16:47.877,1381,CC BY-SA 2.5,
8393,5551,0,"@whuber, I think it is possible to use recursion here. Maybe it is even possible to get analytic solution. I would be pretty hopeful that there is some combinatorical trick which provides it.",2010-12-16T19:27:01.123,2116,CC BY-SA 2.5,
8395,5342,0,"@J. M.: R is open source, so you can find the code pretty easily.",2010-12-16T19:31:12.273,2116,CC BY-SA 2.5,
8396,5551,0,"Yes, recursion works: it leads to the dynamic programming solution.  You won't get a solution that's any simpler than the description of the urn, which involves the array (p_i) of individual probabilities.",2010-12-16T19:48:05.070,919,CC BY-SA 2.5,
8399,5571,0,"You want to compare the models' *fit* using a statistical test, right? What kind of hypothesis would you like to test?",2010-12-16T20:56:07.783,1583,CC BY-SA 2.5,
8400,5573,2,added R tag for appropriate syntax highlighting..,2010-12-16T20:58:32.027,696,CC BY-SA 2.5,
8401,5571,0,"@Firefeather For example, I would like to test whether the fit of `model.nb.inter` is _significantly_ better than that of `model.pois.inter`. Yes, the AIC is lower, but how much lower constitutes _significantly better_?",2010-12-16T21:00:14.590,1973,CC BY-SA 2.5,
8402,5554,0,First thanks to all of you for your answers. I think i'm going to try what @mpiktas suggested with the p.adjust() function and holm method in r.,2010-12-16T21:03:41.283,2402,CC BY-SA 2.5,
8403,5571,0,Note: the answer to this question need not actually include the AIC.,2010-12-16T21:07:01.390,1973,CC BY-SA 2.5,
8404,5571,0,"I don't know the answer to this question, but I can give a start. I know you can use an $F$ test to compare `model.pois` against `model.pois.inter` (and similarly compare `model.nb` against `model.nb.inter`), but I can't guarantee that comparisons between a Poisson model and a negative binomial model would work. I wonder if an $F$ test to compare the variances of each pair would be reliable.",2010-12-16T21:16:51.833,1583,CC BY-SA 2.5,
8405,5571,0,"@Daniel, note that if you're doing more than one comparison between pairs of models, then you'll need to adjust for multiple comparisons (e.g., use [Scheff√©'s method](http://en.wikipedia.org/wiki/Scheff√©'s_method)).",2010-12-16T21:19:43.500,1583,CC BY-SA 2.5,
8406,5571,1,"@Firefeather, yes I'm aware of the need to control the familywise confidence level. Would Scheffe be more appropriate here than, say, Bonferroni?",2010-12-16T21:40:04.240,1973,CC BY-SA 2.5,
8407,5571,0,"@Daniel, I'd have to do some digging to remind myself whether Scheff√© can be applied to Poisson or negative binomial models. However, I do know that other methods are typically preferred over the Bonferroni correction, because it is commonly regarded as too conservative (you give up more power than you should have to).",2010-12-16T21:48:28.563,1583,CC BY-SA 2.5,
8408,5578,0,"(+1) Good to know, thanks. I should look again in Michel Tenenhaus's work -- I'll let you know if I find sth interesting.",2010-12-16T22:01:28.173,930,CC BY-SA 2.5,
8409,5577,1,"(+1) Shouldn't it be `cut2(x, ...)`? Also, I don't think you need to assign a value to `m`: The only constraint in the question was about the No. groups.",2010-12-16T22:08:16.717,930,CC BY-SA 2.5,
8410,5575,1,"(+1) Huh, `Hmisc`... a really great package!",2010-12-16T22:10:01.220,930,CC BY-SA 2.5,
8412,5549,0,"Just to clarify: by ""P(1 Blue)"" you must mean ""probability of exactly one blue and *no other color appears exactly once*,"" etc.  Otherwise the equation is not correct.  Similarly in the second sentence you must restrict to combinations of all other colors where none of them appear exactly once.",2010-12-16T22:16:29.133,919,CC BY-SA 2.5,
8413,5572,1,Is treatment randomly allocated? Is allocation concealed? Are the participants blinded to the treatment allocation? How long do you expect it will take for the treatment to have an effect?,2010-12-16T22:24:16.367,449,CC BY-SA 2.5,
8414,5572,0,"@onestop, some good questions. I see that random allocation would have consequences for choosing a model, so let's assume random allocation, though if you have an model which uses self-selection (or ITT) into the treatment, I am very interested in that answer. Otherwise, say allocation is concealed, the participants know whether they are assigned to the control or treatment group, and the treatment should have an effect within several days, if not sooner.",2010-12-16T23:05:09.203,743,CC BY-SA 2.5,
8415,5566,0,"I suggest you have a look at ""L. D. Brown, T. T. Cai and A. DasGupta, Interval Estimation for a Binomial Proportion, Statistical Science, Vol. 16, pp. 101-117, 2001"". It includes a very nice discussion of interval estimation for the Binomial distribution.",2010-12-16T23:20:24.160,530,CC BY-SA 2.5,
8416,5581,0,+1 Well done!  I'm shocked this hasn't been mentioned yet.,2010-12-16T23:28:00.023,919,CC BY-SA 2.5,
8417,5577,0,@chl - doh! You are right on both accounts. I'm not sure why I thought it was necessary to pass arguments to both `g` and `m`. Code modified accordingly.,2010-12-16T23:29:15.117,696,CC BY-SA 2.5,
8419,5548,0,"Makes sense. Might want to add a bit about that to description, so others don't think I have answered the question you're asking.",2010-12-16T23:41:49.860,1146,CC BY-SA 2.5,
8420,5573,0,@Chase - I didn't realize the tag did that. Good to know!,2010-12-17T00:08:13.273,1583,CC BY-SA 2.5,
8421,5560,0,"The terminology you are using in your question is a little bit nonstandard: for example, your title asks for a ""likelihood"" but your problem description asks for a probability distribution on $(\nu_{1},\nu_{2})$.  In short, a Bayesian solution would start with a (conjugate) Dirichlet prior on $(\nu_{1},\nu_{2})$, then you would observe a multinomial likelihood with $(n_{1},n_{2})$, then the posterior would be Dirichlet again, equivalent to your $F(n_{1},n_{2},n_{3})$, above.  This gives you a density, but it isn't entirely clear from your description that this is what you want.",2010-12-17T01:59:27.050,,CC BY-SA 2.5,user1108
8422,5560,0,"By the way, the solution outlined above is written out in more detail at the bottom of this other problem: http://stats.stackexchange.com/questions/3194/how-can-i-test-the-fairness-of-a-d20",2010-12-17T02:04:31.287,,CC BY-SA 2.5,user1108
8423,5557,0,"So in this particular case, I should calculate a z-score for probabilities of .99 and .01  (i.e. 2.326), then multiply that number by the sampling standard deviation and add or subtract to the mean to get the upper and lower limits?",2010-12-17T03:28:16.503,2385,CC BY-SA 2.5,
8424,5563,0,"chl, yes i think so.",2010-12-17T04:38:58.587,1700,CC BY-SA 2.5,
8426,4595,1,I'd say any model that involves lots of sines/cosines or decaying exponentials would make for badly-behaved problems.,2010-12-17T05:27:30.520,830,CC BY-SA 2.5,
8429,5508,0,"@mpiktas, @onestop has it more or less right. The particular example is from a 1993 paper I was reading which used the explicit phrase, ""intractible."" I've since closed the PDF, but the history of computation intuitively seems to support my characterization.",2010-12-17T07:14:02.440,53,CC BY-SA 2.5,
8430,5586,1,Are you fitting all three models to one data set? Why would youbwant to average those parameters?,2010-12-17T07:14:44.313,25,CC BY-SA 2.5,
8431,5540,0,"Thanks, that's helpful. I'd combine this with @onestop's response for a full answer -- looking into the GHK algorithm provides the math behind what you're explaining. That is, while multidimensional integrals are hard, it doesn't necessarily seem to me that they'd cause extreme issue: we tend to simulate these things anyway, so adding another dimension is just generating one more set of random draws. Looking at how GHK smooths this process, though, helps demonstrate why it takes some time (I think).",2010-12-17T07:18:04.573,53,CC BY-SA 2.5,
8432,5589,0,Thanks Mike.  That's what I thought.  It's just in common practice in econometrics literature to present several models to show that your result is robust across specifications. Your point about omitted variables is well taken.,2010-12-17T07:18:27.473,,CC BY-SA 2.5,Thomas
8433,5589,1,"@Thomas, if it is the same data for all three models, then I do not suggest averaging. The common practice you mention is used  to show that there is no omited variable problem. But since the same data is used to estimate all three models, then you cannot assume independence. If there is no omitted variable bias, then the average will be unbiased, but the standard error of such average cannot be calculated easily, since the coefficients will be clearly correlated.",2010-12-17T07:36:21.993,2116,CC BY-SA 2.5,
8434,5583,1,That's why I asked if the treatment is randomly allocated - if it is then the groups would be expected to be balanced for all known and unknown covariates and matching or propensity score approaches aren't likely to be of use.,2010-12-17T07:44:08.013,449,CC BY-SA 2.5,
8436,4138,0,@gregor if you are not interested in an analytical approach it should be necessary to fix $\Sigma$ and $\mu$  ?,2010-12-17T07:52:51.657,223,CC BY-SA 2.5,
8437,4837,6,"there must be a condition such $<a,b>=0$ missing ? for example if n=2 $a_i=b_i=1$ $X_1+X_2$  and $X_1+X_2$ are not independant.",2010-12-17T08:38:41.850,223,CC BY-SA 2.5,
8440,5557,0,"@user2385 You got the idea, but don't forget that the 1% are distributed in both tails so that you have to work with .005 ($Z$ will be $<0$, $X$ will be $<68$) and .995 ($Z$ will be $>0$, $X$ will be $>68$), for the total area exceeding the limit equals 1%. No need to substract to the mean, you already know that $X=Z\mu +\sigma$. Just use the correct $\sigma$ to go back from the std normal to the sampling distribution of your empirical mean.",2010-12-17T10:48:04.140,930,CC BY-SA 2.5,
8441,5591,0,You would still have to model your experiment somehow to be able to compute the likelihood-function.,2010-12-17T11:06:02.340,2036,CC BY-SA 2.5,
8444,5428,0,I guess the `lm` object is the way to go! Thanks for a great summary... sometimes in the application I get too far from the theory.,2010-12-17T13:33:48.930,1445,CC BY-SA 2.5,
8445,5591,12,"Pete Dixon wrote an article back in 1998 called ""Why scientists value p-values"" (http://www.psychonomic.org/backissues/1631/R382.pdf) that might be an informative read. A good follow-up would be Glover & Dixon's 2004 paper on the likelihood ratio as a replacement metric (http://pbr.psychonomic-journals.org/content/11/5/791.full.pdf).",2010-12-17T14:18:39.503,364,CC BY-SA 2.5,
8447,5592,4,"Actually, most coins are actually very close to fair, and it's hard to come up with a physically plausible way to bias them very much -- see e.g. http://www.stat.columbia.edu/~gelman/research/published/diceRev2.pdf",2010-12-17T14:32:11.207,2126,CC BY-SA 2.5,
8448,5592,10,"Being very close to fair is not the same thing as being exactly fair, which is the null hypothesis. I was pointing out one of the idiosyncrasies of hypothesis testing, namely that we often know that the null hypothesis is false, but use it anyway.  A more practical test would aim to detect whether there is evidence that the coin is significantly biased, rather than significant evidence that the coin is biased.",2010-12-17T15:13:41.473,887,CC BY-SA 2.5,
8450,5592,0,BTW thanks for the link to the paper - I'm sure it will be useful.,2010-12-17T15:29:12.023,887,CC BY-SA 2.5,
8451,5600,4,"I'd just like to reinforce the point about the money (for projects where there is little novelty in the statistics)! The other point I'd make is that for projects that do advance the statistics, make sure you put as much effort into helping your collaborator in publishing his paper in stats journals that you would want from him in publishing the primary result in a biology journal. I know from painfull experience that this does not always happen - I have had a couple of projects where the stats paper went unpublished as my collaborator lost interest :-(",2010-12-17T15:55:49.317,887,CC BY-SA 2.5,
8452,5591,3,"Mike, that looks suspiciously like a good answer to me.  What's it doing in the comments?",2010-12-17T16:20:05.863,71,CC BY-SA 2.5,
8453,2432,0,"This may be a moot/foolish question, but you say ""timing"": is this for use with time series? I believe most of the answers below assume this (""changepoint, etc""), though LOESS can be applied in non-time-series situations, with discontinuities. I think.",2010-12-17T16:52:41.593,1764,CC BY-SA 2.5,
8456,5602,0,example: get a list of  flickr images with the according metadata. someone sorts them in to two groups (the ones he likes and the one hi does not like so much). then i want to find out if there is possible metadata upon which i could have guessed his choice.,2010-12-17T17:24:12.280,2423,CC BY-SA 2.5,
8457,5604,8,The best thing I think for you to do right now is explain what you're trying to do in general (forget about removing outliers explain what your overall goal is). I highly encourage against removing data unless you know it was somehow recorded incorrectly.,2010-12-17T19:11:12.193,1028,CC BY-SA 2.5,
8458,5548,0,"I have updated my question, and have proposed an answer. Having pondered this further, I can see why my approach is not ideal, e.g. I would be loosing information by assuming a distribution rather than using the full posterior mcmc chain; on the other hand, my model is relatively insensitive to the choice of priors so it is difficult for me to justify using the alternative approach.",2010-12-17T20:11:37.143,1381,CC BY-SA 2.5,
8459,5606,0,is `sd.y` the appropriate estimate of $\sigma$ to use in the prior?,2010-12-17T20:23:23.660,1381,CC BY-SA 2.5,
8460,5606,0,this still doesn't answer your question about the mixing of summary statistics and independent observations. It is not clear if the model handles independent observations appropriately.,2010-12-17T20:25:11.943,1381,CC BY-SA 2.5,
8461,5607,0,"""For the continuous parts where Xi‚àâD, the flat sections of F correspond to intervals where Xi has 0 probability so they don‚Äôt really matter when considering F‚àí1(Yi).""
That is it! That was a beautiful answer. Thank you.",2010-12-17T20:41:44.150,2399,CC BY-SA 2.5,
8462,5500,0,"Good point - smoothing the data is the first problem, since I already have some methods that work for the second part. I'll give that a go and see how it does.",2010-12-17T21:06:28.460,1750,CC BY-SA 2.5,
8463,5584,0,"This is along the lines of what I have been thinking. I am considering running a separate  multiple regression model for each subject using all of their pre-treatment data, then putting the result in as covariates in the ANCOVA. But, this seemed a lot like a multilevel model to me. I just can't think of how to structure the model.",2010-12-17T21:44:13.360,743,CC BY-SA 2.5,
8464,5572,0,"@mbq Originally, the question was also tagged panel-data and mixed-model. The reason that I included these tags was because the data clearly falls into these categories. I was hoping that someone with experience using those models would have some suggestion. Economists, especially, often work with time-series panel-data data that changes as the result of some event.",2010-12-17T21:49:03.743,743,CC BY-SA 2.5,
8465,5572,0,"Sorry, fixed.",2010-12-17T21:51:09.310,,CC BY-SA 2.5,user88
8467,5597,1,This sounds like CW material.,2010-12-17T22:57:44.693,919,CC BY-SA 2.5,
8468,4837,2,"@robin good catch.  I've been puzzling over the implicit quantifiers, too.  Unfortunately, all I have access to is that (titillating) quotation from the review, not the book.  It would be fun to find it in a library and browse through it...",2010-12-17T23:03:37.137,919,CC BY-SA 2.5,
8470,5557,0,"Ah I see.  Thanks for all the help, I really appreciate it!",2010-12-17T23:31:41.010,2385,CC BY-SA 2.5,
8472,5599,1,"Who was it that said ""calling a statistician after an experiment has been completed is like calling a doctor for an autopsy"" or something like that?",2010-12-18T08:48:45.893,1795,CC BY-SA 2.5,
8473,5588,1,Thanks for the derivation of likelihood function. It is clear that intercept term cancels out from the numerator and denominator.  Cheers,2010-12-18T13:37:33.717,,CC BY-SA 2.5,user2264
8474,5618,1,"Thanks Dason, that helped.  Also, after reading your reply, it suddenly became clear to me that I am not fully sure how this generalizes in case we are having more factors.  Could you advise?  Thanks again.  Tal",2010-12-18T14:21:25.903,253,CC BY-SA 2.5,
8475,5618,2,You can test multiple contrasts simultaneously.  So for example if A had three levels and B had 2 we could use the two contrasts: C1 = (A1B1 - A2B1) - (A2B1 - A2B2) and C2 = (A2B1 - A2B2) - (A3B1 - A3B2) and use a 2 degree of freedom test to simultaneously test if C1 = C2 = 0.  It's also interesting to note that C2 could equally have been (A1B1 - A1B2) - (A3B1 - A3B2) and we would come up with the same thing.,2010-12-18T14:23:50.153,1028,CC BY-SA 2.5,
8476,5617,3,"I don't have the reputation to allow me to edit, but I think you want $H_0 = \mu_{A1}=\mu_{A2}$ (or $\mu_{A_1}$ if you want a double subscript) [oops, it has automatically tex-ified that:  `H_0 = \mu_{A1}=\mu_{A2}` or `\mu_{A_1}`]",2010-12-18T15:12:04.290,2126,CC BY-SA 2.5,
8478,5617,1,"Oups, didn't see that you are using capital letters to denote factor name *and* their levels --  fix it (following @Ben notation).",2010-12-18T16:12:34.120,930,CC BY-SA 2.5,
8480,5619,0,@clare Interpretation of ESs generally have no mathematical/statistical justification.,2010-12-18T18:01:53.980,930,CC BY-SA 2.5,
8482,5599,3,@cespinoza : Ronald Fisher : To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.,2010-12-18T19:36:56.380,1124,CC BY-SA 2.5,
8483,5599,2,"+1 apart from money, the first and foremost thing I want a researcher to bring me is a hypothesis! and not -again- some garbled dataset and the question : can you get something out of this? Yes I can. No, it won't mean a bloody damn thing.",2010-12-18T19:37:55.660,1124,CC BY-SA 2.5,
8485,5604,0,Do you expect your 'bad data' to be from those who visited the site many times or those who viewed it few times.  Do you have actual access times?  Can you use those to tell the difference between good data and bad data.,2010-12-18T19:41:34.783,196,CC BY-SA 2.5,
8486,5625,1,"+1 Great advice, especially #1.  Concerning #2, be careful not to overestimate what your potential client might know!  (I must have had hundreds of professional conversations in which the client said ""I took *x* statistics courses in graduate school but have forgotten everything""; *x* typically is 3 or 4 and the clients were anywhere from zero to 40 years out of school.)",2010-12-18T21:21:28.177,919,CC BY-SA 2.5,
8487,5619,0,"@chl It is possible your remark could be misinterpreted.  I *think* you are saying that one should seek non-mathematical justification of the interpretation of a particular ES, and *not* that you are saying interpreting an ES is altogether unjustified!",2010-12-18T21:30:17.950,919,CC BY-SA 2.5,
8488,5619,0,"@whuber Indeed, I originally meant that interpreting ESs is highly task or domain-dependent (with some general rules of thumb for e.g., Cohen's d or Fleiss' Kappa), and that a good literature review is generally welcome.",2010-12-18T21:52:25.123,930,CC BY-SA 2.5,
8491,711,1,How much wider are robust standard errors than regular standard errors when the assumptions of OLS are not being violated?,2010-12-19T01:47:04.343,196,CC BY-SA 2.5,
8492,5589,1,"+1.  But what would the interpretation of the average of the $\beta_1$ be?  You might just as well average the price of bananas in Costa Rica with the answers to question #1 in the latest Gallup poll.  And the ""CI"" would require careful interpretation indeed; it certainly doesn't reveal anything about the relationship between $X_1$ and $Y$!",2010-12-19T06:27:41.223,919,CC BY-SA 2.5,
8493,711,1,"Not always wider at all - in fact they can sometimes be narrower. See the latest post on the blog for Angrist & Pischke's book :
http://www.mostlyharmlesseconometrics.com/2010/12/heteroskedasticity-and-standard-errors-big-and-small/",2010-12-19T07:44:32.550,449,CC BY-SA 2.5,
8494,5547,0,"Thanks ""user"".  This doesn't answer my question - but it is a valuable reference for me - thank you.",2010-12-19T08:57:42.537,253,CC BY-SA 2.5,
8496,5605,2,"Hmm, two down votes.  If the answer is so bad it would be nice have some commentary.",2010-12-19T10:57:51.740,1739,CC BY-SA 2.5,
8497,4687,1,"I may be misunderstanding this question, but it seems that you're asking how to specify a prior on a node value when the node has parents.  But you don't need to.  Only nodes without parents need priors.  You may want priors on some parameters that define the *mapping* from parent(s) to child.  But these are priors on parameters not priors on node values not on values.",2010-12-19T13:45:44.123,1739,CC BY-SA 2.5,
8499,5635,0,"I had to munge the data so it was in columns rather than rows, but that's on it's way to working. Thanks.",2010-12-19T16:26:54.410,1531,CC BY-SA 2.5,
8500,5635,0,@sprugman: There should be a setting to choose wether data is in columns or rows,2010-12-19T16:39:36.350,582,CC BY-SA 2.5,
8501,5635,1,"No setting is needed: Select the range of cells in the two rows, invoke the Chart Wizard, specify ""XY (Scatter)"", and you have it.",2010-12-19T17:02:15.140,919,CC BY-SA 2.5,
8503,5607,1,+1 for the rigor and for addressing the second question about the EDFs.,2010-12-19T17:16:37.560,919,CC BY-SA 2.5,
8504,5635,0,"@whuber: I remember to be able to select rows or columns (well, at least OpenOffice has that option). Of course Excel will choose automagically the longest direction, but that's not always what you want! :)",2010-12-19T17:33:51.180,582,CC BY-SA 2.5,
8505,5543,2,"Not really an anwer, but I found this cheat sheet useful, [Some Useful Distributions in Bayesian Analysis with Models from Educational Measurement](http://faculty.ksu.edu.sa/69424/Primary/Distributions.pdf) (R.J. Mislevy, 2001) -- it mainly covers BUGS distributions.",2010-12-19T21:21:11.660,930,CC BY-SA 2.5,
8506,4837,0,This feels like a generalization of G. Jay Kerns' (currently #1) answer.,2010-12-19T21:53:05.980,1670,CC BY-SA 2.5,
8508,1268,1,"it would also help to know _why_ the series are of different lengths.  For example, if they represent the trajectory of a pencil during a handwriting task, say the X displacement while writing out a digit, then you might want to align the time series so that they are the same length.  It is also important to know what type of variation you are interested in retaining, and what you are not.",2010-12-19T22:02:56.880,1670,CC BY-SA 2.5,
8509,5622,1,A really impressive answer Caracal - thank you.,2010-12-19T22:05:45.640,253,CC BY-SA 2.5,
8510,2723,3,"Yes!  Look before you leap.  Please, look at the data.",2010-12-19T22:09:36.317,1670,CC BY-SA 2.5,
8511,5124,0,But was he _really_ a statistician?,2010-12-19T22:27:36.527,1670,CC BY-SA 2.5,
8512,4727,6,I think that to get good advice you will need to describe your model for us.,2010-12-19T22:42:47.683,1585,CC BY-SA 2.5,
8514,2944,0,See here for a recently developed method for approximating the eigenvectors of a large matrices: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ejs/1291126498,2010-12-20T00:09:35.853,1670,CC BY-SA 2.5,
8515,5630,1,"@whuber Well, it was just ""Is there a programming language with built-in overloadable operators so I could implement small error propagation?"", so I have moved it to StackOverflow and that's why it was deleted here -- also the OP edited it on SO, so I think it should stay deleted here.",2010-12-20T00:28:29.947,,CC BY-SA 2.5,user88
8516,904,0,"It‚Äôs not just that it is defined on an ordered set.  Otherwise, how would you distinguish time series analysis from functional data analysis?  I agree with @user549 in that it boils down to the types of questions that are asked.  They are specific to the structure of the data.",2010-12-20T04:04:22.090,1670,CC BY-SA 2.5,
8518,812,2,"Smoothing _is_ a big part of functional data analysis, and it can be converted into a vector mean estimation problem by projection onto an appropriate basis (e.g. Fourier or wavelet), but there are other problems in functional data analysis depending on the functional structure that don‚Äôt translate as easily.  Take for example functional regression where you are interested in predicting a functional response from covariates.",2010-12-20T04:11:15.893,1670,CC BY-SA 2.5,
8521,5603,0,Your example still sounds like a first order Markov chain.,2010-12-20T04:20:26.343,1670,CC BY-SA 2.5,
8522,967,2,This needs to be corrected.  It was Hal Varian that said it. http://www.nytimes.com/2009/08/06/technology/06stats.html,2010-12-20T04:23:04.797,1670,CC BY-SA 2.5,
8526,1150,29,+1 The extremely pathological case where $X = Z$ highlights this further.  $Y = b_0 + b_1 X + b_2 Z + e$ and $Y = b_0 + (b_1 + b_2) X + 0 Z + e$ would be indistinguishable.,2010-12-20T06:41:04.350,1670,CC BY-SA 2.5,
8527,5401,0,Thanks for the comments! I am studying it. But please also have a look at my answer!,2010-12-20T07:27:07.073,2344,CC BY-SA 2.5,
8530,5591,0,"John D Cook posted an excellent answer to a question of mine, which i think you would find interesting: http://stats.stackexchange.com/questions/1164/why-havent-robust-and-resistant-statistics-replaced-classical-techniques",2010-12-20T08:12:15.477,438,CC BY-SA 2.5,
8531,5628,0,Thanks a lot Ben.  This looks like it will point me in the right direction.  I just need to spend some time reading through the help and related articles for MCMCglmm to see if I can wrap my head around what is happening.,2010-12-20T08:26:35.490,196,CC BY-SA 2.5,
8532,4359,1,"Geary, R. C. (1936), ‚ÄúThe Distribution of ‚ÄòStudent‚Äôs‚Äô Ratio for Non-Normal Samples,‚Äù Journal of the Royal Statistical Society, Suppl. 3, 178‚Äì184.",2010-12-20T09:11:23.793,1670,CC BY-SA 2.5,
8538,5648,6,"`extractAIC()` seems to work too (e.g., `extractAIC(m)[2]`).",2010-12-20T14:29:57.597,930,CC BY-SA 2.5,
8539,1286,0,"I'm sure you have some reason for doing this in R, but wouldn't a low-level language like C be more appropriate if you are looking for speed? It would be nastier to code, but would run faster, no doubt.",2010-12-20T14:45:26.710,1118,CC BY-SA 2.5,
8540,5630,0,"@mbq Fair enough.  Although the question was not well formulated, between it and its title it appeared there was interest in error propagation, which would seem to fit with the interests of a stats site.",2010-12-20T15:17:30.000,919,CC BY-SA 2.5,
8541,5382,0,"I cannot find any information in your question that relates ""work"" to ""rating"".  As far as I can tell, ""work"" is something ""allotted"" to a worker and ""work done"" could mean--well, anything.  Without such a connection it's impossible to justify any mechanism to use the ""W"" and ""WD"" values to modify the self-rating data.",2010-12-20T15:23:09.693,919,CC BY-SA 2.5,
8542,5644,1,"I cannot tell whether it's right because I find it meaningless: although you call this formula ""Bayesian"" you haven't provided a probability model, you haven't defined a ""team"", and you haven't (yet) formulated a clear question.",2010-12-20T15:24:16.950,919,CC BY-SA 2.5,
8543,5635,0,"Numbers is pretty limited (at least the ~08 version I was using). I basically had to put the data in one row like: 5, 20, 3, 30, 2, 10, 5, 5. It was dumb, but I got the result I wanted.",2010-12-20T17:03:51.930,1531,CC BY-SA 2.5,
8544,5124,3,That's a tough one...when did statistics become a real field? Many of the fathers of stats were not statisticians.,2010-12-20T18:23:52.967,74,CC BY-SA 2.5,
8545,5433,1,Where can one find this package?,2010-12-20T19:28:07.463,1084,CC BY-SA 2.5,
8546,5433,0,@Adam: I added a link to zoo in my answer.  That page also has a link to zoo's R-forge project page.,2010-12-20T19:51:38.893,1657,CC BY-SA 2.5,
8547,3405,0,"@Matt its only infuriating if you are ""storing"" data in Excel. Its a great feature when using Excel purely as a front end for reporting and chart creation.",2010-12-20T22:02:03.123,1577,CC BY-SA 2.5,
8548,3399,0,"great points, good to hear someone using it the right way, and benefiting.",2010-12-20T22:09:10.433,1577,CC BY-SA 2.5,
8549,5650,3,"What is a ""LOESS/ARIMA model""? Do you meant a nonlinear time trend with ARIMA errors where the nonlinear trend is estimated using LOESS?",2010-12-20T22:33:35.577,159,CC BY-SA 2.5,
8550,711,1,"+1, with @onestop's caveat in comment above that robust se's could very well be bigger or smaller, though typically we expect them to be bigger and thus ""conservative"" wrt type I error. And yes, I always use either heteroskedastic robust or cluster robust se's in my work, as does everyone I know.",2010-12-20T22:39:45.990,96,CC BY-SA 2.5,
8551,5654,2,"It's a great question.  The optimal solution will depend on some additional things, including (a) the principal elements of the cost, including the cost of including one farm and the cost of measuring one subject; and (b) the experimental design.  E.g., will both the treatment and control solutions be applied at each farm (a good choice, but not without its potential problems) or will you be able to apply only one solution at each farm?  Will subjects be clustered (physically) within farms or treated and sampled truly at random?",2010-12-20T22:47:06.967,919,CC BY-SA 2.5,
8552,4687,0,"I think I'm missing a subtlety in the last part of your comment, but also think you may be close to making sense out of my question and resolving my confusion.  With a fully discrete network, my understanding is that the prior probability of a child node is specified in terms of its parents' combined states.  How this notion of dependence carries forward to a network with continuous variables is what confuses me. Does that give you enough to elaborate?  Is there a simple two node (parent -> child) network that exemplifies this kind of 'mapping'?",2010-12-20T23:06:58.403,1474,CC BY-SA 2.5,
8555,5657,0,"Thanks for the observation, John. I have changed it a little to make it a factor. All I did was remake the original vector from the table though, so if there's a way to skip that step, this will be faster.",2010-12-21T03:11:24.053,1118,CC BY-SA 2.5,
8556,5633,0,"if i understand correctly what you wrote, wouldn't $D_n^{(r)}$ be the same for all $r$? also - i can see how to get a monte-carlo estimated critical value for $D_n$; but how about for $D_n^{(r)}$?",2010-12-21T03:36:37.917,1112,CC BY-SA 2.5,
8557,5592,1,"Hi there, maybe I am mistaken but I thought in science, you can never say that the alternative hypothesis is true, you can only say that the null hypothesis is rejected and you accept the alternative hypothesis. To me the p value reflects the chance you will make a type 1 error, i.e. that you will reject the alternative hypothesis and accept the null hypothesis (say p=.05 or 5% of the time. It is important to distinguish between type 1 error and type 2 error, and the role that power plays in your modelling of events.",2010-12-21T07:06:47.437,2238,CC BY-SA 2.5,
8558,5613,0,"Brett, this is interesting and a great example. The model here to me seems to be that people expect the order of heads and tails to occur in a random fashion. For example, if I see 5 heads in a row, I infer that this is an example of a non-random process. In fact, and I may be wrong here, the probability of a toin coss (assuming randomness) is 50% heads and 50% tails, and this is completely independent of the previous result. The point is that if we threw a coin 50000 times, and the first 25000 were heads, provided the remaining 25000 were tails, this still reflects a lack of bias",2010-12-21T07:26:32.933,2238,CC BY-SA 2.5,
8559,5522,0,@chl can you pls tell which useR was this?,2010-12-21T08:39:08.157,1307,CC BY-SA 2.5,
8560,5522,0,@suncoolsu [UseR! 2006](http://www.r-project.org/useR-2006/).,2010-12-21T09:00:30.340,930,CC BY-SA 2.5,
8561,5644,0,Then can you suggest how to clearly declare such things properly and what is missing in my question which is not clear? Thx!,2010-12-21T10:11:10.193,2344,CC BY-SA 2.5,
8562,5382,0,"@Whuber: Though I think I explained it the best I could, I'll try again to define relationship between W and WD. W is work given to a team member, WD is the work done by him. W >= WD >= 0 (in a given component). I'm trying to formulate a method which can take this info in account and use his self-rating to find out his (normalized) rating in the team.",2010-12-21T10:14:47.943,2344,CC BY-SA 2.5,
8564,5662,0,@CHI Thanks. I have studied the caret package and have used it to tune meta parameters. very useful!.,2010-12-21T13:03:16.817,2040,CC BY-SA 2.5,
8565,5662,0,"@chl +1, nice one. I wrote my function solely because code a[levels(a) %in% lf] <-""Other"" does not work, so I assumed that factor level change is complicated affair. As usual it turned out, that R is not complicated, I am :)",2010-12-21T13:04:19.987,2116,CC BY-SA 2.5,
8566,5650,0,"sorry I mean LOESS or ARIMA model. e.g I use LOESS to find the the residuals as:   residuals(loess(x ~ time)).  because the data x is vector with 1000 values, which updates every 15 minutes.  How can I efficiently get the residuals, but not rerun the whole datset as input everytime?  as only 1 value update everytime, the other 999 values is still same as last time.",2010-12-21T13:16:40.080,2454,CC BY-SA 2.5,
8567,5665,1,"In particular, 10%, 20%, 30% etc. are called Deciles.",2010-12-21T13:41:18.463,582,CC BY-SA 2.5,
8568,5665,0,"I've read about percentiles, quartiles and deciles, but they did not quite seem to fit the bill. Maybe I've misunderstood? I thought that a percentile was a specific value, like that the median is really the 2-quantile. I'm looking for a name for a certain range of values. What I'm looking for is to tell people something like ""in the ordered list of mean scores for all the talks, your talk ended up in this 'box'"".",2010-12-21T14:01:27.003,2470,CC BY-SA 2.5,
8569,5657,0,Thanks to everyone who responded. My R is weak but the ability to do this with so few lines of code is testament to how powerful it is and makes me want to learn.,2010-12-21T14:16:25.880,2040,CC BY-SA 2.5,
8570,5665,1,"Deciles are then what you need. Households for example are divided into deciles according to their income. Poorest are in the first, richest in tenth.",2010-12-21T14:20:47.933,2116,CC BY-SA 2.5,
8571,5633,0,@ronaf Could you provide more detail about $D_n^{(r)}$?  What is R?  I don't see that permuting the categories does anything at all: notice that no permutation will change the sum of absolute differences of their counts.,2010-12-21T14:30:13.353,919,CC BY-SA 2.5,
8572,5666,0,"Good start (+1).  The formula needs some fixing.  The variance of the difference of the proportions equals p0(1-p0)/n0 + p1(1-p1)/n1.  With n0 = n1 = n and p0 = .5, p1 = .2, that equals 0.41/n, implying n = 41 z^2.  Note, too, that this is a one-sided test, so z = 1.65 works fine.  (Some precision is needed here because the result is sensitive to the squaring of z.)  Regardless, these calculations establish that approximately 10^2 *independent* subjects will need to be tested *if* this model is correct.  (I do *not* expect the bleach or the new formula to have ""random"" effects.)",2010-12-21T14:39:05.797,919,CC BY-SA 2.5,
8573,5665,3,"@mpiktas Right.  So to give a clear, final answer to the original question, we might say the first category is the ""top (i.e., tenth) decile,"" the next category consists of the ""eighth and ninth deciles,"" the third category is the ""fifth through seventh deciles,"" and the last category contains the ""lowest four deciles"" (or, to be more circumspect, is ""not in the top six deciles"" or even ""did not make it in the top 60%"").",2010-12-21T14:52:48.250,919,CC BY-SA 2.5,
8574,5666,0,"Mike Anderson and Whuber, thank you for your suggestions.  You asked good questions, which I'll try to answer.  The poultry experimental units will be random, not from clusters.  As of now, cost is not a consideration.",2010-12-21T14:55:55.090,2473,CC BY-SA 2.5,
8575,5659,0,"... and then, of course, we conclude $|\rho| \le 1$.  This illustrates the answer provided by @vqv.  @ronaf: why didn't you upvote @vqv's answer then??",2010-12-21T15:15:03.940,919,CC BY-SA 2.5,
8576,5662,0,"@mpiktas Thx. You can work at the vector level with e.g., `a[as.character(a) %in% lf] <- lf[1]; a <- factor(droplevels(a), labels=c(""Other"",LETTERS[3:5]))`.",2010-12-21T15:27:38.873,930,CC BY-SA 2.5,
8577,5645,0,"(+1) Thanks for this, especially Friedman's paper.",2010-12-21T15:30:05.797,930,CC BY-SA 2.5,
8579,5630,0,"@mbq A clarification posted on the SO version makes it abundantly clear that the OP is about error propagation.  There is no longer any question that this is relevant to stats.  Apparently there's no longer any option for us to vote to migrate it back, but it definitely belongs here and should be re-opened.",2010-12-21T16:12:30.660,919,CC BY-SA 2.5,
8580,1286,0,"@Christopher: Yes, it probably would be faster in C but since the outputs are being used in R it seems clearer from a reproducible research standpoint to use only R code.  Moreover, (the real answer) I'm more comfortable in R than C and haven't learned how to call C code from R yet.",2010-12-21T17:02:54.573,196,CC BY-SA 2.5,
8581,5054,0,"... or maybe the question is asking whether models of heteroscedasticity exist, such as GARCH.",2010-12-21T17:28:30.517,919,CC BY-SA 2.5,
8582,5664,0,"Please pay close attention to dmk38's comments, below. *Who* is rating each speaker, and how much variability there is in each speakers' ratings make a difference. That 4.5-rated person has a higher mean than someone rated, say 4.3, but depending on whether their audiences had significantly different personal grading criteria and how much variation in grades there was around each mean, there may be no real-life difference between the two. The numbers will look authoritative, since you ran them through a computer, but the question is: are they saying what you think they're saying?",2010-12-21T17:32:42.900,1764,CC BY-SA 2.5,
8583,4172,0,"I'm just a learner around here, but I noticed that you say that you matched up the ""closest geographically"" point from setup 2 to setup 1, seemingly without knowing that the two devices actually were at the same location when the respective measurements were taken. That seems to my naive eye to be cheating in favor of setup 2. I defer to my statistical superiors on that question.",2010-12-21T17:46:43.577,1764,CC BY-SA 2.5,
8585,5644,0,"Yes: please provide a probability model and define terms like ""team"" and ""effective rating.""  If we have to guess at the meanings our chances of answering the intended question go down.  (Normally it's too much to ask for an explicit probability model, but since you've already done a probability calculation you must have one!)",2010-12-21T18:31:37.020,919,CC BY-SA 2.5,
8586,5662,0,"+1. a[levels(a) %in% lf] <-""Other"" sure saves a ton of code lines. Clever and efficient!",2010-12-21T18:40:09.760,1118,CC BY-SA 2.5,
8587,5630,0,"@whuber I still think it is more about operator overloading than about error propagation (not to mention what I think about error propagation), but you have it opened again.",2010-12-21T18:41:42.583,,CC BY-SA 2.5,user88
8590,5629,0,"On the SO version of this question (which I can no longer find), you had a link to an ""error propagation"" site.  Could you provide that here?",2010-12-21T18:46:18.983,919,CC BY-SA 2.5,
8592,5259,2,This is practically identical to http://stats.stackexchange.com/questions/726/famous-statistician-quotes/2044#2044 but has been attributed to a different person!  Who's right?,2010-12-21T18:49:57.643,919,CC BY-SA 2.5,
8593,5259,3,"Google searches suggest, by 20 to 1, that Easterbrook originated this quotation, but he didn't really start writing until after Coase was quoted in print.  The best evidence I can find concerning this (and it's still not very good) is Coase's Wikipedia page, http://en.wikipedia.org/wiki/Ronald_Coase .",2010-12-21T18:58:58.760,919,CC BY-SA 2.5,
8594,5629,0,Found the SO version!  http://stackoverflow.com/questions/4483266/implementing-error-propagation,2010-12-21T19:03:36.400,919,CC BY-SA 2.5,
8595,5670,0,"I only have a german text-book as a reference for it, but the authors state that a) Spearman's $r_{S}$ systematically overestimates true $\rho$ for normally-distributed data, and that b) $r_{S}$ is difficult to interpret when not near 0, 1 or -1. (B√ºning H & Trenkler G. 1994. Nichtparametrische statistische Methoden. 2. Aufl. Berlin: de Gruyter. With regards to a), they also state that typically, Kendall's $|\tau| <$ Spearman's $|r_{S}|$.",2010-12-21T19:15:05.250,1909,CC BY-SA 2.5,
8596,5670,0,"I edited too slowly... With regards to a), B&T cite Walter (1963) and also state that $r_{S}$ is only nonparametric under independency. Walter E. 1963. Rangkorrelation und Quadrantenkorrelation. Z√ºchter Sonderhefte 6: Die Fr√ºhdiagnose in der Z√ºchtung und Z√ºchtungsforschung II, 7-11.",2010-12-21T19:24:17.833,1909,CC BY-SA 2.5,
8597,5665,1,"But the wikipedia article states that a decile is ""any of the nine values that divide the sorted data into ten equal parts, so that each part represents 1/10 of the sample or population"" - thus a single value, not a range. The R function ¬¥quantile¬¥ also returns a single value. Is it not more correct to say for instance ""between the 8th and 9th decile""?",2010-12-21T19:29:57.490,2470,CC BY-SA 2.5,
8598,5115,5,If it weren't CW it would have to be closed as subjective and argumentative!,2010-12-21T19:37:00.287,919,CC BY-SA 2.5,
8599,5668,0,"The conference had almost 500 participants, at all time divided into three tracks. Not all of our participants gave us ratings, of course, not all participants were at a talk at all times, and they were not evenly distributed among the tracks. I'd say we have about 50% feedback rate, based on the numbers. Some tracks has as much as ~89 feedbacks, some as low as ~22. Since I do not have full access to the raw data, I cannot do stuff like eliminate outliers etc. When will the data be unsafe to draw conclutions from?",2010-12-21T19:45:17.407,2470,CC BY-SA 2.5,
8600,5676,0,"ok thanks, stating the statistic was a mistake. I meant z or t table.",2010-12-21T19:46:53.867,1395,CC BY-SA 2.5,
8601,5675,0,Is this what you're looking for? http://www.weibull.com/DOEWeb/confidence_intervals_in_simple_linear_regression.htm,2010-12-21T19:51:02.257,1118,CC BY-SA 2.5,
8602,5629,0,Copied SO version; I'll ask mmyers to remove it from SO.,2010-12-21T20:03:01.953,,CC BY-SA 2.5,user88
8603,5629,0,K‚Äãa‚Äãb‚Äãl‚Äãa‚Äãm‚Äãm‚Äão.,2010-12-21T20:39:37.457,877,CC BY-SA 2.5,
8604,5668,0,"I don't grasp the layout fully. But if you had groups of evaluators judging multiple speakers, you should perform an appropriate inter-rater reliability test (e.g., ICC) for each group. If that test shows sufficiently high agreement among evaluators--and you believe they were competent to judge (presumably; why else ask? still you are assuming the validity of your measures)--compute the mean scores & standard errors for each speaker. You must use judgment at that point to decide how much imprecision you are comfortable with in making your rankings--much as you would when grading on a curve.",2010-12-21T20:42:52.650,11954,CC BY-SA 2.5,
8605,5630,0,"@whuber and others... Thank you. Sorry for the poorly worded question/title. I debated here v. SO, as the question is both about error propagation AND software implementation of it. Anyway, this is an excellent answer. Thanks.",2010-12-21T20:44:34.300,957,CC BY-SA 2.5,
8606,5671,0,I assume that you meant the steps reversed. But if this is so simple why do we need Gibbs sampling?,2010-12-21T20:52:05.417,2116,CC BY-SA 2.5,
8608,5665,0,"@mranders, decile means both the single value and the range. So you can say both, between 8th and 9th decile or in 9th decile.",2010-12-21T20:58:30.887,2116,CC BY-SA 2.5,
8609,5662,0,"But note that a[a==""a""] <- ""Other"" will not work, which for me is quite natural to assume that it should. Especially since a[a==""a""] is perfectly valid.",2010-12-21T21:04:01.210,2116,CC BY-SA 2.5,
8610,5662,0,"@Christopher, a[levels(a) %in% lf] <-""Other"" will not work, levels(a)[levels(a) %in% lf] <-""Other"" on the other hand will.",2010-12-21T21:06:06.777,2116,CC BY-SA 2.5,
8615,5677,0,"@whuber: Much obliged. It's quite embarassing that I have been able to contribute little to Cross Validated (compared to other Stack Exchange sites), considering I got my undergrad degree in the university's department of statistics. ;-)",2010-12-21T21:27:14.153,1583,CC BY-SA 2.5,
8616,5677,0,"@Firefeather Be patient. Simple mistakes are no problem; as you've seen, you can edit them ;-).  But sooner or later you'll find some questions where it's obvious you have something valuable and special to contribute.  I look forward to seeing them.",2010-12-21T21:49:10.720,919,CC BY-SA 2.5,
8617,5679,0,"(+1) It seems to me this advice is complementary rather than countervailing.  After all, using Student's t matters when the degrees of freedom are small and that's usually where you can't detect heteroscedasticity anyway.  So with lots of data you can check assumptions and protect yourself somewhat with robust SEs and, furthermore, you won't really care whether you're using t tables or z tables.",2010-12-21T21:54:16.813,919,CC BY-SA 2.5,
8618,5680,2,"Could you link to some of those threads?  My gut instinct is ""NOOO no no no"", but I'm hardly an expert and I'd be interested in reading some of those arguments.",2010-12-21T21:54:31.573,71,CC BY-SA 2.5,
8619,5680,4,You sure can't trust any p-values derived from F distributions with those kinds of data!,2010-12-21T21:56:42.740,919,CC BY-SA 2.5,
8620,5677,0,"@whuber: Why, thank you! It may come easier once I start back at school again to get a PhD in Statistics, too.",2010-12-21T22:15:52.380,1583,CC BY-SA 2.5,
8621,5680,3,"Many cite the ANOVA's robustness as justification for using it with non-normal data. IMHO, robustness is not a general attribute of a test, but you have to precisely state a) against which violations of its assumptions a test is robust (normality, sphericity, ...), b) to what degree these violations have no big effect, c) what the prerequisites are for the test to show robustness (large & equal cell size ...). In your split-plot design, I'd love to have somebody state the precise assumptions of sphericity & equality of covariance matrices. It's already mind-boggling in the 2-factorial case.",2010-12-21T22:26:45.443,1909,CC BY-SA 2.5,
8622,5670,0,"@caracal I guess Siegel's book is also a good reference. I also know that Kendall's $\tau$ has better statistical properties (but this has already been discussed in previous Q&A). But it seems to me that the question has to do with the interpretation of an ES, and although we're not informed of the sample size, it is likely to be small. Anyway, your proposed transformation should work fine and there exist transformation for converting an $r$-based measure into a Cohen's $d$ one. But as you didn't address directly the question, I wasn't sure of where you want to go. Now, I've +1 your response.",2010-12-21T22:32:57.727,930,CC BY-SA 2.5,
8623,5662,0,"@mpiktas `a[a==""a""]` is _subsetting_; `a[a==""A""] <- ""B""` would be valid because ""B"" is one of the level of `a`, but not ""Other"". `a[as.character(a) %in% lf]` is _indexing_ (we create a boolean vector of length `length(a)`). This is the vector view. Now, using `levels()` or `relevel()` allows to directly work with attributes of `a` (check `attr(a, ""levels"")`).",2010-12-21T22:44:26.387,930,CC BY-SA 2.5,
8624,5670,0,"Thanks! Actually, I deliberately tried to answer only the first part of the question (significance test of 0.33), since I wasn't sure about using $\rho$ as an ES measure.",2010-12-21T22:48:05.973,1909,CC BY-SA 2.5,
8625,5670,0,"@caracal Ok, I mostly focused on the other point. The references I gave are mainly opposing the Hypothesis Testing approach *vs.* interpretation of ES measures. I still prefer giving confidence intervals for $\hat r$ or $\hat\rho$, but anyway I hope the asker would find good directions for further investigations.",2010-12-21T22:55:26.573,930,CC BY-SA 2.5,
8626,5680,1,"Some thoughts: is there any way you can aggregate any of your groups (e.g. averaging individual values) and still do meaningful analyses? [Murtaugh, Paul A. 2007. Simplicity and Complexity in Ecological Data Analysis. Ecology 88, no. 1: 56-62. http://www.esajournals.org/doi/abs/10.1890/0012-9658%282007%2988%5B56%3ASACIED%5D2.0.CO%3B2] I would be quite worried about a situation with 90% of observations equal to zero ...",2010-12-21T22:56:33.883,2126,CC BY-SA 2.5,
8627,5662,0,"Put differently, you can't change the levels of an existing factor by changing its (vector) elements.[1] I think factors are a bit harder to grasp precisely because attributes usually don't play such a big role in working with R, but with factors, they often need to be explicitly manipulated. [1] Ok, maybe `drop=TRUE` for subsetting counts.",2010-12-22T00:39:54.143,1909,CC BY-SA 2.5,
8628,5681,1,"Are you asking for multivariate hypothesis tests? For your first question, if you're looking for a multivariate equivalent to a t-test, you might consider Hotelling's T-Squared. For the second question, are you trying to compare k groups? Is MANOVA appropriate?",2010-12-22T03:08:51.987,1118,CC BY-SA 2.5,
8629,5659,0,"@whuber - on further consideration, i realize that my answer - to be useful - requires knowing the covariances between the coordinates of $x_1$ and $x_2$ [$\Sigma_{12}$, in @vqv's notation]. $\Sigma_{12}$ is not mentioned specifically in the OP's question - so perhaps it should not be thought of as part of the known 'data' for the problem [a point that managed to escape my notice, i'm afraid]. in that case, @vqv's answer - and yours - are certainly more germane. i am upvoting both of your answers - and thanx for making me stare a bit harder at the issues involved.",2010-12-22T04:23:49.250,1112,CC BY-SA 2.5,
8631,54,0,For cross-reference: it was also [asked](http://math.stackexchange.com/questions/15098) @ m.SE ...,2010-12-22T07:00:40.410,830,CC BY-SA 2.5,
8632,5686,7,"I'd say NA is more of a ""placeholder""; NaN is for (IEEE) arithmetic purposes.",2010-12-22T07:04:17.460,830,CC BY-SA 2.5,
8633,5686,0,@JM. good way to summarize.,2010-12-22T07:09:59.047,1307,CC BY-SA 2.5,
8634,5662,0,"@chl,  do not follow, how a==""a"" is different from as.character(a) %in% ""a""? They both create boolean vectors of length length(a).",2010-12-22T07:20:56.040,2116,CC BY-SA 2.5,
8635,5686,0,"and there is also Inf, which stands for expressions like for instance 1/0...",2010-12-22T08:43:34.067,573,CC BY-SA 2.5,
8636,5592,3,"For frequentist tests, I would use an even weaker statement, which is that you either ""reject the null hypothesis"" or you ""fail to reject the null hypothesis"", and don't accept anything.  The key point being that (as in the case of the biased coin) sometimes you know a-priori that the null hypothesis is not true, you just don't have enough data to demonstrate that it isn't true; in which case it would be odd to ""accept"" it.  Frequentist tests have type-I and type-II error rates, but that doesn't mean that they can talk of the probability of a particular hypothesis being true, as in the OP.",2010-12-22T08:50:08.897,887,CC BY-SA 2.5,
8638,5662,0,"@mpiktas Right. Wrote to quickly (neither is of length `length(a)`): `a[a==""a""]` is _subsetting_, `a==""a""` is _indexing_. The point was about assigning an unknown level to `a` which is a factor. The `%in%` operator or simply `intersect()` is cool when you want to match more than one case, e.g. `as.character(a) %in% c(""a"",""c"")` is equivalent to `a==""a"" | a==""c""`.",2010-12-22T09:52:13.793,930,CC BY-SA 2.5,
8639,5665,1,The decile value is just the border of the range. Values higher are within this range (until the next decile).,2010-12-22T09:52:19.127,144,CC BY-SA 2.5,
8643,5654,0,I have integrated your extension answer into the question.,2010-12-22T11:05:49.980,,CC BY-SA 2.5,user88
8644,5691,1,"An answer to the last comment in your code: `library(reshape); untable(data.frame(X1=gl(2,1),X2=gl(2,2)), c(12, 5, 7, 7))`.",2010-12-22T11:31:25.010,930,CC BY-SA 2.5,
8646,5646,0,Thx. But both bootstrap estimates have a normal distribution and I'd really like to give a joint probaility for the two occuring at the same time.,2010-12-22T13:18:29.787,1291,CC BY-SA 2.5,
8647,5671,1,"No, i meant steps 1 and 2 in the order I wrote. After all, the distribution of y is specified conditional on X, so you must generate an X before Y. As for Gibbs sampling, that is a more complicated solution meant for more complicated problems. Yours, as you describe it, is pretty straighforward, IMHO.",2010-12-22T14:15:25.387,2472,CC BY-SA 2.5,
8648,5695,0,@CP I like your Bayesian approach. You also justify you name!,2010-12-22T14:27:37.543,1307,CC BY-SA 2.5,
8649,5680,0,@whuber I have since tried a bunch of transformations. A square root transformation results in a distribution with a skew of 2.77 and kurtosis of 6.21. Would ANOVA results be more trustworthy from this distribution? Are there any guidelines for how far a distribution can deviate from normality and still work with ANOVA?,2010-12-22T14:43:52.083,2322,CC BY-SA 2.5,
8651,5680,3,"@Matt It sounds like 90% of the residuals are zero.  If that's the case, no transformation is going to make the residuals remotely close to normal.  Simulation studies have shown that p-values from F-tests are highly sensitive to deviations from normality.  (In your case it's fairly likely that some denominators in the F-tests will be zero: a sharp indicator of how far things can go wrong.)  You need a different approach.  What to do depends on *why* so many residuals are zero.  Lack of sufficient precision in measurements?",2010-12-22T15:11:49.190,919,CC BY-SA 2.5,
8652,3174,4,A valuable resource for learning about generalized linear mixed models (GLMMs) and how to fit and interpret them is maintained by Ben Bolker and others at http://glmm.wikidot.com/,2010-12-22T16:28:32.137,1979,CC BY-SA 2.5,
8653,5691,1,Do you need to reproduce any of the marginals in the table?,2010-12-22T16:32:00.587,919,CC BY-SA 2.5,
8654,5680,1,"@whuber I guess you could say that the 0s resulted from lack of precision. Quite simply, it was an easy task with high accuracy that pushed the variance of interest into response times. Another approach I've taken is to use GEEs with a negative binomial distribution. I was not able to get a model that was equivalent to the ANOVA to converge, but I was able to make multiple reduced models with key variables of interest. Does this sound like a reasonable approach?",2010-12-22T16:35:13.283,2322,CC BY-SA 2.5,
8655,5680,2,"@Matt that's sounding more appropriate, assuming your data are counts.  Another attractive consideration is a zero inflated negative binomial response (http://www.ats.ucla.edu/stat/r/dae/zinbreg.htm ).",2010-12-22T16:42:37.240,919,CC BY-SA 2.5,
8656,5650,0,"one possible method, maybe use the first 1000 values to predict the next 1000 values(although LOESS only support predict 4 values), then calculate the residual as the difference between actual value and corresponding predict value.Then retrain the model every 1000 values  However, this is not the original LOESS model I want at all :-(",2010-12-22T16:59:26.460,2454,CC BY-SA 2.5,
8657,5693,0,"OK, you're right in my misuse of terminology.  I've been incorrectly calling them priors or sometimes 'conditional priors' because I specify the conditional probability table as part of modeling the network. I'll update my question so hopefully future travelers aren't as confused.",2010-12-22T17:06:29.787,1474,CC BY-SA 2.5,
8658,5693,0,"That's nice. Does the other part of the answer helps you, too ? Or does it need some refinement ?",2010-12-22T17:31:18.213,264,CC BY-SA 2.5,
8659,5700,3,What is the policy on cross-posting? The exact same question was asked on math.stackexchange.com: http://math.stackexchange.com/questions/15214/finding-the-change-point-in-data-from-a-piecewise-linear-function,2010-12-22T17:58:31.557,2116,CC BY-SA 2.5,
8662,5693,0,"Yes, I think you've nailed it, but I haven't had time to fully digest it yet.  I'll come back and mark accepted when I've had time to digest.",2010-12-22T18:22:25.503,1474,CC BY-SA 2.5,
8665,5696,0,It looks like situation where functional data analysis methods can be applied. Did you consider them?,2010-12-22T19:57:29.070,2116,CC BY-SA 2.5,
8666,5708,3,"Not to mention that every individual rater probably creates their own scale, with their own variations on the differences between levels... this stuff is absolutely loathsome. Gary King's papers on [anchoring vignettes in survey research](http://gking.harvard.edu/category/research-interests/methods/survey-research) provide a good overview of the problems here, and an interesting solution (that can't be applied to this particular case).",2010-12-22T20:08:07.280,71,CC BY-SA 2.5,
8667,5706,0,"You can use LaTeX for formulas, enclose them in dollar signs as in normal tex document. What is the reasoning of your second-last equation. As far as I understood $S_2$ is the number of trains with non-failed units, so your equation does not make sense then.",2010-12-22T20:14:49.960,2116,CC BY-SA 2.5,
8668,5708,0,"Holy crap, this just gets worse... Unfortunately, due to the way we counted up the scores from the feedback forms, I do not have the data for how many of each score the speakers received. I think I will include a disclaimer in the feedback email, detailing some of the uncertainties of the data interpretation.",2010-12-22T20:21:53.237,2470,CC BY-SA 2.5,
8669,5664,0,"Thank you for all the help, people. I really appreciate it! Also, thanks for pointing out the drawbacks with this way of measurement. I have lots to learn about this stuff.",2010-12-22T20:27:28.523,2470,CC BY-SA 2.5,
8671,5703,2,"Only knowing $\rho_{1}, \rho_{2}$, the interval containing $\rho(x, y_{1} + y_{2})$ must include $\rho_{1}$ and $\rho_{2}$: for each $y_{1}, y_{2}$ could have very small values (while having any rank-order), and thus simply ""jitter"" the values in $y_{1}$ when added to $y_{1}$. Thus the rank-order of $y_{1}$ wouldn't be affected. I don't know if the interval can exceed the $\rho_{i}$.",2010-12-22T20:36:33.967,1909,CC BY-SA 2.5,
8672,5706,0,"Thanks for TeXing that up for me. This is my first time on a TeX-enabled SE, and the WYSIWYG for TeX wasn't working for me, so I left it out. Also, I fixed the last two equations, in which I accidentally put in $S_2$ where I meant $S_0$.",2010-12-22T20:39:10.920,,CC BY-SA 2.5,user2491
8673,5703,2,"@caracal Good observations.  The interval definitely can be wider than the $\rho_i$: just consider the case where both correlations are zero.  The correlation with the sum can easily be nonzero--it can range all the way from -1 to 1.  E.g., x = (1,2,3,4,5); y1 = (3,-10,2,10,1); y2 = (-8,9,-2,-9,4); y1+y2 = (-5,-1,0,1,5) has $\rho_1=\rho_2=0$ but $\rho=1$.",2010-12-22T20:57:32.963,919,CC BY-SA 2.5,
8674,5703,0,"@whuber: this seems to imply only trivial bounds exist (i.e. $l = -1, u = 1$). Perhaps I have to throw another constraint at the problem.",2010-12-22T21:13:35.290,795,CC BY-SA 2.5,
8675,5703,0,"@shabbychef No, you have posted a nice problem: it's not trivial.  In case $\rho_1 = \rho_2 = 1$, for instance, the *only* possibility is $\rho = 1$.  I suspect the bounds are nontrivial except when $\rho_1 = \rho_2 = 0$; they must get narrower as $\rho_1$ and $\rho_2$ approach $\pm 1$.",2010-12-22T21:21:29.073,919,CC BY-SA 2.5,
8676,5703,0,"@whuber Neat idea. Geometrically, I think I understand your principle in terms of moving points into the desired quadrants of the centered scatter-plot. However, I think there's another issue: Spearman's $\rho$ only assumes ordinality, thus sums or differences like $y_{1} + y_{2}$ are not a-priori meaningful.",2010-12-22T21:22:46.420,1909,CC BY-SA 2.5,
8677,5703,0,"@Caracal There's nothing faulty about @shabbychef's formulation.  Spearman's rho can be computed for so-called ""continuous"" data just fine.  You have actually put your finger on the entire difficulty, which is characterizing the interaction between addition and ranking.  One potential application is this: suppose $y_1$ is a response variable and $-y_2$ is a vector of unknown errors.  What can we say about the Spearman correlation between $x$ and the *true* values $y_1 - y_2$ given a measured correlation with $y_1$ and an assumed correlation with $-y_2$?",2010-12-22T21:28:16.120,919,CC BY-SA 2.5,
8678,5709,0,"Thanks! In my case, the true value of $n$ is on the order of 30, and the worst case I've seen is $S_2$ around 20 and $S_1$ around 10, making $\hat{S_0}/(S_1 + S_2)$ on the order of a few percent. Does that qualify as $S_2$ being small enough to make $\hat{S_0}$ very biased? I'm not sure how to best express my loss function, but it's negligible for $(S_0 - \hat{S_0})/n$ of, say 5% or less, but rises nonlinearly as that error gets away from 5%. The bottom line is that if the method of moments estimator is ""close enough,"" I'd be biased toward using it rather than something more sophisitcated.",2010-12-22T21:32:56.593,,CC BY-SA 2.5,user2491
8679,5692,1,"Hi Suncoolsu - you got it right, that's what I was wondering on how to do.  And my question was - is it a reasonable solution for that question.  From your answer, I deduce that you believe it is valid to do it this way (Also chl directed me on where to look for implementing this within R, which is great).  Thanks for you input.  Tal",2010-12-22T21:59:09.853,253,CC BY-SA 2.5,
8680,5709,1,@Isaac Simulations suggest the bias in your case isn't bad at all: around 0.5 on average.  It also seems the variance of this MM estimator is approximately the square of the estimate itself.  For your data this means (if the model is correct!) that you can be confident $S_0$ is between 0 and 4 or so.,2010-12-22T21:59:28.760,919,CC BY-SA 2.5,
8681,5694,0,thanks mpiktas. chl suggestion is also a good direction on how to handle such data.,2010-12-22T22:00:00.130,253,CC BY-SA 2.5,
8682,5691,0,"Whuber - That was a question I didn't think about.  Let's say we wish to create a CI for the chi square statistic using bootstrapping, should we then preserve the marginals of the bootstrapped table to be the same as the ones we had?",2010-12-22T22:17:18.410,253,CC BY-SA 2.5,
8683,5703,1,"Here‚Äôs another pathological case.  Suppose that $x = y_1$ and $y_1 = -y_2$. Then $\rho(x, y_1 + y_2) = 0$, but $\rho_1 = 1$ and $\rho_2 = ‚àí1$.  

It might be enlightening to think about a  simpler,  probabilistic version of the problem.
Let $X$, $Y_1$, and $Y_2$ be random variables, each with marginally Uniform distributions.  Now let $G$ be the CDF of $Y_1 + Y_2$.  What can we say about $Cov(X, G(Y_1 + Y_2))$ based on $Cov(X,Y_1)$ and $Cov(X,Y_2)$?",2010-12-22T23:56:50.410,1670,CC BY-SA 2.5,
8684,5703,0,"One more note.  Since $G$ is a non-decreasing function, it feels like there might be a subadditive relationship between $Cov(X, G(Y_1 + Y_2))$ and $Cov(X,Y_1)$ and $Cov(X, Y_2)$.  The relationship would have to take into account the signs of the the latter two covariances.",2010-12-23T00:10:28.500,1670,CC BY-SA 2.5,
8685,4687,0,"Pick some function f(p1,p2,c), and define conditional probability as f(p1,p2,c)/sum_c f(p1,p2,c). Replace sum with integral when c is continuous",2010-12-23T00:12:19.447,511,CC BY-SA 2.5,
8686,5703,0,@vqv: this is a good point. One of the copula inequalities might be relevant here.,2010-12-23T00:32:22.637,795,CC BY-SA 2.5,
8687,5691,1,"@whuber - I was wondering why did you ask this question. I am curious, can you pls tell. The only reason I can think of is bootstrapping might not produce the same marginals. Am I thinking correctly.",2010-12-23T00:37:46.000,1307,CC BY-SA 2.5,
8688,5713,0,"What can I say, all your points are valid, so it is possible that the authors of the articles in question are doing the wrong thing. Without more context it is impossible to say anything concrete.",2010-12-23T07:19:41.880,2116,CC BY-SA 2.5,
8689,5646,0,"@Misha, why do bootstrap estimates should have normal distribution? Usually bootstrap is used when the distribution is unknown. Or you know that the distribution is normal and you use bootstrap to estimate the covariance matrix?",2010-12-23T10:38:31.373,2116,CC BY-SA 2.5,
8690,5646,0,"I was uncertain of the distribution, but as it happens both A & B turn out normally distributed. However what I really would like to know is to estimate the probability of both A & B occuring at the same time.",2010-12-23T11:09:44.180,1291,CC BY-SA 2.5,
8691,5712,0,"You made a (critical) typo. ""Na*N* is 'Not a Number'""",2010-12-23T11:22:54.047,190,CC BY-SA 2.5,
8693,5640,0,This is helpful. I've added more details in the question description. Any thoughts?,2010-12-23T15:49:09.393,446,CC BY-SA 2.5,
8694,5696,0,"I had thought about functional data analysis, but it's not an area that I'm very familiar with. I'm working thru Ramsay and Silverman's book now.  But I don't immediately see how to deal with a multinomial outcome variable...?",2010-12-23T16:58:08.457,,CC BY-SA 2.5,dschulman
8695,5712,0,"@Peter Smit: ouch, thanks. my X server is lagging keystrokes...",2010-12-23T17:23:56.437,795,CC BY-SA 2.5,
8696,5712,2,Several language have equivalent constructs. For instance PHP and Javascript have null and NaN.,2010-12-23T17:55:50.470,582,CC BY-SA 2.5,
8697,5720,3,"Unfortunately, you have to have at least 50 reputation in order to post comments.  The purpose of that restriction has never been clear to me, as commenting seems like a pretty low-cost way for new users to start participating.",2010-12-23T17:56:48.360,71,CC BY-SA 2.5,
8698,5687,21,"Of note, `is.na()` returns `TRUE` for both NA and NaN, which differs from `is.nan()` e.g. `is.na(c(0/0,NA))` *vs.* `is.nan(c(0/0,NA))`.",2010-12-23T18:04:40.117,930,CC BY-SA 2.5,
8699,5717,3,"Perhaps we have different takes on partialling out? In my mind it would involve i) e1 <- resid(lm(y ~ Sex)), ii) e2 <- resid(lm(X ~ Sex)), and finally iii) lm(e1 ~ e2) . i) residualises y with respect to Sex, ii) residualises the other covariates (X) with respect to Sex, iii) fits the partial regression. In that case it doesn't matter how one codes Sex. In the above, we aren't really interested in the effect of Sex nor the interpretation of the coefficients. If we are model building, i.e. controlling for Sex as a Null, then how we parametrise the model *is* an important consideration, however.",2010-12-23T18:38:43.347,1390,CC BY-SA 2.5,
8701,5722,0,But the real problem is that ranks don't add.  See my comment to the question.,2010-12-23T19:59:20.577,1670,CC BY-SA 2.5,
8702,5722,0,@vqv but if $y_1$ and $y_2$ are permutations of the integers $1\ldots n$ then they are exactly the same as their ranks.,2010-12-23T20:52:56.087,449,CC BY-SA 2.5,
8703,5723,1,"Good info. But you may want to update that link to Wicklin's book =) For example, to: http://support.sas.com/publishing/authors/wicklin.html",2010-12-23T21:02:39.000,1934,CC BY-SA 2.5,
8704,5722,0,"half the sum of permutations need not be a permutation; But this is very close, and answers the question for Pearson, I believe.",2010-12-23T21:23:34.157,795,CC BY-SA 2.5,
8705,5696,0,"Is there a gold standard available for those measurements (i.e., do you know when the event(s) of interest occur(s))? How many coders are included in this study? How many distinct events can we expect?",2010-12-23T21:37:27.713,930,CC BY-SA 2.5,
8706,5690,0,"Could you pls give some indication about sample sizes, time frame, % survival, etc. so that we get a better idea of the design of your study?",2010-12-23T21:40:50.050,930,CC BY-SA 2.5,
8707,4752,4,"...recapitulating prior work by Finch and Chater (1992, 1994), Sch√ºtze (1993), and others. HAL, LSA, and other prior art work by generating a similarity measure for words by computing their contextual similarity.  (This is Shephard's 'second order' similarity: 'first order' similarity is when word a occurs near word b; 'second order' similarity is that word a occurs near the same sorts of words as word b does).",2010-12-23T23:07:24.413,1739,CC BY-SA 2.5,
8708,4752,3,"Comparing and contrasting: For LSA the context is the complete document. For HAL and others it is a textual window surrounding the target word. LSA measures distance in a linear subspace extracted via SVD/PCA, and the other deal with distances in the original space of surrounding word counts.",2010-12-23T23:09:12.307,1739,CC BY-SA 2.5,
8709,5202,2,"and maybe also Bishop, Svensen and Williams for putting SOM on a clean probabilistic footing via the Generative Topographic Mapping.  (And I didn't down vote anything either...)",2010-12-23T23:13:30.263,1739,CC BY-SA 2.5,
8711,5720,0,I was somewhat frustrated with that restriction myself.  I remember having something along the lines of 49 reputation and wanted to make a comment and I couldn't but I definitely knew it wasn't appropriate to be a standalone answer...,2010-12-24T05:35:20.887,1028,CC BY-SA 2.5,
8712,5727,1,I know MAD (http://en.wikipedia.org/wiki/Median_absolute_deviation) is used a lot for wavelet shrinkage.,2010-12-24T06:31:31.627,223,CC BY-SA 2.5,
8715,5731,0,"Hi, thank you very much for the reply. A(t) is time varying coefficient associated with state variable X(t). For some textbooks, when they deal with time varying coefficient, they assume the coefficients is a function of constant variable lamda, however, in this particular question, I don't know the function. Should I treat A(t) as another state variable which follows AR(1) process? Thank you.",2010-12-24T13:05:33.003,2510,CC BY-SA 2.5,
8716,5731,0,"@user2510: You may include in the state whatever you please, but you cannot (within the framework of the standard Kalman filter) include non-linear functions of the state in the observation equation; and $A(t)X(t)$ would be non-linear if both $A(t)$ and $X(t)$ are components of the state.",2010-12-24T13:59:47.977,892,CC BY-SA 2.5,
8717,5708,0,"That's too bad. If it's any consolation, Amazon uses the average of 5-star ratings to rank their data (as one option), so you've got good company.",2010-12-24T16:23:23.600,2489,CC BY-SA 2.5,
8719,5730,0,"I think for practical purposes what I said about above and below the diagonal can be ignored.  My problem isn't really a classification problem, but in terms of reducing the matrix to a rectangular matrix for other purposes your approach may be useful.",2010-12-24T17:19:43.093,196,CC BY-SA 2.5,
8720,4372,0,"Naively, it appears that Y_(t-1) could be moved into an expanded theta_t and with an expanded Phi (include F, essentially), could keep updating the Y_(t-1), as a seasonal ARIMA would work when translated into DLM... Except that it would not include e_t, while the actual Y_t does. :-(",2010-12-24T17:51:19.310,1764,CC BY-SA 2.5,
8721,5124,1,"I know my choice is kind of arbitrary, because many are important but this is my favourite one, and his method allowed me to do lots of things.",2010-12-24T18:35:22.593,1808,CC BY-SA 2.5,
8722,5742,0,I think you have your 'quadratic' and your 'cubic' models mixed up.,2010-12-24T19:36:50.053,1028,CC BY-SA 2.5,
8723,5742,0,"@David If the $g_i$ are the models and $Y$ is the data, your question already is Bayesian, because it requires a prior and Bayes' theorem to compute $\Pr(g_i|Y)$.",2010-12-24T19:41:29.160,919,CC BY-SA 2.5,
8724,5742,2,"@David It's legitimate to use AIC for *nested* model comparison (as in null - linear - quadratic - cubic), but not (AFAIK) for completely different models, such as ""monod"" versus ""log"".",2010-12-24T19:42:48.927,919,CC BY-SA 2.5,
8727,5742,0,"@whuber sorry, I got these backwards since my recent work has been in a Bayesian framework that considers data fixed and models variable. But the underlying point- that I would prefer to retain the original likelihood-based approach, remains.",2010-12-24T19:56:17.127,1381,CC BY-SA 2.5,
8728,5742,0,@David With the fix it looks like Bayes' Theorem but you have omitted the (requisite) prior probabilities for the models.  Once you specify that prior you can obtain (in the usual way) a posterior distribution for the models based on the data.,2010-12-24T19:58:11.020,919,CC BY-SA 2.5,
8729,5742,0,"@whuber I think that is exactly my problem. I was just hoping that there was a more direct way to estimate these probabilities from AIC (or another penalized likelihood like BIC if that is more appropriate for this set of models). I think that Aikake weights provide something along these lines, but I am still not sure of the interpretation. I'll post an answer describing this in more detail.",2010-12-24T20:28:39.653,1381,CC BY-SA 2.5,
8730,5740,0,(+1) There's some code available in the Scipy [Sandbox](http://projects.scipy.org/scipy/browser/trunk/Lib/sandbox?rev=2844) which is based on Fortran code available on [Netlib](http://www.netlib.org/).,2010-12-24T21:41:56.333,930,CC BY-SA 2.5,
8731,5740,0,"Cool, thanks. I bet it comes from the same original work.",2010-12-24T22:00:35.133,334,CC BY-SA 2.5,
8732,5736,0,"I'm interested in sampling possible returns for a given asset.  Since the distribution isn't normal AND the assets are correlated over time, that creates a challenge in choosing a distribution.    I'm exploring monte carlo methods for portfolio optimization.",2010-12-24T22:05:32.213,2566,CC BY-SA 2.5,
8733,5743,0,This approach seems implicitly to place a uniform prior on the models you are using.  Whether that's appropriate or not depends on the application.,2010-12-24T22:59:23.563,919,CC BY-SA 2.5,
8734,5743,0,@whuber That is pretty much what I am going for; I have plenty of data for informative priors on these models but that would be another paper altogether. Thanks for the feedback.,2010-12-24T23:06:26.630,1381,CC BY-SA 2.5,
8735,5596,0,"hadn't heard of him before, but I'll check him out. Thanks for the intro!",2010-12-24T23:59:42.247,74,CC BY-SA 2.5,
8736,5690,0,are there censored values in the data - other than for the largest values?,2010-12-25T04:31:42.477,1112,CC BY-SA 2.5,
8737,5685,0,@mbq thanks for having the patience to correct my lazy typing.,2010-12-25T04:39:36.560,1112,CC BY-SA 2.5,
8738,5685,0,"and what if the distribution is not multivariate normal? Also, the vector might have 0 in certain locations (not necessarily the same for all subjects e.g. X1=0, and X7=0 for one subject and X3=0 for another etc.). Is it still ok to use MANOVA?",2010-12-25T05:03:29.853,,CC BY-SA 2.5,dana
8739,5742,0,"I'm going to agree with whuber. If you put a non informative discrete uniform prior on the models, then the posterior likelihood of model k given data is proportional to the likelihood of the data under the model k. This seems much  simpler then trying to derive an approximation of posterior model likelihoods from AIC.",2010-12-25T05:31:36.920,2144,CC BY-SA 2.5,
8740,5736,1,"@Noah,  have you considered various stochastic volatility models, such as GARCH?",2010-12-25T14:44:30.263,2116,CC BY-SA 2.5,
8742,5748,1,"@Nice. I think you mean ""A and **B** are not correlated"" in the very last part of your last sentence.",2010-12-25T21:05:10.417,1307,CC BY-SA 2.5,
8743,5749,1,Taking the means of the x's is probably not the smartest thing to do if you have high skew. I would choose to regress y on x instead of y on mean(x). This will allow you to determine outliers during your residual analysis.,2010-12-25T21:54:25.523,1118,CC BY-SA 2.5,
8744,5752,4,"The reason why I wanted to do it like this was to have an open mind towards understanding my data, so maybe in a way I am fishing for correlations, which I did not think of before, for the purpose of getting enlightened. I am certainly not doing this to satisfy my boss or something abitrary. I would rather not get into the specifics of the data, as I want a general answer to this question, so I can use it in all situations in the future.",2010-12-25T22:43:11.960,888,CC BY-SA 2.5,
8745,5751,2,"Unfortunately, I am not an R user.  So, the codes above mean less to me than they mean to you.",2010-12-25T23:07:06.773,1329,CC BY-SA 2.5,
8746,5748,0,"Yes, Nico with suncoolsu correction... this is a reasonably good explanation.  You are partially describing Path Analysis.",2010-12-25T23:10:46.970,1329,CC BY-SA 2.5,
8747,5747,48,"An example is to take $A=X$, $B=Y$, and $C=X+Y$.  We can take $X$ and $Y$ to be independent, yet both $A$ and $B$ are correlated (positively, Pearson) with $C$.",2010-12-25T23:51:49.903,,CC BY-SA 2.5,user1108
8748,5747,1,"Thanks, that's actually a great comment.  Short, but it captures the essence of the reason why it is so.",2010-12-26T00:07:08.530,1329,CC BY-SA 2.5,
8750,5753,1,"whuber, what are ""multinormal variables""?",2010-12-26T00:19:14.987,1329,CC BY-SA 2.5,
8751,5753,3,http://en.wikipedia.org/wiki/Multivariate_normal_distribution,2010-12-26T00:20:25.823,919,CC BY-SA 2.5,
8752,5746,0,"What exactly is different about your problem?  The solution to the Secretary problem does not depend on the underlying distribution.  (It only requires that any possible ties can be resolved.)  If there is lack of independence among successive observations, you will need to quantify this (or at least bound it somehow) in order to make any progress.",2010-12-26T00:24:08.973,919,CC BY-SA 2.5,
8753,5751,2,"@Gaetan Lion: in this code, $x_1$ and $x_2$ are independent root normals, and $y = 3x_1 + 2x_2$ plus a normal noise term with standard deviation of 0.3. Clearly $y$ is positively correlated to $x_1$ and $x_2$, which are independent.",2010-12-26T06:08:51.307,795,CC BY-SA 2.5,
8754,5722,0,"The ranked values of $y_1 + y_2$ are in general a nonlinear function of $y_1 + y_2$ ‚Äî even if $y_1$ and $y_2$ are each a permutation of the integers $1,\ldots,n$.  Here‚Äôs an example: $y_1 = (1,2,3,4)$ and $y_2 = (2,3,1,4)$. Then $y_1+y_2 = (3,5,4,8)$ and $rank(y_1+y_2) = (1,3,2,4)$.  Plot $y_1+y_2$ against $rank(y_1+y_2)$ and you‚Äôll see that there is no linear relationship between the two.  The above assertion that $\rho(x,y_1+y_2) = Cov(x,y_1+y_2) / \cdots$ **is in general false**, even under the assumption that $y_1$ and $y_2$ are permutations of the integers.",2010-12-26T06:16:14.497,1670,CC BY-SA 2.5,
8755,5748,0,"Yes, sorry, I got mixed up with the letters ;)",2010-12-26T08:17:19.517,582,CC BY-SA 2.5,
8756,5750,6,This is a great example of why one needs multiple hypothesis testing.,2010-12-26T13:44:44.447,,CC BY-SA 2.5,user88
8759,5757,0,"What do you call ""every experiment""? Do you have several outcomes measured at the same time (for the control and the active group), or do you have repeated measurements for both groups at different dates (like in a sequential trial)? What kind of outcome is it: continuous or categorical?",2010-12-26T14:39:23.987,930,CC BY-SA 2.5,
8761,5746,0,"Sorry, I wasn't clear, have tried to clarify.",2010-12-26T15:18:14.340,557,CC BY-SA 2.5,
8762,5757,0,"@chl, I meant every study. Assume the simplest case for my question. I don't know the difference between ""continuous"" and ""categorical"".",2010-12-26T15:22:38.863,1497,CC BY-SA 2.5,
8763,5760,0,"+1 for modelling unpredictable deviations as random variables, the tickets from a box analogy and critical region references. I've updated my question with a more concrete (though hypothetical) example of what I'm interested in. I'll check out those books. I was hoping a widely accepted method exists for flagging significant results in a set of data based on prior data.",2010-12-26T16:49:00.580,1497,CC BY-SA 2.5,
8764,5690,0,"There are indeed censored values in the data and the total population is approx 1500, median overall survival is 18 months (range 300-600 days)... the time frame is the period 2000-2007.",2010-12-26T17:10:25.537,1291,CC BY-SA 2.5,
8765,5729,0,"Yes, this can be done if the classification algorithm cannot utilize weights directly (e.g. Naive Bayes is able to). However it is important to note that a mindful validation is absolutely necessary, since this approach may induce a Sampling Bias into the model.",2010-12-26T20:55:23.917,264,CC BY-SA 2.5,
8766,5763,0,"Thank you very much! It's just that my supervisor specifically instructed me to use a logistic regression, sorry I forgot to add that there are three groups involved autism, language impaired and typically developing kids and I need to look at whether langauge scores of the autistic kids and language impaired kids are also a signifcant predictor of scoring low on a social attribution task in later life. Please can you help with this??",2010-12-26T22:29:30.613,,CC BY-SA 2.5,Miranda
8767,5710,10,"Is it churlish to wince when I read ""data is""?",2010-12-26T23:06:44.780,199,CC BY-SA 2.5,
8768,5756,0,+1 correlation in terms of an angle between multi-dimensional vectors is intuitive for me.,2010-12-26T23:55:58.960,1497,CC BY-SA 2.5,
8769,5736,0,"I have looked at GARCH models.  However, GARCH wouldn't solve this problem.  I'm looking at generating correlated random samples from multiple time series.  Random multi-variate norm sample work, but they require the assumption that returns are normally distributed and  in this case, they are not.",2010-12-27T09:02:37.307,2566,CC BY-SA 2.5,
8770,5765,0,"It would be useful to know more about the scale these values come from - is the data continuous or categorical? Are differences meaningful, or are the measurements just ordinal? A dispersion measure for, e.g., unordered categorical needs to be different from such a measure for continous interval data.",2010-12-27T14:29:47.030,1909,CC BY-SA 2.5,
8771,5761,0,"@James  Good points (+1).  I just want to clarify a possible misunderstanding concerning an earlier comment of mine.  In your example *there is no single underlying distribution:* if there is a trend over time, then *a fortiori* the distribution is changing.  When the distribution does *not* change in any way over time, when there is serial independence of the observations, and the distribution is continuous, then the answer to the Secretary problem remains the same no matter what shape this distribution has.",2010-12-27T17:22:23.120,919,CC BY-SA 2.5,
8772,5765,0,"The simplest possible one is the *range*: 40 - 0 = 40.  The mean density of the numbers can be estimated as the quantity divided by the range; e.g., 5/40 = 1/8 (one value for every difference of eight).",2010-12-28T00:41:52.507,919,CC BY-SA 2.5,
8773,5770,0,"Thanks very much, Suncoolsu, for the explanation as well as the great resources!

Yes, you are right. My split-plot treatment, or subject, should be the 4 individuals within each combination of the two fixed factors, they were sampled 4 times (with one day interval), forming a longitudinal sequences. (But I am totally confused by the terminology of repeated measurement, as I did destructive sampling, but the samples should have temporary correlation in between).",2010-12-28T01:09:53.450,2536,CC BY-SA 2.5,
8774,5770,0,"The BLOCK in my question referred to three whole plots (I call them groups) with 5 days interval (destructive sampling). As a result, all the individuals were sampled in 12 samplings (ÔΩõ3 groupsÔΩù*ÔΩõ4 samplings in each groupÔΩù), the dates from the first sampling is 1-4, 6-9, 11-14 (I also tested the model where all the sampling were taken as one random factor, but it is still not correct, as I only have pseudo-replications, right?).",2010-12-28T01:10:35.557,2536,CC BY-SA 2.5,
8775,5770,0,"The reason for the complicated design is because I have a little bit long term variables (i.e., biomass) which takes effect on the group level, and short term variables (i.e., 14C activity) which could respond quickly within one day. Each group was fertilized, irrigated, and labeled together, and received different water, nutrient, but the same labels. 
Now, I would like to arrangethe whole experiment like this

model<-aov(Var~Group+A*B+Error(Group/A/B/Block),data)

But, I do not if it is correct.",2010-12-28T01:11:01.843,2536,CC BY-SA 2.5,
8777,5772,0,"I think most statistical packages will output sth like $\log\big(\Pr(Y=2)/\Pr(Y=1)\big)=b_{10}+\sum_ib_{1i}x_i$ and $\log\big(\Pr(Y=3)/\Pr(Y=1)\big)=b_{20}+\sum_ib_{2i}x_i$; and the interpretation of a regression coefficient is straightforward: for a one-unit increase in a (continuous) $x$, the log of the ratio of either of these probabilities will be $b$.",2010-12-28T08:16:00.277,930,CC BY-SA 2.5,
8778,5779,0,"mpiktas, I am missing something basic here.  When should one use the normal distribution approach I outlined vs. simply multiplying the p directly with no normal approximation?",2010-12-28T08:48:26.680,1279,CC BY-SA 2.5,
8779,5779,0,"However, the chance that the person who wins the second round is the same as the first is 1/8 (p = 0.125) assuming H0 for all players.",2010-12-28T09:00:00.797,183,CC BY-SA 2.5,
8780,5779,0,"@stats-teacher-stats-studentZZZ, normal distribution approach is an approximation. You use it when you do not know the exact distribution, or the exact distribution is too hard to calculate. The approximation error decreases as the sample length (in your case number of rounds) increases.",2010-12-28T09:14:10.423,2116,CC BY-SA 2.5,
8781,5779,0,"@stats-teacher-stats-studentZZZ: A rule of the thumb is, that if n*p*(1-p) >= 9 one can approximate the binomial distribution with a normal distribution.",2010-12-28T09:26:11.740,264,CC BY-SA 2.5,
8782,5736,1,"@Noah, how about multivariate GARCH? Each individual series is GARCH with innovations from multivariate normal with non-diagonal covariance matrix. Then the returns will not have normal distribution, but they will be correlated.",2010-12-28T09:39:49.830,2116,CC BY-SA 2.5,
8783,5779,1,"Simple question, but why should the null hypothesis be rejected? What are you comparing the result of 0.015625 to conclude that the nullH should be rejected?",2010-12-28T10:07:33.740,1497,CC BY-SA 2.5,
8784,5779,0,"@FreshCode, I assume the usual: if the p-value is lower than 0.05, the null hypothesis is rejected.",2010-12-28T10:10:11.633,2116,CC BY-SA 2.5,
8785,5744,0,"Hi Ronaf.  This came up when I learned about Cramer's V statistic for contingency tables.  I was wondering what the CI for it are, and found a way to get them - but hoped to validate my results using bootstrapping.  Cheers, Tal",2010-12-28T10:35:47.380,253,CC BY-SA 2.5,
8786,5782,1,Delta method is used for asymptotic distributions. You cannot use when you have only one random variable.,2010-12-28T14:35:31.440,2116,CC BY-SA 2.5,
8787,5778,0,"(+1) Thanks for the link.It is also possible to consider a heterogeneous correlation matrix (see, e.g. `hetcor()` from the [polycor](http://cran.r-project.org/web/packages/polycor/index.html) package). Provided the VC matrix is SDP, it should do the work--mostly in the spirit of Factor Analysis. Nominal variables might be dummy coded.",2010-12-28T15:24:20.967,930,CC BY-SA 2.5,
8788,5779,0,"Ok, so with only 2 trials, it is smarter to just to the raw probability calculation.  Once you get to 9, it's a pain to multiply to the 9th power, so you THEN start using normal distribution estimations, b/c they are close enough.  I should do both calculations and compare, in fact.  Thanks for the insight.  BTW, my friend DID indeed win both rounds of Texas Hold'em.  Clearly, some skill!",2010-12-28T15:48:53.747,1279,CC BY-SA 2.5,
8789,5788,0,Are these the same statistical units that are measured at the two time points?,2010-12-28T17:55:05.517,930,CC BY-SA 2.5,
8790,5789,0,"(+1) I won't post my answer since it's basically your 2nd ¬ß (less well explained) + some digressions about an additional test for equal intercept (if we cannot reject the null for the test of the equality of slopes, compute a common slope for both regression lines and ask whether the new lines are parallel or identical)--but, it's always under the assumption that the sampled units are independent at the two time points.",2010-12-28T18:12:13.147,930,CC BY-SA 2.5,
8791,5789,2,"@chl I refrained from commenting on the option of computing a common slope because that implicitly assumes homoscedasticity in the combined year 1 + year 2 datasets.  Given that the slope appears to have changed substantially I would want to check for a change in residual variance, which seems easiest to do by conducting a separate regression for each year--which the OP already has done.  The advantage of modeling the combined datasets comes from the additional power it offers of evaluating serial correlation of the residuals.",2010-12-28T18:27:33.117,919,CC BY-SA 2.5,
8792,5789,1,"@chl You bring up an interesting question concerning equal intercepts.  However, an intercept is an artifact of the origin of the x coordinate system.  We can handle that problem by comparing two models in which a pair of linear splines (plus one common intercept) is nested within two slopes+two intercepts.  The splines would of course have a ""knot"" at the transition from year 1 to year 2.  Assuming the splines aren't a significantly worse fit than the bigger model, we can then test for equality of their coefficients to see whether the slope changed.",2010-12-28T18:33:46.343,919,CC BY-SA 2.5,
8793,5793,3,"+1, pressed post the same second your answer appeared. I would add that for a full tree rsq.val[nrow(rsq.val),] is probably what OP wants.",2010-12-28T20:34:49.110,2116,CC BY-SA 2.5,
8794,5789,0,"Thanks for the clarification and added comment. I'm afraid to say that my initial ideas were not that advanced, but I definitively have to think about this last suggestion.",2010-12-28T20:35:42.197,930,CC BY-SA 2.5,
8795,5793,0,"@mpiktas Oups... Needless to say, your comment is very appreciated.",2010-12-28T20:39:18.217,930,CC BY-SA 2.5,
8796,5793,0,"Thanks a bunch.  I am (obviously) new to R, and never once did I think to look at the function as you first mentioned.  I was trying something similar to your answer, but that is perfect! Thx",2010-12-28T20:50:15.650,569,CC BY-SA 2.5,
8797,5789,0,"Awesome, thanks, all.  In summary, calculate t where $t=\frac{b1-b2}{se_{b1-b2}}$.  To estimate the denominator, you can calculate this as $\sqrt{se_{b1}^2+se_{b2}^2}$.  You can also use the sums of squares and a different formula if you like, but you'll get the same result.  Then just pull out the df, and voila, answer.",2010-12-28T20:52:23.447,101,CC BY-SA 2.5,
8798,2207,0,They're both distributions and loss functions in different contexts.,2010-12-28T21:36:38.513,1119,CC BY-SA 2.5,
8799,5785,0,Great point. I also fell into the trap assuming that 1. is the true setup :(,2010-12-28T21:49:13.193,264,CC BY-SA 2.5,
8800,2207,0,"I pressed enter too quickly on the previous reply - exponential loss is widely associated with boosting (see Friedman Hastie and Tibshirani's Statistical View of Boosting), where it's a loss rather than a distribution, logistic regression coresponds to log loss, laplace is a distribution but corresponds to absolute value loss - so for the most part I was being extremely sloppy, thanks for pointing it out.

But while L1 loss has a geometric solution, it isn't analytically closed form, so I would hardly call its solution easy.",2010-12-28T21:49:28.133,1119,CC BY-SA 2.5,
8801,5786,0,(+1) A very interesting question ...,2010-12-28T21:51:34.753,264,CC BY-SA 2.5,
8802,5780,0,"Thanks Tal, I'll check out that Barnard's test code and have a think about your Wilcoxon suggestion.",2010-12-28T21:55:06.147,174,CC BY-SA 2.5,
8803,5780,0,"My pleasure.  I'm not sure the Wilcoxon test would answer something you want to know - but if you are interested in over all dropout ""earlier"" in the stages of one group vs the other - that seems to me to be a good way to test in.  Cheers.",2010-12-28T21:56:25.320,253,CC BY-SA 2.5,
8804,5773,1,"Basil, I love it! +1 for a plain English explanation.",2010-12-28T22:35:02.523,1329,CC BY-SA 2.5,
8808,5765,0,"How is Mean Density of Numbers and Quartiles different? I mean, statistically, dont you think quartiles convey this information in a better way?",2010-12-29T04:15:21.960,2535,CC BY-SA 2.5,
8809,5791,0,"What assumptions are you making about the distributions? Are they all, for example, lognormal?",2010-12-29T05:10:38.657,919,CC BY-SA 2.5,
8810,5791,0,"@whuber each distribution has it's own assumptions, priors are either beta, gamma, logN, or weibull and only six are updated with data; in these cases the posterior distributions are simulated",2010-12-29T05:51:38.330,1381,CC BY-SA 2.5,
8811,5795,0,"Hey Mariana, thanks for your answer, but we had a misunderstanding: I by ""kernel methods"" mean methods such as the Support vector machine using the ""kernel trick"", not kernel smoothing methods.",2010-12-29T10:00:44.477,2549,CC BY-SA 2.5,
8813,5782,0,"@mpiktas: Actually I dont know much about Delta method, I've just read something on wikipedia. This is quotation from wiki: ""The delta method uses second-order Taylor expansions to approximate the variance of a function of one or more random variables"".",2010-12-29T13:08:40.030,1643,CC BY-SA 2.5,
8814,5790,0,"I dont need to know the exact value of the variance, approximation should works for me.",2010-12-29T13:10:18.733,1643,CC BY-SA 2.5,
8815,5782,0,"it seems wikipedia has exactly what you want: http://en.wikipedia.org/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables. I will reedit my answer, it seems that I underestimated Taylor expansion.",2010-12-29T13:39:10.650,2116,CC BY-SA 2.5,
8816,5790,0,"Indeed, the approximate formula for $\mathbb{E}[f(X)]$ in the OP is often used in risk analysis in economics, finance and insurance.",2010-12-29T14:50:19.560,2036,CC BY-SA 2.5,
8817,5790,0,"@Raskolnikov, yes but it contradicts my admitedly stale knowledge of Taylor expansion. Clearly the remainder term must be taken into account. If the random variable is bounded, then no problem, since polynomials approximate continuous functions on bounded interval uniformly. But we deal with unbounded random variables. Of course for random normal  we can say that it is effectively bounded, but still in general case, some nasty surprises can arise, or not. I will fix my answer when I'll have the clear answer.",2010-12-29T15:11:47.007,2116,CC BY-SA 2.5,
8819,5784,0,Thank you for very much for your help. much appreciated!!,2010-12-29T15:28:14.227,,CC BY-SA 2.5,Miranda
8820,5772,0,Thank you for very much for your help. much appreciated!!,2010-12-29T15:29:29.017,,CC BY-SA 2.5,Miranda
8821,5763,0,Thank you for very much for your help. much appreciated!!,2010-12-29T15:30:00.293,,CC BY-SA 2.5,Miranda
8822,5790,0,"@mpiktas: I've done some simulations and accuracy of the approximation strongly depends on the distribution, for unimodal distribution it seems to work fine, but the error could be significant for uniform distribution. I was testing $f(x)=\sqrt{x}$ and in case of $\chi^{2}$ the relative error was about 3%, but in case of uniform distribution it was more than 10%. I'm talking about the variance, because mean is approximated far better.",2010-12-29T15:46:50.130,1643,CC BY-SA 2.5,
8823,5809,0,That's just plain wrong. One of the points of doing ANCOVA is to adjust for covariate effects that might be confounded with the predictor.,2010-12-29T16:34:53.240,279,CC BY-SA 2.5,
8825,5809,2,"No. ANCOVA is meant as a way to enhance power when you have a covariate related to the predicted variable but unrelated to the predictor variables. It is lamentably common that folks fail to understand this and instead attempt to use ANCOVA to ""control for""/""regress out"" effects of the covariate when they are correlated with the predictors. See Miller 2001: http://www.ncbi.nlm.nih.gov/pubmed/11261398",2010-12-29T16:38:26.250,364,CC BY-SA 2.5,
8826,5810,3,I thought that interaction between the covariates and the factors indicates violation of the homogeneity of regressions assumption (another assumption also mentioned in the first link I gave),2010-12-29T16:53:05.757,339,CC BY-SA 2.5,
8827,5810,3,"This approach tests whether the effect of the predictor variables (on the predicted variable) is affected by the covariate (or vice versa). In other words, this tests assumption 2 on the wikiversity entry linked in the question. This routine cannot speak to assumption 3, the focus of the poster's question, which is about the relationship between the values of the covariate and the values predictor variables. My answer speaks to this assumption.",2010-12-29T16:56:38.473,364,CC BY-SA 2.5,
8828,5809,2,I stand corrected.,2010-12-29T17:10:30.507,279,CC BY-SA 2.5,
8829,5809,2,"I should note that I myself was taught and employed (http://www.ncbi.nlm.nih.gov/pubmed/19154749) the inappropriate practice of using ANCOVA to ""control for""  a covariate with a known relationship with other predictor variables. It's only been through later self-directed study that I discovered the Miller & Chapman paper and the errors of my ways!",2010-12-29T17:24:40.367,364,CC BY-SA 2.5,
8832,5812,0,@mbq: Can you explain this idea of re-sampling CV? Are you proposing an algorithm where you create 10 groups (typical 10 fold CV) but within each fold you create 1 bootstrap sample and determine the expected error on those observations not in the training set of the bootstrap? So you still have 10 error measurements to average?,2010-12-29T18:42:13.923,2040,CC BY-SA 2.5,
8833,5812,0,"@mbq: Also, Are you suggesting this (compared to say the regular 10 fold cross validation) because of the small sample size?",2010-12-29T18:46:43.170,2040,CC BY-SA 2.5,
8834,5813,1,"I don't think there's a single best answer, so this has probably to be made CW. Moreover, I think you need to clarify what you intend to do, and what's your background.",2010-12-29T18:52:58.033,930,CC BY-SA 2.5,
8835,5820,8,Another possible source of confusion is that the pdf of a _discrete_ random variable (also called pmf - probability mass function) indeed cannot exceed 1.,2010-12-29T20:40:04.977,279,CC BY-SA 2.5,
8836,5820,0,@Aniko: This is indeed a source of confusion. I think I understand now.,2010-12-29T20:48:17.387,977,CC BY-SA 2.5,
8837,5812,0,"@user2040 I can't find a good reference, so I'll edit the answer to be more precise; due to the second thing, yes, I think OP needs a quite small N (to remove this possible homogeneity bias) for N-fold CV, and so may get too few repetitions for a meaningful result.",2010-12-29T21:09:09.893,,CC BY-SA 2.5,user88
8838,5796,0,"I should add, don't use implementations based on the Laplace approximation - the posterior is highly skewed, and a symmetric approximation centered on the mode generally wont work very well.",2010-12-29T21:16:21.637,887,CC BY-SA 2.5,
8839,5813,1,Not sure this is answerable without a bit more info on the company's area(s) of activity and the job specification. 'Medical research' and 'programmer' are a bit vague.,2010-12-29T21:18:22.730,449,CC BY-SA 2.5,
8840,5814,0,I'm having trouble following the question. It might help  a little if each opening parenthesis was matched by a corresponding closing parenthesis. Also are the vehicle flow rates for each lane averages? If not how do you allow for overtaking within a lane? Cycle speeds and times depend on the cyclist.,2010-12-29T21:38:17.883,449,CC BY-SA 2.5,
8841,5802,0,"Adam great answer.  That's exactly what I was asking.  However, you added a tid bit of information that I have had a hard time finding more information on.  The Mallow's CP stop criterion.  I've looked through the R source code, but it's too hard to figure how they are choosing to stop.  I roughly have an idea, but I need a paper or some discussion on how it works so I can finish implementing it.  Do you have any information about it?",2010-12-29T21:44:51.537,1929,CC BY-SA 2.5,
8842,5722,0,"@vqv You're quite right. I was too hasty to attempt an answer before leaving for the christmas break. I hadn't come across that inequality relating the Pearson correlation of three variables before. Here's another ref for it complete with 3D visualisations: http://www.jstor.org/stable/2684832 . I still think it might have *some* relevance, so I won't delete my answer, though I can't see how to fix it either.",2010-12-29T21:50:31.677,449,CC BY-SA 2.5,
8846,5823,1,I think the robust analogue of CV would be the Winsorized standard deviation divided by the trimmed mean.,2010-12-30T00:08:38.837,795,CC BY-SA 2.5,
8847,5812,0,"@mbq: Thank you. From what I read of your edit, this is the usual bootstrap validation (e.g. Elements of Statistical Learning 2nd edition page 249), but with 10,15 or 30 iterations (compared to 100's or 1000's) typically used with large N data set.",2010-12-30T00:35:54.757,2040,CC BY-SA 2.5,
8848,5822,0,"in your dataframe there should only be 100 trials in each cell that is a combination of the 2 tasks. Also as an aside, I'm not wed to nlme. I've just begun using (or trying to use) mixed-models. nlme() is the first function that I've been introduced to.",2010-12-30T03:32:38.170,2322,CC BY-SA 2.5,
8850,5802,0,"So, I checked the rpart reference manual, and apparently I was incorrect. While one of the criterion for stopping in rpart is called ""cp"" this is short for ""complexity parameter"" and is merely the minimum amount r^2 must increase by in order to pursue a particular split. I have corrected my post above to reflect this. The lack of statistical tests in splitting rules is one of the reasons why I use the party package over the rpart package. With the party package the default method implements a Bonferroni corrected p-value as a stopping criterion (default p=0.05). For details see the vignette.",2010-12-30T06:14:04.810,2166,CC BY-SA 2.5,
8851,5802,0,"I feel the fact that rpart does not use a stopping criterion is an asset, rather than a liability. A split may appear to be worthless, and yet open the way for subsequent splits further down the tree, which may be quite significant. A ""grow, then prune"" policy as implemented in rpart will avoid early stopping and seems to me a sensible approach.",2010-12-30T09:00:44.500,892,CC BY-SA 2.5,
8852,5790,3,"@Tomek Tarczynski, the third derivative of $\sqrt{x}$ goes to zero quite quickly for large $x$, but is unbounded near zero. So if you picked uniform distribution with support close to zero, the remainder term can get large.",2010-12-30T09:04:21.300,2116,CC BY-SA 2.5,
8853,5790,0,"@mpiktas: Great answer, You put a lot of effort in it. I'm very grateful.",2010-12-30T09:09:52.267,1643,CC BY-SA 2.5,
8854,5790,0,"@Tomek Tarczynski, you're welcome.",2010-12-30T09:31:59.680,2116,CC BY-SA 2.5,
8855,5825,1,"+1 for mentioning L-moments! Glad i'm not their only proselytizer around here. I'm not convinced that normalizing/standardizing by dividing by a measure of location is the best approach here though, which is why I suggested dividing by a measure of dispersion instead. See point number 2 involving temperature in the question.",2010-12-30T09:49:15.027,449,CC BY-SA 2.5,
8856,5812,0,"Thanks for the replies. The (small) dataset is not that homogeneous : the coefficient of variation reaches 26% for the modelled property they are modelling using 3 predictors. That is why I was looking more at the robustness of BRTs to the inclusion or exclusion of very few individuals (1 in case of LOOCV), and thus, more generally speaking the validity of LOOCV for BRTs.",2010-12-30T10:15:03.470,2561,CC BY-SA 2.5,
8857,5826,0,What are the practical issues in using the gam fit?,2010-12-30T10:20:42.933,364,CC BY-SA 2.5,
8858,5812,0,"Nevertheless, the resampling CV algorithm proposed for validation the whole BRT models is interesting and yes seems equivalent to bagging, already performed within the BRT fitting procedure.",2010-12-30T10:23:25.380,2561,CC BY-SA 2.5,
8859,5796,0,"Thank you Dikran! Could you explain to me the relation of KLR und Kernel smoothing? The KLR-model is built similar to the svm [loss + penalty]-formulation and solved via gradient descent. But the same time references (e.g. in ""Kernel Logistic Regression and the Import Vector Machine"", Zhu and Hastie 2005) on KLR go to the smoothing-literature (e.g. ""Generalized Additive Models"", Hastie and Tibshirani 1990).",2010-12-30T10:59:08.260,2549,CC BY-SA 2.5,
8860,5826,0,"A bunch of things, but they mostly boil down to: whatever I fit has to be reproducible in SAS' `proc reg`.",2010-12-30T11:25:05.680,1569,CC BY-SA 2.5,
8861,5830,0,Thanks Rob. I noticed that the `gam` in the mgcv package can also do penalised regression splines; would that be the same as in SemiPar?,2010-12-30T11:26:34.120,1569,CC BY-SA 2.5,
8863,5812,0,"@user2040 Well, the real criterion for T is large enough to stabilize result and small enough to finish analysis in a reasonable time.",2010-12-30T11:36:52.413,,CC BY-SA 2.5,user88
8864,5812,0,"@backflip Again, you just need to test it making CV with larger tests, and I proposed resampling CV as the easiest solution to the problem with small overall number of objects. About bagging, yes it is the same. Indeed at first I thought BST is just a plain boosting, so I haven't mentioned bagging.",2010-12-30T11:43:26.323,,CC BY-SA 2.5,user88
8865,5796,0,"I am not that familiar with the smoothing literature, but kernel models are closely related to spline smoothing.  I think the best place to look would be the publications by Grace Wahba (http://www.stat.wisc.edu/~wahba), whos work spans both smoothing and kernel methods.",2010-12-30T11:43:41.410,887,CC BY-SA 2.5,
8866,5796,0,"Thanks, i will have a closer look at wahba's publications. Can you recommend an implementation of KLR, at best in R?",2010-12-30T12:03:16.293,2549,CC BY-SA 2.5,
8867,5809,0,Do you mean that I'll have to perform a 3-way anova and look for significant effects?,2010-12-30T12:24:25.050,339,CC BY-SA 2.5,
8868,5830,0,"It's similar, but a different implementation, so the results are not the same. I notice that Matt Wand (author of SemiPar) comments at http://www.uow.edu.au/~mwand/SemiPar.html that mgcv now does most of what was intended for SemiPar and that he advises people use mgcv instead.",2010-12-30T12:44:25.727,159,CC BY-SA 2.5,
8869,5814,0,"@onestop - In Traffic engineering, flow rates in this context could be considered as a snapshot. Let's say we're interested in finding the number of vehicles queued at an intersection every five minutes. You Look at the intersection, jot down the number of vehicles queued in the intersection, then you close your eyes for five minutes, open them up and count the vehicles again",2010-12-30T14:19:34.560,59,CC BY-SA 2.5,
8870,5816,0,"...always remembering that you are reinventing the wheel with that: use it as a ""proof-of-concept"" that you can program, if they want to align short DNA fragments to a genome they would use BLAST.",2010-12-30T14:56:33.600,582,CC BY-SA 2.5,
8871,5827,0,"May I suggest that you talk to your friendly neighborhood statistician? This project seems to be too large and important to mess up, and it would be easy to do it incorrectly.",2010-12-30T15:18:25.700,279,CC BY-SA 2.5,
8872,5791,0,"@David Given these distributions come from different families, it seems that CVs will be difficult to interpret and compare among distributions.  Could you clarify then why you are tracking the change in CV and how you intend the CVs to be interpreted?",2010-12-30T15:23:44.140,919,CC BY-SA 2.5,
8873,5820,1,This question is a duplicate of http://stats.stackexchange.com/q/4220/919 .,2010-12-30T15:28:18.707,919,CC BY-SA 2.5,
8874,5827,2,"May I just emphasize what Aniko writes.  The biggest concern in my mind, after reading this question, is not about statistical procedure or software, but about what all this might *mean*.  Unless your data are an exhaustive survey (of a well-defined population) or obtained through a formal randomized procedure, considerable thought--of both a statistical and epidemiological nature--will be necessary to assure that you can learn *anything* from these data that is validly generalizable.",2010-12-30T15:55:32.787,919,CC BY-SA 2.5,
8875,5836,0,So... why don't you write it as in the top graph?,2010-12-30T15:59:58.460,582,CC BY-SA 2.5,
8876,5834,0,"You need to clarify what you mean, exactly, by ""DTMC.""  For instance, let the states be $0, 1, \ldots, n, \ldots$.  Define a Markov chain with transition probabilities $p_{i,i+1} = 1$.  Because $f_{ii}=0$ for all $i$ it is transient.  Because every state is eventually reached from $0$ it is irreducible.  Nevertheless, *your intended conclusion is obviously not true* in this example.",2010-12-30T16:03:33.963,919,CC BY-SA 2.5,
8877,5791,0,"@whuber I would like to quantify the amount of information that we have about each parameter before and after adding data. I would like the figure to show something like CV as an index of how well a parameter is constrained, so that this can be compared to sensitivity of the model to the parameter and the contribution of the parameter to model output variance.",2010-12-30T16:15:40.353,1381,CC BY-SA 2.5,
8880,5835,0,"I'd agree with most of this except that instead of including age as a time-varying covariate you'd almost certainly be better using age as the time scale. See Kom, Graudbard & Midthune 1997 http://aje.oxfordjournals.org/content/145/1/72.",2010-12-30T16:57:08.577,449,CC BY-SA 2.5,
8881,5834,0,Perhaps the questioner forgot to add the condition that the state space is finite.,2010-12-30T17:25:04.563,449,CC BY-SA 2.5,
8884,5836,0,"@nico Lack of expertise on how to do it in R, basically.",2010-12-30T17:39:11.713,2443,CC BY-SA 2.5,
8886,5836,2,"`paste` is your friend. Just construct a string like `legend.string <- paste(""In traffic [Max"", max(values),""]"")` and then pass that as the `legend` parameter of `legend`",2010-12-30T17:53:48.597,582,CC BY-SA 2.5,
8887,5834,0,"@onestop I initially suspected that, but then it occurred to me that in the finite case there must exist either an absorbing or a recurrent state; neither of those can be transient.  Thus a finite transient Markov chain does not exist.",2010-12-30T18:07:28.647,919,CC BY-SA 2.5,
8888,5839,0,(+1) Your solution is at least more elegant than mine because you don't duplicate the grouping variable!,2010-12-30T18:20:49.017,930,CC BY-SA 2.5,
8892,5847,1,"2500 - 2000 = $500.  Whether that's ""meaningful"" or not depends on how much you value this money ;-).",2010-12-30T20:09:48.667,919,CC BY-SA 2.5,
8893,5847,0,"@whuber I can tell that one made more money, but I want to know if how to tell if that difference is likely due to chance, or if its due to the inherent superiority of the other page.",2010-12-30T20:12:08.597,142,CC BY-SA 2.5,
8894,5847,1,"You can't answer that with the information given.  If your pages are offered randomly (using a random number generator) to all comers during the same period *and* you have the individual ""conversion"" values, then you can use simple, standard techniques for testing whether the difference can be attributed to non-chance mechanisms.",2010-12-30T20:15:18.240,919,CC BY-SA 2.5,
8895,5844,0,"what kind of connection you are looking for? KLR is for classification, smoothing splines are for non-linear approximation of conditional expectation. The way I see they are non-linear extensions of logistic and ordinary regressions respectively.",2010-12-30T20:15:20.470,2116,CC BY-SA 2.5,
8896,5848,0,(+1) Ah... we cited the same book :),2010-12-30T20:21:05.730,930,CC BY-SA 2.5,
8897,5849,0,"@chi and @whuber: Thank you, I was wondering if The Good book was a *good* choice. For the problem types I laid out - basically ANCOVA with violations, am I on the right track with permutation or bootstrapping?",2010-12-30T20:22:29.047,2040,CC BY-SA 2.5,
8898,5809,0,"@gd047: If your covariate is numeric, then yes, you can do an ANOVA predicting the covariate from the 3 predictor variables and their interactions. If your covariate is nominal, then you'll have to do a multinomial regression (again, predicting the covariate from the 3 predictor variables and their interactions). If you find suggestion that any of the predictor variables or their interactions predict the covariate, then you may need to reconsider using the covariate in the full model.",2010-12-30T20:44:09.887,364,CC BY-SA 2.5,
8900,5847,0,"I probably misunderstood the question, but where is the >100% value coming from?",2010-12-30T21:09:36.650,582,CC BY-SA 2.5,
8901,5844,0,"KLR seems to be coming out of nowhere: I found the name KLR to be mentioned first around 2000 (for example in ""kernel logistic regression and the import vector machine"", Zhu and Hastie, 2001). But they say in there it is well known and reference smoothing spline literature. My Question is: When and where was KLR first introduced and why do they always reference smoothing-splines?",2010-12-30T21:15:13.230,2549,CC BY-SA 2.5,
8902,5753,0,"As usual, a most thorough explanation you get a well deserved ""Best Answer"" check mark.",2010-12-30T21:17:48.767,1329,CC BY-SA 2.5,
8903,5753,0,@Gaetan Lion You are very kind.  I have enjoyed reading *all* the answers to this question (and marked them all up).,2010-12-30T22:13:12.627,919,CC BY-SA 2.5,
8904,5849,0,"@user2040 It's hard to go wrong with permutation tests.  Good has chapters specifically on multifactor designs, categorical data, and multivariate analysis (including MANCOVA).  Although I don't fully comprehend your specific problem, I'm sure you're find something useful there.",2010-12-30T22:20:36.867,919,CC BY-SA 2.5,
8905,5848,0,@chl I don't think so: they're by the same author but have slightly different titles and different publishers.  Maybe we should each say a little more about them so we can determine which might be more appropriate for the OP.  I added a few details in a comment to your response.,2010-12-30T22:23:49.373,919,CC BY-SA 2.5,
8906,5848,0,I deleted mine after seeing yours.,2010-12-30T22:26:57.533,930,CC BY-SA 2.5,
8907,5849,0,"@user2040 I will add some references but I found your two points difficult to understand as well. To my knowledge, there's no *exact* permutation test when the covariate is continuous.",2010-12-30T22:29:37.763,930,CC BY-SA 2.5,
8908,5849,1,"@chl I think, if I correctly understand your comment about continuous covariates, that exactness depends on the role played by randomness in the data.  When randomization occurs *by design*, it doesn't seem to matter what kind of data you have.  The permutation test takes the data as given and simply lets us glimpse how the statistical results would turn out if our random number generators had (for example) resulted in different assignments of subjects to treatment and control groups.",2010-12-30T22:36:17.477,919,CC BY-SA 2.5,
8909,5848,0,"@chl Ah, I see.  So there's no redundancy.",2010-12-30T22:37:05.633,919,CC BY-SA 2.5,
8910,5834,0,@whuber - thanks for clarifying that. I started suspecting as much as I pondered after submitting my previous comment so tried to add a question mark to its end but was outside the arbitrary 5-minute edit window.,2010-12-30T22:49:11.437,449,CC BY-SA 2.5,
8911,5847,2,"@Nico You're right to wonder that.  The references to percentages and, indeed, to chi squared are red herrings.  In the new situation the observations are total dollar returns on page visits, not a percentage of anything (so of course a chi-squared calculation is wholly invalid).  In the old situation the observations are counts of ""conversions"" and visits for each page.  Chi squared enters as one (of several) possible approximations to a *test of proportions.*  (In both situations it's necessary that conversions be serially independent, which is a dubious but often necessary assumption.)",2010-12-30T23:53:24.303,919,CC BY-SA 2.5,
8912,5849,0,"@chi and @whuber, Thanks again. I will see which of the Good books is the best (many puns intended). As far as my problem, basically it is a two sample experiment (treatment and control/no treatment) where there exist a pre-experiment baseline measure and a post treatment measure, the latter being the dependent variable (actually it is change in the measure from pre to post). So it would be a typical ANCOVA or ANOVA (depending on if the change is the dependent or post is, with the pre as a covariate) except that many of the post measurements are zero (the customer did not purchase anything).",2010-12-31T00:55:11.070,2040,CC BY-SA 2.5,
8913,5780,0,"Wow, that Barnard's code is *slow*. The test doesn't seem to give substantively different results on my first trials, so I'll probably stick with the faster/more conservative Fisher. Haven't yet tried Wilcoxon -- overall dropout might be of interest, but only in a very peripheral way. Anyway, thanks for your help. Holm correction it is.",2010-12-31T02:16:26.887,174,CC BY-SA 2.5,
8914,5799,7,"(+1)  This is a great idea generally.  For large correlation matrices, however, there are so many statistics and so many of them can simultaneously be spuriously large that it usually pays to adjust.  Otherwise you wind up chasing a large number of misleadingly ""significant"" correlations that just disappear in the hold-out data.  (Run a simulation with, say, a few hundred draws from 50 uncorrelated standard normal variates.  It's an eye-opener.)",2010-12-31T05:14:57.070,919,CC BY-SA 2.5,
8915,5775,1,(+1) Good advice in light of the OP's comment to @Peter Flom's response.,2010-12-31T05:16:26.850,919,CC BY-SA 2.5,
8916,5837,1,"Thank you very much for the advise, I will think about this.. I agree that I need a statistician but: 1. I underestimated the problem, the previous smaller questions were easier and could be handled by ourselves; b. I'm in a progrma for a master in public health with special in epidemiology, the advanced statistics modules are only in 2011 and I wanted to have an idea now; c. I like to understand my statistical friend when we are discussing this. I'm a professor ath the University of Antwerp, we even have a epidemiology and biostatistical department ;-) (politics....)",2010-12-31T05:18:13.403,2571,CC BY-SA 2.5,
8917,5835,0,"Thanks! Can you recommend any books on a. survival analysis of this kind; b. programming advise in any program (I'm not married to Stata, my university has a licence for all major programs; I tried R but the learning curve of both statistics and R was a bit to steep...)",2010-12-31T05:24:20.093,2571,CC BY-SA 2.5,
8918,5853,2,"If your set changes time from time it is impossible to fit it to the same bell-curve. Suppose you have the set $A=\{1,2,3,4\}$, so 4 should be 800, but if a new observation 5 arrives, then it should become 800.",2010-12-31T07:43:23.000,2116,CC BY-SA 2.5,
8919,5854,0,what exactly are you displaying? It is not clear for me what kind of data your one data point represents.,2010-12-31T07:49:23.180,2116,CC BY-SA 2.5,
8920,5854,1,How about a kernel density plot? http://www.statmethods.net/graphs/density.html,2010-12-31T09:14:26.310,144,CC BY-SA 2.5,
8922,5854,0,"@mpiktas: My data is Census data for villages. My website will allow the user to select an area on the map, and then will find all the villages in that area. The census data for a village consists of various values like: Male Population, Female population, Average household income etc for that village.  I hope to show the data distribution for a particular value (eg: Total Population) for all the villages falling in the user selected area.",2010-12-31T09:25:56.697,2586,CC BY-SA 2.5,
8923,5856,1,(+1) Forgot about that. It might be handy.,2010-12-31T10:04:09.160,930,CC BY-SA 2.5,
8925,5849,0,"@user2040 *Resampling Methods* is really a light version of the book that @whuber refered to, with computer illustrations (R, SAS, Stata, and even C++); so I'll recommend the latter for serious work. There are practical suggestions for dealing with classical exp. setups, unbalanced design, outliers, etc. (Chap. 6, 7 and 11). If you use a difference score, then it is basically a two-group comparison, but see [The Analysis of Pre-test/Post-test Experiments](http://j.mp/gbdPhn) by GE Dallal, and a permutation test should do the job. Bootstrap might be more interesting with unbalanced design.",2010-12-31T10:56:16.317,930,CC BY-SA 2.5,
8926,5861,3,"I work in biological research. I know some colleagues (I mean, people with a PhD) who cannot really grasp boxplots. I would not use them to target a general audience.",2010-12-31T11:17:57.843,582,CC BY-SA 2.5,
8927,5863,0,"Note I wasn't suggesting he use violin plots for his applications, but a histogram with logarithmically spaced bins.  Violin plots was the answer to the question in the title (which was rather different to the question in the post itself).",2010-12-31T11:21:42.303,887,CC BY-SA 2.5,
8928,5857,3,"Quoting a friend of mine: if you want to ""hide"" something in a paper, put it in the text rather then in a figure. If you want to make sure nobody ever reads it put it in a table! ;) Just joking of course, but having a website with interactive maps for users to click etc. all of that to get a table... well that would be disappointing!",2010-12-31T11:28:12.830,582,CC BY-SA 2.5,
8929,5853,1,"Welcome to CrossValidated, NealWalters. You may find that a common response to asking a statistician ""How do i do this"" is ""Why do you want to do that?""",2010-12-31T11:37:51.180,449,CC BY-SA 2.5,
8930,5857,0,"@nico, yeah but sometimes tables are much more informative than graphs. I for example prefer table instead of a bad graph. In this case the table still can be represented by graph, and I suggested quantiles because they do not have problems with outliers.",2010-12-31T11:46:09.140,2116,CC BY-SA 2.5,
8931,5849,0,"@whuber I was referring to nuisance parameters like those encountered in fMRI studies (e.g., age) that might invalidate the exchangeability assumption, as discussed by [Tom Nichols (2008)](http://j.mp/dTr6O2), with reference to [Permutation Tests for Linear Models](http://j.mp/fxkhLe) by Anderson and Robinson, 2001, but this is probably off-topic there. Good makes it clear that ""we must assume that the residuals are exchangeable and both the concomitant variables and the regression coefficients are unaffected by the treatment"". (p. 101)",2010-12-31T11:47:17.017,930,CC BY-SA 2.5,
8932,5861,2,"@nico That's a fair point. But, this is not a reason not to use efficient graphical summary. A schematic illustration of what a boxplot actually does might help the reader.",2010-12-31T11:53:31.160,930,CC BY-SA 2.5,
8933,5861,2,"it really depends on what the target audience is and what the aim of the site is. Explaining boxplots would definitely help, but still, some people struggle a lot with the concept of distribution.",2010-12-31T12:00:25.313,582,CC BY-SA 2.5,
8934,5863,3,"You will probably like [Many Eyes](http://www-958.ibm.com/software/data/cognos/manyeyes/), [dataviz](http://www.improving-visualisation.org/visuals/), [datavisualization.ch](http://datavisualization.ch/), and [Ideas2evidence](http://www.ideas2evidence.com/showcase.html), to name a few.",2010-12-31T12:12:31.200,930,CC BY-SA 2.5,
8935,5861,0,"@nico Yes, I agree. Although boxplot is not mentioned in [A Tour through the Visualization Zoo](http://queue.acm.org/detail.cfm?id=1805128) -- but these are for large and complex data sets, I simply like it and I'm sorry to see that it is not much used in experimental sciences. Superimposing raw data is a way to help the reader visualizing the distribution.",2010-12-31T12:17:41.333,930,CC BY-SA 2.5,
8936,5861,1,"I know! I always try to ""convert"" my colleagues to boxplots, at least when it comes to writing papers, making presentations etc., but sometimes is though!",2010-12-31T12:25:38.093,582,CC BY-SA 2.5,
8938,5862,0,Good point. Histograms ( or density plots with experiment with bandwidth) are a great solution here.,2010-12-31T12:51:33.050,1307,CC BY-SA 2.5,
8939,5864,0,"With the inclusion of all the non-donators, it seems that 80-90% of the data will be 0. Is the $t$-test still sufficiently robust if there's such a strong deviation from normality? It would also be interesting to know if the data is essentially categorical because people typically donate 5, 10, 15 dollars, ... That would mean a lot of ties, and Mann-Whitney wouldn't be appropriate.",2010-12-31T12:52:11.800,1909,CC BY-SA 2.5,
8940,5802,0,"@Tusell that's the whole idea behind CART's research is that choosing a stop criterion is suboptimal.  Grow a full tree then use pruning to find an optimal tree.  However, as @Adam has pointed out rpart doesn't exactly work like that.  Part of it I think is an optimization for large datasets.  If it can chop off a subtree early without exploring it can save way more CPU time than going through a full expansion and pruning session.  As you point out though that means some trees aren't fully explored and you have to tweak the cp param to make it work.",2010-12-31T14:57:09.867,1929,CC BY-SA 2.5,
8941,5853,0,"@onestop - see my link to previouspost.  In school, I remember of hearing of teachers who graded ""on the curve"". So no matter what score you got on a test, only a certain percentage of people would get each grade.  I have no idea how credit scores work, but that's very similar to what I'm simulating.  I give people points for example when they pay off a loan.  But some people will pay off dozens of loans.  I want to condense my wild points into a set of normalized points.",2010-12-31T14:59:10.517,2585,CC BY-SA 2.5,
8942,5802,0,@Adam is R^2 calculated from the testing dataset or is it using the pruning set to calculate MSE?,2010-12-31T15:04:15.197,1929,CC BY-SA 2.5,
8944,5873,0,If your goal is to provide evidence that the true proportion is greater than 95% then you probably want to switch your null and alternative hypothesis around.  Typically you put what you want to show in your alternative hypothesis.,2010-12-31T16:10:21.523,1028,CC BY-SA 2.5,
8945,5873,0,"@Dason, Good point. One of my profs said that you draw a stronger conclusion from rejecting the null hypothesis.",2010-12-31T16:16:23.927,2591,CC BY-SA 2.5,
8946,5870,0,"The quality of an approximation tends to depend on the values of the parameters.  Can you stipulate anything about the values of $n$ or $\beta$ that you are interested in?  For example, how small can $n$ be?  How close to $1/2$ can $\beta$ be?",2010-12-31T16:17:29.817,919,CC BY-SA 2.5,
8947,5870,0,"@Whuber: It can be assumed that $\beta$ is far from $\frac{1}{2}$ lets say $\beta \approx 0.1$. Case when n is big is simple, so I'm more interested in case where n is rather small: $10 \le n \le 20$",2010-12-31T16:45:35.107,1643,CC BY-SA 2.5,
8948,5872,0,"@Didier Piau: I'm looking for the distribution of $R_{n}$ for fixed n, but I would be also satisfied with first two moments. When n approaches infinity it is easy to calculate them.",2010-12-31T16:48:03.467,1643,CC BY-SA 2.5,
8949,5873,3,"""Sample 10% of the population"" has almost no basis in statistical theory or practical application.  (In fact, it is contrary to statistical theory, which shows that usually the absolute sample *size* matters, not its percentage of the population.)  More importantly, any such rule is assuming something about the trade-off between the costs of sampling and the value of improved decision making *to you, in your particular circumstance*.  That's *exactly* the sort of thing you don't want a stats cookbook or glib Web site to decide for you!",2010-12-31T16:48:55.250,919,CC BY-SA 2.5,
8950,5873,2,A good guiding principle is **Wald's sequential testing theory**.  The Wikipedia article includes a (brief) example identical to your question (but testing for a 99% proportion rather than a 95% proportion): http://en.wikipedia.org/wiki/Sequential_probability_ratio_test,2010-12-31T16:54:31.203,919,CC BY-SA 2.5,
8952,5871,0,"What is true, however, is that if there exists a state $i$ such that $f_{ii}=1$, or if there exist two different states $i$ and $j$ such that $f_{ij}=f_{ji}=1$, then the Markov chain is recurrent.",2010-12-31T17:08:32.753,2592,CC BY-SA 2.5,
8953,5872,0,"@Tomek: ""to calculate them"" in the sense of ""to compute some simple asymptotic equivalents of them"", I presume.",2010-12-31T17:13:39.393,2592,CC BY-SA 2.5,
8954,5872,0,"@Didier Piau: Yes, of course I meant to calculate asymptotic equivalent.",2010-12-31T17:22:06.170,1643,CC BY-SA 2.5,
8955,5875,0,"Calculating this by brute force is deffinitely a solution, but I wanted to avoid, but if there is no better idea I will use it. Thanks for the code, it will save me some of the work.",2010-12-31T17:25:31.080,1643,CC BY-SA 2.5,
8956,5872,0,@Tomek: then the asymptotic equivalents you are interested in are $2\beta n$ for the expectation of $R_n$ and $2n(\frac14 -\beta^2)$ for its variance.,2010-12-31T17:37:47.843,2592,CC BY-SA 2.5,
8957,5875,0,"@Tomek For $n \lt 10$ we can't hope to find good approximations.  Given that the brute force computation is easy to code and executes fairly quickly, the main reason for looking for *any* approximations would be to improve our understanding of the behavior of the sample range.  If that's your situation, the sum in your question can be reduced to a case of the hypergeometric function.  That's a trivial result but it suggests analytical ways of studying the range and creating approximations.",2010-12-31T17:52:03.073,919,CC BY-SA 2.5,
8958,5872,0,"@Didier Comparisons with the examples I posted for $n=20$ show we'll need much larger $n$ before these approximations are reasonably close.  Note, too, that for extreme values of $\beta$ (including near $0$), the distribution tends to be quite skewed, implying that more than the expectation and variance are needed for decent characterization.  Until $n$ gets huge, then, @Tomek's suggestion to use Normal approximations looks more worthwhile.",2010-12-31T17:56:28.640,919,CC BY-SA 2.5,
8959,5872,0,"@whuber: thanks for your comment. Why do you write that the linear approximations of the expectation and variance are not ""reasonably close"" to the actual values? I see nothing about this in your post, so... did you compute these values and choose not to post them? About the last sentence of your comment: the equivalent of the variance I wrote in my last comment is precisely a consequence of the normal approximation you advocate.",2010-12-31T18:18:20.023,2592,CC BY-SA 2.5,
8960,5876,0,"+1, I've deleted my answer, since your was way better and made mine obsolete :)",2010-12-31T18:42:04.997,2116,CC BY-SA 2.5,
8961,5868,0,"What exactly are you trying to estimate? If you want to estimate maximum of the data as each new observation arrives the most simple estimate is the sample maximum. Maybe you want some distribution properties, like mean, variance, quantiles?",2010-12-31T18:49:43.930,2116,CC BY-SA 2.5,
8962,5872,1,"@Didier Good questions. (1) $2\beta n$ in my examples equals about 0, 6.7, and 13.3.  The distributions have modes at 4, 8, and 15, respectively.  Especially in the case $\beta=0$ the approximation is pretty far off (more than 1 SD wrong).  (2) The normal approximation @Tomek proposes differs from yours: one approximates each of the $X_i$ with a Gaussian and from that does the integration to estimate the distribution of the range.  The result is decidedly non-normal yet, when $(1/2-\beta)n \ge 5$ or so, should work quite well with a continuity correction.",2010-12-31T19:45:05.410,919,CC BY-SA 2.5,
8963,5878,0,"Hmm... you think it will be enough? Well, I guess that would be easier for me. And, on second thought, after I gather my data, I will always be able to change the algorithm if I see that another one gives better results.",2010-12-31T20:14:38.340,2590,CC BY-SA 2.5,
8964,5878,0,"(Oh, btw, you have a typo - it's ""winsorized"", not ""windsorized"". Took me a moment to figure it out, since I hadn't heard about it before.)",2010-12-31T20:18:27.963,2590,CC BY-SA 2.5,
8965,5878,0,"@Vilx, thanks, I fixed a typo and added wikipedia links. I posted answer via mobile, so that is why I missed typo and did not add links. Also in your place I would wait for other answers, since you cannot expect a lot of them in new year's eve :)",2010-12-31T20:40:57.217,2116,CC BY-SA 2.5,
8966,5872,0,"@Didier I did a calculation to check @Tomek's normal approximation for $n=20, \beta=1/3$.  It qualitatively reproduces the pdf.  The errors in the pdf are usually less than 0.01 and the error in the cdf has a max of about 0.02 at $k=14$.  We can expect better performance for $\beta$ closer to $0$ and worse performance for smaller $n$.  The numerical integration, which takes some effort, will become computationally more efficient than exact calculation somewhere around $n=50$ I guess.",2010-12-31T20:43:24.003,919,CC BY-SA 2.5,
8967,5878,0,"I know, but recently I learned that the ""accepted"" mark can be removed and re-assigned at any time, so I better accept the answer now, because later I'll just forget to do it. If there is a better answer, I'll accept that. :)",2010-12-31T20:47:29.807,2590,CC BY-SA 2.5,
8968,5878,0,"@Vilx, cheers:)",2010-12-31T21:26:23.327,2116,CC BY-SA 2.5,
8969,5835,0,"Applied Survival Analysis by Hosmer and Lemeshow is good; if you use SAS, then Survival Analysis Using SAS by Allison is very good and clear.",2010-12-31T22:08:25.927,686,CC BY-SA 2.5,
8970,5878,0,Cheers to you too! :),2010-12-31T22:13:24.523,2590,CC BY-SA 2.5,
8971,5879,0,"Sounds complicated and unstable. On one hand I understand that statistics and predictions are inherently unstable and imprecise, on the other hand the programmer in me wants an absolute and 100% correct answer. XD I think now that it might indeed better to start out simple (simple winsorized mean sounds good), and then, when I have gathered a significant amount of raw data, I can start thinking about upgrading the algorithm.",2010-12-31T22:18:54.023,2590,CC BY-SA 2.5,
8972,5879,0,P.S. Happy new year! :) (It's 2011 here already),2010-12-31T22:19:16.033,2590,CC BY-SA 2.5,
8973,5879,0,"@Vilx Definitely more complicated than simple averages! -- but you stated you are ""going for maximum accuracy,"" which I took as a strong hint that complications wouldn't be an obstacle for you.  As far as ""unstable"" goes, it's hard to tell what you mean here.  What you might do is monitor the total completion time for the first few users just to see how much variation there is.  This will indicate how much (or little) would be gained by accounting for their individual differences.",2010-12-31T22:22:00.267,919,CC BY-SA 2.5,
8974,2763,1,would you be willing to share your ascii_hist function by any chance?,2011-01-01T00:02:14.660,2594,CC BY-SA 2.5,
8975,5857,0,"That is what I am currently doing(Showing the deciles on a graph), but after showing it to some of our target audience, we received feedback, that the graphs were not easy to understand.",2011-01-01T05:04:10.337,2586,CC BY-SA 2.5,
8977,5879,0,"Well, no, complications aren't an obstacle, as long as someone helps me write out the relevant formulas. XD And sorry about the ""unstable"". :( I know that the estimates are going to be just that - estimates, and that they will only approximately represent the users remaining time. But since you're even guessing at the method to calculate them (logarithms? square roots?) then that gives the impression of _very_ approximate results. But then - you know your craft much better than I do and it's not really for me to say. Sorry! :(",2011-01-01T08:36:06.650,2590,CC BY-SA 2.5,
8978,5872,0,"@whuber: Three comments. First, about ""Tomek's normal approximation"": to what post/comment are you referring? Once again, the asymptotic variance $2n(\frac14-\beta^2)$ follows from a **normal** approximation. Second, when $\beta>0$, did you compare $2\beta n$ and the **expectations** (not the **modes**) of your simulations? Third, when $\beta=0$, $R_n$ is of order $\sqrt{n}$ and $2R_n/\sqrt{n}$ converges in distribution to a non degenenerate positive random variable $\max(N_1,N_2,N_3)-\min(N_1,N_2,N_3)$ with $(N_i)$ i.i.d. standard normal.",2011-01-01T11:55:46.493,2592,CC BY-SA 2.5,
8980,5869,0,Thanks Dikran -- however book publishers have also studied extreme values; would anyone know of an online introduction ?,2011-01-01T16:00:05.403,557,CC BY-SA 2.5,
8981,5872,0,"@Didier (1) The original question.  (2) Look at the plot in my response.  The expectation for $\beta=0$ is larger than the mode; the expectations for the other values are close to the modes.  (3) Fine.  But ""convergence"" tells us what happens for arbitrarily large $n$.  Without additional information it tells us *nothing* about particular values of $n$.  When we turn to the actual distributions, it's clear the asymptotic estimates are not very good even for small $\beta$ and $n=20$ and it's also apparent the distributions are not approximately normal for $\beta$ near $0$ or near $1/2$.",2011-01-01T17:13:57.870,919,CC BY-SA 2.5,
8982,5879,0,@Vilx The business about logarithms or roots has to be a guess *for me* because I don't have the data.  For *you* it won't be a guess because there are standard and rigorous ways to ascertain appropriate re-expressions of data; they are part of exploratory data analysis.,2011-01-01T17:16:30.210,919,CC BY-SA 2.5,
8984,5869,2,http://books.google.com/books?id=kXCg8B5xSUwC&lpg=PP1&dq=Statistics+of+Extremes+gumbel&pg=PP1&hl=en#v=onepage&q&f=false,2011-01-01T17:21:27.897,919,CC BY-SA 2.5,
8985,5870,0,"Just for clarification (because it came up as a question in some comments), I take it that your allusion to the normal approximation should be interpreted to mean we can approximate each $X_i$ with a normal variable and perform the integrations to estimate the distribution of the range.  (Note that this is *not* the same as a ""sum"" of normal distributions, nor is it easy to calculate.)  Is this what you were thinking of, or have I misinterpreted your idea?",2011-01-01T17:26:51.590,919,CC BY-SA 2.5,
8986,5879,0,"Ahh, I see. Well then, it's even more paramount that I get the data first. I'll start with that, and then, when I have the data, I'll return here. :) Thank you for the explanation!",2011-01-01T18:07:30.787,2590,CC BY-SA 2.5,
8987,5870,0,"@whuber: To be honest I was considering both versions, but the one that I wrote about was different from what you proposed. $X_{1}$ and $X_{3}$ can be approximated using normal distribution and then we just approximate the range by the $X_{1}-X_{3}$ which have normal distribution. It works quite well for big n. Your method gives better results, but it is much harder to calculate.",2011-01-01T21:52:06.110,1643,CC BY-SA 2.5,
8988,5870,0,"Thank you for the clarification.  Yes, when $n$ is large, the problem is an easy exercise, because you can be practically certain the range equals $X_1 - X_3$.  Thus the challenge is to compute the distribution when there's an appreciable chance that $X_2$ will exceed $X_1$ or be less than $X_3$.  This happens when $2\beta^2n/(1-2\beta^2)$ is less than 6 or so.",2011-01-01T22:06:39.237,919,CC BY-SA 2.5,
8989,5883,0,Actually this is pretty close to the kind of stuff I'm looking for.,2011-01-02T00:12:52.640,,CC BY-SA 2.5,user2293
8992,2763,0,@btown the margin is too small to contain it!,2011-01-02T05:34:12.510,795,CC BY-SA 2.5,
8999,5887,1,"How did you divide the data? Randomly, by hand, or some other method? Though, in truth, the part about ""successfully created a model"" is a MUCH larger part of the issue. Before doing expensive things, you should see if you're using the appropriate type of model, if you've overfit your training data, and if you have the appropriate data for what you are trying to predict.",2011-01-02T15:06:44.310,1764,CC BY-SA 2.5,
9000,5879,0,"""on the other hand the programmer in me wants an absolute and 100% correct answer"". I'd suggest changing your user base to automatons of some sort.",2011-01-02T15:22:57.477,1764,CC BY-SA 2.5,
9001,5874,0,"How much freedom do you have to change question ordering? For example, a first page that has three or four questions that divide users into several classes (business, private person one job over last X years, private person Y jobs over last X years, private person who does consulting on the side, etc) that might have more homogenous question paths and times. Also, can questions be skipped or must they be answered in order?",2011-01-02T15:56:05.983,1764,CC BY-SA 2.5,
9002,5874,0,"Also, I'd suggest that you display a range, like ""Remaining time: 20-45 minutes"" that narrows as time goes by, not just a point estimate.",2011-01-02T15:59:22.563,1764,CC BY-SA 2.5,
9004,5871,0,"understood, but it is true that for a transient DTMC Lt k->infinity pij(k) =0 for all i,j . How to prove that? We know that E(Mj|X0=i)=sigma k=1 to infinity pij(k). Here Mj= number of visits to state j. We can show that E(Mj|X0=i)=fij/1-fij . We need to show that for transient DTMC E(Mj|X0=i)=finite then it is obvious that pij(k) when k->infinity is zero. But we can't say that fij<1 for all i,j. Then how to prove that this expectation will be finite. Hope I am making my point clear.",2011-01-02T17:54:44.460,2516,CC BY-SA 2.5,
9005,5893,2,"presumably you could get something about geographic location as well, if you could get the appropriate matching data? You could also use information about popularity of first names over time (google ""baby name wizard"") to make inferences about age ...",2011-01-02T18:25:22.133,2126,CC BY-SA 2.5,
9006,5871,0,"The point here is that, if the Markov chain is transient, then $f_{jj}<1$ for every state $j$. To visit $j$ at least $n+1$ times starting from $i$, one first has to reach $j$, which happens with probability $f_{ij}$, and then to come back to $j$ at least $n$ times, which happens with probability $(f_{jj})^{n}$. Thus $E(M_j|X_0=i)$ is the sum over $n\ge0$ of $f_{ij}(f_{jj})^n$, which is finite if and only if $f_{jj}<1$, and then $E(M_j|X_0=i)=f_{ij}/(1-f_{jj})$. (Be careful: your comment is awfully written and you write ""equals zero"" instead of ""converges to zero"".)",2011-01-02T19:22:26.973,2592,CC BY-SA 2.5,
9007,5896,2,"It would be interesting to expand that response a little bit, especially to show how you get this result.",2011-01-02T19:55:59.290,930,CC BY-SA 2.5,
9008,5897,0,Thanks. That's what I started with. But was using an old edition of Walpole which did not have much information on it.,2011-01-02T19:58:52.403,2610,CC BY-SA 2.5,
9009,5874,0,"@Wayne - yes, I too had a range in mind, though didn't voice it. I think that's a rather minor implementation detail. But anyway, nice to know that others think the same way. :) About the ordering - for the most part I can order them any way I like. It makes more sense to group similar questions together though. And, of course, if a question depends on a previous question, then that order is fixed. But big class-defining questions will definitely go up front.",2011-01-02T19:59:06.223,2590,CC BY-SA 2.5,
9010,5874,0,"Questions cannot be skipped as per se, but some can be left empty. Like, if you didn't have any income abroad, then you just leave that table empty. However in most cases I think that will be preceded by a yes/no question in the form ""did you have any income abroad""? Or something like that. If you answer ""no"", then the question with the table will be skipped. It's the ""not applicable"" point in the original question.",2011-01-02T20:00:47.737,2590,CC BY-SA 2.5,
9011,5879,0,"@Wayne, ehh, wishful thinking....",2011-01-02T20:02:02.337,2590,CC BY-SA 2.5,
9012,5899,0,"Why does candidate 1 have only 3 votes? Does that mean he also has 2 ""no hire""? In that case one may choose number 2!!!",2011-01-02T21:23:31.923,582,CC BY-SA 2.5,
9013,5901,0,Why limit tries to 27?,2011-01-02T22:34:17.020,449,CC BY-SA 2.5,
9014,5899,1,Or were two members of the committee out to lunch while candidate 1 was being interviewed? In that case one might treat their votes as 'missing data'.,2011-01-02T22:40:19.187,449,CC BY-SA 2.5,
9015,5901,0,I was thinking that there had to be some kind of limit and that after checking 27 times that you would stop. But thats a presumption that I probably should not have made.. What would you suggest?,2011-01-02T23:32:29.293,,CC BY-SA 2.5,Sycren
9017,5899,0,"These kinds of examples are also better understood if you list all of the ballots cast.In the example you gave, the ballots are based on a point system, so a board member could theoretically give +2 for several candidates, while another board member might only give one candidate a +2.",2011-01-03T02:10:43.323,660,CC BY-SA 2.5,
9018,2563,0,"This isn't just a data analysis question, because many users become aware of the voting system and vote strategically. In other words, the kind of measurement is changing the behavior. Economics (political economy and game theory in particular) has a lot of interesting things to say about choosing ranking algorithms.",2011-01-03T02:16:17.643,660,CC BY-SA 2.5,
9019,5862,0,"You are completely right, that the Histogram is the most understood way to show a distribution. I will try to make histograms with both the axes in log scale.",2011-01-03T03:47:14.080,2586,CC BY-SA 2.5,
9020,5871,0,"yes, I understood my mistake of finding the expectation after posting this. Thanks for pointing it also.",2011-01-03T04:26:12.783,2516,CC BY-SA 2.5,
9021,5887,0,"BTW, I forgot to turn on the cynicism mode before the ""successfully created a model""",2011-01-03T07:28:19.093,2604,CC BY-SA 2.5,
9022,5862,2,"I'm only suggesting using a log scale for the x-axis. I don't think a log scale for the frequency axis would be a good idea, as then the shaded area of each bar of the histogram wouldn't be proportional to the number of observations.",2011-01-03T09:06:18.880,449,CC BY-SA 2.5,
9023,5900,1,"Looks set to kick Gilks, Richardson & Spiegelhalter (1996) into the long grass when that comes it in May.",2011-01-03T09:35:05.333,449,CC BY-SA 2.5,
9026,5910,0,"What about people who have the same name as you... there are a number of ""Dean Harding""s out there, one of them was even a professional footballer! The ""DeanHarding"" on twitter is not me, there's hundreds of ""Dean Harding""s on Facebook, etc etc...",2011-01-03T02:06:36.493,,CC BY-SA 2.5,Dean Harding
9027,5910,0,"That depends on chance, of course. Usually you can find out which one is it by profession, location, etc. though I saw cases where there were 3 persons with the same full name, in the same profession and living roughly in the same area. Then of course it becomes harder :)",2011-01-03T04:02:50.243,,CC BY-SA 2.5,StasM
9028,5761,0,"@James, could you point to some (real or synthetic) data for your problem ?",2011-01-03T11:19:51.490,557,CC BY-SA 2.5,
9029,5905,0,"Good info Dave. Why do you have strong reservations about my proposed ""preference ranking"" based voting system? Note that the CEO bit was just an example... my domain is actually different.",2011-01-03T11:19:56.217,2605,CC BY-SA 2.5,
9030,5904,0,I added some code using JAGS where I seem to be getting a different answer. Where is my mistake? Is this happening because of the priors?,2011-01-03T12:20:31.243,2617,CC BY-SA 2.5,
9031,5896,2,"@chl: Yessir! The OP asks for $E(N)$ where $N\ge1$ is the date of the first success in a series of independent trials, each with probability $p=1/27$ of success. The event $[N\ge n+1]$ corresponds to the $n$ first trials being unsuccessful, hence (by independence) $P(N\ge n+1)=(1-p)^n$. Now, for every nonnegative integer valued $N$, $E(N)$ is the sum of $P(N\ge n+1)$ from $n=0$ to $+\infty$. For every complex number $a$ such that $|a|<1$, the sum of $a^n$ from $n=0$ to $+\infty$ is $1/(1-a)$. For $a=1-p$, one gets $E(N)=1/p=27$.",2011-01-03T13:30:12.853,2592,CC BY-SA 2.5,
9036,5917,1,rajah9's suggestion to have the stakeholders divvy up $100 is a variation of a range voting system (with a rating ballot).,2011-01-03T15:11:50.763,660,CC BY-SA 2.5,
9037,5905,1,"@varuman - I tend to prefer voting systems when you have relatively large constituencies that are unlikely, unwilling, or unable to engage in dialog and deliberation as part of the selection process. On the other hand, small groups that already have open lines of communication would benefit strongly by attempting to select the CEO by a deliberative and consensus-oriented approach. If the group does end up needing to vote numerically, I would be agnostic about which voting system to use until I know more details about the problem space.",2011-01-03T15:16:08.700,660,CC BY-SA 2.5,
9040,5926,1,"Who says they shouldn't be ""too strongly correlated"", i.e. what's the source of that quote?",2011-01-03T17:38:14.300,449,CC BY-SA 2.5,
9041,5915,0,+1 For noticing the strange treatment of critical values in the OP's code!,2011-01-03T17:59:28.553,919,CC BY-SA 2.5,
9042,5918,0,"@user2040 (+1) Those questions are very good ones, indeed! A somewhat related question can be found here: [Feature selection for ‚Äúfinal‚Äù model when performing cross-validation in machine learning](http://stats.stackexchange.com/q/2306/930).",2011-01-03T19:02:13.523,930,CC BY-SA 2.5,
9044,5927,2,"The determinant is not invariant with respect to reparameterization of the problem.  Thus, you cannot expect there to be any universal measure of flatness: you need to translate the ""flatness"" into something *meaningful for the problem.*  Looking at simultaneous confidence regions (which are typically estimated with the Hessian) might be a good place to start.",2011-01-03T19:13:31.297,919,CC BY-SA 2.5,
9045,5823,0,"you make a good point about reporting the change in means and the change in variability separately, and your proposed metrics are nice and intuitive. I am going to have to play around with these, and research some of the leads given by @shabbychef before deciding which path to choose, thanks for your answer",2011-01-03T19:20:26.693,1381,CC BY-SA 2.5,
9046,5825,0,"thanks for these leads, the windsorization approach seems to be the most straightforward, but the L-moments approach also looks promising based on the Vigilone 2010 article, so I will check them out. Can you recommend a good textbook reference?",2011-01-03T19:22:02.763,1381,CC BY-SA 2.5,
9047,5825,1,"@David, my favorite text on 'robust statistics' is Wilcox' 'Introduction to Robust Estimation and Hypothesis Testing.' (2nd edition; 3rd ed to appear in July) http://amzn.to/hcpZm2 This text describes trimmed means, the McKean-Schrader estimate, etc, but does not put them together in terms of coefficients of variation. The author has an R package, see http://www-rcf.usc.edu/~rwilcox/  I do not have a good reference on L-CV other than Viglione, but I believe the L-CV is just the Gini coefficient, up to a rescaling. @onestop would know better than I about that.",2011-01-03T19:41:02.243,795,CC BY-SA 2.5,
9048,5825,0,"@David, Wilcox has a more 'accessible' text, 'Applying Contemporary Statistical Techniques', but I prefer the 'Introduction' text. YMMV.",2011-01-03T19:42:46.080,795,CC BY-SA 2.5,
9050,5927,0,"whuber's caveat notwithstanding, I do think you're on the right track: The expectation of the Hessian (i.e., the inverse Fisher-Information) is the asymptotic covariance of the (asymptotically normal) MLE, which is the main reason you can use it to construct the confidence regions whuber mentions. Its determinant is a suitable (albeit nor invariant, see above) measure of the ""flatness"" of the likelihood around the MLE.",2011-01-03T20:18:15.233,1979,CC BY-SA 2.5,
9056,5927,0,"@whuber, @fabians,",2011-01-03T22:00:46.083,1004,CC BY-SA 2.5,
9057,5927,0,"@whuber, @fabians,  What about using the largest eigenvalue of the hessian of -2 x loglikelihood, ( the hessian should be ideally postive definite at the mle), if largest eigenvalue is ""small"" in some sense, then the likelihood is flat. Also, is there a way to reduce to simultaneuous regions into one number?",2011-01-03T22:06:19.777,1004,CC BY-SA 2.5,
9058,5893,1,I have merged the transferred question with the duplicate.,2011-01-03T22:20:21.507,,CC BY-SA 2.5,user88
9059,5927,0,"That idea does not overcome the arbitrariness-of-scale objection.  For example, suppose you're estimating location and scale for a distributional family and you find that both eigenvalues are 1000.  What does this mean, pray tell?  Why, we don't even know what the units of measurement are!  Unless I give you more information, these statistics are *absolutely meaningless.*",2011-01-03T22:23:43.643,919,CC BY-SA 2.5,
9061,5936,2,"@user2040. I don't understand how would you do the ""type 3"" test? Is it something SAS related? (sorry my SAS knowledge is very limited). I would have done a logistic regression as you suggested but with 2 dummy variables. Also, given that I understand correctly, if you do logistic regression, testing if some or all the coefficients are 0 is done by deviance (or likelihood ratio) and it IS asymptotically Chi-Sq (not necessarily with df = 1)",2011-01-03T22:52:01.170,1307,CC BY-SA 2.5,
9063,5937,0,can you define 'tiny'? Does 'manual' calculation include use of a calculator?,2011-01-03T23:05:15.167,1381,CC BY-SA 2.5,
9064,5944,1,I still need to think about the contrast. I will correct it whenever I get time. Sorry about that,2011-01-03T23:30:56.427,1307,CC BY-SA 2.5,
9065,5942,0,Is it just me or is their closed form incorrect?  Seems like their *n* changes meaning to 365-*n* in the final step.,2011-01-03T23:35:12.740,2622,CC BY-SA 2.5,
9067,5936,1,"@suncoolsu: Yes, practically speaking you should get the same conclusion. I should not have said ""equivalent"" (I work with big data so they end up the same). I added some code in the answer to help clarify.",2011-01-03T23:42:11.503,2040,CC BY-SA 2.5,
9068,5802,0,"If you use ""rsq.rpart(fit)"", where fit is your rpart model, you can see that cp is calculated using the decrease in relative error.",2011-01-04T08:47:36.180,2166,CC BY-SA 2.5,
9069,5937,0,Hi David - calculator is fine - but the idea is to not have too many numbers (Since they all need to be written on the board),2011-01-04T09:24:35.620,253,CC BY-SA 2.5,
9070,5940,0,Hi David - the site you linked to is really great - thank you.,2011-01-04T09:37:36.967,253,CC BY-SA 2.5,
9071,5938,0,"Good idea wolfgang, I didn't think of it.",2011-01-04T09:38:00.350,253,CC BY-SA 2.5,
9073,5949,0,Good example Andy - I've learned something new today :),2011-01-04T09:39:00.427,253,CC BY-SA 2.5,
9074,5956,0,@Gavin (+1) Far more complete response than mine; I just like it.,2011-01-04T10:25:53.780,930,CC BY-SA 2.5,
9075,5956,0,"@Gavin The ""Via the segmented package"" section didn't work for my data. I got a ""No breakpoint estimated"" after running the `segmented` command.",2011-01-04T10:35:10.893,339,CC BY-SA 2.5,
9078,5946,10,"I might just be showing my ignorance here, but isn't the assumption behind ANOVA that the residuals are normal? In that case it doesn't really matter if the variable itself is non-normal, as long as the residuals fit the pattern.",2011-01-04T10:56:08.293,656,CC BY-SA 2.5,
9079,1185,3,"That is very true, and also in psychology, there's an annoying confusion between non-parametric and non-normal, which seems to hinder understanding.",2011-01-04T10:58:48.107,656,CC BY-SA 2.5,
9080,5956,0,"@gd047: Apologies, there was an error in the code I showed. You need to supply argument `seq.Z` with a one sided formula of the variable(s) that have a segmented relationship with the response. I've edited my answer to include `seq.Z = ~ sqft` and added a note about having `segmented` choose values of `psi` for you.",2011-01-04T11:52:34.943,1390,CC BY-SA 2.5,
9081,5956,0,@gd047 I would like to remove my answer as this one addresses your original question in a more better way. Would mind accepting this one instead of mine?,2011-01-04T12:00:51.510,930,CC BY-SA 2.5,
9082,5956,0,"@chl Of course, even though I still get an error : Error in if (model) objF$model <- mf : 
  argument is not interpretable as logical
In addition: Warning message:
In if (model) objF$model <- mf :
  the condition has length > 1 and only the first element will be used",2011-01-04T12:16:19.787,339,CC BY-SA 2.5,
9083,5769,0,"PyIMSL is not open source, is it?",2011-01-04T12:19:47.663,2454,CC BY-SA 2.5,
9084,5956,0,"@gd047: Sorry, first day at work for 2 weeks and the brain isn't working. I've edited the code and it now works for me on R 2.12-patched with the latest version of segmented (0.2-7.3). This did work fine for me at home so I think I introduced errors when copying code into the answer. Check if it works now and if not, check your R and packages are up to date; Vito released a new version of the package in early December.",2011-01-04T12:29:58.170,1390,CC BY-SA 2.5,
9085,5956,0,"@gd047 The last command from @Gavin works provided we pass `model=TRUE` when calling `segmented(...)` (don't know why, even after looking at the source code of `segmented.lm`, line 104?!).",2011-01-04T12:30:33.610,930,CC BY-SA 2.5,
9086,5956,0,"@chl: strange, both work for me without `model = TRUE`, and that is the default for this argument in `segmented`, so I wouldn't expect this to cause a problem.",2011-01-04T12:33:34.423,1390,CC BY-SA 2.5,
9087,5956,0,"@Gavin I'm on a Mac (os x 10.6) with R 2.12 and `segmented` 0.2-7.3. And yes, `model=T` is the default (didn't notice that BTW). And now, it works! Ok, found it: there was an extra `~ sqft` in your original formula.",2011-01-04T12:37:23.337,930,CC BY-SA 2.5,
9088,5956,0,"@chl: Yes, that's right. In my first edit to fix code, I didn't see `seg.Z` explicitly, so added it not realising I had included it as an unnamed argument already. I fixed that problem with my second edit. Not sure why the first version (before all my editing) failed as it worked for me at home with up to date packages and R. My first edit was in error. Apologies for the confusion!",2011-01-04T12:47:07.547,1390,CC BY-SA 2.5,
9089,5956,0,@Gavin No probs. Congrats for such a nice job and thx for the follow-up!,2011-01-04T12:52:44.567,930,CC BY-SA 2.5,
9090,5921,0,Thanks for all your wonderful advice; it‚Äôs much appreciated! My colleague [posted some more info here](http://stats.stackexchange.com/questions/5913/interpreting-two-sided-two-sample-welch-t-test/5961#5961); does this affect anything?,2011-01-04T13:20:03.547,2616,CC BY-SA 2.5,
9091,5915,0,Thanks for all your wonderful advice; it‚Äôs much appreciated! My colleague [posted some more info here](http://stats.stackexchange.com/questions/5913/interpreting-two-sided-two-sample-welch-t-test/5961#5961); does this affect anything?,2011-01-04T13:20:38.007,2616,CC BY-SA 2.5,
9092,5916,0,Thanks for all your wonderful advice; it‚Äôs much appreciated! My colleague [posted some more info here](http://stats.stackexchange.com/questions/5913/interpreting-two-sided-two-sample-welch-t-test/5961#5961); does this affect anything?,2011-01-04T13:22:53.933,2616,CC BY-SA 2.5,
9093,5918,0,"@chi Thank you, I had seen that post. Do you think I am on the right track with my thought process at least? It seems that an independent test set allows us to be more liberal in our use of CV for feature selection and model tuning / selection. Otherwise, nested loops appear required to train, tune and estimate error generalization all using the same training data.",2011-01-04T13:25:25.510,2040,CC BY-SA 2.5,
9094,5915,0,"@Mathias Bynens, if you are really going the way of rounding degrees of freedom, the only fix is to drop the division by 2.",2011-01-04T13:25:33.103,2116,CC BY-SA 2.5,
9095,5960,5,"Intriguing question. I've know nothing about this so won't attempt an answer, but googling bimodality+test gives quite a few promising leads. 
http://www.google.co.uk/search?q=bimodality+test",2011-01-04T13:29:09.263,449,CC BY-SA 2.5,
9096,5942,0,"I don't think so. Just write out the definition of the binomial coefficient, and you'll see that the equality works. Note that ${n\choose k} = {n\choose {n-k}}$",2011-01-04T13:54:29.007,279,CC BY-SA 2.5,
9098,5916,0,"A sample size of 5 is a bit low. Can you increase the minimum a bit, say to 10 or 20? It's not just a question of rounding the d.f. The test is more sensitive to non-normality with small samples, and strictly speaking, your rule of sampling until the margin of error is below some threshold should affect the critical values of the test.",2011-01-04T14:33:00.493,449,CC BY-SA 2.5,
9099,5961,1,Please delete the t-table: it is pointless.  It would suffice to say your are using the upper 97.5 percentile for your critical region.  Of more import is a description of how the testing works and some examples of how the timings are distributed.  These variances differ so greatly that it makes no sense to test for equality of means.,2011-01-04T16:06:17.813,919,CC BY-SA 2.5,
9100,5948,1,"How does it help to suggest that this is an ""undergrad"" question?  Is that relevant to the answer?  Does it suggest ways to search for an answer on the Web?  I hope this remark wasn't intended to belittle the questioner.",2011-01-04T16:29:06.123,919,CC BY-SA 2.5,
9102,5916,0,"@onestop: Could you please explain a bit further? Especially the part about affecting the critical values of the test. A larger sample changes the degrees of freedom, since `df = sample size - 1`, but beyond that I‚Äôm afraid I don‚Äôt really understand what you mean. Thanks in advance!",2011-01-04T17:01:38.713,2616,CC BY-SA 2.5,
9103,5961,0,"@whuber, The t-table is used to calc the margin of error of each benchmark result (each benchmark has a sample of runs).

Because of such difference in variances would a simple comparison of mean+-margin of error suffice?

You can see a sample test http://jsperf.com/benchmark-js-test-page.

Testing is done by determining the min time to run a test (in most cases the smallest unit is 1ms so the time needed for a percentage uncertainty of 1% is 50ms ((1 / 2) / 0.01), so a test is executed repeatedly until the time passed is >= 50ms, the time is clocked, and the executions per second are computed",2011-01-04T17:11:06.040,2634,CC BY-SA 2.5,
9104,5961,1,"@John-David You might must as well show us a multiplication table to illustrate the multiplying you do :-).  The sample test page is well done.  You could enhance it slightly by indicating how many iterations each result is based on.  You might also screen out obvious outliers.  What I'm still unsure of is what you are comparing to what!  If you're comparing my ops/sec to someone else's for the same benchmark, then you are probably best off using *logarithms* of the timing for your comparison.  That should equalize the variances, eliminating that complication.",2011-01-04T17:25:16.357,919,CC BY-SA 2.5,
9105,5967,1,Why does heteroscedasticity in the *independent* variables bother you?  Are you using a mixed model in which they are treated as random variables?,2011-01-04T17:28:16.057,919,CC BY-SA 2.5,
9106,5961,0,"I believe they are comparing different approaches, not different users (though it is not clear if users are aggregated). That could justify different variances, as one algorithm might be amortized or somesuch. However, since runtimes may not be normal, I would recommend a brief look at some data to see if, for example, an Erlang distribution offset from below and censored from above would be a better fit than the Normal. Also, the test stopping condition isn't independent of the data, which should ideally be addressed.",2011-01-04T17:47:56.697,2456,CC BY-SA 2.5,
9107,5916,0,The property that distinguishes the t-distribution from the normal is that it takes into account the sample size. See for example http://www.math.unb.ca/~knight/utility/t-table.htm - note how the critical values are substantially larger at small degrees of freedom.,2011-01-04T17:55:42.493,2456,CC BY-SA 2.5,
9108,5961,0,@whuber I posted the table to show we are not using a formula to resolve the critical value but rather a simple table and to show you/others the values in the table in case you/others took issue with them or caught something.,2011-01-04T17:57:37.937,2634,CC BY-SA 2.5,
9109,5961,0,"@whuber, 

""You might also screen out obvious outliers"" >
So one pass compute standard deviation (SD) and things, remove outliers, then a second pass recompute SD and margin of error, and things?",2011-01-04T18:00:19.450,2634,CC BY-SA 2.5,
9110,5961,0,"@whuber, ""What I'm still unsure of is what you are comparing to what!"" >
In the example *(I linked to earlier)* when you click ""Run Tests"", and it finishes, it will rank each test (""Sort ascending"", ""Sort descending"", ""Don‚Äôt sort at all"", and so on) based on their own mean ops/sec *(so it's comparing different tests to each other)*. The Browserscope results show the results of other browsers users ran the benchmarks in.",2011-01-04T18:01:32.070,2634,CC BY-SA 2.5,
9111,5967,0,"whuber, I am not sure in what direction our model will ultimately go.  I don't think we will use our independent variables as random variables.  And, if we do I'd be concerned that their respective standard error around the regression coefficient is unstable because of heteroskedasticity.  Otherwise, I may be completely wrong to be bothered by heteroscedasticity in the independent variable.  But, I don't know if I should or should not be bothered.  That's the essence of my question.",2011-01-04T18:12:13.110,1329,CC BY-SA 2.5,
9112,5974,0,"I'm measuring the time taken to perform a specific operation, and then running an application that performs that operation repeatedly.  The output is just a dump of the latencies for each operation.",2011-01-04T18:29:45.347,2638,CC BY-SA 2.5,
9113,5961,0,"@John-David Thank you for explaining.  I was confused because the heading for the Browserscope says ""Results"" and it is the only tabular summary.  I now understand you are (a) expressing standard errors as percentages and (b) expressing mean speeds as percentages of the max.  Both indicate you are already thinking in terms of *relative* (*i.e.*, proportional) comparisons, which strongly suggests basing the analysis on logs.  (A difference in logarithms translates to a proportional difference in the original units.)",2011-01-04T18:32:32.773,919,CC BY-SA 2.5,
9114,5961,0,"@John-David As far as the t-table goes, it's just laying out the results of 100 calculations, exactly like a multiplication table would lay out 100 multiplications.  It doesn't matter *how* you get the t-values; what matters is *what they represent.*  You are using them to construct two-sided 95% confidence intervals around the mean; that's all we (or anyone else) need to know.",2011-01-04T18:34:17.450,919,CC BY-SA 2.5,
9115,5974,1,"so if you're getting different regression outputs for each test right? i may be misunderstanding, but can you're question also be stated as ""does this model A offer more explanatory power than this model B""?",2011-01-04T18:37:41.833,2644,CC BY-SA 2.5,
9116,5961,0,"@whuber, Thanks for your advice. Would you point me to some resources, or specific formulas for comparing logarithms, I can use/learn about?",2011-01-04T18:37:54.657,2634,CC BY-SA 2.5,
9117,5963,0,How will you identify the centers of the two normal distributions?,2011-01-04T18:41:59.587,2535,CC BY-SA 2.5,
9118,5973,0,@user2643: Do you have any references to share on how you created the nested CV? Was it along the same lines as the pdf I linked to in my question? Also.....is this data marketing data by chance?,2011-01-04T18:50:18.943,2040,CC BY-SA 2.5,
9119,5916,0,"Oh, that's not actually a relevant. The question was about your sampling rule affecting the test. The issue there is that the t-statistics you are comparing to are built with the assumption that consecutive measurements are independent, but yours are not. Since you stop after a margin is sufficiently low, you are stopping all cases where the margin dips below the threshold but would soon come back up, and therefore you slightly overstate significances.",2011-01-04T19:05:20.607,2456,CC BY-SA 2.5,
9120,5973,0,"@user2643 The problem with that approach (which is correct) is that it only yields a single criterion for accuracy (classification) or precision (regression); you won't be able to say ""those are the features that are the most interesting ones"" since they vary from one fold to the other, as you said. I've been working with genetic data (600k variables) where we used 10-fold CV with embedded feature selection, under a permutation scheme (k=1000, to be comfortable at a 5% level) to assess reliability of the findings. This way, we are able to say: ""our model generalizes well or not"", nothing more.",2011-01-04T19:09:30.337,930,CC BY-SA 2.5,
9121,5980,0,May I ask you to clarify your answer.  I can't tell if you think the independent level variables are a problem and should be transformed or not.,2011-01-04T19:13:13.093,1329,CC BY-SA 2.5,
9122,5977,0,"Canyou clarify answer.  You start by suggesting that heteroskedasticity is a problem.  And, you move on by suggesting that multicollinearity is more of a problem and that heteroskedasticity is Ok.",2011-01-04T19:15:42.783,1329,CC BY-SA 2.5,
9123,5968,0,"+1 but (1) You can format your code using indentation (with 4 spaces or through the online editor); (2) Some comments around your code might be appreciated because users that are not familiar with Matlab syntax will have hard time to understand what the above code chunk actually does. As far as I understand, predicted class memberships on the test sample are stored at each iteration, and then averaged to yield the classification CR.",2011-01-04T19:16:37.083,930,CC BY-SA 2.5,
9124,5972,1,"You would probably want to treat k/n>0.25 cases as $\alpha=0.5$ and not 0, but that does not change the bias issue.",2011-01-04T19:17:24.453,279,CC BY-SA 2.5,
9125,5962,2,"Re disappointment: You wouldn't expect each problem to have the same parameters, so why *would* you expect the problems to share good values for the hyperparameters (log gamma and C)?",2011-01-04T19:19:26.017,1739,CC BY-SA 2.5,
9126,5972,0,"@Aniko: Of course You are right, I've already changed it.",2011-01-04T19:21:16.220,1643,CC BY-SA 2.5,
9127,5945,1,"I'm not really up on SVM theory, but the 'flatness' in the kernel-machines discussion you link to seems to amount to: 'has small *second* derivative' (think of the typical motivation for spline smoothing models).",2011-01-04T19:32:00.497,1739,CC BY-SA 2.5,
9128,5873,0,"@whuber, thank you for directing me toward Wald's SPRT. What's the correct way to compute the likelihood ratio for this experiment?And does http://www.quantitativeskills.com/sisa/statistics/sprt.htm count as a glib web site, or does their calculator take everything into account?",2011-01-04T19:34:23.270,2591,CC BY-SA 2.5,
9129,5977,0,"well, without seeing the data I cant tell either way, which is a problem. I was simply stating the actual effects of multicoll and heterosked, because it seems as if in your answer you are concerned with heterosked and the standard errors around the regression plane, but as I understand it, multicollinearity is more of a problem when concerning the standard errors and when estimating coefficients, not heterosked. I'm just unsure of which one actually might be the problem.",2011-01-04T19:41:54.587,2644,CC BY-SA 2.5,
9130,5950,1,so this is basically regression with a 'tube' loss function (0 penalty for points +/- epsilon of the prediction) rather than the quadratic loss function from OLS?,2011-01-04T19:43:51.930,1739,CC BY-SA 2.5,
9131,5982,0,Could you add a bit more context please? e.g. what R function and package are you using?,2011-01-04T19:50:44.087,449,CC BY-SA 2.5,
9132,5962,0,"@Conjugate Prior: The training sets are subsets of the same experiment, and the negative training sets are sampled from the same population, so I had hoped that the same RBF kernel width …£ would be effective.  Because the positive sets are being discriminated from the same background (negative) population, I had hoped that the ideal penalty C would be similar as well.  If this is not the case, it makes SVM really hard to apply.  Gentle boosting, for instance, seems much easier to tune.",2011-01-04T19:51:39.807,220,CC BY-SA 2.5,
9133,5950,0,"@Conjugate Prior: yes, usually kernel regression minimizes an 'epsilon-insenstive loss' function, which you can think of as $f(x) = (|x| - \epsilon)^+$ see e.g. http://kernelsvm.tripod.com/ or any of the papers by Smola _et al_.",2011-01-04T19:51:47.690,795,CC BY-SA 2.5,
9134,5962,0,"Aha.  But it seems to me that although it is the same experiment in the physical sense, you are nevertheless attacking separate and different problems in the statistical sense.  Particularly if the negative cases are resampled for each treatment.",2011-01-04T19:59:07.473,1739,CC BY-SA 2.5,
9135,5950,0,@shabbychef  Thanks.  I always wondered what was going on there.,2011-01-04T20:00:03.967,1739,CC BY-SA 2.5,
9136,5950,0,"@Conjugate Prior: I don't think this is actually the desired loss function, but the math ends up working out well, so they ran with it. At least that's my suspicion.",2011-01-04T20:02:19.327,795,CC BY-SA 2.5,
9137,5982,0,A link explaining why random-walk needs a burn-in also would help.,2011-01-04T20:12:08.037,2116,CC BY-SA 2.5,
9138,5980,0,"@Dmitrij Celov, I've upvoted your answer, since I was intending to write something similar, but please fix the language",2011-01-04T20:18:44.600,2116,CC BY-SA 2.5,
9139,5948,0,"@whuber, no the remark was made with a smile and happy memory of Prof. Bentley's combinatorics class. No disparagement was intended.",2011-01-04T20:21:04.110,2591,CC BY-SA 2.5,
9140,5948,1,I am very glad to read your clarification.  It's easy to misunderstand parenthetical comments when they are only written and not spoken.,2011-01-04T20:24:56.530,919,CC BY-SA 2.5,
9141,5977,0,I think heteroskedasticity is a problem regarding the standard error and the related confidence intervals around a specific regression coefficient.,2011-01-04T20:33:34.450,1329,CC BY-SA 2.5,
9142,5873,1,"@rajah9 That site looks balanced and reasonable.  I did not read it in detail, but I looked for unsupported assertions (like ""always do this"" or ""10 is a minimum"", etc.) and didn't find any.  I did find a straightforward applet and lots of references, both of which are good signs of quality.  As always, don't rely on *anybody's* software until you have checked it out with data where you know the answer.",2011-01-04T20:33:46.427,919,CC BY-SA 2.5,
9143,5961,0,"@John-David I'm sure the Web is full of such resources.  I posted some materials relevant to statistics, for post-graduate professionals who may have forgotten all about logarithms, at http://www.quantdec.com/envstats/notes/class_03/gm_etc.htm .",2011-01-04T20:36:29.170,919,CC BY-SA 2.5,
9144,5980,0,"Even though you try to apply stepwise selection it is not a good practice to pool into one set of the variables those that are in levels (trendy one) and flows (% changes, changes, etc.), especially when you try to model variable that is a % change. My guess is if you do include the lagged level of mortgage loans you could be surprised even more. In this case think about error-correction models if they can help you. Pardons for bad English.",2011-01-04T20:37:40.580,2645,CC BY-SA 2.5,
9145,5967,0,"whuber, giving this more thought here is why an independent level variable bothers me...  It is because of its temporal dimension.  Let's say you consider what happens to the dependent variable when the Dow is at 5000.  Well, a couple of decades ago this may be an economically bullish indicator several thousands points above the relevant level at the time.  But, today it has a completely different meaning.  It would entail another wrenching crash with dire economic implication.  Thus, I don't see how such level variables could really work.",2011-01-04T20:38:42.967,1329,CC BY-SA 2.5,
9146,5982,0,"Well we will use diffinv() directly, but the burn-in default could be found in arima.sim() function.",2011-01-04T20:39:01.733,2645,CC BY-SA 2.5,
9147,5969,0,It seems that you're on the right track but you might want to look at the interaction between cognitive ability and your other predictors.,2011-01-04T20:42:10.907,601,CC BY-SA 2.5,
9148,5969,0,@John that's exactly what I want to do. Are you suggesting that I'm not able to do that with mixed-models?,2011-01-04T21:01:46.387,2322,CC BY-SA 2.5,
9149,5973,2,"If features vary from fold to fold it means that there isn't enough information to confidently identify the useful features, so I'd view that as an advantage of cross-validation (as just looking at the results from a single model is likely to have over-fit the feature selection criterion and hence be misleading).  For problems with many features and few observations, ridge regression often gives better performance, so unless identifying features is a key goal, it is often better not to do any feature selection.",2011-01-04T21:08:48.567,887,CC BY-SA 2.5,
9151,5989,1,Hah!  I think my proposal is worse than yours :-).,2011-01-04T21:18:28.043,919,CC BY-SA 2.5,
9152,5986,0,"I've got two questions (at least for now):$

$
1) What is the value of estimator g when n=16 and k=8? I'm asking because it seems to be complex.
2) Could You elaborate $h_{\lambda}(k,n)=\lambda(n)k/n$? I'm not sure what $\lambda(n)$ is.",2011-01-04T21:20:53.630,1643,CC BY-SA 2.5,
9153,5989,0,The no-free lunch theorem suggest that there will be problems where it works well and other algorithms dont! ;-P,2011-01-04T21:21:59.787,887,CC BY-SA 2.5,
9154,5990,0,"+1 If you wanted a straw man, that would indeed be a good one!",2011-01-04T21:25:13.497,887,CC BY-SA 2.5,
9155,5986,0,"@Tomek Good catch: I simply left out the most crucial part of the definition of $\delta$!  It's in now.  It is the obvious one: when the argument of the square root is negative, just set it to zero.  In other words, estimate $\alpha = 1/2$ whenever $n/4 \le k$.  (There was no such error in the plots, however: this was only a typo.)  $\lambda$ is simply a number but it can change from one $n$ to another.  For a fixed $n$ that makes $h$ a linear function of $k$.",2011-01-04T21:47:29.780,919,CC BY-SA 2.5,
9156,5942,1,"Yes, ""more manageable"" for the question of doubles, assuming only a d6 and not a d365.  For triples, etc., you need more powerful methods.",2011-01-04T22:04:56.787,919,CC BY-SA 2.5,
9157,5947,2,"I just want to comment that anyone familiar with the Binomial theorem and multinomial coefficients can easily use this generating function to write down a closed-form formula for any specified pattern (a partition of the number of dice).  It is a product of two multinomial coefficients.  The point of this response is that you can go pretty far without any such knowledge, solely by exploiting generating functions.",2011-01-04T22:08:29.437,919,CC BY-SA 2.5,
9158,5989,0,Thank you.  Do you have a favorite reference or references to these NFL theorems?,2011-01-04T22:09:39.887,919,CC BY-SA 2.5,
9159,5990,0,"@Dikran You have read my answer as it was intended: tongue in cheek.  It's amusing to try to think of anything that could be worse and somewhat amusing to think of more sophisticated attempts that, in the end, haven't performed any better than a coin flip!",2011-01-04T22:11:05.517,919,CC BY-SA 2.5,
9160,5989,0,"The original papers were by David Wolpert, there are some references and info at http://www.no-free-lunch.org/ .  ""no free lunch for the sandwich"" would be a favourite reference, but only because of the title ;o)",2011-01-04T22:51:10.740,887,CC BY-SA 2.5,
9161,5986,0,"@Whuber: Great, now I know how to make the calculations that You have made, just to clarify $h_{\lambda}=\lambda(n)k/n$ should be changed to $h_{\lambda}=min(0.5,\lambda(n)k/n)$. Thank You very much for the answer, I'm still considering whether something better might be done.",2011-01-04T23:16:15.717,1643,CC BY-SA 2.5,
9162,5980,0,"Dmitrij, intuitively I really agree with you.  And, that is why I am asking the question.  But, what is the mathematical argument supporting our position?  I came up with a reasonably good logic argument as shown in my comments above answering whuber.  But, logic is not math...",2011-01-04T23:16:41.550,1329,CC BY-SA 2.5,
9163,5962,1,"BTW, grid search is rather inefficient, the Nelder-Mead simplex optimisation algorithm is very effective, as are gradient descent optimzation methods.  Grid search is simple, but a bit ""brute force"".",2011-01-05T00:59:36.573,887,CC BY-SA 2.5,
9164,5990,1,"I am curious if one can do even _worse_ than a coin flip in the case of unknown proportions of classes, but it seems probably not.",2011-01-05T01:07:32.037,795,CC BY-SA 2.5,
9165,5986,0,"@Tomek Yes, good observation.  In fact, the $h$ I ended up using is $\min(1/2, \lambda k / n, \lambda (n-k) /n)$.  Although this is not precisely linear in $k$, it is linear in a ""constrained"" sense.  Linearity doesn't really matter; what matters is to find an effective estimator of *any* form.",2011-01-05T01:42:04.977,919,CC BY-SA 2.5,
9168,5969,0,not at all... I'm suggesting that what you want to find is an interaction between cognitive ability and your other predictors.  All you need to do is add them to the model.,2011-01-05T03:39:37.527,601,CC BY-SA 2.5,
9170,5950,0,"@shabbychef: I'm still lost. Consider the one-dimensional case: $y = \theta x$. All minimizing $\theta$ does is give you a more *horizontal* line. It seems to have nothing to do with the second derivative, which I think you're referring to (""smoothness""). And if my sample points are (0,0) and (1,1e9), why would I prefer a flatter line? I.e., say my $\epsilon$ tolerance is 1 - why would I prefer the flatter line from (0,0) to (1,1e9-1) ($\theta=1e9-1$) instead of the line through (1,1e9) ($\theta=1e9$) or the line through (1,1e9+1) ($\theta=1e9+1$)?",2011-01-05T05:43:10.450,1720,CC BY-SA 2.5,
9171,5950,0,"@Yang: I do not see it as a ""smoothness"" issue either. The page at http://kernelsvm.tripod.com also refers to flatness as  ""model complexity,"" which is more in line with my view. For example, if most of the elements of the vector $\theta$ were zero, you would consider this a less complex model. Flatness is a more continuous version of this. The main problem with your counterexample is that the $\epsilon$ tolerance is far too small for the units in which you measure the response $y$.",2011-01-05T06:29:28.210,795,CC BY-SA 2.5,
9172,5996,0,"Well, I do know that the process is not stationary. Burn-in period's purpose is to be more or less confident that the starting history values are already present in the stochastic trend, in the sense that we may assume that we have started from the long long past. It is not clear why I have to go from zero if I don't want to. And else it would be interesting to hear some suggestions about the same questions when the process is random walk with drift component.",2011-01-05T07:12:00.373,2645,CC BY-SA 2.5,
9173,5980,0,"Gaetan, search for the information in Brooks ""Introductory Econometrics"" or whenever about the ""spurious regression"", ""error correction models"". Working with time series non-experimental objects is a bit (but significantly!) different from what you see in general statistics. So you may be concerned about at least several things: are your regression results significant purely at random (an easy check is to perform rolling regression and see the evolution of parameter estimates), are the level parameters more or less the same (then linear restriction? but opposite in sign!). Be cont.",2011-01-05T07:25:59.357,2645,CC BY-SA 2.5,
9175,5973,0,"@Dikran I should have give more precision about my study: Yes, the objectives was feature selection (in an $n\ll p$ context, as commonly found in [genome-wide association studies](http://en.wikipedia.org/wiki/Genome-wide_association_study)). When the outcome is univariate, I still prefer the elasticnet criterion, but anyway it's not the purpose of the question. I like your response (+1).",2011-01-05T07:54:24.203,930,CC BY-SA 2.5,
9176,5963,0,"@venkasub, the center of normal distribution is its mean. If you know that bimodality comes from two normal distributions you can fit mixture model to estimate the means, which will be two modes.",2011-01-05T09:00:10.597,2116,CC BY-SA 2.5,
9177,5990,0,"I suspect it is possible to make a classifier that is worse than coin flipping by using an overly complex model (such as an ANN with a large hidden layer) and too little data.  While it will out-perform coin flipping on the training set, it may well be worse on the test set.  AFAIK, there is no guarantee that an overfit model won't be arbitrarily bad on the test set.",2011-01-05T09:07:42.863,887,CC BY-SA 2.5,
9181,5969,1,"I think it would be great if you could provide a few example of the exact nature of the independent variables (i.e., how many factors and how many levels). I think you can get pretty far with traditional GLM depending on the exact nature of your designs.",2011-01-05T10:16:57.187,442,CC BY-SA 2.5,
9183,5990,0,I would say flipping a coin is the best you can do when the information you have is not related with the classification you want to make. Hence there are worst things to do ?,2011-01-05T13:18:03.583,223,CC BY-SA 2.5,
9184,6007,1,for this purpose you can see the stringr package of H. Wickam,2011-01-05T13:56:03.123,1154,CC BY-SA 2.5,
9186,5990,1,"In addition to Dikran, the worst classifer I can imagine would be a classifier trained on a small amount of data which performs sometimes better and sometimes worse than random so that the negation is not possible, i.e. the is quality systematically unpredictable (but this seems ridiculous, so I'll stick with whuber).",2011-01-05T14:06:01.827,264,CC BY-SA 2.5,
9188,6009,0,@mpitkas (+1) Nice one!,2011-01-05T14:13:48.870,930,CC BY-SA 2.5,
9189,6009,0,"@mpitkas Ah, and yours is faster (by 0.004, on my fake data set). This the way to go...",2011-01-05T14:18:52.020,930,CC BY-SA 2.5,
9190,6009,0,And sorry to have mispelled your name (two times) :(,2011-01-05T14:23:14.263,930,CC BY-SA 2.5,
9191,5990,1,"@steffen I think you're right: complete unpredictability can be worse than random.  There is a difference!  But although there is no probability model available, we can analyze this situation with game theory.  It's you *vs* the classifier.  Assume the classifier is out to get you: it will do its worst.  What is your optimal strategy?  The answer, in the absence of any other information, is to flip the coin: that guarantees you won't lose more than half the time in the long run. What ruins this analysis is the eternal hope that *our* classifier is actually good, so we rarely use this strategy.",2011-01-05T14:23:57.053,919,CC BY-SA 2.5,
9193,6009,0,"@chl, no worries, my surname is constantly misspelled by my native language speakers, so my expectations for not misspelling the nickname are really low :)",2011-01-05T14:35:39.157,2116,CC BY-SA 2.5,
9194,5817,0,"Given the silence here thusfar, you should try sending this query to the R-SIG-Mixed-Models mailing list.",2011-01-05T14:44:09.813,364,CC BY-SA 2.5,
9195,5817,1,"Also, during the Bayesian Data Analysis session at the recent annual meeting of the Psychonomics Society, it was mentioned that Bayesian approaches can capture scenarios like this where you have two tasks but want to analyze the data simultaneously, letting the data from each task influence inference about the other. I'm working my way through the happy puppy Bayes book (http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/), but still not to the point where I can describe how to achieve this analysis.",2011-01-05T14:48:45.440,364,CC BY-SA 2.5,
9196,5969,0,"@Henrik Does traditional GLM do well with repeated measures (i.e. can it remove the subject variance)and continuous IVs? If so, can you provide me with examples or references?",2011-01-05T15:19:14.377,2322,CC BY-SA 2.5,
9197,6011,0,"Thanks - I know I will need the log-likelihood to get the DIC, but I still don't know how to actually calculate P(D|theta)",2011-01-05T15:39:35.677,2654,CC BY-SA 2.5,
9199,5714,4,"I develop milk. If either of you run into any problems, please let me know by email (lpc at cmu dot edu). Bug reports generally get fixed in under 24 hours.",2011-01-05T16:25:30.213,2067,CC BY-SA 2.5,
9201,5987,0,"Mathematically speaking, in binary classification, knowing the worst is equivalent to knowing the best. If you know the best you reverse the rule and it becomes the worst. Why do you want the worst rule ? what do you mean by worst ?",2011-01-05T17:07:26.863,223,CC BY-SA 2.5,
9202,6011,0,"Thanks onestop - I suspected this was model dependent. So if I have a model that is say, a Poisson regression with a bunch of covariates, then for each Y_rep data point, I calculate the probability that it came from a Poisson(lambda) distribution? - where lambda is a fitted parameter value.",2011-01-05T17:35:15.517,2654,CC BY-SA 2.5,
9203,6005,0,Just a programming note: it is useless (and will slow down your script) to create a function that just calls another function. There is no real advantage in calling `find_domain` if the only thing it does it's calling `grep`. You are just adding the overhead of another function call.,2011-01-05T17:47:59.513,582,CC BY-SA 2.5,
9204,6007,0,Thanks for pointing out the rstring library!! Awesome approaches! Thanks!,2011-01-05T17:53:09.863,18462,CC BY-SA 2.5,
9205,6005,0,you are right! this was a dirty approach and just sounded sort of wrong design though it worked. Am glad that i asked!,2011-01-05T17:55:52.870,18462,CC BY-SA 2.5,
9206,6011,0,"Yes, that's it.",2011-01-05T18:02:56.263,449,CC BY-SA 2.5,
9207,5916,0,"@sesqu One of the reasons for stopping it after it reaches 1% is so tests don‚Äôt run forever or all run for 8 seconds. Would it be better to just stop after some time (like 8 seconds) with no percent threshold or lower the threshold to below 1%? We also use 1% as a cutoff for calibrations that are used to counter the cost of some testing overhead, and taking it down to 1% can take some time and 750+ sample sizes. Is there a way to factor in the cutoff to counter any negative affects on significance?",2011-01-05T18:04:54.577,2616,CC BY-SA 2.5,
9208,5996,2,"If I understand correctly, you want to simulate a stochastic trend. Why not initialize $Y_0$ to a random value, say a sample from $N(0,\sigma^2)$ with $\sigma^2$ large?",2011-01-05T18:22:51.777,1670,CC BY-SA 2.5,
9209,5996,0,"One more comment.  The problem with burn-in is that, in addition to being non-stationary, the process is not ergodic.  So what you are asking for is not really possible.  You will always be able to infer (roughly) what the initial value of the process is.",2011-01-05T18:39:40.617,1670,CC BY-SA 2.5,
9211,592,0,"That's all? I'm surprised, cos it is a really good book.

The same author wrote an excellent introduction to Time Series Analysis as well.",2011-01-05T19:41:23.207,57,CC BY-SA 2.5,
9212,5916,0,"Yes. The easy way would be to stop at 8 seconds every time. To counter the effects, you would divide each p by 0.95^i-1, depending on what you're really doing.",2011-01-05T20:01:31.050,2456,CC BY-SA 2.5,
9213,6000,1,"Depending on how this suggestion is interpreted, it may amount to either Fisher's Exact Test or Barnard's Test.  See http://en.wikipedia.org/wiki/Fisher%27s_exact_test .",2011-01-05T20:06:21.827,919,CC BY-SA 2.5,
9214,6015,2,"+1 for addressing the apparent (rather than the stated) question, which is much more to the point.",2011-01-05T20:07:45.010,919,CC BY-SA 2.5,
9215,5817,0,"@Mike Lawrence Thanks for the tip. You'll probably see this on the R-Sig list soon. Although I'd lose power doing it, I can address the problem you mention below about the cell with 0 trials by running 2 separate models. I missed the Bayes session at pnomics, but find the idea of using a Bayesian approach to dual-tasks intriguing. I don't know much about Bayesian analysis, but it has me thinking about how the Bayes approach may be different from fitting a covariance matrix in mixed-models.",2011-01-05T22:28:17.900,2322,CC BY-SA 2.5,
9216,5603,0,"I think my use of the word ""state"" probably does imply that -- although I did want to think about a ""continuous"" situation, where the ""birth rate"" of the process declines gradually (perhaps precipitousness) as the population grows.",2011-01-06T00:29:29.797,446,CC BY-SA 2.5,
9218,5769,0,"Correct. The PyIMSL algorithms are Python wrappers to the IMSL C Numerical Library (~500 math and stat functions), which is not open source. PyIMSL is free to use for non-commercial use though.",2011-01-06T04:20:35.717,1080,CC BY-SA 2.5,
9219,6020,1,"the correlation is calculated for each pair of variables separately. So conceptually there is no difference, you just need to repeat the analysis; in R you could use lm(y~x)",2011-01-06T06:41:54.410,1381,CC BY-SA 2.5,
9220,6025,1,"Thanks for the advice.  Re: lognormal - I could make the ratios of percentiles to median work out by subtracting 7077 from everything, then adding it back in at the end.  How bad an idea would that be?",2011-01-06T09:04:44.860,2665,CC BY-SA 2.5,
9221,6020,0,"Thanks, I will look into using 'lm'.  Do you have any further information of how I can analyze such a dataset?  Is it called correlation or regression?",2011-01-06T09:26:53.213,2664,CC BY-SA 2.5,
9222,6014,0,"Thank you for the great answer! I do have a couple of questions: is it correct to assume that ""chance of chi-square variate exceeding X"" is essentially the chance that a random selection between 1-2.3E-111 would produce the number? or? And, is there any software that I can use to quickly calculate these metrics, or is doing it by hand the best option? Thank you!",2011-01-06T10:56:36.447,,CC BY-SA 2.5,Sam Thropton
9223,6025,1,"Good point, that would give a 'shifted log-normal distribution'. The log-normal and the log-logistic are pretty similar in shape apart from the heavier tails of the latter, so you could try both and compare results.",2011-01-06T11:40:34.420,449,CC BY-SA 2.5,
9224,6030,0,"(+1) I forgot the `Sys.sleep()` function. I think the $x$- and $y$-axis range should bet set up in advance, no?",2011-01-06T12:26:01.880,930,CC BY-SA 2.5,
9225,6024,0,"Seems to me there might be some better way of formulating or parameterising the problem, but it's hard to tell without knowing a bit more.. could you give us some idea what the $x_i$s are and how you know the underlying model is of that form?",2011-01-06T12:33:08.347,449,CC BY-SA 2.5,
9226,6030,0,"@chl, yes the $x$ and $y$ axis ranges should be set up in advance.",2011-01-06T13:30:45.077,2116,CC BY-SA 2.5,
9227,6030,1,"@mpiktas Or just init the plot with a single call to `plot()`, possibly with `0,0,type=""n""` if there is nothing to plot yet... Indeed this is much easier than ding `plot.new()` and adding all the stuff like axes or labels by hand.",2011-01-06T13:49:09.310,,CC BY-SA 2.5,user88
9228,6033,4,This is similar to http://stats.stackexchange.com/questions/5700/finding-the-change-point-in-data-from-a-piecewise-linear-function,2011-01-06T14:09:13.907,2116,CC BY-SA 2.5,
9229,5996,0,"So if I understand well building up some common stochastic trend (to have cointegrating relationship) do not affect the Monte Carlo simulation results whatever the starting value will be? On the other hand it is the ergodicity property that insures the ""memory"" length, that after some period of development you can not tell what the starting value was.Again in Gaussian random walk case how could you infer (roughly)the initial value if you drop sufficiently large burn-in part, if you don't or do know the length of burn-in (say T) the confidence interval of +/- 2$\sqrt{T}$ is too rough isn't it?",2011-01-06T14:15:01.373,2645,CC BY-SA 2.5,
9230,6019,0,"Thanks for a pointer to some sample code, I'll check it out!",2011-01-06T15:10:51.720,2659,CC BY-SA 2.5,
9231,5996,1,"Suppose $Y_t = \alpha Y_{t-1} + \epsilon_t$ where $\epsilon_t$ are i.i.d. $N(0,1)$ and $|\alpha| \leq 1$.  The conditional distribution of $Y_t$ given $Y_0$ is $N(\alpha^t Y_0, \sigma_{\alpha,t}^2)$, where $\sigma_{\alpha,t}^2 \leq t$ depends only on $\alpha$ and $t$.  If $\alpha = 1$, note that $Y_t$ given $Y_0$ remains centered at $Y_0$ even as $t \to \infty$.  In that sense, you can always infer $Y_0$.  On the other hand, if $|\alpha|< 1$ then the conditional mean of $Y_t$ given $Y_0$ tends to 0 as $t\to\infty$ so that asymptotically $Y_t$ is conditionally independent of $Y_0$.",2011-01-06T15:42:26.200,1670,CC BY-SA 2.5,
9232,6014,0,"@Sam The chi-square statistic is a measure of association of the two variables (""attribute"" and renewing).  When there is no association, the chi-square statistic should act like the 500 renewers occurred at random in the population of buyers, regardless of the values of ""attribute.""  In this model, the number 3E-111 estimates the chance that chi-square could be as large as it turned out to be.  You can do all the calculations in Excel, although stats software like R (free), Stata, SAS, JMP, Mystat (free), etc., is easier and more reliable.",2011-01-06T15:48:31.877,919,CC BY-SA 2.5,
9233,5996,1,"If $|\alpha| = 1$, then $\sigma_{\alpha,t}^2 = t$. If $|\alpha| < 1$, then  $\sigma_{\alpha,t}^2 = (1-\alpha^t) / (1-\alpha)$.  So although you can infer $Y_0$ from $Y_t$ when $\alpha = 1$ the standard error will be of the order $\sqrt{t}$.  I think to better answer your question you need to be less specific and explain what you will use $Y_t$ for.",2011-01-06T15:49:48.410,1670,CC BY-SA 2.5,
9235,6033,0,"Yes, and the answers will be the same, too.",2011-01-06T15:54:15.817,919,CC BY-SA 2.5,
9236,6032,2,Any three-parameter distribution is guaranteed to fit three quantiles *perfectly.*  Thus it makes sense to use this approach to fit only one or two parameters.  It also does not make any sense to compare a one-parameter fit to a two-parameter fit (with a different family) based on likelihood alone.,2011-01-06T16:04:42.640,919,CC BY-SA 2.5,
9237,6027,1,Polynomials and splines are unlikely to be valid CDFs.,2011-01-06T16:05:25.043,919,CC BY-SA 2.5,
9238,6025,0,Compare how?  The shifted lognormal is guaranteed to fit the quantiles perfectly.  Almost any three-parameter family will fit perfectly.  How do you compare two perfect fits?,2011-01-06T16:31:41.153,919,CC BY-SA 2.5,
9240,6025,0,@whuber I meant compare the resulting predictions for the percentiles corresponding to other values,2011-01-06T17:13:01.523,449,CC BY-SA 2.5,
9241,6025,0,"I'm missing something: what other values?  The OP states that *only* three percentiles are available, nothing else.",2011-01-06T17:38:31.793,919,CC BY-SA 2.5,
9243,6027,0,"Good observation. In this case, the usual quadratic polynomial fails to work, but there are infinitely many quadratic splines to choose from (think B√©zier) that should not have the same problem (though some might still require domain cropping). Similarly, it should be possible to find a suitable monotonic cubic spline. I am aware of spline algorithms that guarantee monotonicity, but am unable to find one just now, so I have to leave the matter at ""pick something you like that works as cdf"".",2011-01-06T17:46:40.633,2456,CC BY-SA 2.5,
9244,6039,4,"Since Principal Components Analysis and sampling have almost nothing to do with each other--one analyzes data and the other determines what data to collect--I am wondering whether you mean something else by ""PCA"". ""Point of Closest Approach""? ""Prompt Corrective Action""? ""Physical Configuration Audit""? ""Permanent Corrective Action""? ""Power Control Algorithm""? ""Process Control & Automation""? ""Parallel Cellular Automata""? ""Product Complaint Analysis""? ""Probable Course of Action""? ""Poodle Club of America""?",2011-01-06T17:47:26.590,919,CC BY-SA 2.5,
9245,5996,0,"Thanks for the explanation, and even though it is correct I'm still a bit confused. My point was since you do drop the burn-in period, you can't track back to the original point, but do continue from some point $Y_T$, pretending that there was some history to get you to the point. Therefore you can't say for sure what was the original $Y_0$ value you started from, and even more due to self-similarity it will be $Y_T$ the new point to build the mean value (again for a particular realization! not on average). Building the question I was just curious what usually people do for RW in Simulations.",2011-01-06T18:24:04.470,2645,CC BY-SA 2.5,
9247,5996,0,"I was not originally planning to give more details on the experiment we do perform, but it seems that we need more details on the DGP in simulation experiments. Suppose we try to simulate to factors $F_{i,t} = \alpha_i F_{i,t-1} + \beta_i G_{t} + \nu_{i,t}$, $i = 1,2,\dots,k$, where $\nu_{i,t}$ (and any other residuals) are i.i.d $N(0,1)$, where $G_t$ is a common stochastic trend. Then we build many ""factor-based"" series $x_{j,t} = \sum_i \lambda_{i,j}F_{i,t} + \varepsilon_{j,t}$, $j = 1,\dots,n$, where $\lambda_{i,j}$ are randomly drawn from $N(0,\sigma_j^2)$. The same factors are used for",2011-01-06T18:42:36.060,2645,CC BY-SA 2.5,
9248,5996,0,"the dependent variable $y_{t} = \sum_i \gamma_{i}F_{i,t} + \varepsilon_{t}$. The point is how to bring common stochastic trend into the model, and when we can say that the trend is nice. May be in this case it indeed doesn't matter if we start from ""looking as in real data"" trend or actually take any part of it... in any case we will standardize the auxiliary processes latter on, but would like to see at least by simulations if the problem we are working on (actually it is temporal disaggregation) could be done for non-stationary but co-integrated data. Thanks again for details you provided :)",2011-01-06T18:48:58.830,2645,CC BY-SA 2.5,
9249,6028,0,Thanks a lot for your reply! I'm having a hard time de-ciphering your text-based formulas. Can you humanize them? ;) Great article you're referencing. I'll read it thouroughly and get back...,2011-01-06T19:05:54.843,2652,CC BY-SA 2.5,
9250,6043,0,"Looking at this document, http://support.sas.com/publishing/pubcat/chaps/59498.pdf, their also appears to be differences in definitions between SAS and SPSS that could cause confusion. Formats in SPSS appear to be referred to as informats in SAS. Value labels in SPSS appear to be referred to as formats in SAS.",2011-01-06T19:10:07.580,1036,CC BY-SA 2.5,
9251,6038,3,+1 **You need more data** and **what you intend to use the results for** deserve extra emphasis.,2011-01-06T19:20:11.310,1670,CC BY-SA 2.5,
9252,6028,1,Ahhh... Internet Explorer failed to render the formulas. Firefox fixed this. :),2011-01-06T19:21:15.630,2652,CC BY-SA 2.5,
9253,6038,2,It sounds like there's lots of wisdom in your answer.  I'll have to consult more with the people who posed me the problem about just what they want.  Thank you for the links and the advice.,2011-01-06T19:29:03.600,2665,CC BY-SA 2.5,
9255,6038,1,@Mark Best of luck!,2011-01-06T19:33:47.233,919,CC BY-SA 2.5,
9256,6044,1,"To keep the responses relevant to the question you have in mind, could you say a little more about what these data are, what you mean by ""difference in percentage,"" and why that's important to you?",2011-01-06T19:40:15.887,919,CC BY-SA 2.5,
9258,6037,3,"I'm actually unclear as to how a likelihood ratio (LR) does not achieve everything that an effect size achieves, while also employing an easily interpretable scale (the data contains X times more evidence for Y than for Z). An effect size is usually just some form of ratio of explained to unexplained variability, and (in the nested case) the LR is the ratio of unexplained variability between a model that has an effect and one that doesn't. Shouldn't there at least be a strong correlation between effect size and LR, and if so, what is lost by moving to the likelihood ratio scale?",2011-01-06T19:54:51.660,364,CC BY-SA 2.5,
9259,6037,1,"Mike -  You've got me interested, but do your points extend to effect sizes as simple as mean differences between groups?  These can be easily interpreted by a lay person and can also be assigned confidence intervals.",2011-01-06T20:22:53.617,2669,CC BY-SA 2.5,
9260,6044,0,Alright. I will edit the post.,2011-01-06T20:46:12.147,2676,CC BY-SA 2.5,
9261,6032,0,"@whuber, re: ""Any three-parameter distribution is guaranteed to fit three quantiles perfectly"". I hadn't realized that, so good to know! re: ""It also does not make any sense to compare a one-parameter fit to a two-parameter fit (with a different family) based on likelihood alone."" Ah yes, indeed; I failed to mention that one would have to apply some complexity correction (AIC, BIC, ...) if comparing fits to distribution flavors with different numbers of parameters. Thanks for pointing that out.",2011-01-06T21:10:59.380,364,CC BY-SA 2.5,
9263,6044,3,"It's still not clear what you mean. Do you mean the number of points which are different? Do you mean the percent difference in the averages? Do you mean the mean percent difference for each of the pairs of points? There's no obvious way to interpret what you're asking for. Percentage difference is not something people normally think about for sets of points. Percent difference can fully specify how two individual points are different, but you need a lot more information to fully specify how different two sets of points are.",2011-01-06T21:24:02.667,1146,CC BY-SA 2.5,
9264,6043,0,Thank you for the suggestion Andy.  I forwarded it to my friend - and I hope she will find it useful.,2011-01-06T21:53:52.777,253,CC BY-SA 2.5,
9265,5951,2,I'd even say [box-counting dimension](http://en.wikipedia.org/wiki/Box-counting_dimension) for a more practical statistic.,2011-01-06T22:01:53.663,2036,CC BY-SA 2.5,
9266,6045,2,+1 Nice find!  The paper also has a brief discussion of the PCA approach (as well as some other methods).,2011-01-06T23:08:31.400,919,CC BY-SA 2.5,
9267,6032,0,"I exaggerated a little bit, because I was thinking of two of the parameters being scale and location and the third comprising a wide range of shapes.  Even so, most three-parameter families have sufficient flexibility to fit three percentiles provided they are all distinct.",2011-01-06T23:11:12.367,919,CC BY-SA 2.5,
9268,6037,0,"Ah, so by effect size, you mean absolute effect size, a value that is meaningless unto itself, but that can be made meaningful by transformation into relative effect size (by dividing by some measure of variability, as I mentioned), or by computing a confidence interval for the absolute effect size. My argument above applies to the merits of LRs vs relative effect sizes. There may utility to computing effect CIs in cases where the actual value of the effect is of interest (eg. prediction), but I still stand by the LR as a more intuitive scale for talking about evidence for/against effects.",2011-01-06T23:11:56.483,364,CC BY-SA 2.5,
9269,6037,0,"I guess the use of LRs vs CIs will likely vary according to the context, which may be usefully summarized as follows: More exploratory stages of science, where theories are roughly characterized by the existence/absence of phenomena, may prefer LRs to quantify evidence. On the other hand, CIs may be preferred in more advanced stages of science, where theories are sufficiently refined to permit nuanced prediction including ranges of expected effects or, conversely, when different ranges of effect magnitudes support different theories. Finally, predictions generated from any model need CIs.",2011-01-06T23:18:49.550,364,CC BY-SA 2.5,
9270,6031,0,"Thank you very much, that was a great pointer in the right direction!",2011-01-07T00:14:10.003,2664,CC BY-SA 2.5,
9271,6044,0,"@John: Exactly, percentage difference works fine for a pair of points, but when someone says that certain result (as a function or as a set of numbers) is inaccurate by 10% or 15% with respect to another result, how can you arrive to such conclusion?",2011-01-07T00:52:54.747,2676,CC BY-SA 2.5,
9272,6044,0,"By the way, I don't mean any particular question as ""How to calculate percent difference"". I want to know what is the correct way to reach the kind of conclusion I mentioned before.",2011-01-07T01:00:21.773,2676,CC BY-SA 2.5,
9273,6050,0,"I think it is hard to answer this question in a general context. If possible, can you pls provide some insights about what is your data? what do you want to infer? possible assumptions, etc. Providing these details might help us in answering your questions better.",2011-01-07T01:02:48.120,1307,CC BY-SA 2.5,
9274,6049,0,"Thanks for your answer. Unfortunately, those questions are not what I want to know. I updated the question, though.",2011-01-07T01:02:52.487,2676,CC BY-SA 2.5,
9275,6052,1,"Thanks for the response. I guess my concern is that, in your example, if one country of birth is a fantastic predictor of a disease, and all the other countries of origin are poor predictors, then the backward elimination model will tell me to remove country of birth, even though country of birth is hardly useless as a covariate. In other words, it might make sense to have a single variable for 'are you Lithuanian?' even though country of birth as a whole does not predict well. Are there sensible ways to handle that sort of problem?",2011-01-07T01:53:30.930,2308,CC BY-SA 2.5,
9276,6050,0,"@suncoolsu I'm not really at liberty to discuss any of that. If it's possible to answer the question with a decision tree ('if you're assuming X, then do Y'), that could be really helpful.",2011-01-07T02:11:31.843,2308,CC BY-SA 2.5,
9277,6050,1,"For clarity, are you interested in interpreting the model as causal or are you simply interested in prediction?",2011-01-07T02:18:23.813,1036,CC BY-SA 2.5,
9278,6052,2,"(+1) If one country is a great predictor, then the whole group of countries will be significant and it will be retained.  @Matt Parker is correct: keep groups of dummy variables together and base the entering and leaving criteria on the joint significance level for the entire group, not on the significance levels for the individual dummies.",2011-01-07T02:18:47.520,919,CC BY-SA 2.5,
9279,6052,2,"@whuber OK one more potentially dumb comment from me and then I'll be at peace when I understand why I'm wrong :-P It seems to me that if I'm looking at AIC and I exclude, say, 200 country dummies as a group, the AIC will improve because the new model will have 200 fewer variables, and it will get worse only to the extent that those dummies were good predictors. If only one of the variables was a good predictor, it seems to me that on net the AIC will make the model with 200 fewer variables look better, but then I'll be missing out on that one variable...",2011-01-07T03:03:50.557,2308,CC BY-SA 2.5,
9280,6030,0,"Indeed this seems the simplest... but too bad that the axes are not updated. As I want to update online a time series, and I know at which speed the x axis moves, I guess that I can replot everything every n updates.",2011-01-07T03:06:12.740,1709,CC BY-SA 2.5,
9281,6052,4,"@nerdbound That's not dumb at all.  However, if only one dummy in 200 works well, you really have a different categorical variable: it's a new binary (or reclassified) variable.  @Matt Parker addresses this issue in his second paragraph.  This looks like a matter for exploratory analysis (which is what stepwise regression is anyway).  It's perfectly fine to torture your data in this way, but after all the smoke clears, you need to test your model on independent data.",2011-01-07T03:09:20.440,919,CC BY-SA 2.5,
9282,6044,0,"Let me give you an example, say you have 1 population of 50 individuals and 2 bathroom scales. You want to know if both bathroom scales work equally well? Or if one scale with give readings higher or lower than the other?",2011-01-07T04:11:41.240,1540,CC BY-SA 2.5,
9283,6044,0,"@carlosdc: No, sorry. Did you read the update?. I put an example there of what I want to know.",2011-01-07T05:16:40.097,2676,CC BY-SA 2.5,
9285,5358,0,"@onestop Thanks, I did mean the sum.  I haven't done the calculations, but it's clear many of the problems are due to having some largish p's in with a mix of smaller ones.  But since the OP hasn't responded to a request for clearer characterization of the distribution of p's, we should try to keep the discussion fairly general and at least characterize when various suggested approaches work and when others should be considered.  My second response to this thread covers so many of the possibilities with perfect accuracy that I would be reluctant to use an approximation now.",2011-01-07T05:35:51.733,919,CC BY-SA 2.5,
9286,5969,2,"@Matt If you have two levels on a within subject variable you can use the difference as dv (no need to go for repeated measures models). The same logic applies if you have two two-level within subject variables. The interaction is the difference of the differences (avoiding repeated measure models). However, if one of your within-variables has more than two levels, this approach does not work anymore, but you have to go multilevel. I recommend you read the special issue of the Journal of Memory and Language referred to in chl's answer: J. Mem. Language, 2008 59(4): Emerging Data Analysis",2011-01-07T09:41:57.480,442,CC BY-SA 2.5,
9287,6060,0,(+1) Nice answer. The [arules vignette](http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf) also have some good references about *lift*.,2011-01-07T09:45:59.490,930,CC BY-SA 2.5,
9288,6042,0,Invite your friend here!,2011-01-07T11:02:38.473,,CC BY-SA 2.5,user88
9289,6042,0,"I totally want to, but she's too busy writing academic articles.  In general, I wonder how many non-R statisticians we've got on the site.",2011-01-07T11:48:47.160,253,CC BY-SA 2.5,
9291,6044,0,"@Robert Smith, Yeah, sorry what you said doesn't make much sense. I'm trying to put it in statistical terms.",2011-01-07T12:43:35.713,1540,CC BY-SA 2.5,
9292,6022,0,i've added the r tag so the R code is highlighted in my comment,2011-01-07T13:50:04.063,2116,CC BY-SA 2.5,
9293,6065,1,"Great!  Thanks for all the effort that went into this, mpiktas.  I'm not familiar with R, but your code is explained well enough that I can still easily tell what you're doing.",2011-01-07T13:57:07.503,2665,CC BY-SA 2.5,
9294,6048,2,"While I don't doubt the veracity of your claims, it may be more useful to the OP and to the community in general if you describe how the lme4 package handles missing values.",2011-01-07T14:11:13.067,1036,CC BY-SA 2.5,
9295,6046,0,"How many participants (i.e., animals) do you have? If the ratio of animals to missing data is too big, this is not that good. In all cases (e.g., after using mixed effect modeling as proposed by Mike Lawrence) you should compare the results of the data with listwise deleted cases (i.e., all animals with missing data omitted) with the modeled results...",2011-01-07T14:19:52.913,442,CC BY-SA 2.5,
9296,6047,1,I think it depends on the context.  Note that $$Q = \frac{\Pr(A|B)}{\Pr(A)} = \frac{\Pr(B|A)}{\Pr(B)}$$ so that $\Pr(A|B) = Q \Pr(A)$ and $\Pr(B|A) = Q \Pr(B)$.  This form has more of a Bayesian inference flavor.,2011-01-07T14:23:19.830,1670,CC BY-SA 2.5,
9297,6060,0,"Thanks, that's probably where I've seen it before.  I think I've seen lift with a slightly different definition in machine learning context before though...I hate that sometimes there is a lack of consensus over a definition while other times there are many terms for the same concept.",2011-01-07T14:59:04.023,2485,CC BY-SA 2.5,
9298,6048,0,"Thanks for answers. Sorry, but I'm not an expert neither a statistician,  I don't handle R. I've just two statistical softwares, SPSS 16 or STATISTICA 7.0 from StatSoft and I'm quite lost! I've tried mixed models with SPSS with fixed effect for reproductive status and dose as the repeated effect. however I'm quite confuse, I'dont know how interprete the results.",2011-01-07T16:04:05.243,,CC BY-SA 2.5,Rom
9299,6055,0,"I took the problem to not be actually writing a usable file, but to transporting the value labels to SAS. I don't have a copy of SAS handy to see if the export to SAS7dat files normally saves this or not.",2011-01-07T17:00:38.500,1036,CC BY-SA 2.5,
9300,6066,3,"This is a problem with stepwise regression and categorical variables.  See this (very) recent question for more information:

[How should I handle categorical variables with multiple levels when doing backward elimination?](http://stats.stackexchange.com/questions/6050/how-should-i-handle-categorical-variables-with-multiple-levels-when-doing-backwar)",2011-01-07T17:01:57.017,71,CC BY-SA 2.5,
9302,6044,0,@carlosdc: See the answer posted by Dennis.,2011-01-07T17:23:35.203,2676,CC BY-SA 2.5,
9303,6049,1,+1 For a valiant attempt at deciphering the question and providing good advice.,2011-01-07T19:29:48.183,919,CC BY-SA 2.5,
9304,6071,0,"Are $e_i$ and $\delta_i$ (assumed to be) independent? Also, what are the distributional assumptions? In particular, are $e_i$, $\delta_i$, and $g_i$ assumed to be normally distributed? Finally, did you really mean $\beta \mu_i$ and not $\beta + \mu_i$?",2011-01-07T19:31:33.107,1934,CC BY-SA 2.5,
9306,6042,2,"I think you should change the title to reflect the concern with keeping the formats and labels. Actually saving the file in the other softwares format is trivial, keeping the labels and value labels is the hard part.",2011-01-07T20:20:27.340,1036,CC BY-SA 2.5,
9307,6067,7,"It is hard to imagine how this could be. Perhaps you are cutting the predicted probability at 0.5? If so, try varying the cutoff.",2011-01-07T20:30:02.617,279,CC BY-SA 2.5,
9308,6071,0,"They could be correlated, but literature indicates that this is a minor concern. All errors are assumed normal, and the variance is **known** for $e_i$ and $\delta_i$",2011-01-07T20:33:37.757,1364,CC BY-SA 2.5,
9309,6071,0,"yes $\beta \mu_i$, indicating the relationship between $X_i$ and $Y_i$.",2011-01-07T20:34:41.533,1364,CC BY-SA 2.5,
9310,862,0,"As a note, by default (so, if you don't specify `breaks`) R",2011-01-07T21:36:26.267,582,CC BY-SA 2.5,
9311,6066,0,"I don't understand. If one categorical variable group (or partition, or what I have called ""comparison"") is not significant and all the groups have to be removed from the model, why in my case I find that one group is retained for nominal regression and the other two ones are discarded?",2011-01-07T23:13:25.313,1219,CC BY-SA 2.5,
9312,6074,4,"I doubt it is a Naive Bayes, rather some decision tree extended each time it fails to recognize someone.",2011-01-07T23:39:24.007,,CC BY-SA 2.5,user88
9313,6075,0,Wow... very nice follow-up.,2011-01-07T23:52:00.940,71,CC BY-SA 2.5,
9314,564,0,Does anyone know how to estimate a difference in difference regression in gretl? Do I have to work with OLS or panel data?,2011-01-07T23:54:44.187,,CC BY-SA 2.5,Pyca
9315,564,3,"@Pyca It sounds like an inappropriate use of comments there. You should post a new question, with reference to this one.",2011-01-07T23:54:44.267,930,CC BY-SA 2.5,
9316,6074,1,"Wouldn't such decision tree be too unwieldy to update? Plus, I see no easy way to account for accidentally-wrong/honest-mistake answers and still get it right in the end with decision tree",2011-01-07T23:55:07.883,2696,CC BY-SA 2.5,
9317,6074,5,"Looks like a reincarnation of the twenty year old 20-questions guesser, now at http://www.20q.net/. Here's a popular explanation how it works http://www.mentalfloss.com/blogs/archives/13725",2011-01-08T01:01:21.187,511,CC BY-SA 2.5,
9318,5347,0,"Sum of IID bernoulli variables is known as the Binomial random variable, so what you want is binomial tail probability. Quick google search seems to give some R recipes for getting it exactly",2011-01-08T01:05:09.560,511,CC BY-SA 2.5,
9319,6078,0,"Sorry to ask, but do you want to test trails and patients are independent? If no, can you please specify what does the Chi-sq test intend to do?",2011-01-08T03:49:57.583,1307,CC BY-SA 2.5,
9320,6071,0,The 'simex' package is popular for measurement error models....,2011-01-08T04:08:14.980,2310,CC BY-SA 2.5,
9321,6079,0,"Could you explain where the 82/15/3 values come from?  You appear to be using numbers that you haven't yet shared with us, such as ""the monthly CDDs"" (whatever those may be) and the ""total predictions for the year"" (whatever those may be).",2011-01-08T04:25:48.910,919,CC BY-SA 2.5,
9322,6078,0,"Yes, that's what I am looking for. I have edited the question to reflect that. Please let me know if anything is unclear.",2011-01-08T05:24:30.163,52,CC BY-SA 2.5,
9323,6075,0,"I am somewhat surprised it reduced to 201 words. Although for first word played, our house rules accept 'I' and 'A' as words, which would probably further reduce the number of minimal words.  I was hoping to see someone bust out the inclusion-exclusion analysis, which should be pretty hairy...",2011-01-08T06:05:39.157,795,CC BY-SA 2.5,
9324,5993,0,"Whatever approach I end up using, Monte Carlo will certainly be used to test the implementation.",2011-01-08T06:10:29.017,795,CC BY-SA 2.5,
9325,6074,5,"Excuse me, but I think that using ""artificial intelligence"" and ""neural networks"" without any context hardy counts as explanation. And I cannot see how one could use neural net for this kind of thing - what would be the output function, for example?",2011-01-08T07:03:31.570,2696,CC BY-SA 2.5,
9326,6062,0,You're welcome Robert; could you post what you ended up using (in a week or so) ?,2011-01-08T11:36:57.057,557,CC BY-SA 2.5,
9327,6082,4,"Good answer. Regarding the last part (with respect to running the three models): The model with the intercept alone won't give you an $R^2$ value. Also, the $R^2$ values from the two models with just one predictor are probably not going to add up to the $R^2 = .85$ value for the full model with both predictors. That will only happen when the two predictors are uncorrelated. In other words, the two predictors will probably account for overlapping parts of the variance in the dependent variable.",2011-01-08T13:03:19.417,1934,CC BY-SA 2.5,
9329,6082,0,"Thanks! I've filled in more information, above, if that makes any difference to your answer.",2011-01-08T14:20:21.247,1764,CC BY-SA 2.5,
9331,6086,10,"If you don't know the operational class frequencies, they can be estimated by EM without knowing the labels of the test/operational samples.  The details are in Saerens et al. ""Adjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure"", Neural Computation, vol. 14, no. 1, pp. 21-41, 2002 ( http://dx.doi.org/10.1162/089976602753284446 ).  I've used this a couple of times and was impressed at how well it worked.  Note however that the theoretical correction is not normally optimal, and setting it via e.g. cross-validation is often better.",2011-01-08T17:17:37.383,887,CC BY-SA 2.5,
9333,6075,0,"@shabbychef There are no 1-letter words in the lexicon.  Most minimal words are 2- and 3-letter words.  Here is the full distribution of minimal word lengths: 2: 73, 3:86, 4:31, 5:9, 6:2.  The 6-letter words are: GLYCYL and SYZYGY.",2011-01-08T17:33:36.907,1670,CC BY-SA 2.5,
9334,6045,1,"Thanks very much, I think that this is the closest to what my colleague was looking for.",2011-01-08T19:17:28.807,,CC BY-SA 2.5,user1157
9336,6083,0,"Thank you Dwin.  I wonder which of the different residuals should be explored (besides the Within one).  Cheers, Tal",2011-01-08T19:37:57.800,253,CC BY-SA 2.5,
9337,6083,0,"npk.aovE is a list of three elements. The first two are parameter estimates and normality is not assumed for them, so it wouldn't seem appropriate to test anything except $Within. `names(npk.aovE)` returns `
[1] ""(Intercept)"" ""block""       ""Within""`",2011-01-08T19:57:36.283,2129,CC BY-SA 2.5,
9338,6075,0,@shabbychef I updated my answer to include a sketch of an exact inclusion-exclusion approach.  It is worse than hairy.,2011-01-08T20:18:57.030,1670,CC BY-SA 2.5,
9339,6087,0,"Thanks gd047 - the question is what do we do when we have a more complex scenario of aov(yield ~  N*P*K + Error(block/(N+K)), npk)  would the test you propose do the work?",2011-01-08T20:46:08.047,253,CC BY-SA 2.5,
9340,5996,0,"@Dmitrij I don‚Äôt think I can help you further.  But generally, when performing simulations you want the model to be close to reality as possible.  I sometimes fit my model to some typical data and then simulate from the fitted model as if it were true.  When simulating from a stationary random walk, people often want to sample from its stationary distribution, but its stationary distribution is hard to sample from directly so that initializing $X_0$ to the stationary distribution is not possible.  In that case burn-in makes sense so that $X_t$ for some large $t$ is approx. stationary.",2011-01-08T21:07:20.940,1670,CC BY-SA 2.5,
9341,6087,0,"Would you be kind enough to explain the difference between the scenarios? Unfortunately I am not familiar enough with the use of the Error term in the model (by the way, can you suggest a good book on that?). What I just proposed is the SPSS way of checking the normality assumption, as I have learned it. See this as an example  http://goo.gl/p45Bx",2011-01-08T21:38:08.413,339,CC BY-SA 2.5,
9342,862,3,"@nico. The default in R is breaks=""Sturges"" which does not always give good results.",2011-01-08T21:48:45.580,159,CC BY-SA 2.5,
9343,6087,0,"Hi gd047. Thank you for the link.  The things I know about these models are all linked to from here: http://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/   If you'll get to know of other resources - I'd love to know about them.  Cheers, Tal",2011-01-08T22:16:07.067,253,CC BY-SA 2.5,
9344,862,0,"for whatever reason my comment was truncated... I meant to write ""by default (so, if you don't specify breaks) R uses the Sturges algorithm""... odd!",2011-01-09T00:15:54.283,582,CC BY-SA 2.5,
9345,6093,0,possibly related http://stats.stackexchange.com/questions/421/what-book-would-you-recommend-for-non-statisticians,2011-01-09T00:52:24.687,22,CC BY-SA 2.5,
9347,6090,0,"OK, I have two predictors (hdd and cdd), and the squared semi-partial correlations for cdd ~ hdd is 0.755, and for hdd ~ cdd is 0.055, for a total of 0.810 versus the R-squared of 0.845.",2011-01-09T06:25:34.327,1764,CC BY-SA 2.5,
9351,5268,0,"I'm not clear on whether you are after unsupervised clustering (where all variables are treated equally) or in supervised classification (where you have a Y variable and a set of X variables). K-NN is a supervised method, while k-means clustering and PCA are unsupervised.

Can you explain the purpose of clustering the users?",2011-01-09T12:49:34.520,1945,CC BY-SA 2.5,
9352,5268,0,"Both of you are right, I may have jumped the gun on clustering.  The goal is to find ""archetypes"" for users in the database, that is, some number of user groups much less than the total number of users, where each group can be described using the fewest number of features while minimizing the average distance between the description and each user.  This is definitely unsupervised.",2011-01-09T12:49:34.597,,CC BY-SA 2.5,DanB
9353,5876,0,"Where are you getting the 5 in the 1-5?  4 is the average. Should it be 1-4?  I'm testing today with Python, thanks again.",2011-01-09T15:56:10.060,2585,CC BY-SA 2.5,
9354,5876,0,"@NealWalters Sorry, that was a slip of the finger.  I have fixed it now so it reads (1-4)/2*100+500.",2011-01-09T16:55:27.400,919,CC BY-SA 2.5,
9355,6097,1,Have you considered vector-valued autoregressive processes as a model?  http://en.wikipedia.org/wiki/Autoregressive_model .,2011-01-09T17:51:59.960,919,CC BY-SA 2.5,
9356,6097,0,"Your second equation, $O^1_j = f(O^1_{j-l}, O^2_{j-l}, O^3_{j-l}) + \epsilon$, is confusing.  Is this the same $f$ as previously?  (I hope not, because that wouldn't make much sense.)  What exactly do you mean by setting $l$ to 1, 2, 3?  That seems to give three separate equations for $O_j^1$.",2011-01-09T17:53:48.807,919,CC BY-SA 2.5,
9357,6100,0,"So something like `var calibrated = a.mean - cal.mean;` and then take their standard errors `var error = Math.sqrt( Math.pow(a.SEM, 2) + Math.pow(cal.SEM, 2));`? How does that get factored in for reporting margin of error?",2011-01-09T17:55:05.247,2634,CC BY-SA 2.5,
9358,6097,0,"@whuber: No, the f is different (but note that it is unknown). You're right, probably the equation is confusing... I meant that essentially you have 9 parameters, $O^1$, $O^2$ and $O^3$ for the past 3 observations. I will have a look to the autoregressive models, I am not completely familiar with them but I guess it's a good pointer as a start.",2011-01-09T18:01:40.330,582,CC BY-SA 2.5,
9359,6100,0,"@John Sure; for the second thing, I'm afraid I don't understand... how to call it in a report?",2011-01-09T18:53:42.537,,CC BY-SA 2.5,user88
9360,6100,0,"So how would I report the `error`, if it's not margin of error *(MoE)*, or standard error of the mean *(SEM)*? Would I do something like `moe = error * getCriticalValue(a.size - 1);` similar to if `error` was the SEM?",2011-01-09T19:36:07.133,2634,CC BY-SA 2.5,
9361,6102,0,"Might help if you told us what software package you're using, and what command(s) you typed.",2011-01-09T19:55:56.937,449,CC BY-SA 2.5,
9362,6102,0,"@onestop, the article in the link mentions stata, so probably stata is used. Does not matter though, since any statistical package would encounter problems in this particular case.",2011-01-09T20:17:26.303,2116,CC BY-SA 2.5,
9363,6104,0,"If you have shown independence and zero means, you only need to check whether the standard errors are ones. In your case if we put $\sigma^2$ inside the parenthesis, your statement will follow if you prove that $EY_i^2/\sigma^2=1$",2011-01-09T20:53:47.177,2116,CC BY-SA 2.5,
9364,6106,0,"Thank you mpiktas, I will have a look at those functions and see if they can help me with this problem.",2011-01-09T21:09:02.207,582,CC BY-SA 2.5,
9365,6090,0,(+1) A nice step-by-step illustration.,2011-01-09T21:17:57.083,930,CC BY-SA 2.5,
9366,6101,1,Quick look at http://en.wikipedia.org/wiki/Queueing_model says that the case N servers one queue is also well analyzed.,2011-01-09T21:19:56.920,2116,CC BY-SA 2.5,
9367,6107,0,"This method is equivalent, and would probably be the way to go, had I not already calculated the means and variances of the Yi's. Either way, it gets stuck at the same point, I believe.",2011-01-09T21:22:23.557,1118,CC BY-SA 2.5,
9368,6048,0,"@Rom, if you do not know how to interpret results, try formulating it as a new question.",2011-01-09T21:34:10.980,2116,CC BY-SA 2.5,
9369,6094,0,"if you use sapply, you won't need to use unlist",2011-01-09T21:36:23.770,2116,CC BY-SA 2.5,
9370,6107,0,"Not really. You have (or at least I now have)  proved that ((Y1^2 + Y2^2 + Y3^2) == (X1^2 + X2^2 + X3^2) and surely that is sufficient to establish the chi-sq-ness when you have Xi ~ N(0, sigma^2)",2011-01-09T21:43:42.297,2129,CC BY-SA 2.5,
9371,6090,0,"One more question: this illustrates how each predictor contributes to the R-squared ""variance accounted for"". Is it legitimate to use the coefficients & intercept as I mentioned (in the edited version of) the original posting, to talk about each component's contribution to a yearly total? If so, how do you see the R-squared playing into this?",2011-01-09T23:17:08.603,1764,CC BY-SA 2.5,
9372,6110,0,"Well, I'm no expert in physics. I googled few ""Fermi-Dirac statistics"" articles, but I can't see any biparametric distribution. Also please note that I am not interested with higher variance but in variance as a parameter.",2011-01-09T23:23:47.650,217,CC BY-SA 2.5,
9373,6107,0,That's where the problem lies. I'm looking for how to do that. I'm not sure how to go from having a sum of three squared normal pdfs to a chi-square pdf with df=3.,2011-01-10T00:32:55.853,1118,CC BY-SA 2.5,
9374,6107,1,"I'm confused. Isn't that precisely the definition of a chi-sq variate with 3 df? If you are worried about the sigma being different than unity, it's just a constant and so easily factors out. http://en.wikipedia.org/wiki/Chi-square_distribution",2011-01-10T01:09:50.357,2129,CC BY-SA 2.5,
9375,6104,1,"@Christopher (I TeXified your question to make it readable.)  You should double-check the distributional form for $Y_i^2$, because it is incorrect: it is not Gaussian, but exponential.  (That is, the argument of exp should be linear in $x$, not $x^2$.)  There also needs to be a factor of $x^{-1/2}$.  Once you fix that, all you need to show is that a sum of Gamma distributions is also Gamma.",2011-01-10T02:05:15.223,919,CC BY-SA 2.5,
9376,6097,0,"You may find it helpful, both conceptually and notationally, to think of $O$ as a sequence of **vectors** $(O_j^1, O_j^2, O_j^3)$ indexed by ""time"" $j$.  You can then write $O_j = f(O_{j-1}, O_{j-2}, O_{j-3}) + \epsilon$ (and $\epsilon$ is a random vector with zero expectation).  It would help immensely if you can assume $f$ is linear, at least to a good approximation: that's the AR model.  (If this is a correct representation of the situation, then your last formula needs additional fixing...)",2011-01-10T02:24:17.157,919,CC BY-SA 2.5,
9377,6112,3,"It's simpler to note that the probability of 4 or more agreements equals the probability of 3 or fewer and, since that exhausts all possibilities, both numbers must equal **1/2** exactly.",2011-01-10T02:30:06.123,919,CC BY-SA 2.5,
9378,6114,0,"The ""closed formula"", expressed as a polynomial in the original $p_i$, has 2187 terms--more than the number of values you would have to enumerate!",2011-01-10T02:37:02.077,919,CC BY-SA 2.5,
9379,6117,0,(+1) good and a very fundamental point.,2011-01-10T02:47:43.493,1307,CC BY-SA 2.5,
9380,6117,0,"I thought, may be @Lew is trying to estimate the parameter and the corresponding CI.",2011-01-10T02:49:00.327,1307,CC BY-SA 2.5,
9381,6108,3,"First hit when Googling ""negative binomial confidence interval"": http://www.bepress.com/ucbbiostat/paper242/ (abstract only).  The seventh hit gets you a full paper along with its references to the literature: http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000180 .",2011-01-10T02:51:30.860,919,CC BY-SA 2.5,
9382,6104,0,"@Whuber: Thanks for TeXing me out! $Y_i$ is distributed normal, being a linear transformation of two random normal variables. I didn't specify the distribution of $Y_i$, but I'd figure it to be $\chi^2(1)$. The sqrt(x) also confused me. I was not sure where that should come from. No one in the normal pdf could I see a sqrt(x) coming from. I was hoping someone would have pointers.",2011-01-10T04:13:50.747,1118,CC BY-SA 2.5,
9383,6104,0,@Christopher. If Yi are the sum of normal variates with mean zero then they cannot be œá2(1).,2011-01-10T05:33:36.017,2129,CC BY-SA 2.5,
9386,6111,0,"added r tag, so the R code in the comment is highlighted",2011-01-10T07:17:19.807,2116,CC BY-SA 2.5,
9387,6104,0,"Sorry if I was unclear, I meant each $Y_i$ was distributed $\chi^2(1)$.",2011-01-10T07:33:10.153,1118,CC BY-SA 2.5,
9388,6121,0,"Thanks, mpiktas. I figured there must've been a mistake somewhere in my pdf.",2011-01-10T08:31:01.340,1118,CC BY-SA 2.5,
9389,6111,0,"@mpiktas This question is quite independent of R, IMHO. Moreover, code highlighting is handled through [Google Prettify](http://code.google.com/p/google-code-prettify/) on SE sites, so there's no need to add extra tag to enable syntax highlight.",2011-01-10T09:10:59.933,930,CC BY-SA 2.5,
9390,6111,0,"@chl, I asked the question on meta concerning adding r tags, since I felt too, that adding r tag just for syntax highlighting is not appropriate. Google prettify did not work for me.",2011-01-10T09:22:26.953,2116,CC BY-SA 2.5,
9391,6090,0,"@Wayne You need to calculate the semi-partial correlation between your DV `elec` and each of your predictors. Taking the coefficients as a measure of ""influence"" doesn't seem like a good idea to me: a) the coefficients are scale dependend b) for a coefficient $b_{i}$, it is $\hat{Y}$ that changes by $b_{i}$ units when predictor $i$ changes 1 unit, not $Y$ itself. Before delving further into regression theory, I'd take the increase in $R^{2}$ as a measure for the improvement in prediction wenn adding a predictor.",2011-01-10T09:25:06.707,1909,CC BY-SA 2.5,
9393,6084,0,"I'm not really interested in the effect of the other coravirates, I just put them in the model to control for the effect they have. If I understand you correctly, my statistic ""indicates a genuine effect even though it is only borderline significant""?",2011-01-10T10:41:55.630,2652,CC BY-SA 2.5,
9394,6085,0,Can I acheive this interval-censored model by calculating a new variable as the LN(age)? I remember reading about that somewhere...,2011-01-10T10:46:22.073,2652,CC BY-SA 2.5,
9395,6108,0,I downvoted the question because it's eminently google-able.,2011-01-10T13:20:59.427,1979,CC BY-SA 2.5,
9396,6048,1,"...if by ""handle"" you mean ignore/remove all observations with missing values for any of the covariates or responses.",2011-01-10T13:22:41.157,1979,CC BY-SA 2.5,
9397,6090,0,"@Carcal: I think your point B is pretty important: the distinction between $\hat{Y}$ versus $Y$. I'm wanting to compare $\sum_t{b_i} p_{it}$ to $\sum_{t=1}^n{\sum_i{b_i p_{it}}} + n a$ where the intercept is $a$ and predictor $i$ at time $t$ is $p_{it}$. Not comparing coefficients, but rather how much that predictor contributed to the sum. Does that make sense?",2011-01-10T13:29:43.207,1764,CC BY-SA 2.5,
9398,6128,0,"Thanks for your great answer! 2 more questions: Does your second example mean, that coin in fact does provide all possible permutation and is an exact test? Is there any benefit of not providing an exact test in my case?",2011-01-10T13:45:37.237,442,CC BY-SA 2.5,
9399,6104,1,"@Christopher I suspect that in computing the pdf of $Y_i^2$ you made a substitution but did not back-substitute again.  Letting $y=x^2$, you can rewrite $C\exp(-2x^2/(2\sigma^2))dx$ = $C\exp(-y/\sigma^2)dy/(2\sqrt{y}),$ which is a (scaled) gamma(1/2) distribution.",2011-01-10T13:46:10.027,919,CC BY-SA 2.5,
9400,6129,0,The scores are not proportions but estimates by participants on a 0 to 100 scale (the presented data are means of estimates on several items with that scale).,2011-01-10T13:57:04.573,442,CC BY-SA 2.5,
9401,6128,13,"(+1) It is no surprise that the (unpaired) t-test yields essentially the same p-value, 0.000349.  Despite what the reviewer said, the t-test *is* applicable to these data.  The reason is that the *sampling distributions* of the means are approximately normal, even though the distributions of the data are not.  Moreover, as you can see from the results, the t-test actually is more conservative than the permutation test.  (This means that a significant result with the t-test indicates the permutation test is likely to be significant, too.)",2011-01-10T14:09:22.393,919,CC BY-SA 2.5,
9403,6129,0,"Then non-parametrics would seem the traditional way to go. That said, I've wondered if such scale data might usefully be inferred to derive from a binomial process and thereby analyzed as such. That is, you say that each score is the mean of several items, and let's say each item is a 10 point scale, in which case I'd represent a response of, say, ""8"" as a series of trials, 8 of which have the value 1 and two of which have the value 0, all labelled with the same label in an ""item"" variable. With this expanded/binomial-ized data, you could then compute the binomial mixed effects model.",2011-01-10T14:24:13.833,364,CC BY-SA 2.5,
9404,6129,0,"Following from my previous comment, I should note that in the expanded/binomial-ized data, you could model the ""item"" variable as either a fixed or random effect. I think I'd lean toward modelling it as a fixed effect because presumably you might be interested in not only accounting for but also assessing the item differences and any possible interaction between item and other predictor variables.",2011-01-10T14:26:21.960,364,CC BY-SA 2.5,
9405,6128,2,"@Henrik For certain situations (chosen test and numerical complexity), `coin` can indeed calculate the exact permutation distribution (without actually going through all permutations, there are more elegant algorithms than that). Given the choice, the exact distribution seems preferable, but the difference to a Monte-Carlo approximation with a high number of replicates should be small.",2011-01-10T14:26:48.900,1909,CC BY-SA 2.5,
9406,6128,1,"@Caracal Thanks for the clarification. One question remains: The data I presented is paired. Hence, I need the equivalent to the paired t-test. Is `oneway_test` the accurate function? And if so, which one is the correct for non-paired data?",2011-01-10T14:34:16.303,442,CC BY-SA 2.5,
9407,6128,0,"@Henrik An exact permutation test needs 19! = 121 645 100 408 832 000 permutations.  Assuming a really fast 10^9 applications per second, that would require 3.87 years of computation.  (A clever algorithm would exploit all the repetitions among the x's to reduce that to one second, but I think you would have to hand-code that.)",2011-01-10T14:34:31.077,919,CC BY-SA 2.5,
9408,6128,0,"@Henrik You're in trouble for paired data with a nonparametric test: the p-value is around 40%.  The problem is that there are some (inconsequential?) increases from x to y, a number of tied values, and some really big decreases from x to y.  By ignoring the magnitude of the decreases you lose too much information.",2011-01-10T14:36:26.800,919,CC BY-SA 2.5,
9409,6131,0,Attack of the R code syntax highlighting! Is it possible to not have the code highlighted?,2011-01-10T14:50:12.483,1036,CC BY-SA 2.5,
9410,6128,0,"@whuber: when using `wilcox.test` from stats with paired = T, I get a p-value of .02, which is reasonable. So I cannot replicate the .4 you mention. However, I am still unable to get a paired test to work with coin. (When using `wilcoxsign_test` I get an error: Not an unreplicated complete block design)",2011-01-10T14:52:20.547,442,CC BY-SA 2.5,
9411,6048,0,"@AndyW & fabians: I've updated the post to reflect my understanding of how mixed effects models handle missing data, please feel free to let me know if my understanding is erroneous in any way!",2011-01-10T14:57:27.813,364,CC BY-SA 2.5,
9412,6130,0,I wouldn't suggest resorting to python to accomplish this task unless you need it to be standardized to run on multiple datasets with different variable names. The UCLA website is full of useful examples though of interest to anyone who uses SPSS.,2011-01-10T14:58:54.863,1036,CC BY-SA 2.5,
9413,6085,0,"Interesting suggestion, to which I have not had prior exposure. I am reasonably sure that would not achieve the precise goal I had in mind. It might achieve similar objectives by dampening the effect of age, though. The effect of age on the hazard scale would now only be linear rather than exponential. Whether that would correspond to the ""true"" age effect on the response under study would still be open to question.",2011-01-10T15:21:08.637,2129,CC BY-SA 2.5,
9414,6084,0,"Hardly. I would not accept that as a fair paraphrase of what I said. I say one would first need to do a LRT examination before doing further discussion of whether the borderline Wald test even met conventional standards for significance. And  I think the work ""genuine"" is a far reach in the absence of better context.",2011-01-10T15:25:37.367,2129,CC BY-SA 2.5,
9416,6131,0,"@Andy What's wrong with highlighting SPSS code?  Code highlighting isn't restricted to R.  If you don't want it highlighted, don't indent it...",2011-01-10T15:38:56.377,5,CC BY-SA 2.5,
9417,6131,0,"@Andy The highlighting engine used on SE guesses language on its own; it is even not capable of highlighting R (it is  interpreted as C); this seems to be interpreted as Pascal because of ""end"".",2011-01-10T16:10:32.273,,CC BY-SA 2.5,user88
9418,6114,0,"Yes and No answers have a probability of 0.5. This is correct: 0.2734375 0.1640625 0.0546875 0.0078125. There are 16 possible combination of Yes answers for two users: {4,4} {4,5} {4,6} {4,7} {5,4} {5,5} {5,6} {5,7} {6,4} {6,5} {6,6} {6,7} {7,4} {7,5} {7,6} {7,7}. And we can estimate the probability of each of these 16 pairs occuring. However once a pair occurs - the probability of a match is different. In other words for a {4,4} the probability is 1/35 as there are 35 combinations of 4 out of 7. For {4,5} the probability of a match increases. How do we take those in account.",2011-01-10T16:20:43.080,,CC BY-SA 2.5,user2715
9419,6131,1,"@Shane, it is not the blocked code highlighting (the grey background) that is the problem, it is the fact that the wrong keywords within the syntax are highlighted. For instance my loop variable in the simulated data is highlighted with a lighter grey shade, suggesting the code after the pound sign is a comment. I think it is much easier to read as a blocked code segment, but the highlighting can only be confusing. Is there anyway to keep the blocked code but not have highlighting?",2011-01-10T16:22:26.883,1036,CC BY-SA 2.5,
9420,6128,0,@Henrik Wilcox.test assumes the distribution of x-y is *symmetric* about its mean.  It is also unclear whether wilcox.test handles ties appropriately.  Thus I think your p-value of 0.02 is invalid.,2011-01-10T16:25:35.913,919,CC BY-SA 2.5,
9421,6128,0,"@whuber, re: exhaustive permutations -> The exhaustive variant of the permutation test is sometimes termed a ""randomization test"" (eg, by Mewhort et al, 2009, http://www.springerlink.com/content/pg17u7141845w082/) and advances by Gill (2007, discussed in the Mewhort paper) dramatically reduce the computational requirements.",2011-01-10T17:04:50.217,364,CC BY-SA 2.5,
9422,6134,0,"I'm curious if you'd consider the following an appropriate quasi-visualized approach: bootstrap estimates for the moments of the two groups (means, variances, and higher moments if you so desire) then plot these estimates and their confidence intervals, looking for the degree of overlap between groups on each moment. This lets you talk about potential differences across the variety of distribution characteristics. If the data are paired, compute the difference scores and bootstrap the moments of this single distribution. Thoughts?",2011-01-10T17:21:47.220,364,CC BY-SA 2.5,
9423,6128,0,"@Mike Thank you.  I don't have access to the paper, so I can only guess that it might be doing something similar to what I had in mind when I suggested 4 years of calculation could be reduced to one second :-).",2011-01-10T17:54:49.470,919,CC BY-SA 2.5,
9424,6132,2,"Exactly right. Aggregating prematurely is likely to obscure important relationships. If meat prices are going up in concert, is it because energy or transportation prices went up in prior intervals? I see no mention of methods for examining auto-correlation or cross-correlation which would clearly be needed for a proper treatment.",2011-01-10T17:56:47.283,2129,CC BY-SA 2.5,
9425,6134,2,"(+1) Good analysis. You're perfectly right that the results are obvious and one doesn't need to press the point with a p-value.  You may be a little extreme in your statement of (3), because the t-test does not require normally distributed data.  If you are concerned, there exist adjustments for skewness (e.g., Chen's variant): you could see whether the p-value for the adjusted test changes the answer.  If not, you're likely ok.  In this particular situation, with these (highly skewed) data, the t-test works fine.",2011-01-10T17:58:13.803,919,CC BY-SA 2.5,
9426,6135,0,"+1 Adjoining the lexicon and then re-minimizing it is a clever idea.  The algebra is beyond me, but it feels like you are calculating a multinomial probability, rather than hypergeometric.  So the probability is for sampling _with_ replacement.  I think that explains why your answer of 1.08% is so much larger than my estimate of 0.4%.  Is there a way to modify your approach to handle sampling without replacement?",2011-01-10T18:08:04.573,1670,CC BY-SA 2.5,
9427,6131,0,@Andy (+1) Good that you provide a pure SPSS solution.,2011-01-10T18:21:50.923,930,CC BY-SA 2.5,
9428,6134,0,(+1) Nice catch! And very good comments.,2011-01-10T18:26:32.507,930,CC BY-SA 2.5,
9429,6114,0,"I'm not sure I fully understand your question, but in the `for()` loop, each number of pairwise agreements is considered separately (4, 5, 6, 7 in your case): for each of these numbers, all possible answer patterns leading to that agreement are enumerated (`poss`), and their respective probabilities calculated. One way to check that the result is correct, you can set `k` (all numbers of agreement) to `0:7`. The sum for all probabilities should then be 1.",2011-01-10T19:11:03.180,1909,CC BY-SA 2.5,
9430,6128,0,@whuber: The preprint of the Mewhort paper can be accessed via his website: http://psyc.queensu.ca/~hiplab/LAB_PUBS/BSC903_Mewhort_Kelly_Johns_BRM.pdf,2011-01-10T19:47:49.060,364,CC BY-SA 2.5,
9431,6135,2,"@vqv Yes.  Now that we have a list of the half million or so racks with no words, it's straightforward (by changing the last two lines of code) to compute the chance of each rack (without replacement) and obtain the hypergeometric result.  The exact answer equals 349870667877/80678106432000 = **0.43366%**.  With N=100K trials your SE is 0.021%, so your answer should have come between 0.38% and 0.49% (two-sided 99% CI).  I'm so glad our answers agree!",2011-01-10T19:48:48.140,919,CC BY-SA 2.5,
9433,6135,0,@whuber Could you run the calculation using the Words With Friends (WWF) tile distribution?  My estimate of 0.4% is based on the WWF lexicon and WWF tile distribution.  I think you are using the Scrabble tile distribution with the WWF lexicon.,2011-01-10T20:34:57.437,1670,CC BY-SA 2.5,
9434,6114,0,"@Rado It looks like you are conditioning on the numbers of yes answers; *e.g.*, {6,4} means respondent 1 had 6 yeses and respondent 4 had 4 yeses.  This is a valid approach but is more complex than necessary.  @caracal is modeling it question by question: on question $i$, the chance of agreement is like flipping a coin with probability $p_i^2 + (1-p_i)^2$.  By looking at all possible outcomes of these 7 coin flips--there are only 128 of them--we can compute the chance of any pattern of agreement.",2011-01-10T20:37:55.523,919,CC BY-SA 2.5,
9435,5853,0,"reading your last comment, I wonder what approach is best: give, e.g. 1 point for each loan paid off, other points for other things, then try to tame the enormous score at the end, or perhaps do something more meaningful with each component that makes up the final score? For example, if you give points for loans paid off you could calculate your Loans Paid Off (LPO) points as log (1 + NumberOfLoansPaidOff). So someone with zero loans paid off gets an LPO of 0, someone with 3 loans paid off gets an LPO of 1.1, while someone with 100 loans paid off gets an LPO of 4.6. Cap LPO at 5.",2011-01-10T20:38:38.050,1764,CC BY-SA 2.5,
9436,6114,0,"I might be missing somenthig - ( res ) is probability of selecting 4, 5,6 or 7 ""yes"", e.g. for 4 it is 35/128. sum (res) - are you suggesting that this is the probability of the the two picking the same answers? To me sum (res) is the probability of anyone selecting 4 or more Yes out of all possible combinations.",2011-01-10T20:40:38.370,,CC BY-SA 2.5,user2715
9437,6135,0,"Oops.  The exact answer actually is 349870675899 (I was 8022 off due to an error in my dictionary.)  This makes no practical difference, fortunately.",2011-01-10T20:43:07.797,919,CC BY-SA 2.5,
9438,6135,0,"@vqv I'm not familiar with the various tile distributions.  I copied mine directly from your code (and I used your dictionary) :-).  If you mean the distribution at http://osxreality.com/2010/01/01/beginners-guide-to-words-with-friends-2/ , then I obtain **1.15444%** (with replacement), **0.43366%** (without replacement).  The second number actually does differ from the Scrabble frequencies at the 8th significant figure.",2011-01-10T20:51:44.200,919,CC BY-SA 2.5,
9439,6139,0,You might be interested in the recent thread at http://stats.stackexchange.com/q/5750/919 .,2011-01-10T20:55:32.173,919,CC BY-SA 2.5,
9440,6114,0,"@Rado No, `res` contains the probabilities for exactly 4, 5, 6, or 7 pairwise agreements. I.e., the probability that for exactly 4 (5, 6, 7) questions, either both participants answer ""yes"" or both participants answer ""no"". `sum(res)` therefore is the probability of 4 or more pairwise agreements.",2011-01-10T20:57:18.700,1909,CC BY-SA 2.5,
9441,6135,0,"@whuber I have both in my code.  The Scrabble distribution was commented, but looking above it seems like you used the Scrabble distribution.  btw, great follow up.  We now have a complete solution for the specific problem in the title.",2011-01-10T20:59:10.453,1670,CC BY-SA 2.5,
9442,6114,0,"@Rado It sounds like you're both right, at least when all $p_i$ are 1/2.  In both cases the outcomes can be modeled by seven flips of a fair coin.  In your case you interpret the sides of the coin as ""yes"" and ""no"" on one response; in @caracal's case the sides of the coin mean ""agree"" and ""disagree"" for a given pair of responses.",2011-01-10T21:11:46.343,919,CC BY-SA 2.5,
9443,6134,0,"We seem to be accepting the notion that the underlying distribution is ""similar"" to the random instantiation. So couldn't one pose the question: are these both from beta(0.25, 0.25) and then the test would be whether they have the same (non-)centrality parameter. And wouldn't that justify using either a permutation test or Wilcoxon?",2011-01-10T21:13:05.857,2129,CC BY-SA 2.5,
9444,6135,0,"@vqv I see it now.  I just happened to grab the Scrabble frequencies.  BTW, simulation (at least with N < 1000000 or so) is faster.  My aim was to find a solution that required little coding and still was computationally efficient.  For the greatest computational efficiency, I think both methods can be beaten by a suitable Boyer-Moore type of algorithm.  That is, one needs to sequentially construct a DAG whose nodes represent subsets containing no words and whose leaves are words.  (This is basically what the polynomial algebra is doing automatically.)",2011-01-10T21:20:33.543,919,CC BY-SA 2.5,
9445,6140,0,and a good short discussion and with two excellent references on [Paul Hippel's pages](http://www.sociology.ohio-state.edu/people/ptv/faq/weights.htm),2011-01-10T21:31:40.460,1739,CC BY-SA 2.5,
9446,6135,0,"@whuber @vqv The algebra is also beyond me (although clearly exposed), but I'd like to say that I still admire your tenacious efforts (and the follow-up you gave).",2011-01-10T22:01:27.513,930,CC BY-SA 2.5,
9448,6114,0,thanks that clarifies it. I will test the R code shortly.,2011-01-10T22:10:05.067,,CC BY-SA 2.5,user2715
9449,6145,0,"Thank you for the answear.
But is the first model pooled? I have added the LAND term wich is the entity dummy variable (as it's a facor variable.
And as I can se the parameter estimates (EURO) are identical?",2011-01-10T22:27:09.540,2724,CC BY-SA 2.5,
9450,6114,0,"[C(7,4) + C(7,5) + C(7,6) + C(7,7)]*(1/2)^7 = (35+21+7+1)*1/128 = 64/128 = 1/2.",2011-01-10T22:30:24.480,,CC BY-SA 2.5,user2715
9451,6145,0,"@Skolnick, I've missed that at first, so I changed my answer accordingly",2011-01-10T22:31:03.123,2116,CC BY-SA 2.5,
9452,6135,0,"@chl ""algebra"" = ""magic"" ;-).",2011-01-10T23:05:25.670,919,CC BY-SA 2.5,
9454,6119,4,"You simply must read Mosteller & Wallace, *Applied Bayesian and Classical Inference, The Case of the Federalist Papers.*  It is a classic, it's readable, and if by page 12 you aren't thoroughly convinced it has plenty to show you, just return it to the library or bookseller.",2011-01-10T23:42:25.577,919,CC BY-SA 2.5,
9455,6151,1,"please clarify your question - it is not clear that you need a 'treatment' to answer your question. can you provide a list or some examples of the questions in the survey? Do you plan to have 100 individuals, 50 sibling pairs, or something else?",2011-01-11T01:21:45.663,1381,CC BY-SA 2.5,
9456,6153,0,"I am not sure if it is clear, but the histograms are of the 30 values of variance, and the barplots are the raw values of the variance, i.e. `var <- c(0,0,1,3,10,100,150), hist(var), barplot(var)`, so I interpret this as a few parameters explain most of the variance, not that most of the explained variance is in the tail. Does that make more sense? Sorry if it was unclear.",2011-01-11T04:07:48.073,1381,CC BY-SA 2.5,
9457,6154,1,"I am not an expert on SAS or SEM, but if you get t-values, can't you calculate p-values yourself?",2011-01-11T04:24:26.237,2116,CC BY-SA 2.5,
9459,6155,0,"I have tried to use http://cran.r-project.org/web/packages/graph/vignettes/graph.pdf but it seams that the package ""Rgraphviz"" is not available anymore. Anyone have another solution?",2011-01-11T05:03:10.347,1709,CC BY-SA 2.5,
9460,6155,1,Rgraphviz is on bioconductor: http://www.bioconductor.org/packages/release/bioc/html/Rgraphviz.html,2011-01-11T05:07:07.760,159,CC BY-SA 2.5,
9461,6151,0,"Jealousy____ Competition for parents' attention____ Difference in terms of gender____     These are sample choices in the questionnaire. They have to rank them from 1 to 15. 1 being the top and 15 as the least., I have 50 pairs of siblings as my respondents, How can I detect if there is really a rivalry between siblings, what will be my statistical treatment on these?, Thanks David for your response, I'll be waiting for your next post",2011-01-11T05:20:58.280,,CC BY-SA 2.5,noli macnoli
9462,6067,5,"The area under the ROC-curve is .585, a rather poor result. This implies that there really isn't a cutoff value where the specificity/sensitivity trade-off is worth it. Fiddling with the cutoff won't improve classifications much, as it would just decrease the specificity by roughly as much as it increases the sensitivity.",2011-01-11T07:52:43.050,2690,CC BY-SA 2.5,
9463,6086,0,"Yeah, I should have mentioned that the results from the ROC-curve weren't convincing also. In this case I think there isn't a threshold that gives satisfactory results.",2011-01-11T07:58:11.313,2690,CC BY-SA 2.5,
9464,6126,0,"Our current policy is to not weight, since I, like you, decided that the weights were rather arbitrary and were probably no better than the unweighted results.  When we sample, we target a specific distribution using each segment's proportional size, but this is also using a relatively arbitrary target distribution, so we may need to change this policy as well.",2011-01-11T09:38:03.150,1195,CC BY-SA 2.5,
9465,6140,0,"thanks very much, that's the answer I was looking for.  It was more of a curiosity thing and having the answer on hand to be able to explain to someone if they asked me about it, but I agree with you and @Mohit",2011-01-11T09:39:27.500,1195,CC BY-SA 2.5,
9467,6128,0,@caracal. Thanks for the update. But what is the rational for not using the exact statistics but an approximation? (The results are markedly different...),2011-01-11T09:50:47.767,442,CC BY-SA 2.5,
9468,6134,0,"+1 Thanks for the detailed answer. One point: The data is paired. I.e., Each data pair is responses from a single participant on different trial types. We constructed the trial in such a way to expect differences in that way. Furthermore, I have to present the results (i.e., there exists differences) as briefly as possible. So, I have to see how I can use your analysis.",2011-01-11T10:22:31.297,442,CC BY-SA 2.5,
9470,6156,1,"(+1) It seems my response came after yours. As your response highlights somewhat a different perspective (cross-platform, algorithms), I feel like our responses are not so redundant, but I can remove mine without any prob.",2011-01-11T10:23:29.347,930,CC BY-SA 2.5,
9471,6160,0,"(+1) Never used this package, but your overview suggest I shoud try it. Seems good at first sight.",2011-01-11T10:26:01.560,930,CC BY-SA 2.5,
9473,6138,0,"That's a good start, but I think there is more to the problem. For example: how do they get the answers to the questions? Presumably they use the answers given by previous players (a reinforcement learning problem), in which case we face the tradeoff of choosing a question that (a) solves the problem for the current player, and (b) provides information for future players.",2011-01-11T11:10:53.033,495,CC BY-SA 2.5,
9474,6132,1,"Yes, collinearity is a major problem, but the relationships change over time, since one factor may affect one group of commodity prices at one point, then affect a different group when the factor changes in a different way.",2011-01-11T13:09:26.660,1195,CC BY-SA 2.5,
9475,6163,0,"Are you trying to calculate the variance of y_pred in Deming Regression. I am sorry, its not clear to me.",2011-01-11T14:34:32.687,1307,CC BY-SA 2.5,
9476,6163,0,Sorry for being unclear. I would like to know the prediction error when I use a new x (not one used to fit the model) trying to predict y (y_pred). The standard error of x is known.,2011-01-11T14:44:22.687,2732,CC BY-SA 2.5,
9477,6154,0,"Yes which is what I do, but in every other SAS procedure the p-value is given along with the test statistic.",2011-01-11T14:46:10.473,2310,CC BY-SA 2.5,
9478,6162,0,Thanks.  I should have made it clear that I know how to calculate a p-value given a test statistic but I find it strange that they don't output the p-value for you as in almost all the other SAS procedures.,2011-01-11T14:47:12.937,2310,CC BY-SA 2.5,
9479,6110,0,Comparing Gamma random variables is exactly the kind of idea I needed. How can I download the article?,2011-01-11T14:51:50.003,217,CC BY-SA 2.5,
9480,6166,1,"Good answer. However, I think, if @johanvdw wants to predict for new x, the prediction error will be larger than you estimate. If I understood him correctly, var(pred_y) = var(\hat y) + var(error).",2011-01-11T15:30:39.607,1307,CC BY-SA 2.5,
9481,6128,0,@caracal I would be grateful if you could give some hints. as the difference is huge: p = .01 (approx) and p = .15 (exact),2011-01-11T15:31:41.460,442,CC BY-SA 2.5,
9482,6166,0,"thanks, that was fast!
@suncoolsu I think the error in y should not be added. The regression coefficients already include that uncertainty. You wouldn't add it in a normal linear regression either.",2011-01-11T15:45:59.693,2732,CC BY-SA 2.5,
9484,6149,0,"I think they discuss the (central) hypergeometric distribution, if I am not mistaken. Do correct me if I am wrong.",2011-01-11T17:19:19.993,2728,CC BY-SA 2.5,
9485,6166,1,"@johanvdw. I think we do need to add the error in normal linear regression for prediction of a new Y (for a new X). Please see : equation 2.38 in section 2.5 of Applied Linear Statistical Models - Kutner, Nachtsheim, Neter, and Li (2005). McGrawHill",2011-01-11T18:12:56.457,1307,CC BY-SA 2.5,
9487,6114,0,"@Rado Your formula answers two questions: (1) the chance of four or more ""agrees"" in seven draws and--reversing the sides of the ""coin""--(2) the chance of four or more ""disagrees"".  These events are complementary: they don't overlap and they include all possibilities.  Thus the chance of each of these events must equal 1/2.  No calculation needed!  The point of @caracal's response is to show how to compute the chances when the frequencies of ""yes"" responses to one or more questions depart markedly from 1/2.  This might be a more meaningful (and accurate) approach.",2011-01-11T18:58:33.843,919,CC BY-SA 2.5,
9489,6149,0,"@highBandWidth you are correct, I have changed my answer. Perhaps it would be better to rephrase the question 'Is there a conjugate prior...'",2011-01-11T19:20:34.707,1381,CC BY-SA 2.5,
9490,6128,2,"@Henrik The `coin` author wrote me that `oneway_test()` cannot calculate the exact distribution for the dependent case, you have to go with the MC approximation (only `wilcoxsign_test()` is suitable for the exact test). I didn't know this and would prefer an error in that case, but MC should be accurate enough with a high number of replicates.",2011-01-11T19:30:15.453,1909,CC BY-SA 2.5,
9491,6151,0,"it is still not clear what your questions are, and I think that you are getting your predictors and responses confused. For example, 'difference in terms of gender' would most commonly be used as a predictor, since 'sex' can be assumed to be binary (male or female) whereas 'gender' can have more nuanced meaning; to evaluate ""how different is your gender"" as a response on a scale of 1 to 15 would require a clear definition of gender.",2011-01-11T19:33:11.693,1381,CC BY-SA 2.5,
9492,6138,0,"At the first glance, ability to draw analogy between 20 questions and Huffman coding hinges on the ability to ask ""range questions"". That is, instead of ""Have you character ever been to space?"" we are asking ""blanket"" questions like ""Has he ever been to space, or is a male, or is bald, or was in a movie, or ... (100500 other options)?""

Am I right? If so, then I should probably edit my question to make it clear that I am interested in ""ask one by one"" variety",2011-01-11T19:41:37.317,2696,CC BY-SA 2.5,
9493,6138,0,"Plus, most of the articles that use Huffman codes as model for 20 questions, imply that questioner is free to make up his own questions, which in essence boil down to ""Does bit $i$ in codeword for the object is $0$""? However, in my case (or, rather, akinator.com's case) the set of questions is predefined, and it (obviously) has nothing to with Huffman codewords.

Right now I can't see how to make transition from my question to Huffman codes. Perhaps you could elaborate?",2011-01-11T19:52:44.150,2696,CC BY-SA 2.5,
9494,6166,0,"@suncoolsu and @johanvdw,  I've updated the answer to reflect your discussion. @suncoolsu is right, we need to add the error, but only if we want to forecast true $y$, if we want the forecast of $y^*$ the addition is not necessary. @johanvdw in normal linear regression error addition is necessary, since normal linear regression does not postulate the latent variable $y^{*}$",2011-01-11T20:14:45.600,2116,CC BY-SA 2.5,
9499,6173,4,"(+1) A possible exception is when we have a set of indicators (e.g., items in a questionnaire) with a fixed number of categories (e.g., Likert-type items), as discussed in [Visualizing Likert Item Response Data](http://stats.stackexchange.com/questions/3921/visualizing-likert-item-response-data), or a single crossing factor (e.g., gender), in which case an horizontal (we are better at visually discriminating along the horizontal than the vertical axis) stacked barchart might be interesting, IMO.",2011-01-11T20:39:46.760,930,CC BY-SA 2.5,
9500,6172,0,"Thanks, this does indeed look very useful. Do you know how one would go about predicting future values from the pgml() function?",2011-01-11T20:44:05.303,2704,CC BY-SA 2.5,
9501,6110,0,Added link. Wasn't able to find source of it that was free but perhaps you can get it through your university library.,2011-01-11T20:45:39.443,2129,CC BY-SA 2.5,
9502,6173,1,"Excellent point - that mostly negates the concern about varying group sizes, assuming non-response is about even across the questions.  I still have difficulty comparing the ""slightly concerned"" proportions across groups, but the horizontal layout does seem easier.  That post is a great resource - thanks for linking it here.",2011-01-11T20:50:15.710,71,CC BY-SA 2.5,
9503,6167,1,"How many classes are there?  If there are substantially fewer than $10^4$, then why not estimate all the probabilities directly from the data as $\hat{P}(A>B)$ = # cases where A is preferred to B / # cases where A is compared to B?  (This is ML and it's as simple as they get.)",2011-01-11T20:56:23.833,919,CC BY-SA 2.5,
9504,6174,0,Thanks David...I will start with that.  Another small question I had....is there some guidence on how I can interperate the notation you have used above?,2011-01-11T20:56:46.753,2734,CC BY-SA 2.5,
9505,6172,0,"@Thomas Jensen, forecasting the future values is the weak point of plm package. There are no readily available functions even to get fitted values. So I do not expect that pglm will have them. If you chose to estimate model via pglm, you will have to code the prediction yourself.",2011-01-11T20:59:10.867,2116,CC BY-SA 2.5,
9507,6174,0,"@mike $P(Y_{t}=y_{t})$ is the probability that you sell $y$ units in a time $t$; $P(Y_{t}=[0,1,2,3]$ is the probability that you sell 0, 1, 2, or 3 units, $\lambda$ is the poisson parameter, the average rate of sales such that the average units sold in time $t$ is $Y_t=\lambda t$.",2011-01-11T21:06:54.340,1381,CC BY-SA 2.5,
9508,6176,2,"math.stackexchange.com might be more appropriate place to ask this, and it may already contain the answer.",2011-01-11T21:06:55.433,2116,CC BY-SA 2.5,
9509,6135,0,"@whuber +1, what can I say, awesome follow-up.",2011-01-11T21:17:45.157,2116,CC BY-SA 2.5,
9510,6175,0,"Thanks a lot for this! I am still a bit new to all of this, how would one write a function for forecasting? Is it a simple matter of fixing the independent variables, and then feed the predicted values back into the lag term in the model to get new predicted values?",2011-01-11T21:20:08.783,2704,CC BY-SA 2.5,
9511,6174,0,"Thanks..is there a general document somewhere on the notation you are using?  Is it some statistical notation?  For example, in ""\frac{e^{-\lambda t}(\lambda t)^{y_{t}}}{y_{t}!}$$"" I am not clear on the parameter precedence and do I interperate the ! as factorial, or does it have a special meaning there (and what does ""frac"" mean?).  Thanks for the help.",2011-01-11T21:27:08.853,2734,CC BY-SA 2.5,
9512,6175,1,"@Thomas Jensen, something like this. When you have data up until period $t$, you can calculate the forecast of $y_{t+1}$. You use this forecast when forecasting $y_{t+2}$ and so on. To implement this some kind of loop construct will be necessary.",2011-01-11T21:28:02.933,2116,CC BY-SA 2.5,
9513,6176,3,"@mpiktas Good suggestion, but bear in mind that the stated interest is *technique* rather than *theory*.  Recommendations at math.SE will likely favor the latter.  Moreover, you don't need to know measure theory (beyond absolute basics) to learn about NP Bayes methods, so the main focus here should be on intros to probability that focus on statistical applications.",2011-01-11T21:28:44.447,919,CC BY-SA 2.5,
9514,6175,0,Great I really appreciate the help!,2011-01-11T21:35:33.307,2704,CC BY-SA 2.5,
9515,6174,0,Ok...ignore that last message (and my first one!) (although I will leave them there in case anyone else has the same question).  My browser was not resolving the notation into its MathJax image.,2011-01-11T21:50:55.207,2734,CC BY-SA 2.5,
9516,6173,1,"+1 For the distinction between bar charts and histograms.  (The difference becomes very clear when you consider how best to plot histograms with varying intervals, as illustrated at http://www.stata.com/support/faqs/graphics/histvary.html.)",2011-01-11T21:52:29.137,919,CC BY-SA 2.5,
9517,6174,0,@Mike thats okay. The programming language used by MathJax is $\LaTeX$,2011-01-12T00:20:26.517,1381,CC BY-SA 2.5,
9518,6177,1,thank you for the explanation and proper terminology / reference. The audience is readers of a scientific journal and the topic is variance decomposition; understanding the concept of a log transform is a pre-requisite but I still wasn't sure if this presentation required further justification - roots are a good alternative. Thanks.,2011-01-12T00:30:34.880,1381,CC BY-SA 2.5,
9519,6173,0,I updated to reflect I want bar plots not histograms.,2011-01-12T00:58:36.020,559,CC BY-SA 2.5,
9520,6182,0,"The traditional way you could do this is through hypothesis testing. This will allow you to test whether the plausibility that the lifetime is equal to some value, say 10,000 times.  http://en.wikipedia.org/wiki/Statistical_hypothesis_testing",2011-01-12T01:39:06.967,1118,CC BY-SA 2.5,
9522,6169,1,Good question. I have been always confused about their uses. Thanks for asking.,2011-01-12T02:11:16.280,1307,CC BY-SA 2.5,
9523,6182,3,"You have to make strong assumptions about the lifetimes.  Without them, you would need to test approximately 3/(0.1%) = 3000 devices.  For example, suppose 1% of the devices have a relatively rare defect that causes them to fail after 100 reprogrammings.  Your chance of encountering one of them during your testing of only four devices is approximately 4%: pretty low.  Thus, *based on the data alone,* you can have very little confidence that such defects don't exist in the population of microcontrollers.",2011-01-12T02:35:24.480,919,CC BY-SA 2.5,
9524,6181,0,Isn't http://en.wikipedia.org/wiki/Multiple_correlation good enough?,2011-01-12T02:39:35.053,919,CC BY-SA 2.5,
9525,6184,5,"(+1) Nice answer.  This will become a reference for similar questions in the future, I think.  However, cross-posting is unusual; in fact, I believe it is frowned on, because it screws up many aspects of the feedback/referencing/threading/commenting system.  Please consider removing one of the copies and replacing it by a link in a comment.",2011-01-12T02:49:48.307,919,CC BY-SA 2.5,
9526,6184,0,@whuber thanks for the feedback. I have removed the other copy.,2011-01-12T02:53:43.990,1381,CC BY-SA 2.5,
9528,6182,0,@whuber - is my answer consistent with your point about assumptions?,2011-01-12T05:20:53.527,1381,CC BY-SA 2.5,
9529,6188,2,another useful function is `any`. For instance if you want to check that at least one element of a vector is `= 10` you could write (`any(v==10)`).,2011-01-12T07:38:28.980,582,CC BY-SA 2.5,
9530,6185,1,+1 for the R code illustrating fast solution of the problem,2011-01-12T07:40:39.780,2116,CC BY-SA 2.5,
9531,6188,0,"@nico yeah, but **10 %in% v** has 9 characters and variant with **any** 10 :). Although for numeric vectors it is better to use equality, since R is smart and if your data is integers mixed with real numbers it will correctly recognize that you have 10 in your data set.",2011-01-12T07:42:35.283,2116,CC BY-SA 2.5,
9532,6189,0,what is the mean or median?,2011-01-12T08:04:06.537,2116,CC BY-SA 2.5,
9533,6180,0,Those statements sound fine to me.,2011-01-12T08:25:19.317,449,CC BY-SA 2.5,
9535,6188,0,what about the case for regular expression? assuming that you do not want to work with an exact match? Do they have something close to ~= as in other languages? what is most close  to that expression in R,2011-01-12T08:56:35.180,18462,CC BY-SA 2.5,
9537,6192,4,"@onestop This prompts me to think whether IQR should be an interval or a scalar. From Wikipedia, it is defined as a scalar and is consistent with what I have learnt.",2011-01-12T09:37:29.040,2742,CC BY-SA 2.5,
9539,6192,2,"I've always known that the IQR is the **difference** between the 3rd and the 1st quartile, hence a scalar.",2011-01-12T10:50:40.063,582,CC BY-SA 2.5,
9540,6161,0,Thanks for the help! I will try to implement the code in Mathematica Environment.,2011-01-12T10:58:04.053,1637,CC BY-SA 2.5,
9542,6188,0,"@Biorelated See `grep` and `agrep`, and related documentation on POSIX 1003.2 or Perl-compatible regex.",2011-01-12T12:26:40.907,930,CC BY-SA 2.5,
9543,6188,0,"@Biorelated, I've update the answer with the example of grep.",2011-01-12T12:37:09.860,2116,CC BY-SA 2.5,
9544,6105,1,"@Ama, your are welcome. At this site and similar stackexchange sites, there is a standard way of thanking for the answers. You simply accept the answer. Just press the check box outline to the left of the answer. I am bringing this up only because I noticed that you haven't accepted any of the answers you asked, so maybe this something you do not know.",2011-01-12T12:44:13.063,2116,CC BY-SA 2.5,
9545,6192,2,"Though IQR might be defined as a scalar, if it is reported as an interval it carries much more information. For example [-1,1] and [499,501] will have the same range, but implications are different.",2011-01-12T13:23:36.243,2116,CC BY-SA 2.5,
9546,6188,0,Thanks a tonne on this!!!! the explanation and the references! awesome.,2011-01-12T13:32:58.177,18462,CC BY-SA 2.5,
9547,6182,1,"@David Yes it is.  Good work.  I was thinking along the lines of borrowing strength from other datasets or knowledge about failure modes and distributions derived from similar devices.  Such information could justify a Bayesian estimate, too.",2011-01-12T15:19:49.223,919,CC BY-SA 2.5,
9548,6185,0,"@David if $n=4$ is too small to justify most models, why is it enough to justify *any* model?",2011-01-12T15:21:01.967,919,CC BY-SA 2.5,
9549,6191,3,"Despite the quip, it helps to know enough measure theory that you won't be afraid to read articles in JASA (or wherever) that could be useful or instructive.  If you're going to work in stochastic processes and mess about with Ito integrals and the like, and if you care to understand the tools you'll be using, then you actually do need a serious dose of measure theory.",2011-01-12T15:37:09.770,919,CC BY-SA 2.5,
9550,6193,3,+1 for offering a classic and for the remark on brevity!,2011-01-12T15:37:52.327,919,CC BY-SA 2.5,
9551,6185,0,"@whuber my point was that, given $n=4$, it might be most appropriate to fit a single parameter model. I can see your point though. Perhaps it would help to incorporate his prior knowledge into a gamma prior with $P(Y<100,000)=0.5$ and $P(Y<10,000)\simeq 0)$?",2011-01-12T15:57:46.283,1381,CC BY-SA 2.5,
9552,6196,0,"Exactly how would you apply a multinomial logit model, whose dependent variable is categorical, to prices, which are not categorical?",2011-01-12T16:00:12.603,919,CC BY-SA 2.5,
9553,6185,1,"@David I don't know enough about this particular situation to comment on any specific proposal.  The point I am making is that there is no magic in statistics that will generate ""99.9% certainty"" from a sample of four.  A single-parameter model might fit beautifully but anybody would be foolish to rely on a 99.9% CI or percentile estimated with it *unless* they had substantial, *independent* evidence that the model is appropriate.",2011-01-12T16:04:21.947,919,CC BY-SA 2.5,
9555,5973,0,"@user2040: Sorry for the late reply. I created my own software implementation of the nested CV approach. Because my research is related to bioinformatics, I'm planning to submit a description of the software soon to a bioinformatics journal. But it can be used in any research domain. If you're interested in trying it out, please let me know. goldfish1434 at yahoo dot com",2011-01-12T16:29:17.633,2643,CC BY-SA 2.5,
9557,6185,0,"@whuber I also don't know much about failure analysis and never think about such small probabilities, but I know that tail probabilities are much more sensitive and difficult to estimate - is that the reason for your concern?",2011-01-12T16:31:54.347,1381,CC BY-SA 2.5,
9558,6185,1,"@David I didn't plead ignorance concerning the *subject matter* :-)  I just don't know what additional information the OP has, so I have no idea whether your gamma prior is reasonable.  (Choice of prior is a big deal with this small sample.)  The concern is only indirectly about high uncertainties attached to tail probabilities.  More fundamentally it's about the model itself: if you use a bad model for the failure distribution, you will likely get bad (or awful) estimates.  How, then, can you tell whether the model is appropriate?  Not by testing just four units!",2011-01-12T16:36:30.997,919,CC BY-SA 2.5,
9559,6182,1,"It may be there is a significant variation in chips from different wafers (batches), but low variation within a batch.  This means that if the four chips are all from the same batch, it may give a misleading view of the lifetime of a chip selected at random from the whole production run.  It may be the low lifetime quoted is because there is occasionally a bad batch and it is easier to quote a low life expectancy than to detect the bad batches.  If your chips are all from the same batch, it may not be possible to estimate the true population lifetime.",2011-01-12T18:28:10.677,887,CC BY-SA 2.5,
9560,6185,0,"@David, thank you for your answer.  We do know that the failure will be due to the Flash memory ""wearing out"".  Any devices failing less than the minimum can be ignored and treated as burn-in failures.  I had one of the four fail with less than 100 events, but it was a ""soft"" failure and I restarted the test.  I am only interested in hard failures where retries are not possible.  Right now the counters on the four devices are approximately 721,000, 166,000, 166,000 and 172,000 so none have failed yet. (continued)",2011-01-12T18:32:05.620,2698,CC BY-SA 2.5,
9561,6185,0,"(continued from above)  I picked the number of 99.9% somewhat arbitrarily -- our current overall return rate for the product in the field after several months is between one and two percent, and I was just wanting to make sure this new process didn't contribute significantly to this return rate.  If I wanted to use your formula in R for other percentages (say 99.5, 99 and 98) I would just subsitute 0.005, 0.01 and 0.02 in the qexp line?",2011-01-12T18:37:18.007,2698,CC BY-SA 2.5,
9562,6185,0,"@David, I have installed R version 2.12.1.  Since you are using the MASS library, would ""Modern Applied Statistics with S"" be worthwhile for me to get (I notice there is an edition available for my Kindle DX), or would it be over my head since I have never taken a stats course?  Or do you recommend other book(s)?  (I do have an MS in computer science.)  Thanks.",2011-01-12T19:35:01.067,2698,CC BY-SA 2.5,
9563,6185,1,"@tcrosley yes, that is correct; MASS is one of the best books for learning R - I don't think that it would be over your head. I used MASS for the `fitdistr`, which provides a maximum likelihood estimate (MLE) of a distribution's parameters given data. So, if you are interested in the theory, use `?fitdistr` and read up on MLE. But you might want to consider comparing the fits of different distributions, perhaps contacting the manufacturer for additional data that could be used in a hierarchical model.",2011-01-12T19:48:29.313,1381,CC BY-SA 2.5,
9564,6198,0,"Sold! That sounds exactly what I'm looking for! As I'm all new to this I'm having trouble comparing suggestions, so I'm wondering how would least squares regression compare to multiple-regression and ""hedonic pricing"". These are suggestions I got in the mathematics site where I initially posted. What am I fixing when using least squares regression for instance? Basically, is there something I need to be aware of when using this approach?",2011-01-12T19:49:04.243,2746,CC BY-SA 2.5,
9565,6196,0,"@Dmitrij Celov: Thanks for your suggestion. I'll try to answer your questions. 1) No price is available, this is the unknown which I'd like to answer by looking at similar cars. 2) I don't know which variable is weighing the most - this I was hoping to get. 3) I would like to based on a list of cars with features and prices be able to price any car with any features.",2011-01-12T19:54:14.860,2746,CC BY-SA 2.5,
9566,6196,0,"@whuber: The""trick""with categorical attributes is to introduce the dummy variables that correspond to $K_j - 1$ level of the $j$-th attribute. The ""not categorical"" prices are introduced as they are, so it is an ordinary independent variable in (multinomial) logit model. Whuber, now I doubt that we need multinomial here, probably it is simple binary dependent variable with $1$ if chosen and else $0$. You do compare with rival collection of attributes, so some kind of differences between estimates like $P(y_i = 1| y_j = 0) = \frac{1}{1 + e^{-\beta^\prime (X_i-X_j)}}$ comparing $y_i$ and $y_j$.",2011-01-12T19:54:58.090,2645,CC BY-SA 2.5,
9567,6198,0,also thanks for this suggestion. It seems very good. I'll have to read up more to get an idea how I can get started to see how to use it.,2011-01-12T19:55:02.537,2746,CC BY-SA 2.5,
9568,6196,0,"@murrekatt: 1) So you just look for the most ""valuable"" attributes? 2) Logit estimated parameters are nicely interpreted like odds and odds ratios, but multinomial logit has a weak feature known as **independence from irrelevant alternatives** 3) Can you be sure that the listed prices are relevant, i.e. that the cars were actually purchased? @whuber: simple regression works here fine, if the dependent is price, but again what price? published where? or is it the actuall transaction?",2011-01-12T20:05:28.357,2645,CC BY-SA 2.5,
9569,6201,0,"@mpiktas: thanks. I understand what you mean. This was something I was thinking about, but didn't know exactly how to ask or add to the question. How does one deal with what you explain? Is this a problem which is separate and as you write to be taken into account when interpreting results, or is this integrated in some other approaches and not part of least squares regression? Not sure how to formulate myself, but what I mean is that are there approaches which take this into account and others which don't? Which means that for the ""don't"" we must interpret results?",2011-01-12T20:06:45.360,2746,CC BY-SA 2.5,
9570,6082,0,"Hi Wolfgang - thanks!  Actually, I think the intercept gives you an R^2 of 0, doesn't it?  But you are right about the overlapping variance, that's why you need do each alone; because each would add a different amount to a null (intercept only) model, and to a model with the other variable in there (unless they are exactly orthogonal, which seems very unlikely)",2011-01-12T20:10:00.153,686,CC BY-SA 2.5,
9571,6196,0,"@Dmitrij Celov: 1) I don't know how to answer this. Maybe I am, but then it's a side-product when pricing a given car. Maybe after analyzing the car list with prices some features are not really important in pricing the car (e.g. color maybe). 3) Yes, they are and yes those prices are real deals. Still back in time, so if e.g. a list if 1 year old, all cars on the list are now one year older too, which will decrease the price. Not sure how to take this into account.",2011-01-12T20:11:16.697,2746,CC BY-SA 2.5,
9572,6201,3,"@murekatt, if you do not have additional data on demand, but you need the model for price, you deal with this by taking extra care. This means less attention to statistical significance of coefficients, but more attention to forecasting performance. Essentialy this means treating regression as black-box and use the model forecasting performance as measure of model validity. This means using cross-validation, data division to train and test samples, etc.",2011-01-12T20:19:42.500,2116,CC BY-SA 2.5,
9573,6202,8,"$R^2$ can also be negative. This happens when other methods are used to calculate the coefficients in linear regression model specification, and the usual $R^2$ formula is used. I think $R^2$ can also get negative if regression is without the intercept.",2011-01-12T20:23:52.543,2116,CC BY-SA 2.5,
9575,6196,0,"@Dmitrij Celov: I'll try to answer your added questions. By the way, I didn't -1 your answer. I'll start with 2) The prices in the list are actual deals and no price if no deal for a car. All prices are deals. If we have to know who set it, let's say seller&buyer agreed. 1) Not sure here how to answer other than let's imagine we want as a seller to generate prices across all cars somehow. Or let's imagine as a buyer you'd like to generate prices across all available cars.",2011-01-12T20:34:54.950,2746,CC BY-SA 2.5,
9576,6196,0,"@murrekatt: than I give up, simple regression actually all that you need here, but if you observe the ""real deals"" and some deals that were not purchased, that logit analysis is still possible. To your reply, it is the buyers position, or the demand side that limits you here, so if you would like to discriminate the prices some how you need to know the features consumers value most, may be you still observe some suggestions that have not passed to real deals? By the way how to model ""hedonic"" prices is a very interesting question :)",2011-01-12T20:35:04.280,2645,CC BY-SA 2.5,
9577,6201,0,"@mpiktas: what do you mean with ""additional data""? Could you please give an example of this in the car context?",2011-01-12T20:37:28.957,2746,CC BY-SA 2.5,
9578,6201,1,"@murrekatt, look at the end of updated Dmitrij's answer. The demand data is important, so if you have how much cars were sold with given price this would help tremendously. Furthemore if you have data of how price changes for the given car with fixed attributes this also should be reflected in your model",2011-01-12T20:47:20.603,2116,CC BY-SA 2.5,
9579,6196,2,"@Dimitrij Price is not an independent variable: it's the *dependent* variable: ""I would like to understand how to model prices for any car based on this base information.""  I fear that with this misapprehension you may be taking @murrekatt very far afield.",2011-01-12T21:01:19.203,919,CC BY-SA 2.5,
9580,6196,0,"Applicability of the logit model aside, I recommend looking into [Biogeme](http://biogeme.epfl.ch/) for a dedicated software package that allows extreme flexibility in estimating a variety of GEV models...and it's free.",2011-01-12T22:57:01.023,696,CC BY-SA 2.5,
9582,6207,0,I hope I am not old fashioned if I use lattice :-),2011-01-13T02:47:52.010,1307,CC BY-SA 2.5,
9583,6207,2,"I also hope that if this is a HW problem, you will not simply copy paste.",2011-01-13T02:54:59.210,1307,CC BY-SA 2.5,
9584,6161,0,"I could run the code in Mathematica Environment, but since I'm not familiar with this topic, can you tell me how can I plot the impulse response of this filter? I just could plot the amplitude response.",2011-01-13T03:06:47.253,1637,CC BY-SA 2.5,
9585,6207,0,Thanks. This is not a HW question and the answer is helpful for me to understand my model.,2011-01-13T04:25:39.350,2755,CC BY-SA 2.5,
9586,6203,0,"@F. Tusell: that was a good description. I already puzzled this together from other posts, but this summarized things well for a beginner like me.",2011-01-13T07:00:20.240,2746,CC BY-SA 2.5,
9587,6201,0,"@mpiktas: I see. Price change of a car I suppose can be gathered by looking at historical data somehow. I have been thinking of this too, but not knowing how to utilize it. Maybe this is something that can be added later on?",2011-01-13T07:03:28.927,2746,CC BY-SA 2.5,
9588,6201,1,"@murekatt, in principle yes. I think you need to start small and add additional features later. The initial results will tell you what direction to take further.",2011-01-13T07:52:51.377,2116,CC BY-SA 2.5,
9591,6207,0,oh yes you are :),2011-01-13T08:09:30.527,2116,CC BY-SA 2.5,
9592,6196,0,"@whuber @murrekatt I was too quick to post the answer, after some sort of discussion I completely remade the suggested solution. Note the threats with least square regression I mentioned. Thanks for a good lesson on how to write the answers here, I'm still a newbie :) And, all attributes are interdependent, including the price, in our discussion I simply wanted to show that the fact of willingness to pay for the attributes comparing to not bought could not be ignored here.",2011-01-13T09:36:14.553,2645,CC BY-SA 2.5,
9593,6208,2,"Shortly - yes.  Simply to allow consistency with other packages (even if the result was ""wrong"" in some way - in which case a warning massage would be fine).  And thank you for your amazing package!",2011-01-13T11:08:45.243,253,CC BY-SA 2.5,
9594,6212,1,"To add one further requirement: It would be totally awesome if the function would also be able to run all kinds of contrasts from this ANOVA, for example, using the `multcomp` package (which, as far as I remeber, requires `aov` objects). Otherwise, I agree totally and normally use `ez` for my daily duty, because it is so easy...",2011-01-13T12:06:38.623,442,CC BY-SA 2.5,
9598,6216,0,"This is pretty clear, but the question was suggesting a restriction between b0 and b1. Should I also create a new variable Z = 2X + 1 and fit a model without intercept?",2011-01-13T13:04:54.677,339,CC BY-SA 2.5,
9599,6216,2,"I think usualy **I** is used instead of eval in formulas,  i.e. Y~I(1+2*X1)+X2+X3-1",2011-01-13T13:21:04.807,2116,CC BY-SA 2.5,
9600,6216,0,"@gd047: I have updated with a code pieces, yes it is as you say. @mpiktas: will change this, yes it is shorter ;)",2011-01-13T13:21:29.613,2645,CC BY-SA 2.5,
9602,6212,1,"adding a ""method"" argument (with possible values as ""oneway"",""aov"", and ""Anova"", and with a ""..."" argument to pass var.eq=FALSE to oneway and type=2 to Anova) is an intriguing idea, and it certainly seems that it would be easy enough to incorporate. Thanks for the suggestion.",2011-01-13T13:32:16.473,364,CC BY-SA 2.5,
9604,6212,0,"@Henrik: I have to be honest that my understanding of contrasts is very limited, and I never use them in my research, so I haven't prioritized incorporation of contrasts into ez.",2011-01-13T13:36:57.420,364,CC BY-SA 2.5,
9605,6211,0,"Ah, I hadn't realized that the type II/III distinction arises whenever predictors are correlated, of which an unbalanced design is only one example. This certainly further undermines my first ""just collect more data"" argument.",2011-01-13T13:46:25.043,364,CC BY-SA 2.5,
9606,6211,2,"I'm rather convinced by your argument that R will gain more SPSS converts if they are provided with easy means of achieving familiar analyses, even if these analyses require more nuanced consideration than is typically applied. Then, once we have them hooked on R, we can attempt to convince them that the familiar approach isn't necessarily always the appropriate approach. I think I'll still put the default as type=2 with a big warning in the documentation that this default may yield results that differ from SPSS and links do documentation on the type II/III distinction.",2011-01-13T13:47:30.627,364,CC BY-SA 2.5,
9607,6212,0,@Mike If you can add the method argument and so your function would be able to return an `aov` element that would be great. So far I am usually to lazy to make my own `aov` element and use you `ezANOVA` instead with t.tests...,2011-01-13T13:49:57.893,442,CC BY-SA 2.5,
9609,1527,0,"I agree with Robin, Anyhow, even if it is not usual to say that in a given set of observations the probability of success is 0.3, it is common to use the word proportion as a synonym of probability: search google for: binomial and ""proportion p of success""",2011-01-13T13:56:55.287,1219,CC BY-SA 2.5,
9610,6212,0,"@Mike I hadn't dared suggesting something like that since it's your work you're putting in. Thanks for that! The `ez` package has some great potential: in psychology, it's already recommended quite often (cf. German book ""R f√ºr Einsteiger"" by Luhmann). Making it even more flexible would certainly be appreciated.",2011-01-13T14:01:05.763,1909,CC BY-SA 2.5,
9612,6211,0,"Other variations on correlated predictors are usually with continuous predictors, but yes, that's the general problem. // That sounds great to me! Sometimes these shifts are slow; the use of sphericity corrections (rather than ignoring the problem) is one that I think progress has been made on in neuroscience. There were times when any mention of it (particularly if you reported the corrected, non-integer df) made referees think you were barking.",2011-01-13T14:42:12.157,2761,CC BY-SA 2.5,
9613,5791,1,"If you want to measure the constrain-ness of a parameter, what about to use st.dev(posterior)? If you want to compare constrain-ness of different parameters, what about to use st.dev.(posterior)/st.dev(prior)?",2011-01-13T14:50:11.210,1219,CC BY-SA 2.5,
9614,6198,3,"I want to acknowledge and express my agreement with the caveats posted by @mpiktas and @dimitrij celov.  Analyses of prices can be--and in many cases *should* be--as complex as the economic systems of which they are a part.  However, due to the intended application (a hobby) and the plainly signaled limitations in the OP's capabilities for statistical modeling, we should place great value on simplicity, ease of use, and interpretability.  Obviously someone not yet conversant with least squares is not going to jump right in and start creating full-blown econometric models.",2011-01-13T15:57:02.667,919,CC BY-SA 2.5,
9616,6220,0,"Not correct: you need to use $n-2$, not $n$.",2011-01-13T16:03:38.700,919,CC BY-SA 2.5,
9617,6213,3,Examples of better plots?,2011-01-13T16:06:31.470,559,CC BY-SA 2.5,
9618,6215,0,"Could you explain the distinction you are making between ""observed"" and ""declared"" level?  (I also suspect a good journalist would be bothered by the implication that you are ""discarding"" values and would follow up with some difficult questions :-).)",2011-01-13T16:08:00.910,919,CC BY-SA 2.5,
9621,6220,0,"good point, fixed.",2011-01-13T16:54:13.177,449,CC BY-SA 2.5,
9622,6225,5,"It depends on what you mean by ""prove.""  As stated, this is a philosophical question, not a statistical one, and has no definitive answer (although, at least since David Hume's time, most people would answer ""no"").",2011-01-13T16:54:16.800,919,CC BY-SA 2.5,
9624,6226,3,NA is not the same thing as NaN. See http://stats.stackexchange.com/questions/5686/what-is-the-difference-between-nan-and-na.,2011-01-13T16:56:51.443,449,CC BY-SA 2.5,
9626,6219,0,"Surprisingly, Wikipedia appears to have no articles related to this subject that explicitly and clearly show the correct formula in this least squares context!  (The formula is buried in articles on analysis of variance and least squares.)",2011-01-13T17:04:57.100,919,CC BY-SA 2.5,
9627,6226,0,"Why are you doing ""data preparation"" in openoffice? Do you really have to? I try and load things into R as soon as possible and make all changes within R itself.",2011-01-13T17:44:26.507,8,CC BY-SA 2.5,
9631,6226,4,"If you save a file as csv and import it to R, blank cells will be represented as NA.",2011-01-13T18:20:55.773,2116,CC BY-SA 2.5,
9632,6226,0,@csgillespie I find it convenient to edit some data in a spreadsheet UI as opposed to a text editor UI.,2011-01-13T18:24:17.120,660,CC BY-SA 2.5,
9633,6226,0,@onestop Thanks for pointing out that NA is different from NaN. I edited the question accordingly.,2011-01-13T18:24:46.527,660,CC BY-SA 2.5,
9634,6226,0,"@mpiktas Thanks, I think your response is useful as an ""answer"" (not just a comment) -- want to submit it?",2011-01-13T18:26:34.077,660,CC BY-SA 2.5,
9636,6219,2,"I think you are all confused, rms stands for your favorite GNU advocate, [Richard Matthew Stallman](http://en.wikipedia.org/wiki/Richard_Stallman)",2011-01-13T19:14:49.907,696,CC BY-SA 2.5,
9639,6219,1,"@Chase Good point; we shouldn't rely on acronyms: see http://stats.stackexchange.com/q/6039/919 .  However, ""Royal Meteorological Society"" gets more Google hits than ""Richard M Stallman"" :-).",2011-01-13T19:34:53.650,919,CC BY-SA 2.5,
9640,6114,0,"Can @caracal's code be generalized to inlude multiple choice questions. i can vary but it will be nice to give a line like # p <- c(0.4, 0.4, 0.4, 0.4, 0.1, 0.1, 0.1) but instead of the probability the user enters the number of choices, assuming equal likelihood.",2011-01-13T19:38:14.240,,CC BY-SA 2.5,user2715
9641,6226,0,"@David, ok, I've posted it as an answer and included additional comments. Hope it helps.",2011-01-13T19:39:32.537,2116,CC BY-SA 2.5,
9642,6075,0,"great work! I love that this question, which could be posed as one sentence (to those with sufficient background), has brought out monte carlo, inclusion-exclusion, DAGs, searching trees, polynomial algebra, and that your simulations are confirmed by the theoretical of @whuber. cheers!",2011-01-13T19:49:03.020,795,CC BY-SA 2.5,
9644,6208,0,"When I wrote ""packages"" I meant other statistical packages (like SPSS and SAS) - I'm glad others have driven the point further home then I did :)",2011-01-13T20:00:22.863,253,CC BY-SA 2.5,
9645,6114,0,"@Rado If a question has $k$ choices with probabilities $p_{j}$, then the probability for pairwise agreement on a single such question is $\sum_{j=1}^{k}{p_{j}^{2}}$. You should only need to change the `getP()` function to reflect this, specifically the lines starting with `pp <- ...` and `qq <- ...`.",2011-01-13T20:50:30.947,1909,CC BY-SA 2.5,
9646,6232,0,I'm having trouble wrapping my head around your design... At what level was the randomization done? At the standtype*treatment level? How many standtype*treatment level 'units' are there? 4√ó3√ó6=72? And then 72√ó6=432 plots?,2011-01-13T20:56:22.890,449,CC BY-SA 2.5,
9648,6131,0,@Andy Interesting suggestion: some mechanism for specifying that you want to shade the code block without highlighting?  I'd say it's unlikely since SO uses an open source code highlighting utility (don't remember the name).  Maybe open a meta ticket to suggest it?,2011-01-13T21:25:02.550,5,CC BY-SA 2.5,
9649,6229,4,"+1 Nice answer.  A simple rendering of the math is that the null and its alternatives are assumed to yield disjoint sets of outcomes; *e.g.,* either there is a zebra in this room or there isn't.  Of course ""prove"" here implicitly includes ""conditional on the model,"" which itself is never established with the same rigor as, say, a mathematical theorem; it implicitly includes ""conditional on the accuracy of the observations;"" and it implicitly includes that the hypotheses can be unambiguously interpreted.  (For criticism of the latter, see George Lakoff's *Women, Fire, and Dangerous Things.*)",2011-01-13T21:29:40.460,919,CC BY-SA 2.5,
9650,6173,0,"Grouped bar chart is always preferable to stacked bar chart, because the latter makes the comparison of different series more difficult than the former. A brief discussion about different versions of stacked bar and grouped bar plot (marimekko chart, weighted grouped bar plot, etcetera) with some links can be found here: http://www.antoniorinaldi.it/data-visualization-for-two-ways-tables-1",2011-01-13T22:02:21.267,1219,CC BY-SA 2.5,
9651,5986,0,"I've asked one of my professors and he said that he would use Bayes estimator: $p = \frac{k+1}{n+2}\cdot \frac{\beta_{k+2,n-k+1}(0.25)}{\beta_{k+1,n-k+1}(0.25)}$ , where $\beta$ denotes CDF of beta distribution and $p=\alpha-\alpha^2$. p is less than 0.25 so it can be easily transformed to $\alpha$. I havent checked those calculations yet, but several simulations, that I've made, have shown that for small n this is far from being great.",2011-01-13T22:04:16.627,1643,CC BY-SA 2.5,
9652,6192,1,"(IMHO) IQR is a scalar: it is the lenght of an interval. Obviously, since it is always possible to pass from the second (lying in a subset of R^2) to the first (lying in R+) but not viceversa, the latter is more informative than the former.",2011-01-13T22:05:50.330,1219,CC BY-SA 2.5,
9653,5986,0,"@Tomek I thought of those Bayes estimators (with conjugate priors) when writing the original answer, and worked out their characteristics, but found them wanting.  The approach is a good one for estimating $\alpha$ but doesn't overcome the difficulties of estimating $\alpha - \alpha^2$.  (One problem is that the prior your professor is implicitly using just doesn't narrow the likely range of $\alpha$ sufficiently.)  But I still think, as I indicated, that this is a fruitful line of investigation.",2011-01-13T22:07:02.280,919,CC BY-SA 2.5,
9654,6235,0,(+1) Nice answer! - I especially like the graphics. I was wondering if there is a way to do this in R?,2011-01-13T22:37:20.210,1307,CC BY-SA 2.5,
9655,6232,0,"In addition to onestops's remark - I would also be interested to know - what was the aim of the experiment, what was the response variables, and any other specifics that are important in the data analysis. I am guess this to be a split-plot-type design, but we can help you better if we get more details.",2011-01-13T22:42:52.437,1307,CC BY-SA 2.5,
9656,4850,2,"Err, maybe I'm missing something but in some important cases, e.g. all of survey research, probabilities are not at all hypothetical, they're just population proportions.  In the question 'how many Ukrainians think X' the population is pretty clear - all Ukrainians - and the proportion that thinks X from a simple random sample estimates the proportion of the population that thinks X, which is exactly the probability of interest.  For frequentists, this is the easy case (and I, as a non-frequentist would concur with their analysis).",2011-01-13T22:46:41.977,1739,CC BY-SA 2.5,
9657,4850,4,"@Conjugate In some cases a probability may *equal* a proportion but it *is not* a proportion.  What relates a proportion to a probability is the specific procedure of sampling uniformly at random with replacement from a well-defined population (which are rare, by the way: 20 Ukrainians have been born since you wrote your comment!).  This clearly is a special case of other sampling methods, including without replacement, with stratification, etc.  In those other cases the proportions no longer even equal the probabilities.  Doesn't this suffice to show the two concepts are distinct?",2011-01-13T23:05:18.250,919,CC BY-SA 2.5,
9658,6238,2,"The issue here lies in the distinction between *actual* coverage and *nominal* coverage.  You seem to assume the two are equal, but for most discrete distributions they are not.  Worse, the actual coverage depends on the proportion one is trying to estimate!  Before considering the question further, you might want to research the two methods @Michael Lew mentions.",2011-01-13T23:08:06.610,919,CC BY-SA 2.5,
9659,6237,1,"+1 A valiant, tactful, and informative attempt to respond to an unanswerable question!",2011-01-13T23:09:44.783,919,CC BY-SA 2.5,
9660,4850,0,"On the 'special case of other sampling methods', I agree. The whole trick of survey research is to make one's stratified, clustered, etc. sample proportions match (in expectation at least) the population proportion one is interested in.  But I'm not sure that's a criticism, rather a statement of the problem.",2011-01-13T23:56:24.247,1739,CC BY-SA 2.5,
9661,4850,0,"Another tack: whenever a frequentist states a coverage probability she immediately asserts the existence of a population. In some situations the population is indeed infinite, e.g. infinite hypothetical replications of an experiment that takes place once. But in others, e.g. the survey context, the population is not infinite at all.  In my example it's the population of the Ukraine (at t). That's definitely finite and describable using proportions. I'm not disagreeing with your analysis - just pointing out that there is a simple situation where proportions and probabilities coincide.",2011-01-14T00:03:48.340,1739,CC BY-SA 2.5,
9663,4850,0,"@Conjugate I am making a conceptual distinction here, so ""coincidence"" (mathematical equality), although noteworthy, is beside the point. In many cases, a ""population"" is a convenient fiction: one can find many references to *processes* as ""populations,"" for instance. In ideal cases we can indeed construct a rigorous data frame. But that solves only one of the difficulties.  Whatever we're interested in has to be measured and measurements are subject to error. (Even gender!) The ontological status of a ""population"" therefore is questionable: like probability, it too is a modeled construct.",2011-01-14T00:44:12.473,919,CC BY-SA 2.5,
9664,6240,1,"+1.  This is a nice example of the importance of being clear about one's standard of ""proof.""  In many applications the one you invoke here--the ""act as if"" standard, if I may call it that--is so weak that nobody would accept it as ""proof.""  I do not deny its utility, though, and advocate this kind of approach to support rational decision making.  (But maybe Bayesian methods are better... :-)",2011-01-14T00:47:33.357,919,CC BY-SA 2.5,
9665,6220,0,"Can you confirm this thinking:  with n = 3, if order is 2, then k is 3.  So then n - k is zero, by which we don't want to divide. Is my understanding correct that if n = order + 1, there is always an exact fit, so calculating RMSE isn't even necessary?",2011-01-14T02:20:25.313,2767,CC BY-SA 2.5,
9666,6235,1,"I'm not aware of any way to do it; my webvis package provides a wrapper for Protovis, but it would be a lot of work to get it to make this graphic.  Incidentally, this paper introduces the ""arc diagram"" which is related: http://ieg.ifs.tuwien.ac.at/~aigner/teaching/ws06/infovis_ue/papers/arcdiagram_01173155.pdf",2011-01-14T04:10:18.577,5,CC BY-SA 2.5,
9667,6112,0,You're right. I just wanted to give an answer based on undergraduate probability and give where it comes from. Having the formula also gives you insight as to how the value changes for different i's and p's.,2011-01-14T05:28:26.220,1540,CC BY-SA 2.5,
9668,6241,2,"Also note that window(qs,2010, c(2010, 4)) <- 3 will change the qs accordingly.",2011-01-14T06:34:27.537,2116,CC BY-SA 2.5,
9670,6100,0,I ended up computing the difference between means as the calibrated mean -- http://stattrek.com/AP-Statistics-4/Difference-Means.aspx,2011-01-14T07:46:47.717,2634,CC BY-SA 2.5,
9671,6100,1,"@John-David MOE=SeM=deviation of mean=deviation of sample/$\sqrt{\text{size of sample}}$ -- that's why I used the word ""error"". So, my result is identical to the magic formula on that site -- they just get deviation of sample as input, which is quite confusing.",2011-01-14T08:31:40.160,,CC BY-SA 2.5,user88
9672,4850,0,"@whuber Ahh, it's population as an idea you're not happy with. Fair enough.  (on the other hand it seems hard to talk about 'measurement subject to error' without that concept :-)",2011-01-14T08:35:08.237,1739,CC BY-SA 2.5,
9673,5950,0,"@shabbychef: Why is a flatter (""less vertical"" in 2D space) hypothesis the simpler one? Why is the $\theta$ with smaller components any simpler than one with larger components? For my example, what about training on (0,0) and (1,9) instead, then? I don't see why $\theta=8$ is any simpler than $\theta=10$, and similarly for more dimensions....",2011-01-14T08:39:12.630,1720,CC BY-SA 2.5,
9674,6138,0,"@vqv: Re: ""I don‚Äôt think it is really a classification problem. 20 questions is often characterized as a compression problem."" Aren't statistical inference and information compression directly related / the same problem?",2011-01-14T08:42:26.043,1720,CC BY-SA 2.5,
9675,6240,1,(+1) Nice answer. I added a link to an online version of Streiner's article; I hope you don't mind (feel free to remove).,2011-01-14T09:18:59.577,930,CC BY-SA 2.5,
9676,6220,0,"Yes. When n=k, the model becomes an exact fit to the data, i.e. the fitted values are the same as the observed values, so you'd get 0/0 in the above formula, so the result is undefined. But logically, as the 'errors' are all zero, you'd expect the root mean square error to be zero too, and that's what the stats packages i've tried report.",2011-01-14T10:18:59.637,449,CC BY-SA 2.5,
9677,6243,0,It may help if you explain a bit more about *how* you use the difference between means to calibrate the mean.,2011-01-14T10:29:02.463,449,CC BY-SA 2.5,
9678,6234,1,"Related, but for small number of sets (for reference): http://stats.stackexchange.com/questions/4211/how-to-visualize-3d-contingency-matrix",2011-01-14T12:51:06.377,,CC BY-SA 2.5,user88
9679,4850,0,"@Conjugate Measurement refers to individuals, not a population.  But that's taking us off on a tangent (for which I apologize): the question concerns distinguishing probabilities from proportions.  I hope we have succeeded in illuminating some of the differences.",2011-01-14T13:04:28.020,919,CC BY-SA 2.5,
9680,6235,1,"@suncoolsu , the R package diagram may be able to do the same ""arc diagram"" Shane pointed to. It looks like it would be hard work though to get the ""plot web"" to look like the visual above though. http://cran.r-project.org/web/packages/diagram/vignettes/diagram.pdf .",2011-01-14T13:13:32.503,1036,CC BY-SA 2.5,
9681,6221,0,"Great answer, csgillespie. Thanks for taking the time to edit my question and provide your answer. The reason I didn't accept it was that in your step 3, I couldn't figure out where the constant ""2"" came from!  I liked your presentation and clarity over the other answers, except for that.  I did give it +1 though. Thanks for the help.",2011-01-14T13:31:18.227,2767,CC BY-SA 2.5,
9682,6220,0,"THANKS, onestop.  I've accepted your answer.  I appreciate it.  The frustrating thing about asking a statistics question on SO is there's no way I'll ever be able to ""pay it forward"" by helping someone else out.",2011-01-14T13:34:28.783,2767,CC BY-SA 2.5,
9683,6247,0,"Can you construct a simple, reproducible example of your problem? Just now I can't reproduce your difficultly since I don't have `mx`.",2011-01-14T14:00:12.403,8,CC BY-SA 2.5,
9684,6235,0,"and Andy. Thank you for your answers. @Shane, I have seen your webvis package. But I still need to explore it further. I do like protovis graphs a lot. They have a great website.",2011-01-14T14:15:05.277,1307,CC BY-SA 2.5,
9685,6222,0,"Thanks, Nico, for taking the time to answer.  You assumed that a beginner like me knows what ""R"" is. I didn't ... until I just Googled it ... so your R code didn't help.",2011-01-14T14:28:02.460,2767,CC BY-SA 2.5,
9686,6235,0,"@suncoolsu Regretfully, the package is far from ""complete"" and I have very little time to spend on it.  Maybe one of these days.  I'd love to expose things like the above graphic so that they would be trivial to create.",2011-01-14T14:34:17.863,5,CC BY-SA 2.5,
9687,4850,1,"I meant that it's measurement *error* (or any other notion of statistical error) that requires the concept.  But you're right, we've wandered a bit. Hope I'm not the only one who's been illuminated in this little exchange.",2011-01-14T14:37:52.353,1739,CC BY-SA 2.5,
9688,6243,1,@onestop See http://stats.stackexchange.com/questions/6096/correct-way-to-calibrate-means,2011-01-14T15:07:52.343,,CC BY-SA 2.5,user88
9689,6243,0,"Thanks, mbq. Seems to me the overhead would simply cancel out when taking the difference between two overhead-corrected means. Or am i missing something?",2011-01-14T15:18:22.680,449,CC BY-SA 2.5,
9691,6253,1,"I interpreted ""10% bonus"" as an additional 10% towards your return mark. For example, 30% goes to 40% with this bonus. However, it could be that your return mark is increased by 10% (30% goes to 33%). Would you clarify?",2011-01-14T15:25:52.750,8,CC BY-SA 2.5,
9693,6222,1,"Oh, OK! Well... if you did not find it, R is an open-source statistics environment and programming language. You can download it at http://www.r-project.org",2011-01-14T15:37:40.033,582,CC BY-SA 2.5,
9694,6243,0,"@onestop - I correct the variance, moe, sd, and others but I am not sure if sample size also needs correcting and if so, the formula to do it.",2011-01-14T15:43:46.447,2634,CC BY-SA 2.5,
9696,6253,1,30% goes to 40%....,2011-01-14T16:58:12.037,2599,CC BY-SA 2.5,
9697,6257,0,"+! Yup; @Max What do you thing this ""obvious"" clustering would be?",2011-01-14T17:33:42.630,,CC BY-SA 2.5,user88
9698,5950,0,"@Yang: thinking about this in the 1d case may not be instructive. I've amended my answer, see above.",2011-01-14T17:49:17.863,795,CC BY-SA 2.5,
9699,6220,1,@Robert Frank: by asking this question you are effectively helping out anyone else who will in the future have the same doubt as you. :),2011-01-14T18:11:25.637,582,CC BY-SA 2.5,
9700,6257,0,"@mbq: Actually I don't know what would be a good clustering for this. By ""obvious"" I ment that (N-1, 1) is definitely not a good clustering for this. A better clustering would be only one cluster, so no clustering at all. Or maybe some clustering with the number of clusters more than 2.",2011-01-14T18:19:32.460,255,CC BY-SA 2.5,
9702,6138,0,"@Yang Are you referring to Jorma Rissannen‚Äôs argument? Statistical inference and information compression both make use of probabilistic models to describe uncertainty, however their   perspectives and those if the researchers in those areas are generally very different.  What I mean to say above is that 20 questions can be more naturally formulated as a compression (specifically, source coding) problem rather than a classification problem. ‚Ä¶ continued below",2011-01-14T19:48:13.047,1670,CC BY-SA 2.5,
9703,6138,0,"@Yang Here‚Äôs one aspect.  In a classification problem, we typically assume that there are a small number of classes and, certainly, that the number of classes is much smaller than the number of units.  In 20 questions, every unit is its own ‚Äúclass.‚Äù  The goal is to ask (on average) a minimal number of questions to uniquely identify each unit.",2011-01-14T19:48:42.423,1670,CC BY-SA 2.5,
9704,6138,0,@ADEpt I don‚Äôt know what you mean by range questions.  The specifics behind the model that Akinator.com are‚Ä¶ well‚Ä¶ specific to them.,2011-01-14T19:49:41.003,1670,CC BY-SA 2.5,
9706,6243,0,@mbq - Thanks for helping me clear things up. Still unsure what the composite `n` value would be. The tutorials I have seen don't mention a way to compute it.,2011-01-14T20:23:06.307,2634,CC BY-SA 2.5,
9707,6243,0,@mbq I found a site that shows formulas for computing the new `n` value -- http://www.rad.jhmi.edu/jeng/javarad/samplesize/,2011-01-14T21:18:54.690,2634,CC BY-SA 2.5,
9708,6240,1,"couple more things: (1) Treating failure to reject the null as evidence in *support* of null is a shockingly common error & usual occasion for Streiner's point. This mistake essentially turns the strong aversion to type 1 error in ""p < 0.05"" norm into license to make type 2. S says, ""wait--you need power..."" (2) Whuber cites Hume's famous argument. H's pt is actually just as subversive of empirical proofs rejecting the null as of proofs *of* the null. H says induction can't support causal inference. Ok; but there's no alternative for empirical study! Go Pearl (& Bayes), not Hume, on causality!",2011-01-14T23:49:35.133,11954,CC BY-SA 2.5,
9709,6265,0,"This does not directly pertain to the violation of independence, but have you considered multiple imputation to deal with your missing values?",2011-01-15T02:57:15.923,1118,CC BY-SA 2.5,
9710,6243,0,@mbq Seems that site wasn't much help :(,2011-01-15T05:18:28.867,2634,CC BY-SA 2.5,
9711,6265,2,"Why is there missing data? Is the data missing completely at random, or can the reason for a given observation being missing be explained by the non-missing data or the demographic data? The best method for handling missing data depends critically on the reason for missingness.",2011-01-15T08:42:02.533,449,CC BY-SA 2.5,
9712,6243,0,@John This site you linked does not contain any equation valid in your case...,2011-01-15T09:43:40.113,,CC BY-SA 2.5,user88
9713,6270,5,+1 for highlighting the distinction between model-based clustering *vs.* purely distance-based unsupervised clustering.,2011-01-15T10:58:19.733,930,CC BY-SA 2.5,
9714,6248,0,"It's usually considered good practice to save the old parameters before modifying them, so that new settings don't propagate throughout successive calls to `plot.*` functions. An example of use would be: `opar <- par(mar=c(5,1,4,2)); ... plotting goes here ...; par(opar)`.",2011-01-15T11:04:48.077,930,CC BY-SA 2.5,
9715,6267,0,"I've tried using intra in inter cluster distance, but couldn't think of something usefull for a cluster with one point. Also I don't have a center point. I only have distances between points.",2011-01-15T12:52:48.927,255,CC BY-SA 2.5,
9716,6248,0,"@chl, OP uses postscript, so the plotting must end with dev.off(). This resets all the par settings. The same holds for usual plots. If dev.off is called between plot's, settings are reseted.",2011-01-15T13:20:03.463,2116,CC BY-SA 2.5,
9718,6233,0,"Thank you so much for the advice about R import of csv data with blank cells. I didn't know that, and have been spending a lot of time putting NA's into my spreadsheets.",2011-01-15T14:06:55.793,656,CC BY-SA 2.5,
9719,6276,0,"Thank you for the links, the one to this PDF looks especially useful http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf Not sure how i missed that.",2011-01-15T14:32:35.890,656,CC BY-SA 2.5,
9720,6276,0,I don't think that the LearnR blog has any time series references...,2011-01-15T14:34:39.270,5,CC BY-SA 2.5,
9721,6268,1,"At the same time you are showing how good linear regression can be, please also show your audience how it fails in situations where the relationships are not well described by straight lines: `require(mlbench) ; cor( mlbench.smiley()$x );  plot(mlbench.smiley()$x)`",2011-01-15T15:12:05.037,2129,CC BY-SA 2.5,
9722,6278,1,"Thank you, especially for the Zoonekynd reference, i found that before but couldnt get back to it.",2011-01-15T15:41:21.723,656,CC BY-SA 2.5,
9723,6268,0,Will do Dwin... :-),2011-01-15T16:08:43.410,253,CC BY-SA 2.5,
9724,6255,1,"With a uniform prior distribution wouldn't guessing 30 be equivalent to guessing 35, since both will have 65 actual test grades that will result in a passing grade?",2011-01-15T16:12:56.930,1036,CC BY-SA 2.5,
9725,6266,0,"+1, I think this is a better strategy if your goal is to maximize the grade and you have prior distribution of test grades. It seems a bit overkill if your only goal is pass the exam though.",2011-01-15T16:14:50.313,1036,CC BY-SA 2.5,
9726,6276,0,"I searched it for time-series, and got some stuff about ggplot2. Useful, but not really with i was looking for.",2011-01-15T16:16:00.023,656,CC BY-SA 2.5,
9727,6233,0,"@richiemorrisroe, you're welcome. I usually find if something would be convenient, R implements it this way :)",2011-01-15T16:19:28.550,2116,CC BY-SA 2.5,
9729,6281,0,"hm, if you know how to deal with numbers, then get them by multiplying true mean by 0.5%.",2011-01-15T18:23:10.083,2116,CC BY-SA 2.5,
9730,6279,1,why exactly are you testing for non-linearity? And what are you trying to model? What is IV?,2011-01-15T18:28:34.993,2116,CC BY-SA 2.5,
9731,6248,0,That's right. Sorry about that (forgot about the PS issue).,2011-01-15T18:59:15.803,930,CC BY-SA 2.5,
9732,6248,0,"@chl, nothing to be sorry about, au contraire, your comment was useful, since this is good to know if you indent to use par more often.",2011-01-15T19:13:35.233,2116,CC BY-SA 2.5,
9733,6284,0,"That continuous result should be squared. $\sqrt{n}=\frac{Z_{c}\sigma}{E}$, and thus $n=(\frac{Z_{c}\sigma}{E})^2$",2011-01-15T20:07:03.340,1118,CC BY-SA 2.5,
9734,6274,0,I replied to you and @mbq in an answer to my own question.,2011-01-15T20:33:38.270,2634,CC BY-SA 2.5,
9735,6273,0,I replied to you and @onestop in an answer to my own question.,2011-01-15T20:34:11.813,2634,CC BY-SA 2.5,
9737,6284,0,"@Christopher Aden: that is true, thank you for pointing out the error! I corrected in the answer.",2011-01-15T21:22:26.733,2714,CC BY-SA 2.5,
9738,6255,0,"@Andy-w Good point, I should have explained my reasoning. I've now updated my answer.",2011-01-15T21:43:01.577,8,CC BY-SA 2.5,
9740,6265,0,"I shine light on the skin and measure the properties of the light returning. The property I am analyzing is independent of the the mean intensity of the light unless it is below a certain intensity. Thus the device rejects any data with a mean intensity below the cutoff. Mean intensity is determined by the exposure time, set manually by a tech. Sometimes he chooses poorly and the mean intensity is too low. For the most part the missing values are randomly distributed throughout except two body sites need longer exposure times and he is more likely to underestimate the needed exposure time.",2011-01-16T01:41:25.477,2788,CC BY-SA 2.5,
9741,6279,0,IV stands for independent variables. I thought the first step is testing assumption before we start the regression analysis?,2011-01-16T01:47:42.863,2793,CC BY-SA 2.5,
9742,6288,0,"crosstab result is ok, with no zero values.",2011-01-16T01:52:57.037,2793,CC BY-SA 2.5,
9743,6288,0,"crosstab result is ok, with no zero values. The problematic continuous variable gives me this result: B=9.62, SE=7.0, Wald=1.89, Sig=0.16, Exp(B)=15203.83. So, it is not sig., thus no issue of multicollinearity right? But the Exp(B) is so large which I think something wrong here. Another interaction (X*LnX) is sig. at 0.05, thus multicollinearity exists. From what I read from the book, the only solution is to transform this continuous variable to categorical?",2011-01-16T02:00:00.087,2793,CC BY-SA 2.5,
9746,5770,1,@Marco. Can you please make edits to your original question? This will make it easier to answer similar questions in the future. It will also help me comprehend your design and answer it correctly. Thanks in advance.,2011-01-16T04:34:47.567,1307,CC BY-SA 2.5,
9747,6280,0,"I‚Äôm writing a paper for a broad audience an wish to contrast those statements with the equivalent statement for the interval containing 95% of the population proportions that are likely to yield the observed number of successes. That interval is easily accessible from the cumulative beta function, Beta(x+1,n-x+1), and has exact coverage given the observed successes. It is fully frequentist, being a fiducial interval and is at the same time a Bayesian interval with a uniform prior. (Ross 2003, <www.computersinbiologyandmedicine.com/article/S0010-4825(03)00019-2/abstract>",2011-01-16T05:48:55.947,1679,CC BY-SA 2.5,
9748,6279,0,"could you be more precise, your questions are for the results of non-linearity test, or for logistical regression? Also it would help if we knew more about your model, what is dependent variable, etc.",2011-01-16T07:32:41.687,2116,CC BY-SA 2.5,
9749,6288,0,"@lcl23, what is $B$? If it is a coefficient from logistic regression, why are you interested in $\exp(B)$?",2011-01-16T07:33:56.007,2116,CC BY-SA 2.5,
9750,6279,0,"it is the result of nonlinearity test using Ln transformation. My research question is to know the existence(yes/no) of management committee in public company. Predictors involved comprised of categorical & continuous variable, such as board size, auditor, etc....",2011-01-16T10:28:56.517,2793,CC BY-SA 2.5,
9751,6288,0,"yes, B is the coefficient. Exp(B) is the odd ratio. Does this extremely large odd ratio indicate something wrong? or I can just ignore it?",2011-01-16T10:55:47.673,2793,CC BY-SA 2.5,
9752,6280,0,"Then I can see why you don't mention the Jeffreys interval:  Although I can't access the article itself, from the abstract it seems that Ross 2003 offers a Jeffreys' interval with a Beta(1,1) prior, rather than the same interval with a Beta(.5,.5) prior that Brown et al. examined.  It would seem odd if the coverage properties were wildly different for these two, but without the article it's hard to say.",2011-01-16T11:55:50.440,1739,CC BY-SA 2.5,
9753,6096,0,"About the flag -- this is no need to remove this post, it may be still useful for others.",2011-01-16T12:18:59.683,,CC BY-SA 2.5,user88
9754,6294,2,"The estimation is the same for ordinal ""X"" categorical variables as simple categorical ones - the interpretation is different though.  The fact that they are ordinal categories (a,b,c,d,e) simply tell you that a<b<c<d<e and you would need some structure saying in some way ""how much"" bigger each category is compared to the ones below it.  If you want to save a few degrees of freedom, then you would need to specify some additional structure saying how the ordering happens using 3 parameters or less.",2011-01-16T12:34:26.443,2392,CC BY-SA 2.5,
9755,6289,0,"Nice answer and great research. Appears to me from the 1st pdf that although they're defining this as the 'epidemic threshold', they're not declaring an epidemic each time it's exceeded for a given week - they're just declaring it a period of 'excess mortality'. They then focus on periods when it's exceeded for several weeks running (e.g. 10 or more).",2011-01-16T13:11:37.150,449,CC BY-SA 2.5,
9757,6279,0,I am perplexed about your recode of zero cells. Have you tried a sensitivity analysis? What happen if you use for example 0.01 or 0.00000001? Isn't to possible to make a conditional analysis after discarding observations with zeroes?,2011-01-16T13:37:21.180,1219,CC BY-SA 2.5,
9758,6291,0,"""Another thing to ask is what is the range of X values in your data?"" This variable ranges from 2 to 6.",2011-01-16T13:40:26.240,2793,CC BY-SA 2.5,
9760,6192,1,"I expected this to be controversial and realise that it's not the *usual* definition, but i think it's a more useful one. In my experience of the medical literature, it's far more common to report both quartiles as the IQR rather than the difference between them, and I'm glad that's the case.",2011-01-16T13:54:24.460,449,CC BY-SA 2.5,
9761,6295,1,"I would slightly disagree with onestop's advice, prefering to leave the ordinal variable as a categorical variable, and then check how the co-efficients vary as you move up the ordinal categories for some structure.  To fit the data as if it was continuous means you have to chose numerical values for the categories, and the results could easily be sensitive to this choice - example if I have 1,2,3,4,5 it gives the same ordering as -20,0,50,1000,1000000 - and there is no obvious way to chose between them prior to observing the data.",2011-01-16T13:54:52.607,2392,CC BY-SA 2.5,
9762,6295,1,"The problem with leaving it as categorical is that you'll get a test with 4df that will have less power. In epidemiology it's common to perform a 'test for trend' by treating an ordinal covariate as continuous, but then to report the estimates and CIs for the model with it categorical.",2011-01-16T14:07:01.137,449,CC BY-SA 2.5,
9763,6298,2,I suspect they're hoping to keep that commercially confidential!,2011-01-16T14:11:03.977,449,CC BY-SA 2.5,
9764,6289,0,I don't think it takes 10 weeks of exceeding the mean+1.645 sd threshold to make an epidemic call ... more like 2 or 3.,2011-01-16T14:46:42.613,2129,CC BY-SA 2.5,
9765,6288,0,I suppose it's possible that it means something but more likely it means something went wrong. Multi-colinearity will not necessarily show up in the cross-tabulations. You need to run some diagnostics. There should be documentation in your software about how to get a condition number for the XX' matrix or perhaps it will tell you how to get a report on variance inflation factors.,2011-01-16T14:51:02.980,2129,CC BY-SA 2.5,
9766,6192,0,"@onestop: obviously it is more informative to report the two quartiles, just I would not call that IQR! Just nomenclature nitpicking... :D",2011-01-16T14:57:17.147,582,CC BY-SA 2.5,
9767,6288,0,"Addendum: If you are using the SPSS GUI, then check the ""Collinearity diagnostics"" at the time of model specification.",2011-01-16T15:12:48.327,2129,CC BY-SA 2.5,
9768,6225,1,"This is somewhat of an ill-posed question.  We need to know the conditions under which this ""proof"" is to occur.",2011-01-16T15:39:26.503,2392,CC BY-SA 2.5,
9769,6225,2,"Perhaps a better posed question is ""Under what conditions/assumptions is it possible to prove the null hypothesis?""",2011-01-16T15:40:35.193,2392,CC BY-SA 2.5,
9770,5564,2,"Don't forget the differences in how BUGS and JAGS deal with truncation, censoring and prior ordering of those distributions (section 8 of the manual).  In particular, JAGS has a `dinterval` distribution, where BUGS works with I().",2011-01-16T17:01:34.470,1739,CC BY-SA 2.5,
9771,6289,0,"Wow, great research/answer.  Maybe it's the word ""Epidemic"" that threw me off.  To me, the term ""Epidemic Threshold"" is when certain conditions become unstable and the infection progresses at an increasing rate (due to the mechanics of the infection).  Other factors, such as a colder or wetter season could drive the count above the 95% level without it going unstable.  So, before today I would have expected the Epidemic Threshold to rise and fall due to ""other factors"".  But, maybe simpler is better.   I'll play with the flubase package.",2011-01-16T18:19:39.090,2775,CC BY-SA 2.5,
9772,6215,0,"@whuber: I add an example to my answer. About the difficult questions, I suppose that they arise even if you answer refers to the normal distribution (as other answers to this question do), or some other concept. Could be simpler to explain a 50%-trimmed mean? If it is so, then say that the IQR is the length of the range of its observations",2011-01-16T19:00:51.643,1219,CC BY-SA 2.5,
9773,6302,0,Why do you need to report CDF (empirical or estimated) when you are happy with the PDF?,2011-01-16T21:15:40.223,1307,CC BY-SA 2.5,
9774,6302,0,"@Suncoolsu, statement of work requires both",2011-01-16T21:26:02.503,559,CC BY-SA 2.5,
9775,6302,0,Also I think it is a good question regarding if people should use empirical or fitted cdfs.,2011-01-16T21:26:42.283,559,CC BY-SA 2.5,
9776,6280,0,"The coverage properties with a uniform prior and the Jeffrey's prior are indeed very similar when expressed over all possible population parameter values. However, the uniform is superior when the coverage is expressed as conditional on the observed proportion. In that case it is exact. Not 'exact' in the Clopper-Pearson sense, by exactly exact. The Jeffries prior is not quite as good. It is important to note that intervals from the method do not have to be interpreted in the Bayesian manner, as it is entirely derivable as a fiducial interval.",2011-01-16T22:05:18.500,1679,CC BY-SA 2.5,
9777,6292,0,"Thanks for the thoughtful input. In terms of the words ""mean intensity,"" you are correct that I mean the mean of the light across the area of a spot (not of multiple subjects). In terms of the censoring of the data, I think I didn't explain it clearly.  My variable of interest isn't actually the intensity.  It is a different variable called contrast.  But when the intensity is too low, I am unable to reliably measure the contrast( I know this from previous experiments on objects with known properties).  Thus the values of the contrast, when the intensity is too low, are censored.",2011-01-16T23:09:12.063,2788,CC BY-SA 2.5,
9778,6295,0,"It happens that the ordinal variable is a series of equidistant samples on a continuous dimension, so it is ok to treat it as a continuous variable. I didn't include that information because I didn't want to bias responses to thinking only of continuous variables, but this should work. Thanks!",2011-01-16T23:44:55.020,2800,CC BY-SA 2.5,
9779,6303,0,Can I use a Q-Q plot instead? I am currently using Matlab and they have a built-in function for Q-Q plots (qqplot) but not R-R plots.,2011-01-17T03:09:29.573,559,CC BY-SA 2.5,
9780,6303,2,"@Elpezmuerto. Yes pp plot and qqplot try to do the same job. I think QQ plot is as good as PP plot (with subtle differences). But as the great Cleveland, W.S. says, QQ plot is one of the best ways to compare distributions.",2011-01-17T05:00:50.733,1307,CC BY-SA 2.5,
9782,6302,0,"I would go for consistency, if the pdf is fitted, then cdf should be fitted, if pdf is empirical, so is the cdf.",2011-01-17T07:09:01.953,2116,CC BY-SA 2.5,
9783,6304,0,@suncoolsu : it does say 'nearly'...,2011-01-17T07:16:05.660,795,CC BY-SA 2.5,
9784,6305,1,"Hotelling's T^2:  (f ‚àí d + 1)/fd T^2 ‚àº F (d , f + 1 ‚àí d )",2011-01-17T08:04:40.140,2129,CC BY-SA 2.5,
9785,6305,1,"@DWin, not so sure that Hotelling's $T^2$ is really applicable here. At least from formulas from wikipedia page it is not immediately clear that $T$ in OP question can be represented as $T^2$. Can you please elaborate on this more?",2011-01-17T08:25:08.437,2116,CC BY-SA 2.5,
9786,6305,1,"will search for a convolution of $F(1,n)+F(1,n)$, afraid of some hypergeometric things, but have to be known somewhere.",2011-01-17T09:41:14.213,2645,CC BY-SA 2.5,
9787,6307,2,"+1 for the link, did not know that characteristic function of F distribution was so complicated.",2011-01-17T11:03:40.177,2116,CC BY-SA 2.5,
9788,6124,0,"Thanks everyone! I realized I should write a script, I did it in Matlab.",2011-01-17T11:16:36.447,2719,CC BY-SA 2.5,
9789,6303,0,"Yes, a Q-Q plot would be fine too. Late last night I had some half-formed idea that a P-P plot would be a bit better in this case, but I've changed my mind in the cold light of morning so edited my answer to reflect this.",2011-01-17T11:18:13.927,449,CC BY-SA 2.5,
9790,6298,0,"This may be true, yet the video (from summer 2010) suggest that they had been still experimenting by that time; so I posted this Q hoping that some leaks appeared since then.",2011-01-17T11:29:43.167,,CC BY-SA 2.5,user88
9791,6308,3,"For the fair and unfair coins this is a helpful read:
http://www.stat.columbia.edu/~gelman/research/published/diceRev2.pdf",2011-01-17T12:01:32.953,2116,CC BY-SA 2.5,
9792,6293,0,"you can use LaTeX styled math symbols (http://en.wikibooks.org/wiki/LaTeX/Mathematics) in your text between dollar signs `$` inline, or in new line aligned to the center between double dollar signs `$$`. Details: http://meta.math.stackexchange.com/questions/107/faq-for-math-stackexchange/117#117",2011-01-17T13:09:02.530,2714,CC BY-SA 2.5,
9793,6138,0,"@vqv Let me try and put it the other way: all the articles on ""how the huffman codes and 20 questions are essentially the same"" are focused on finding out the best set of questions for the given set of objects. But the problem at hand deals with a predefined set of questions and predefined set of answers. I don't see how to bridge the gap between that and Huffman codes. Do you?",2011-01-17T13:51:18.257,2696,CC BY-SA 2.5,
9794,6310,0,"""Now maximise this quantity with respect to p"": i think you mean minimise.",2011-01-17T13:52:07.243,495,CC BY-SA 2.5,
9798,6311,0,"Thanks. Ok, I read in very beginning of the paper that they used Chi-square test for all categorical variables. The classification table written doesn't refer to a variable in particular, it is the output of a classification task. It isn't very clear! Now I suppose they did a classical test on proportion.. maybe Chi-square..",2011-01-17T14:53:35.530,2719,CC BY-SA 2.5,
9799,6304,0,my apologies. didn't see that.,2011-01-17T15:18:19.507,1307,CC BY-SA 2.5,
9800,6306,1,"this is an interesting subject. I am curious why you (McKay et al 1999)  would choose 'War and Peace' as a control rather than, for example, random strings of letters (perhaps weighted by their observed frequencies). In other words, is it sufficient for the text to be sufficiently long, or does it have to be sufficiently long and comprehensible (or sufficiently long and of some literary value)?",2011-01-17T15:33:42.560,1381,CC BY-SA 2.5,
9801,6306,3,"David, the Choice of ""War and Peace"" as a control text (More precisely the beginning of the Hebrew translation of ""War and Peace"" of the same length as the Book of Genesis) was done by the original researchers. The story according to Aumann is this: When Bob Aumann who carefully followed the experiment told Kenneth Arrow about the marvelous findings in ""Genesis"", Arrow asked what about ""War and Peace"". Aumann then started reporting about the war and peace situation in Israel but it turned out that what Arrow asked about was if the same phenomenon cannot be found in ""War and Peace"".",2011-01-17T16:57:16.910,1148,CC BY-SA 2.5,
9802,6302,0,"Consistency is nice, however if you using the empirical cdf, you can provide support (or argue against) your choice of distribution in the pdf",2011-01-17T17:14:32.247,559,CC BY-SA 2.5,
9803,6318,0,Could be a more focused question?  Perhaps one about likelihood oriented approaches to a specific inference problem?,2011-01-17T17:17:08.907,1739,CC BY-SA 2.5,
9804,6318,2,"But while we're here: On exposition: have you mixed up measures of effect size, usually identified with a parameter, for measures of comparative evidence for a complete model? LRs only look like candidates for the latter. Also, if you want Likelihood functions alone or in combination to tell you everything the data are trying to telling you about a model, then you're basically a Bayesian. Because that's the Likelihood Principle. (Come on in, the water's lovely :-)",2011-01-17T17:24:25.657,1739,CC BY-SA 2.5,
9805,6318,0,Your title and your concluding para seem to disagree on whether you're suggesting using confidence intervals or replacing them.,2011-01-17T17:26:14.253,449,CC BY-SA 2.5,
9806,6318,0,"@onestop: indeed, I just realized I forgot to change the title; I changed my mind regarding confidence intervals while writing the question. I have edited the title now. Apologies for the confusion.",2011-01-17T17:43:24.123,364,CC BY-SA 2.5,
9807,6305,0,"I believe it reduces to your situation when  the variance matrix is diagonal. The off-diagonal elements from a sample should be near zero if the samples were from Normal, but might not be exactly zero if from t. Nonetheless, you asked for something approximate, so I think the answer is probably F under that proviso.",2011-01-17T17:55:45.570,2129,CC BY-SA 2.5,
9811,6320,3,"the answer will depend on what kind of distribution is estimated. Is your goal only to estimate the distribution, or you intend to estimate something else using this distribution? What about the structure of this $n$-dimensional vector? Is it also a sample, is it time-series, or something else? Does the distribution change with time?",2011-01-17T18:55:15.457,2116,CC BY-SA 2.5,
9812,6320,0,"Just *how* large is 'very large'? Are you hoping to estimate a 100-dimensional multivariate distribution with no constraints on its form? You might indeed need a *very* large number of observations, maybe of the order of 10^100.",2011-01-17T19:06:28.510,449,CC BY-SA 2.5,
9814,6321,1,Use the Fisher Transformation or a t-test based on a scaled tangent tranformation: http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient#Determining_significance,2011-01-17T19:11:10.157,919,CC BY-SA 2.5,
9815,6325,0,"I was afraid of that, but I thought that summing would bring in the tails somewhat.",2011-01-17T20:53:01.823,795,CC BY-SA 2.5,
9816,6305,1,"@DWin: it sure does look like a Hotelling with diagonal covariance matrix, but I am somewhat confused: from first principles, it does not seem like the sum of $F(1,n)$ RVs would be distributed like an $F$...",2011-01-17T20:55:53.430,795,CC BY-SA 2.5,
9817,6325,0,"I also thought to produce some sort of Monte Carlo experiments, trying to see for what $n$ and $k$ the approximation could be close enough to $\chi^2(k)$, probably $k(n)$ that we need here. But for small $k$ and especially $n$ it will be very heavy tailed indeed. May be you could add here these two histograms, just for lazy people like me?",2011-01-17T21:22:00.537,2645,CC BY-SA 2.5,
9818,6322,11,For anyone else interested: this can be done in R with `cor.test()`,2011-01-17T21:58:50.760,900,CC BY-SA 2.5,
9819,6325,0,"@Dmitrij The simulations are fast (it takes more time to draw the histograms), so I added 12 of them.",2011-01-17T22:00:19.443,919,CC BY-SA 2.5,
9820,6310,0,@mpiktas (+1) Nice (updated) answer.,2011-01-17T22:01:35.840,930,CC BY-SA 2.5,
9821,6318,0,@Conjugate Prior: Completely agree with your first two sentences. But you *can* accept the likelihood principle without being a Bayesian if you don't like the idea of priors and base inference on likelihoods alone - see books by Edwards http://books.google.com/books?id=2a_XZ-gvct4C and Royall http://books.google.com/books?id=oysWLTFaI_gC. Although someone (and i wish i remember who and where) once likened this to breaking eggs but not eating the omelette.,2011-01-17T22:30:07.057,449,CC BY-SA 2.5,
9822,6320,0,"As mentioned in the post: the sequence is iid, hence stationary. The goal is to estimate the PDF, not something else. I am open to learn about any know results for specific classes, e.g., PDFs of given smoothness, with finite support, with finite n-th moment, etc. I suspect that the literature is not very large.",2011-01-17T23:26:13.440,30,CC BY-SA 2.5,
9823,6267,0,"The higher the inter cluster distance the better, you can measure it by calculating the distances between the center of the clusters.",2011-01-17T23:47:22.953,1808,CC BY-SA 2.5,
9824,6320,0,"it is still not clear for me, are the elements of the $n$-dimensional vector independent and distributed identicaly?",2011-01-18T07:49:44.717,2116,CC BY-SA 2.5,
9825,6293,0,"Thanks you @daroczig for the math stuff, looks much more readable now!",2011-01-18T08:25:48.353,2392,CC BY-SA 2.5,
9826,6318,0,"@onestop The quote is from Savage, I think, to the effect that ""to make the Bayesian omelet, you have to break the Bayesian eggs"".",2011-01-18T10:17:02.780,1739,CC BY-SA 2.5,
9827,6084,0,"Thank you for taking your time to help me. And I'm sorry for the delayed response. I've separated the covariates in 2 blocks in the analysis. The first block contains the 5 covariates I want to control for, the second block contains only the 'risk' covariate. In Block 2 I get a 'Change from previous block'-statistic: Chi-square=6,298, df=1, sig=0,012. Doesn't this mean that the second changed the model significantly? Is this what you mean by Likelihood Ratio Test?",2011-01-18T10:35:30.290,2652,CC BY-SA 2.5,
9828,6318,0,"@Conjugate Prior: You're right, the original metaphor is due to Savage in 1961. My memory had completely scrambled Morris's rejoinder to his 1983 paper on Empirical Bayes, at the end of which he lays out a 2√ó2 table for breaking eggs and enjoying omelettes: http://www.jstor.org/stable/2287105. It's Empirical Bayesians that break the egg without enjoying the omelette though. Likelihoodists aren't mentioned, but I guess they'd belong with frequentists as neither breaking eggs nor enjoying omelettes. Perhaps they're the vegans of statistics?? Time i got out of the kitchen..",2011-01-18T10:52:11.753,449,CC BY-SA 2.5,
9829,6318,0,"I think the problem with p-values and confidence intervals is that they are too easy to interpret as something that they are not.  Likelihood ratios on the other hand have a name which corresponds precisely to what they are.  My issue with the usual null hypothesis testing is that often the alternatives are not well specified, and consideration is normally given only to ""type 1 errors"" or false positive.  A p-value also suffers from the ""double use"" of the data (discussion by James berger here http://www.isds.duke.edu/~berger/papers/interplay.html).",2011-01-18T11:17:03.337,2392,CC BY-SA 2.5,
9830,6335,0,"I think the OP is after some kind of *treatment interference* (non-interference is defined as ""The effects of actions of each drug in the combination should be as good as or superior to its actions when used alone"", e.g. FDA guidelines http://bit.ly/dIxYQH).",2011-01-18T11:38:25.217,930,CC BY-SA 2.5,
9831,6310,0,"I think this example shows you exactly what a confidence interval is.  I find it easiest to interpret a CI as ONE observation from a Bernouli distributed random variable with a probability parameter equal to the level of confidence.  It only makes sense to me to use CIs if you are doing the experiment repetitively.  Another issue is that what is the alternative hypothesis? is it p=7/10, p>0.5, p=1050/2000? p=527/1000?  Another issue is what do we mean by p=$\frac{1}{2}$? is it EXACTLY $\frac{1}{2}$ or is it $p \in \left(\frac{1}{2} \pm \epsilon \right)$ where $\epsilon$ is a small number.",2011-01-18T11:42:52.410,2392,CC BY-SA 2.5,
9832,6312,0,"How can you *estimate* the criterion? One physician's criteria for diagnosing e.g. chronic fatigue syndrome may well differ from another's, but I can't see it's the sort of thing you can *estimate*.",2011-01-18T12:22:07.693,449,CC BY-SA 2.5,
9833,6295,0,"I would respond to the 4df having less power as a direct consequence of making the weaker assumption of ordinal categories.  Without any additional information on how far each category is from the other, one is effectively replacing degrees of freedom with assumptions (which are somewhat arbitrary without more info), and thus leaving the results open to model misspecification (i.e. choosing the wrong set of numbers to label the categories).",2011-01-18T12:57:57.197,2392,CC BY-SA 2.5,
9834,6333,5,"Here's a question: what is the effect if you take logs and they're not called for? I've liked it when working with time series that require a log transform, because (as I understand it) the coefficients are ratios and at small values nearly percentages. (E.g. exp (0.05) = 1.051.)",2011-01-18T13:20:33.297,1764,CC BY-SA 2.5,
9835,6312,0,"Nothing to do with the question, just some na√Øve questions on the text :) :  How would you define ""separation between the distributions""  and what is the added value of the variance of the distribution ? what if the distributions are not gaussian ? have you heard about the total variation distance ?",2011-01-18T13:22:53.507,223,CC BY-SA 2.5,
9836,6338,0,"Thanks for the answer and the links. I can't up-vote it yet :) I've tried the t-test on some of the data. The problem is that in some cases Wilcoxon and t-test give the same/similar results, but in other cases (as above), Wilcoxon gives a p-value ~ 0.05 and t-test gives a p-value ~ 0.5. Some of my data contain (a few) outliers.",2011-01-18T13:51:44.373,2824,CC BY-SA 2.5,
9837,6338,0,@SabreWolfy But you can accept it by clicking on the check-mark icon.,2011-01-18T13:58:54.257,,CC BY-SA 2.5,user88
9838,6338,0,"@Henrik I beg to differ: I do not ""always advocate the t-test.""  In *one particular situation,* because the sample sizes were sufficiently large, the data did not appear to be extremely skewed, the data values were restricted to a finite set, and the t-test was likely more conservative than a permutation test (like the Wilcoxon), I argued that the t-test would perform well.",2011-01-18T14:02:05.143,919,CC BY-SA 2.5,
9839,6338,1,"@Henrik Your response seems to assume, implicitly, that the Wilcoxon test is *not* a permutation test.  However, *both* Wilcoxon tests (the signed rank and rank sum tests) *are* permutation tests.  What other permutation tests are you specifically recommending here?",2011-01-18T14:03:52.440,919,CC BY-SA 2.5,
9840,6338,1,"@SabreWolfy Outliers are always critical to parametric tests (e.g., the t.test). One way of dealing with them, besides the permutation test I highly recommend, is Winsorizing (see e.g.,: http://www.unt.edu/rss/class/mike/5700/articles/robustAmerPsyc.pdf). However, with clinical data, outliers can be meaningful. This depends on what the outcome variable is (e.g., a single dead patient is pretty meaningful). No general solution to this problem.",2011-01-18T14:07:34.040,442,CC BY-SA 2.5,
9841,6338,0,"@whuber, you are absolutely correct in both cases. Sorry for misunderstanding you. No offense intended. Further, I refer to those class of permutation tests that do not take the rank but retain the original values and build an own test distribution (e.g., use `oneway_test`in `coin`). It is absolutely careless to oversee that Wilcoxon is a permutation test as well.",2011-01-18T14:12:20.243,442,CC BY-SA 2.5,
9842,6338,0,@Henrik: So the outliers are the reason for the difference in results between the Wilcoxon and the T-test?,2011-01-18T14:13:08.193,2824,CC BY-SA 2.5,
9843,6338,0,"@sabreWolfy Yes, see my updated answer.",2011-01-18T14:20:38.887,442,CC BY-SA 2.5,
9844,6338,0,@Henrik I neglected to mention that your answer is spot-on and contains good advice.  Naturally I upvoted it right away ;-).  The added outlier example makes an excellent point.,2011-01-18T14:26:53.823,919,CC BY-SA 2.5,
9845,6338,0,@Henrik: Thanks for the update re outliers; missed that in the help for t.test.,2011-01-18T14:40:02.320,2824,CC BY-SA 2.5,
9846,6339,0,Thanks for pointing these out; I hadn't specifically considered these.,2011-01-18T14:46:56.433,2824,CC BY-SA 2.5,
9847,6336,0,"I used this Matlab function to generate the empirical cdf:
http://www.mathworks.com/matlabcentral/fileexchange/4426-cdfplot",2011-01-18T14:54:49.263,559,CC BY-SA 2.5,
9848,6339,0,(+1) I was about to post a response with [Simpson's paradox](http://en.wikipedia.org/wiki/Simpson's_paradox).,2011-01-18T15:02:23.487,930,CC BY-SA 2.5,
9849,6339,0,"Just when I think that I've ""completed"" this analysis, I realize there is more to consider... :)",2011-01-18T15:06:24.577,2824,CC BY-SA 2.5,
9850,6342,0,This question appears to be a continuation of http://stats.stackexchange.com/q/6337/919 .,2011-01-18T15:08:22.710,919,CC BY-SA 2.5,
9851,6340,2,"(+1) *Technical note*: gamma, beta, and hypergeometric functions are analytic almost everywhere in the complex plane.  They're actually fast and easy to calculate to high accuracy.  *All* functions ""require numeric evaluation"" at some point!",2011-01-18T15:10:25.793,919,CC BY-SA 2.5,
9852,6342,0,I thought it best not to mix the questions together as they cover different topics. Here I am asking how to bin the data; the previous question was asking about comparisons between subsets of the data.,2011-01-18T15:11:23.260,2824,CC BY-SA 2.5,
9853,6336,0,"Great advice, especially about overlaying the dots on the line plots.",2011-01-18T15:19:46.327,1080,CC BY-SA 2.5,
9854,6344,1,(+1) The `cut2()` function from [Hmisc](http://cran.r-project.org/web/packages/Hmisc/index.html) is a very handy replacement to base `cut()`.,2011-01-18T15:33:27.190,930,CC BY-SA 2.5,
9856,6336,0,"Maybe its got something to do with the graphics on my PC or this web page, but the plots in the link do look like stair-case functions, but the plots in the question look smooth.  Maybe you have a lot of data and the ""steps"" are so close together you can't see them?",2011-01-18T15:35:20.633,2392,CC BY-SA 2.5,
9857,6084,1,"It's a bit hazy on this side of the crystal ball interface, since you are using some sort of software that has it's own particular terminology. It sounds as though there was a change in the 2*log-likelihood of 6.298 when risk was added. Given the limits of what you have offered,  it appears the evidence in favor of ""risk"" (whatever it might be) being associated with shorter survival time is considerably stronger than was indicated by the Wald test.",2011-01-18T15:42:31.000,2129,CC BY-SA 2.5,
9858,6344,0,"@chl Thanks! Indeed, `cut2()` makes things easier, especially argument `g` and `minmax`.",2011-01-18T15:48:12.147,1909,CC BY-SA 2.5,
9859,6344,0,Thanks for the reply and the example. I'll work through it carefully now. Seems I was on the wrong track trying to use 'hist(age)$...' to split the A variable.,2011-01-18T16:29:31.547,2824,CC BY-SA 2.5,
9860,6331,0,"That was my original logic; however, as caracal pointing out (and which now makes sense to me), it doesn't necessarily indicate that there is a difference between XB and AB (my study may not have been powered sufficiently to detect that difference).",2011-01-18T16:35:03.710,2816,CC BY-SA 2.5,
9861,6335,0,Thanks caracal. Some of the statistical lingo is a bit over my head but your answer has definitely helped me interpret my results. +1,2011-01-18T16:36:24.987,2816,CC BY-SA 2.5,
9862,6331,0,I actually didn't do any post hoc testing - I just used an anova to compare the four data sets. The only group that was significantly different than the others was XB. Our hypothesis expected the effect of B to be negated in the presence of A. +1,2011-01-18T16:40:49.633,2816,CC BY-SA 2.5,
9863,6336,0,"you would be correct, my posted figure is also from pgfplots to make it look pretty for the report",2011-01-18T16:56:47.037,559,CC BY-SA 2.5,
9864,6325,0,+1 for the figure. Illustrations are always nice to see.,2011-01-18T17:16:58.370,2645,CC BY-SA 2.5,
9865,6347,3,"What exactly do you mean by denoise? Do you want to remove some data which satisfies certain criteria, or do you want to estimate the signal from your data?",2011-01-18T17:18:37.373,2116,CC BY-SA 2.5,
9868,6351,7,"I think there is important points to add: in an ANOVA, the normality within each group (not overall) is equivalent to the normality of the residuals.",2011-01-18T20:00:06.630,279,CC BY-SA 2.5,
9869,6353,0,"I'm sure you *can*, but I'm afraid I've no idea *how*!",2011-01-18T20:41:33.160,449,CC BY-SA 2.5,
9870,6320,0,Surely you know SOMETHING about the nature of these 100+ variable observations.,2011-01-18T20:42:13.783,1795,CC BY-SA 2.5,
9871,6352,2,"The assumption is that these $SS$ are $\chi^2$-distributed *under the null hypothesis*, which is that the group means are all equal, i.e. $M_j=M$ for all $j$. When this is the case, $y_{ij}-M_j$ is normal implies $M_j-M$ is normal. So you only need to check the first, i.e. that the observation-level residuals are normal.",2011-01-18T20:51:57.183,449,CC BY-SA 2.5,
9873,6350,3,"You can pretty much ignore anything else those sources that say if they claim the raw data needs to be normally distributed. And who said ""we"" were only checking the raw values with histograms, anyway. Are you in one of those Six Sigma classes???",2011-01-18T20:58:01.260,2129,CC BY-SA 2.5,
9874,6350,1,@Andy W: I've just added a link to what appears to be the relevant section of the Wikipedia article on ANOVA.,2011-01-18T20:58:07.550,449,CC BY-SA 2.5,
9875,6350,0,"@DWin: http://blog.markanthonylawson.com/?p=296 (sorry, *completely* off-topic but couldn't resist)",2011-01-18T21:02:14.390,449,CC BY-SA 2.5,
9876,6340,0,"@whuber - apologies on my abuse of the word *analytic* here.  I meant that these functions usually require a numerical evaluation of an integral or approximation to a sum, which is what it seems like @highbandwidth is trying to avoid.",2011-01-18T21:20:20.203,2392,CC BY-SA 2.5,
9878,6340,1,"Then you should include exp, log, and the trig functions in your list!",2011-01-18T21:37:10.590,919,CC BY-SA 2.5,
9879,6351,3,"@Aniko Could you please elaborate on what you mean by ""equivalent"" in your comment?  It is almost tautological that normality within a group is the same as normality of that group's residuals, but it is false that normality separately within each group implies (or is implied by) normality of the residuals.",2011-01-18T21:39:33.613,919,CC BY-SA 2.5,
9880,6351,10,"I really meant the tautological sense: if the groups are normal then the residuals are normal. The reverse is only true if homoscedascity is added (as in ANOVA). I don't mean to advocate for checking the groups instead of the residuals, but I think this is the underlying reason for the varying phrasing of the assumptions.",2011-01-18T22:14:47.867,279,CC BY-SA 2.5,
9881,732,0,Especially if you are in the financial services sector.,2011-01-18T22:21:46.553,2129,CC BY-SA 2.5,
9882,6353,0,I would suggest typing something like *regression* into the help documentation that comes with SPSS.  Should be bread and butter stuff for any stats package,2011-01-18T22:29:12.657,2392,CC BY-SA 2.5,
9884,6361,0,"That was a bit ""mathsy"" for me, and initially I didn't understand the formula, but I read it carefully about three times and it clicked! This is *exactly* what I was looking for, and your explanation was very clear, even for someone who isn't a mathematician or statistician at all. Thank you very much!",2011-01-19T00:11:22.943,2832,CC BY-SA 2.5,
9885,6364,0,"I think maybe you wanted $S = PP^\top$, where $S$ is the covariance, and $P$ is the Cholesky factor.",2011-01-19T01:39:06.543,795,CC BY-SA 2.5,
9886,6364,0,"Actually I want $S$ (the square root, or in this case, the Cholesky factor) to be positive definite. I have clarified the question though; thanks!",2011-01-19T01:50:27.240,2833,CC BY-SA 2.5,
9887,6365,0,"I am using `cholupdate` but my question is about making `R` (in this case) positive definite. I have a case where my `R` is non-pd, and `cholupdate(R,X,'-')` (a downdate) fails.",2011-01-19T01:51:34.647,2833,CC BY-SA 2.5,
9888,6366,0,"After a bit of maths, the class of distributions above all give $p(|\rho|<1)=\frac{1}{2}$ so that can't be used as a criterion for making the solution unique.",2011-01-19T03:03:21.730,2392,CC BY-SA 2.5,
9889,6340,0,"tuchee @whuber, perhaps it is my ""comfortability"" with exp, log, and trig functions, compared to my somewhat ""phobia"" of gamma and beta functions (for non-integer arguments or their ""incomplete"" version), and the hypergeometric function, bessel functions.  My pure maths skills stop at about this kind of area.",2011-01-19T03:33:07.570,2392,CC BY-SA 2.5,
9890,1336,0,"Isn't this an ""economist"" joke? Saying that the economist Jargon of ""the long run"" we never actually get to ""the long run""",2011-01-19T03:39:56.287,2392,CC BY-SA 2.5,
9891,6350,0,"@onestop thank you. I only requested the link because I am lazy and did not want to look up ANOVA on wikipedia myself, not because it is essential for the question.",2011-01-19T03:44:39.213,1036,CC BY-SA 2.5,
9892,968,0,This is brilliant!,2011-01-19T03:51:22.827,2392,CC BY-SA 2.5,
9893,6360,2,+1 for pointing out (in the last paragraph) the assumption of homoscedasticity.,2011-01-19T03:55:20.120,919,CC BY-SA 2.5,
9895,6366,0,Clearly $\Pr(|\rho| \lt 1) = 1/2$ by symmetry: no calculations are needed.,2011-01-19T04:19:09.550,919,CC BY-SA 2.5,
9896,6369,0,"I could also introduce a location parameter $\mu$ So that $Z_{t}$ effectively gets replaced with $\frac{X_{t}-\mu}{\sigma}$, but then the prior would just get multiplied by $\frac{1}{\sigma}$, so the example I have given doesn't make the task any easier",2011-01-19T05:16:56.250,2392,CC BY-SA 2.5,
9897,6365,1,"all online algorithms of this form (update & downdate) suffer from precision issues like this. I had similar issues in 1d resulting in negative estimates of variance. My suggestion would be to keep a circular buffer of the last k vectors observed, and when `cholupdate` fails, recompute the covariance based on that circular buffer and eat the cost. If you have the memory, and can stand the occasional time hit when this happens, you will not find a better method in terms of accuracy and ease of implementation.",2011-01-19T05:57:06.183,795,CC BY-SA 2.5,
9898,2365,0,"This is a very poor example of the ""inferiority"" of Bayesian methods, of exactly the same type Jaynes speaks of in his 1976 paper.  You need to write down what the *numerical/mathematical equation* that the ML (or other frequentist method) does, *and the corresponding Bayesian method and its numerical answer!* You have written down the model, but no solution to the estimation of anything to do with it!  The rest of your answer would be greatly improved if you wrote down what the frequentist answer using ML actually is.",2011-01-19T06:03:14.493,2392,CC BY-SA 2.5,
9899,6369,0,"@probabilityislogic Yes, you're right that leaving out these parameters does not simplify the problem.",2011-01-19T06:08:28.687,919,CC BY-SA 2.5,
9900,2369,0,"Confidence intervals, in my opinion are *completely and utterly useless* UNLESS the experiment is to be repeated a moderate number of times (10 or more).  Because whether or not an $\alpha$ level CI contains the true parameter is basically a $Bernouli(\alpha)$ random variable which has been ""mixed up"" so that we don't know whether we have observed a ""success"" or a ""failure"".  Also this problem it is impossible to give an ""exact"" CI, because $1^{12}$ times its 0.5 and 1 time its 0.1.  Show me 95% of this set? it doesn't exist!  Wouldn't you just give the set of two numbers {0.5,0.1}?",2011-01-19T06:18:14.933,2392,CC BY-SA 2.5,
9902,6369,0,"I'm thinking from the lack of articles on it, perhaps no transformation group big enough exist? this group was the only one I could think of which left the problem unchanged to someone who was ""completely ignorant"".  I may update my question to reflect this",2011-01-19T08:25:40.500,2392,CC BY-SA 2.5,
9903,2365,0,"@probabilityislogic : I gave the references. This is a discussion site, not a scientific journal. Please read the comments and the reference I gave for more information. and before you call it a poor example.",2011-01-19T09:10:01.387,1124,CC BY-SA 2.5,
9904,6361,4,"Very nice non-technical answer, and an approach I wouldn't have thought of myself. I'd only add that it's possible to add any number of fake 'observations' to each category instead of 1, including non-integer numbers. This gives you flexibility to decide how much you want to 'shrink' towards zero the scores of items with few votes. And if you happen to want an technical-sounding description of this method, you could say you're performing a Bayesian analysis of data from a multinomial distribution using a symmetric Dirichlet prior.",2011-01-19T09:28:14.507,449,CC BY-SA 2.5,
9905,6145,0,"I have tried to get reach of that book but the library has a copy that's not available for the moment. But I can perhaps ask which model that has the most accurate standard errors when HC-robust, LSDV or within?",2011-01-19T09:32:25.873,2724,CC BY-SA 2.5,
9906,6352,0,"@onestop Edited to reflect your clarification, thanks!",2011-01-19T09:34:15.013,1909,CC BY-SA 2.5,
9907,6375,0,I guess @Shane will have something to say here while talking about webvis (parser for protovis) since protovis (dedicated to web visualisation) (http://vis.stanford.edu/protovis/) contains the possibility of interactive graphics...,2011-01-19T10:15:07.973,223,CC BY-SA 2.5,
9908,6071,1,"@Andrew Redd: have never searched for such models but Rseek also finds `gbev` package (or `MeasurementError.cor` for a simple model) as one that may be interesting. Do you have some known methods or do you know how it is implemented in other programs like S-Plus, Stata, SPSS or whenever?",2011-01-19T10:16:58.843,2645,CC BY-SA 2.5,
9911,6363,1,"@ rolando - good answer. That being said, missing pairwise approaches tend to confuse the comparison of effects, as they are based on different numbers of observations. Might be something to keep in mind.",2011-01-19T11:32:46.570,656,CC BY-SA 2.5,
9913,6361,1,"While they may seem like ""fake"" observations, they do have a well defined meaning when it is +1 (as opposed to +2 or higher, which really are ""fake"" numbers, or numbers from a previous data collection).  It basically describes a state of knowledge that it is *possible* for each category to be voted for, *prior* to observing any data.  This is precisely what the flat prior on the (N-1) simplex does.",2011-01-19T13:25:23.337,2392,CC BY-SA 2.5,
9914,6353,0,I don't know what you mean by coding for one categorical variable. Can you give an example in syntax? Is your dependent variable continuous or categorical?,2011-01-19T13:35:16.863,1036,CC BY-SA 2.5,
9915,6380,0,"there's an error in the above program: the line should be ""for(i in 1:plength)""",2011-01-19T13:38:14.083,495,CC BY-SA 2.5,
9916,6363,0,"I think your slightly confused, pair-wise missing only matters if your running entirely separate models (such as using a step-wise model selection procedure). If your entering all variables into the model it still drops missing values list-wise.",2011-01-19T13:39:33.050,1036,CC BY-SA 2.5,
9917,6380,0,"you are rigth, thx!",2011-01-19T13:40:05.257,2838,CC BY-SA 2.5,
9918,6381,0,please help me to modify my script...,2011-01-19T13:47:36.670,2838,CC BY-SA 2.5,
9920,6383,0,"thank you very much. You are right, that with ""apply"" the script could be optimized. I just used my script as a minimal-example in order to get the message through... Thx a lot, your answer is exactly what I was looking for!!",2011-01-19T14:27:43.283,2838,CC BY-SA 2.5,
9921,6381,2,Those are just hiding the loop from you. The real problem with @Produnis code is that forced copying is going on because the results vectors are being extended at each iteration of the loop.,2011-01-19T14:43:04.383,1390,CC BY-SA 2.5,
9922,6379,0,"Do you want to test the null hypothesis that all the players have the same strength, or check the fit of a model of player strength?",2011-01-19T14:58:18.867,449,CC BY-SA 2.5,
9923,6385,0,"To be more specific. I have 8 players and only 18 games. So, there a lot of pair of players that did not play with each other and there a lot of pairs that played only one with each other. As a consequence, I cannot estimate the probability of win for a given pair of players. I also see, for example that there is a player that won 6 times in 6 games. But may be it is just a coincidence.",2011-01-19T15:47:57.473,2407,CC BY-SA 2.5,
9925,6381,0,"snowfall package can extend Gavin's solution like saying ""cake"". Package has a plethora of apply function modified to do multicoring. For apply function, you would use sfApply(<yourarguments as for apply>). Snowfall is also well documented. I should point out that no extra software is needed for performing this on a multi-core processor. See http://stackoverflow.com/questions/4164960/which-list-element-is-being-processed-when-using-snowfallsflapply for a sfLapply example.",2011-01-19T16:54:06.620,144,CC BY-SA 2.5,
9928,6388,0,"(+1) It's funny, I just realized that this Stata command might be used for transmission/disequilibrium test in genetics :) I discussed R packages in an earlier response, http://stats.stackexchange.com/questions/5171/testing-paired-frequencies-for-independence/5258#5258.",2011-01-19T18:21:32.800,930,CC BY-SA 2.5,
9929,6373,0,"In your first main paragraph you seem to have confused $\alpha$ and $1-\alpha$.  Where does the value of 10^12+1 come in?  What do you mean by ""beheaded""??  This text looks like it is need of proofreading and revision.",2011-01-19T18:30:56.600,919,CC BY-SA 2.5,
9930,6388,0,"Indeed, the TDT is one application discussed in the Stata help I linked above. It's also the context in which I first came across this test. Thanks for the link to that previous Q - looks like I was busy with other Qs when it was posted.",2011-01-19T18:38:27.460,449,CC BY-SA 2.5,
9931,6332,0,I've edited my question to include a function I wrote to do that.  Does my new function make sense?,2011-01-19T18:41:26.557,2817,CC BY-SA 2.5,
9932,6394,0,What would be the structural difference between the Weibull and the Gamma distribution to tackle this modeling?,2011-01-19T18:55:05.737,2808,CC BY-SA 2.5,
9933,6394,0,"Depends what you mean by 'structural'. The Weibull distribution is certainly more common in time-to-event analysis. It has a simpler cumulative distribution function, and it's the only distribution that has both the proportional hazards and accelerated failure time properties.",2011-01-19T19:16:32.907,449,CC BY-SA 2.5,
9934,6348,0,"thanks for the info.  With the GQ test, the lower the p value, the more likely the distribution is heteroskedastic?",2011-01-19T19:56:42.823,2817,CC BY-SA 2.5,
9935,6394,0,"From watching some of the plots of weibull and gamma, it seems that weibull has more mass as it gets to the peak of the pdf whereas the gamma has more mass as it departs from the peak of the pdf.  I was wondering if this structure has any explanation.",2011-01-19T20:03:02.367,2808,CC BY-SA 2.5,
9937,6361,0,"One more observation, for future people who find this post: In implementing this in my model I took the final score and multiplied it by 20, which gives a range of -100 to 100 from worst to best possible score (though I suppose technically those are limits you can't ever quite reach, but you get the idea). This makes the output for users in my app very intuitive!",2011-01-19T20:14:47.820,2832,CC BY-SA 2.5,
9938,6384,0,"FWIW, Matlab has the same issues regarding preallocation and expanding vectors, and is a classic code 'blooper'. In addition to your wager, it is probably faster to use the results of `rowSums` to compute the row means (unless I am missing something regarding _e.g._ Na or NaN). The code in your third approach sums each column *twice*.",2011-01-19T20:16:01.170,795,CC BY-SA 2.5,
9939,6367,0,"Thanks for your effort -- unfortunately, it did not work. (I was doing something very similar in my 3-line program: `[V,D] = eig(A); D(D <= 1e-10) = 1e-6; Apd = V*A*V';`). This approach is similar to the one by Rebonato and Jackel, and it seems to fail for pathological cases like mine.",2011-01-19T20:35:10.280,2833,CC BY-SA 2.5,
9940,6365,0,"Thanks, that's something to think about. Unfortunately, my covariance matrix goes through so many transformations that it is not clear at which point I have to a recomputation from the circular buffer. Nevertheless, if all else fails, I should be able to use the last known PD covariance matrix -- with the hope it doesn't produce a bias in my estimates.",2011-01-19T20:38:38.653,2833,CC BY-SA 2.5,
9941,6370,1,"Thanks, that's a technique I could potentially use. It seems like it's been implemented here: http://infohost.nmt.edu/~borchers/ldlt.html",2011-01-19T20:40:12.287,2833,CC BY-SA 2.5,
9944,6400,0,"Could you explain what are $Y$, $X$, and $Z$?",2011-01-19T20:51:14.510,930,CC BY-SA 2.5,
9945,6400,0,"Y is a vector of gene expression values. These are not gaussian, thus they are represented as a vector of ranks. X and Z are vectors of SNP data (genotypes at a particular location) for multiple samples. They are encoded as 0, 1, and 2 representing aa, ab and bb respectively.",2011-01-19T20:54:53.790,2842,CC BY-SA 2.5,
9946,6400,0,"Is that 'first order regression' in the sense of Karaliƒç and Bratko (1997)?? http://dx.doi.org/10.1023/A:1007365207130 
If not, could you explain what you do mean by 'multiple first order regression analysis'?",2011-01-19T20:55:45.727,449,CC BY-SA 2.5,
9947,6400,0,"No, it is first order in the sense that Y ~ X + Z + XZ + e, where X and Z are main effects, AB is the effect of the interaction and e is the error. Beta 1, 2 and 3 coefficients are omitted for clarity.",2011-01-19T20:59:56.547,2842,CC BY-SA 2.5,
9948,6367,0,"Thats too bad. I'd be interested in an example matrix you've found that causes this (and other methods you've tried) to fail if you have time to post one. 
This is such an aggravating problem to keep running into, I hope you find a solution.",2011-01-19T21:02:14.203,1913,CC BY-SA 2.5,
9949,6394,0,"One difference is that the Wiebull has a subexponential tail (one type of heavy tail) when the shape parameter is less than 1, whereas the Gamma distribution has an exponential tail.",2011-01-19T21:15:34.700,449,CC BY-SA 2.5,
9951,6384,0,"@shabbychef you'll be surprised (see my edited answer). Yes the sums are notionally computed twice, but `rowSums` and `rowMeans` are highly optimised compiled code and what we gain in only computing the sums once, we loose again in doing the mean computation in interpreted code.",2011-01-19T21:34:31.453,1390,CC BY-SA 2.5,
9953,6400,1,"1) ""The vectors of y-values do not all follow a normal distribution, therefore I need to implement a non-parametric regression"" is a non-sequitur. Linear regression is reasonably rubust to departures from normality with large samples, and as you're in genetics I suspect you have a *very* large sample. And non-parametric methods are not the only alternative; you could apply a transformation, or use generalized linear models.
2) You still haven't given us any idea information about $Z$, e.g. how many levels does it have?",2011-01-19T21:56:04.523,449,CC BY-SA 2.5,
9954,4547,0,"drknexus You get the issue I was driving at,. The issue is that the national housing stats often report national medial house prices,, but I'm pretty sure they at best get the median values for a state so I believe that the ""national"" median house price statistic is pretty bogus. Particularly when one realizes that the mix of demand by state probably fluctuates quite a bit as well due to local economic conditions and local foreclosure trends (which vary widely). So, Is there a semi valid way to do this is is it as bogus as I suspect??",2011-01-19T22:33:05.793,1963,CC BY-SA 2.5,
9955,6384,0,"@Gavin Simpson: not so fast: try instead `system.time({
 for (iii in c(1:1000)) {
 p1max3 <- apply(p1, 1, max)
 p1mean3 <- rowMeans(p1)
 p1sum3 <- rowSums(p1)
 }
})` and similarly `system.time({
 for (iii in c(1:1000)) {
  p1max4 <- apply(p1, 1, max)
    p1sum4 <- rowSums(p1)
    p1mean4 <- p1sum4 / ncol(p1)
 }
})`; the version which does not recompute the sum takes 1.368 seconds on my computer; the one which does takes 1.396. again, far from exhaustive, but more compelling...",2011-01-20T00:45:49.423,795,CC BY-SA 2.5,
9956,4547,1,"<shrug> It is 'valid' to whatever extent it represents the information you are interested in/the extent to which it represents what it purports to represent.  If the people doing these analyses have the raw data that allows them to find the median by state, I see no compelling reason they would opt to report the median of state medians rather than just the overall median, because (as you suggest) the overall median has a clear interpretation whereas the median of medians does not.",2011-01-20T00:57:53.813,196,CC BY-SA 2.5,
9957,4547,1,"P.S. You can spot check their technique if you can find the median by state.  If their reported median national house selling price matches the median of reported state medians - then they are doing what you fear.  If not, then you can probably assume that they are using the simple median of all houses sold.",2011-01-20T00:59:34.670,196,CC BY-SA 2.5,
9959,6380,1,doesn't this belong on StackOverflow?,2011-01-20T02:14:44.027,2544,CC BY-SA 2.5,
9960,6373,0,"$10^{12}$ is for the trillion fair coins, and 1 is for the unfair coin.  And I haven't confused $\alpha$ and $1-\alpha$ the Clopper Pearson interval listed [here][1]",2011-01-20T02:43:56.857,2392,CC BY-SA 2.5,
9961,6373,0,"[sorry typo] $10^{12}$ (TeX fixed) is for the trillion fair coins, and 1 is for the unfair coin, one over this is a rough approx. to the probability of having the ""bad"" coin.  Beheaded is the consequence of giving the wrong confidence interval.  And I haven't confused $\alpha$ and $1-\alpha$ the Clopper Pearson interval listed on the wiki page (search binomial proportion confidence interval).  What happens is one part of the C-P interval is a tautology $1 \geq \frac{\alpha}{2}$ when one 1 observation.  The side ""flips"" when X=1 to X=0, which is why there is $1-\theta$ and $\theta$.",2011-01-20T02:52:36.367,2392,CC BY-SA 2.5,
9962,6227,5,"More generally the problem in statistics is not that you are unable to prove the null hypothesis, it is that you are not able to make any point estimates with certainty.  That is, just as you can't say ""there is no effect of the variable"" you are unable to say that ""the effect size of the variable is 1.95"".  Statistics always have confidence intervals.",2011-01-20T03:41:55.183,196,CC BY-SA 2.5,
9963,2365,0,"@joris meys - I understand that you did give a reference, but your discussion does not talk about *how* the confidence interval solution is superior to the Bayesian credible interval.  This means that basically the confidence interval basically needs to be *uncalculable* using Bayesian methods.  By showing the Bayesian solution which gives the same interval, you can show what prior information was implicitly contained in the procedure to generate the confidence interval.",2011-01-20T07:17:52.980,2392,CC BY-SA 2.5,
9964,6348,0,"@Zach: exactly, take 5% for instance, of course if you are not planning to go for data mining. I personally start from the model assumptions.",2011-01-20T07:52:47.553,2645,CC BY-SA 2.5,
9965,6384,0,"@shabbychef we must have different ideas on what is or is not compelling ;-) In fact, your more rigorous simulations reinforce my main point, that as `rowMeans` and `rowSums` are implemented in efficient, optimised compiled code they are going to be difficult to beat.",2011-01-20T08:14:17.530,1390,CC BY-SA 2.5,
9966,5740,1,Calling R from your preferred language seams the best solution in your case,2011-01-20T08:56:29.167,1709,CC BY-SA 2.5,
9967,6402,0,"@probabilityislogic It's not clear to me whether your model treats all variables as categorical ones or not, and what is the advantage of modeling ranks using an adjacent or cumulative-logit model. Anyway, when dealing with SNP data, we often assume an additive model (aka, allelic dosage) and treat the frequency of minor allele {0,1,2} as a continuous predictor.",2011-01-20T09:13:45.087,930,CC BY-SA 2.5,
9968,6404,2,"Baseball is many US statisticians' favourite source of examples so a Google (/ Scholar) search will bring up several relevant articles, e.g. Morrison and Schmittlein (1981) http://www.jstor.org/stable/2630890 .
I'll leave it to someone more familiar with both baseball and R to answer your question.",2011-01-20T09:16:32.733,449,CC BY-SA 2.5,
9969,2365,0,"@probabilityislogic : the whole discussion revolves around Felstensteins claim that it is impossible to put a prior without making impossible assumptions about either time or mutation rate. Remember we're talking about phylogenetic trees. This concept makes for quite a different framework, as it's not a classical equational model in a space of real numbers. I'd suggest you read the chapter of his book to see his argument on how under certain condition the Bayesian approach can be proven to be wrong. I'd like to stress this is ONE example. It doesn't say anything about Bayesian in general.",2011-01-20T09:33:33.593,1124,CC BY-SA 2.5,
9970,2365,0,@probabilityislogic : To show the difference in nature of the problem : you talk about confidence intervals. Now try to define a confidence interval around a phylogenetic tree...,2011-01-20T09:34:39.367,1124,CC BY-SA 2.5,
9971,6390,0,"Thanks guys, sorry for not explaining myself more clearly.  I amended the question above...see above",2011-01-20T10:10:12.270,2405,CC BY-SA 2.5,
9972,6390,0,"By the way, I would up-vote the comments but my rep is too low.",2011-01-20T10:23:44.173,2405,CC BY-SA 2.5,
9973,6390,0,some R example code for your solution would echo far. :),2011-01-20T10:46:52.550,144,CC BY-SA 2.5,
9974,6407,0,Thank you very much for the answer. I wonder about the last one about adding no varying dummy variables. If I add only present members of the EURO-area those dummies will still vary since I use data from the years before they introduced the euro. But for countries that hasn't introduced the dummy will have a 0 for the the whole period.,2011-01-20T11:12:00.063,2724,CC BY-SA 2.5,
9975,6355,0,"see Beyer et al. under [when-is-nearest-neighbor-meaningful-today](http://stats.stackexchange.com/questions/6314/when-is-nearest-neighbor-meaningful-today): kNN is sometimes NOT meaningful.  What are your N, dim, k ?",2011-01-20T11:14:39.757,557,CC BY-SA 2.5,
9976,6407,0,"@Skolnick: but if you don't your results could be biased, you would like to know the impact of being in the club as compared to the countries not in the club (hence you need more out of the club countries to be confident enough). If you have a particular question if the entrance into club has had an impact, than introduce one more dummy for the countries that were not originally in euro-zone.",2011-01-20T12:15:15.500,2645,CC BY-SA 2.5,
9977,6404,1,"I would also suggest you check out the work of J.C. Bradbury and his blog, Sabernomics, http://www.sabernomics.com/sabernomics/ . His book on measuring player worth will likely be insightful as to what characteristics are predictive of future productivity.",2011-01-20T13:13:01.910,1036,CC BY-SA 2.5,
9978,6402,0,"The questioner's sample data has as many values for Y as there are observations. If this is true of his real data, which I guess it is as they are ranks (ignoring any ties), maximum likelihood estimation isn't going to work, as the number of $\theta$s to estimate increases with the numbers of observations.",2011-01-20T13:28:46.783,449,CC BY-SA 2.5,
9979,6348,0,@Dmitrij.  Thank you.  I just want to make sure I'm interpreting the output correctly.,2011-01-20T14:30:17.880,2817,CC BY-SA 2.5,
9980,6412,5,What is the purpose of your question? Why do you need such a list? Doesn't google search on statistical packages on linux answer your question?,2011-01-20T16:49:31.663,2116,CC BY-SA 2.5,
9981,6410,0,Can we assume that all machines are identical? Or you suspect that some of them misbehave and you want to detect them?,2011-01-20T17:15:24.900,2116,CC BY-SA 2.5,
9982,6384,0,"@Gavin Simpson. Actually, the problem with my example is that most of the time is taken in the apply part to compute the maximum. I agree with you that a c-based vectorized function like `rowMean` is going to be hard to beat via a general-purpose R tool like `*apply`. However, you seem to suggest that it is faster to sum 10000 numbers *twice* via `rowMean` and `rowSum` rather than only once and use R's builtin division operator. I know R has some efficiency issues (_e.g._ the recent discovery of the curly braces vs. parenthesis issue), but that seems crazy.",2011-01-20T17:53:47.843,795,CC BY-SA 2.5,
9983,6384,0,"@Gavin Simpson: as a followup, try the following: 
p1 <- matrix(rnorm(10000), ncol=100)

system.time({
 for (iii in c(1:20000)) {
    p1sum5 <- rowSums(p1)
    p1mean5 <- p1sum5 / ncol(p1)
 }
})

system.time({
 for (iii in c(1:20000)) {
    p1sum6 <- rowSums(p1)
    p1mean6 <- rowMeans(p1)
 }
})",2011-01-20T17:54:04.507,795,CC BY-SA 2.5,
9984,6390,0,"@Roman Lu≈°trik, I added some sample code in Matlab for equi-depth histograms (I'm not a regular user of R), hope this will suffice!",2011-01-20T18:07:40.860,1913,CC BY-SA 2.5,
9985,6418,1,Maybe some remarks about the desired use?,2011-01-20T21:59:11.277,,CC BY-SA 2.5,user88
9986,6361,0,"@probabilityislogic: surely any strictly positive parameters for the Dirichlet prior describe that all the probabilities are strictly between 0 and 1? And this argument suggests setting them to 2/m, where m is the number of categories, rather than 1: http://en.wikipedia.org/wiki/Rule_of_succession#Generalization_to_any_number_of_possibilities",2011-01-20T22:32:25.490,449,CC BY-SA 2.5,
9987,6418,0,possible duplicate of [Weight a rating system to favor items rated highly by more people over items rated highly by fewer people?](http://stats.stackexchange.com/questions/6358/weight-a-rating-system-to-favor-items-rated-highly-by-more-people-over-items-rate),2011-01-20T22:33:22.697,449,CC BY-SA 2.5,
9989,6373,0,Do you mean @Keith Winstein's answer?,2011-01-20T23:00:31.950,919,CC BY-SA 2.5,
9990,2365,0,"@Joris Meys - I do appreciate the reference to the book (but its seems as though without a link, I am to buy his book in order to read your reference), which is where all the arguments are.  The equation you presented for the model is simple enough (0<P<1, t>0, u>0 with a relation between each), in fact it could be expressed as the $P=Pr(Y<u)$ where Y has an exponential distribution with rate parameter $t$ (or mean $\frac{1}{t}$).  The CI needs to be around one of the parameters defining the model you describe.  The interpretation will depend on the context, as you correctly point out.",2011-01-20T23:17:00.363,2392,CC BY-SA 2.5,
9993,6421,5,See http://stats.stackexchange.com/q/1883/159 for a similar question (which was closed as subjective & argumentative).,2011-01-21T03:00:17.510,159,CC BY-SA 2.5,
9994,6421,1,I was about to bring up the same thread. Smells like a duplicate.,2011-01-21T03:03:42.410,334,CC BY-SA 2.5,
9995,2365,0,"Apologies (again), I wrote the fraction incorrectly (today is just not my day!).  So it should be that you can write $P=Pr(Y<\frac{4u}{3})$ where $Y \sim Expo(t)$ so that $E(Y)=\frac{1}{t}$.  If we do not observe $u$ or $t$ then the model is not identifiable (i.e. there are an infinite amount of $u$ and $t$ values which give the same $P$).",2011-01-21T03:08:29.570,2392,CC BY-SA 2.5,
9996,6412,0,"Purpose: to elicit actual experiences. Need: I think I may have large multivariate logistic regression in my near future. Google searches: no, it doesn't.  Have you done the searches?  Plenty of non-free, plenty of abandoned projects, etc.",2011-01-21T04:25:19.583,2849,CC BY-SA 2.5,
9997,6361,0,"@onestop - I would argue against the wiki page that you describe is only considering the case when 1 of 2 possible outcomes can happen.  So *if* we were indifferent between ""strongly like"" and everything else (the other three categories *combined*), then we are *not* indifferent between ""strongly like"" and ""like"".  But the question as posed clearly give *four* categories, not two, and from the information in the question there is no *a priori* reason to favour any one rating.  I'll have another think about it, because the issue may be more subtle (e.g. why not use the reference prior?).",2011-01-21T04:44:56.133,2392,CC BY-SA 2.5,
9998,6373,0,"@whuber, yes I do mean keith winstein's answer.",2011-01-21T05:39:46.070,2392,CC BY-SA 2.5,
9999,1491,3,gtk is fine on OSX.,2011-01-21T05:44:07.067,46,CC BY-SA 2.5,
10000,6361,0,"Okay, I've had a bit of a think and basically using the $Dir(k,\dots,k)$ prior to give posterior of $Dir(k+n_1,\dots,k+n_m)$.  The [wiki page][1] [1]:http://en.wikipedia.org/wiki/Dirichlet_distribution#Aggregation shows that collapsing categories, you just add the parameters. This means that collapsing to 2 categories gives you a Beta posterior. so the denominator will be the sum of all dirichlet parameters $n_1+\dots+n_m+mk = n+mk$ and the numerator will be the sum of all the categories labelled ""success"" which will be of the form $s+ck$ where c=number of categories for ""success"".",2011-01-21T06:34:55.743,2392,CC BY-SA 2.5,
10001,6361,0,"... continuing, this means the probability of success is given by $\frac{s+ck}{n+mk}$.  So setting $k=\frac{2}{m}$ gives $\frac{s+\frac{2c}{m}}{n+2}$.  Now if m is ""evenly divided"" as the wiki page says, then 2c=m and we are left with the original rule of succession.  So what does this mean for the uniform prior (k=1)?  To me, it means that *mere knowledge of the existence of more than two possible categories (that we consider them possible is essential to this argument) represents important information about an equal aggregation of them*.  more later",2011-01-21T06:47:35.653,2392,CC BY-SA 2.5,
10002,6421,1,"It's subjective, sure, but isn't it still okay for CW?",2011-01-21T06:53:21.470,1118,CC BY-SA 2.5,
10003,6361,0,"...continuing again... So what state of knowledge does the constant $\frac{2}{m}$ represent?  Here's what I think.  It represents that we are only know that one category in the ""success"" labeled group, and one category in the ""failure"" group are *possible*, the remaining $m-2$ categories *we are not even sure that these categories are possible and will only consider them possible after observing them*.  This easily generalizes to a new constant $\frac{q}{m}$ where there are $q$ categories *possible* and $m-q$ categories where we are not sure of even the possibility that they exist.",2011-01-21T06:55:11.717,2392,CC BY-SA 2.5,
10004,6412,0,"what is large? Do you have any special needs? The multivariate logistic regression has pretty standard algorithm for solving it (iterative reweighted least squares), so every software is simply reimplementing it. In this case you usually go with the most popular implementation. Or you can write the algorithm yourself, it is not so hard.",2011-01-21T07:03:22.067,2116,CC BY-SA 2.5,
10005,6361,0,"...continuing again (apologies for the lengthy ""comment"")... Note that changing from $2$ to $q$ changes the probability to $\frac{s+\frac{q}{m}}{n+q}$.  One thing I have wondered with this argument, is do we need to specify which categories within ""success"" are they ones we are willing to assume are *possible a priori*?  Or can we just say ""one of them, but not sure which"" and then spread this out ""evenly"" over the aggregated categories?  If we can, then this gives an interpretation of the Jeffreys prior as assuming that we consider *only one outcome possible, but not sure which one* or $q=1$.",2011-01-21T07:07:19.443,2392,CC BY-SA 2.5,
10006,6361,0,"Apologies, I made a small error in the above comment, the probability should be $\frac{s+\frac{cq}{m}}{n+q}$ instead of $\frac{s+\frac{q}{m}}{n+q}$.",2011-01-21T08:10:24.457,2392,CC BY-SA 2.5,
10007,6390,0,Effort is always a plus in my book.,2011-01-21T08:39:09.223,144,CC BY-SA 2.5,
10008,6416,0,"Just a slight tehcnical note: you say that ""taking averages won't help"", but this is precisely what the Bartlett test does, to a degree.  It calculates logarithm of the ratio of the arithmetic average variance to the geometric average variance, with both averages *weighted* by the group sizes minus 1.  The factor multiplying this logarithm is fixed by the ""design of the experiment"" as it only depends on the number of samples and the number of groups.  So one is effectively rejecting the hypothesis of equal variance if the arithmetic average is ""too much"" bigger than the geometric average.",2011-01-21T09:01:27.957,2392,CC BY-SA 2.5,
10009,6410,0,One important piece of information which is not included above is the number of bars you observed from each machine (i.e. the sample size for each group),2011-01-21T09:09:05.763,2392,CC BY-SA 2.5,
10010,6416,1,"@probabilityislogic, yes but in Bartlett test case this average is used to determine whether the variances are identical or not. If they are not identical there is no point in averaging them, since this average does not estimate any parameter.",2011-01-21T09:10:00.430,2116,CC BY-SA 2.5,
10011,6339,0,"You can *never* complete an analysis! you can only throw your hands in the air and say ""I'm sick of this problem, and I accept the errors that will come from not digging deeper!"" :)",2011-01-21T09:53:18.487,2392,CC BY-SA 2.5,
10012,2365,0,"@probabilityislogic - I have Felsenstein's book, unfortunately his reasoning is faulty as he seems to think that all flat priors are uninformative and vice-versa and thus considers the fact that two flat priors on different parameterisations of the same thing give different conclusions is an indication there is a problem.  The premise is wrong, and the conclusion unsurprising to anyone familiar with the idea of transformation groups.  Essentially an uninformative prior on branch length should be insensitive to the choice of units, which would give a prior that was flat on a logarithmic scale.",2011-01-21T10:11:16.297,887,CC BY-SA 2.5,
10014,6406,0,"I wouldn't say that @TMOD *needs* more data, just that the predictions are likely to be more accurate if @TMOD *had* more data.  There is enough information in the question to generate a prediction.",2011-01-21T10:15:27.860,2392,CC BY-SA 2.5,
10015,2365,0,"@Joris, can you give a specific page number?",2011-01-21T10:16:06.343,887,CC BY-SA 2.5,
10016,2365,0,comment removed - whatever...,2011-01-21T10:20:57.867,1124,CC BY-SA 2.5,
10017,2365,0,"@Dikran : I'll look it up tonight. It's where he demonstrates the effect of the truncation on the t prior. Actually, it's almost a page big, you should have seen it when you read the chapter. It's pretty the center of his story...",2011-01-21T10:21:22.017,1124,CC BY-SA 2.5,
10018,6404,2,"The problem as stated is a bit like an *outlier* problem, but not in the normal way one thinks of outliers. To incorporate the *amazing* result (i.e. the outlier) you would need a ""sampling distribution"" with a heavy tail (Jose's result well over 3 standard deviations away from his average over the past data), so this may help fit you data better, and account for it in prediction.",2011-01-21T10:21:47.903,2392,CC BY-SA 2.5,
10019,6406,0,"@probabilityislogic, yes there is enough information to generate the prediction, but then the model will only have intercept.",2011-01-21T10:23:21.800,2116,CC BY-SA 2.5,
10020,2365,0,@probabilityislogic : the whole point Felstenstein makes is that t and u are linked. Meaning that a flat prior on t gives a greatly biased prior on u and vice versa. You'll have to use a prior that favorizes certain values for either of it in order to have a prior that actually makes **biologically** sense. So you need to know at least something about either the transformation rate or the mutation time to use eg mrBayes in phylogeny.,2011-01-21T10:29:15.623,1124,CC BY-SA 2.5,
10021,2365,0,"@Joris, it is a while since I read the chapter in question, but IIRC Felseneteins problem was that a flat prior on branch length is biologically implausible.  I agree, but a flat prior on branch length is not necessarily an uninformative prior.  Felsensteing seems to think (incorrectly) that only flat priors are uninformative, and hence isn't aware of other choices that may be uninformative and biologically plausible.  I should point out though that if you have knowledge of what is and what isn't biologically plausible, then you are not entirely uninformed, and neither should be your prior!",2011-01-21T10:29:54.257,887,CC BY-SA 2.5,
10022,2365,0,"@Joris ""the whole point Felstenstein makes is that t and u are linked. Meaning that a flat prior on t gives a greatly biased prior on u and vice versa.""  It may be that this bias is what you get if you make a minimally informative prior that includes the prior knowledge that the units of measurement should have no effect on the conclusion (transformation groups).",2011-01-21T10:32:53.317,887,CC BY-SA 2.5,
10023,2365,0,"@joris I can understand what you are trying to say, in setting a prior you are describing a *state of knowledge*, just as if you are setting a sampling distribution. Now in the uniform prior on $P$ you are describing a *state of knowledge* that it is possible for ""no change"" and ""one or more changes"" to occur on a given branch.  Probability theory tells you how to *coherently* transform this into *the same state of knowledge* about $t$, given your knowledge about the relationship between $P$ and $t$.  So a ""flat"" prior for $t$ necessarily is describing a *different state of knowledge*.",2011-01-21T10:55:35.487,2392,CC BY-SA 2.5,
10024,2365,0,That the solutions are different is no more and no less surprising than if you used a different model between P and t.,2011-01-21T10:56:16.577,2392,CC BY-SA 2.5,
10026,4720,0,"Just a brief technical note, the ""arcsine"" transformation is one which has a faster convergence to normality than the logistic transformation.  Set $Y=\frac{2}{\pi}\arcsin{\sqrt{\frac{X}{N}}}$ (where $X$ is number of ""successes"" and $N$ the number of trials), and you can show with the so called ""delta method"" that the variance of $Y$ is approximately constant (and independent of $Y$, as it should be in the normal distribution).",2011-01-21T11:49:05.173,2392,CC BY-SA 2.5,
10027,6431,2,"Well said, especially about what question a CI actually answers.  In the Jaynes' article however, he does mention that CI's (and most frequentist procedures) are designed to work well ""In the long-run"" (e.g. how often do you see $n \rightarrow \infty$ or ""for large n the distribution is approximately..."" assumptions in frequentist methods?), but there are many such procedures that can do this.  I think this is where frequentist techniques (consistency,bias,convergence,etc.etc.) can be used to assess various Bayesian procedures which are difficult to decide between.",2011-01-21T12:13:38.477,2392,CC BY-SA 2.5,
10028,6431,2,"""Jaynes was being a little naughty in his paper..."" I think the point that Jaynes was trying to make (or the point that I took from it) is that Confidence Intervals are used to answer question a) in a large number of cases (I would speculate that anyone who *only has frequentist training* will use CI's to answer question a) and they will think they are an appropriate frequentist answer)",2011-01-21T12:19:32.073,2392,CC BY-SA 2.5,
10029,2356,5,"I would add some clarity to the ""incorrect prior assumptions..."" by stating that the Bayesian answer and the frequentist answer must make use of *the same information*, otherwise you are just comparing answers to two different questions.  Great question though (+1 from me)",2011-01-21T12:26:40.900,2392,CC BY-SA 2.5,
10030,6431,3,"yes, by ""a little naughty"" I just meant that Jaynes was making the point in a rather mischeiviously confrontational (but also entertaining) manner (or at least that is how I read it).  But if he hadn't then it probably wouldn't have had any impact.",2011-01-21T12:28:05.417,887,CC BY-SA 2.5,
10031,6421,1,"That was on a longer time scale. I don't think it's a duplicate. As for argumentative, it's up to the participants. I am not trying to award a trophy here, just to keep abreast of seminal papers I and others may have missed. Since there is no right answer, I am all for a CW. 

I find it interesting that so far all the answers are on bayesian innovations.",2011-01-21T15:39:32.073,30,CC BY-SA 2.5,
10033,6433,1,See this thread: http://stats.stackexchange.com/questions/5347/how-can-i-efficiently-model-the-sum-of-bernoulli-random-variables,2011-01-21T16:05:37.447,449,CC BY-SA 2.5,
10034,2365,0,"I'm a bit curious, how does the ML solution work for $t$ if you just plug in $P$ into your likelihood.  The derivative will be (by chain rule) $\frac{dL}{dt}=\frac{dL}{dP}\frac{dP}{dt}=0$ but from the function for $P$ this means $\frac{dP}{dt}=\frac{4u}{3}e^{-\frac{4}{3}ut}$, so setting $u \rightarrow 0$ and $t\rightarrow\infty$ such that $P$ is unchanged (and equal to $P_{MLE}$) would solve the ML equation?  Or is there something about $u$ which is not stated in the information?",2011-01-21T16:08:43.437,2392,CC BY-SA 2.5,
10038,6419,0,"Scipy has much more than BLAS. In terms of Fortran packs, it binds optimization packs such as minpack, fitpack, and other linear algebra packs. It also has signal or image processing tools, optimization. I personally find that SciPy is much more versatile than R. It addition, you seem to be comparing the complete set of R packages to the single Python package 'scipy'. To be fair, you should take in account the huge variety of Python packages, a lot of them useful for science. Now I will easily acknowledge that for the specific task of statistical data processing, R is much better equipped.",2011-01-21T16:42:21.973,1265,CC BY-SA 2.5,
10039,6402,0,"@chl, This model treats X and Z as categorical in the standard ANOVA-regression (factor contrasts) representation for categorical regressors.  That's why I have the $j$ and $k$ subscripts in the equations (if they were continuous only the $i$ subscript would be there).  And from what it sounds like your ""minor allele"" you have information about the scale of the categories, the ""ordinality"" of X and Z only affect the interpretation of coefficients.  But by treating it as a numerical value, you are effectively replacing ""degrees of freedom"" with ""model assumptions"".",2011-01-21T16:56:32.163,2392,CC BY-SA 2.5,
10041,6402,0,"@onestop - if this is true ($n=I$), then the model as I have described it is *unidentifiable* which means that the problem basically needs more structure in order to get a unique solution.  One way to add this ""structure"" is through the Bayesian approach, setting a slightly informative prior on the probabilities (such as the uniform prior).",2011-01-21T17:07:13.540,2392,CC BY-SA 2.5,
10043,6433,2,"The answer depends, in an essential manner, on precisely *how* your observations depart from independence and how they fail to be identically distributed.  In general the binomial distribution, with a single parameter, is too ""rigid"" to model your sum: you should be looking at families with more parameters to allow for under- or over-dispersion.  Perhaps you could tell us more about these data?",2011-01-21T17:31:25.950,919,CC BY-SA 2.5,
10045,6421,1,I liked the 50-year question. I don't think it needed to be close.  I am just not so sure we needed a new question. <Shrug>,2011-01-21T18:15:11.387,334,CC BY-SA 2.5,
10047,6428,0,"using function melt from package reshape, converting to long format is one line in R, melt(data,id=1:2).",2011-01-21T18:41:35.297,2116,CC BY-SA 2.5,
10050,2365,0,@Dikran : the graph about the truncation of T is shown on page 305 (fig 18.7),2011-01-21T22:55:36.050,1124,CC BY-SA 2.5,
10051,2365,0,"@probabilityislogic : we're talking about trees. The likelihood of the tree is the multiplication of all likelihoods at each site (node) of the tree, which is defined as the sum  over all possible nucleotides that may have existed at the interior nodes of the tree, of the probabilities of each scenario of events. And that probability is defined by a model which involves T (or u), the Jukes-Cantor model being the most easy one. As said, phylogeny does not fit into any classical framework.",2011-01-21T23:00:17.340,1124,CC BY-SA 2.5,
10052,2365,0,"@probabilityislogic : There have been numerous frameworks built up by now around bayesian posterior probabilities as alternative for bootstrap support values, but most of the studies conclude - rightfully - that both cannot be compared. And for the estimates of the prior both birth-death processes (data-based) as theoretical distributions for branch lengths have been used extensively. Bayesian applications like mrBayes can reduce calculation time significantly, but discussion remains whether they perform better or worse, each side of the argument bringing ""proof"" for the claim.",2011-01-21T23:04:51.357,1124,CC BY-SA 2.5,
10053,2365,0,"@probabilityislogic : But again, most studies rigthfully conclude that they can't be compared. And I still follow Felsenstein that, in case no further knowledge is available, the risk on bias is far larger with a bayesian than with an ML estimate for a phylogenetic tree. If you dive into the literature on phylogeny ( and check the papers that are not online as well, science didn't start in 1998), you'll see that this controversy has been debated heavily for the past 50 years. You and @Dikran might disagree, but the comments here are far from the right place to discuss this properly. Cheers",2011-01-21T23:10:39.787,1124,CC BY-SA 2.5,
10054,6439,0,"This is equivalent to my problem, but it is easier to pose the problem in terms of the left tail (smaller quantiles).",2011-01-21T23:11:26.753,795,CC BY-SA 2.5,
10055,6371,0,"Since the Markov chain you consider has some absorbing states (and is irreducible, presumably), its stationary distribution is concentrated on these absorbing states. In other words, the stationary vector you computed should have exactly two non zero coordinates, one for each of the absorbing states. Is this really what you have in mind?",2011-01-22T00:13:15.910,2592,CC BY-SA 2.5,
10056,6444,0,Any particular reason you chose 1 and n as the two variables?  are they ordered in some way?,2011-01-22T00:25:46.063,2392,CC BY-SA 2.5,
10057,6444,0,What is the relationship between $X_{n}$ and $X_{1}$ without including the other variables?,2011-01-22T00:34:36.867,2392,CC BY-SA 2.5,
10058,6406,0,"not necessarily, one could fit an AR(1) or AR(2) model to this data",2011-01-22T00:42:42.820,2392,CC BY-SA 2.5,
10059,6406,0,"@probabilityislogic, ah yes, you are right.",2011-01-22T03:53:56.550,2116,CC BY-SA 2.5,
10060,6445,0,"I think aeWiardll is implying that there is a correlation (or some similar structure) in the data. Therefore, the simple assumption of independence of `lm` is probably not what @eWizardll is looking for. I hope the answer to your question would make things clearer.",2011-01-22T04:50:58.457,1307,CC BY-SA 2.5,
10061,6332,0,"If holdout=0, it is using insample fit which will favour the model with more parameters. But if holdout>0, it makes sense, although you need quite a large holdout sample for the method to select the best model reliably. In general, I would choose the model to use based on other considerations rather than only consider the out-of-sample forecast performance on a short holdout sample from one series. For example, you could consider the out-of-sample performance across the whole ensemble of series (rather than one series at a time), and select the best model class that way.",2011-01-22T07:56:32.553,159,CC BY-SA 2.5,
10062,6449,0,"@mpitkas: Thank you very much for your helpful answer. Just a couple of typos in it for you to edit: The expectation operator is missing before the first parenthesis in the third line of the middle block of equations, and the coefficient $\beta_1$ is missing before the second term in the last line of that block and before the first term in the numerator on the RHS of the final equation.",2011-01-22T10:23:33.217,2866,CC BY-SA 2.5,
10063,6450,1,(+1) I discovered this package only recently. It looks interesting.,2011-01-22T11:17:55.440,930,CC BY-SA 2.5,
10065,6430,0,(+1) Thanks for sharing your experience. I often observed that a good compromise is found just by looking at the scree plot while considering Kayser's rule as a lower limit and simulated data as an upper limit. What I like in `psych` is that it displays simulated scree plots from both PCA and FA.,2011-01-22T11:24:59.580,930,CC BY-SA 2.5,
10066,6414,0,(+1) I came across O'Connor's website some years ago and it has a lot of useful resources. Nice that you link it here.,2011-01-22T11:25:43.980,930,CC BY-SA 2.5,
10067,6402,0,"@probabilityislogic Thanks for clarifications. If you're interested in how to ""scale"" SNP data, there was a nice paper by Waaijenborg & Zwinderman with a pretty nice use of optimal scaling, [Correlating multiple SNPs and multiple disease phenotypes: penalized non-linear canonical correlation analysis](http://j.mp/gIq9mW) (Bioinformatics 2009 25(21):2764). I'm still not clear myself with how we could convert a rank outcome to an ordinal one (hence echoing @onestop's comment), so I'm waiting for an helpful OP's feedback.",2011-01-22T11:32:55.710,930,CC BY-SA 2.5,
10068,6450,4,His paper about it got into *Biostatistics*: http://dx.doi.org/10.1093/biostatistics/kxp050,2011-01-22T11:45:43.160,449,CC BY-SA 2.5,
10070,2365,0,"@Joris, Figure 18.7 on page 305 just shows that using an informative (not uninformative) prior, the maximum likelihood estimate lies outside the Bayesian credible interval.  There is nothing in the least surprising about that.  As has already been pointed out, a flat prior on branch length is unlikely to be uninformative (transformation groups), especially when needlessly truncated (it is possible to use improper priors).",2011-01-22T13:25:32.057,887,CC BY-SA 2.5,
10071,6448,1,The wiki article might help out:  http://en.wikipedia.org/wiki/Continuity_correction,2011-01-22T13:30:21.400,253,CC BY-SA 2.5,
10072,6446,3,"(+1) I've tried a few experiments in Stata on their standard auto.dta example dataset and they agree with this. The two $t$-statistic, and hence the two $p$-values, are always identical. So i think the questioner's intuition is right and he is indeed doing something wrong with the software.",2011-01-22T15:49:29.000,449,CC BY-SA 2.5,
10077,6402,0,"one way to dodge the ""ranks"" issue is to just use broader rankings, such as percentiles, deciles, quintiles, quartiles, etc.,etc..  However this introduces the ""messy"" problem of ""how much to aggregate?"" - could treat this as a decision problem and use decision theory (for a formal approach).  I would suggest going the ""formal"" way in this case because you run the risk of ""quasi-fudging"" the significance (i.e. choosing the aggregation in which the results are most significant).  Apologies for my possible ignorance, but what does ""SNP data"" stand for?",2011-01-22T17:00:15.230,2392,CC BY-SA 2.5,
10078,6449,1,"I'm not sure this is the answer to the question asked, because @John Bentin is not trying to fit the two least square equations simultaneously but *separately*, so there is no endogeneity in terms of the coefficients - only in terms of the model you have posed in your answer; *joint* estimation was not part of the question, only *conditional* estimation was.",2011-01-22T17:30:54.317,2392,CC BY-SA 2.5,
10079,6453,1,"The statement ""hypothesis A is proven true..."" is not something statistics can be much help with *in absolute terms*.  Statistics is much more useful at *comparing* different hypothesis, it would be better put in terms of ""the data supports hypothesis B compared to hypothesis A"" and other comparative statements.",2011-01-22T18:13:26.150,2392,CC BY-SA 2.5,
10080,6459,0,"@bil_080, the OP does not mention R, and help page for Box.test in R mentions the correction and has an argument to allow for the correction, although you need to supply it manualy.",2011-01-22T20:50:33.660,2116,CC BY-SA 2.5,
10081,6449,0,"@probabilityislogic, if both equations are fitted and deemed adequate (with $\beta_1$ and $\alpha_n$ non-zero) you get a contradiction, so it does not matter that they are fitted separately.",2011-01-22T20:59:45.937,2116,CC BY-SA 2.5,
10085,6459,0,"@mpiktas, Oops, you're right. I assumed this was an R question.  As for the second part of your comment, there are several R packages that use Ljung-Box stats.   So, it's a good idea to make sure the user understands what the package's ""lag"" means.",2011-01-22T22:29:06.497,2775,CC BY-SA 2.5,
10086,6461,3,"But what if the two effects have the opposite signs? Likelihood ratios may be an alternative to $p$-values, but I wouldn't recommend meta-analysing $p$-values either unless they're the only thing reported.",2011-01-23T00:03:04.810,449,CC BY-SA 2.5,
10087,6458,1,"Technical Note: ""...However, if you non-randomly select two papers from among many that have been published, it would introduce bias into your study..."" This *bias* you speak of is in the *generality* of the results to other studies, not in the combination of the two pieces of analysis.  The correct analysis will *never* be biased no matter how the data was obtained, as long as the conclusions are contained to populations *sufficiently similar* to that which was observed.  The potential bias comes in when the population you are inferring about is not *sufficiently similar* to the sample.",2011-01-23T02:01:53.797,2392,CC BY-SA 2.5,
10088,6461,0,"@onestop - I'd say the ""opposite sign"" effect is a direct consequence of using a 2-sided test.  In this test, the ""null hypothesis"" is no effect, whereas the alternative is for some effect, regardless of sign $effect = 0$ vs $effect \neq 0$.  If the likelihood is symmetric in the effect, then a change of sign (with no decrease in absolute value) is further evidence in favour of the alternative hypothesis.",2011-01-23T02:11:18.980,2392,CC BY-SA 2.5,
10089,6461,0,"@onestop meta-analysis uses the mean and standard deviation, and $P$ value itself is insufficient since sd can only be estimated from $P$-values if $n$ is also provided, the statistical model is clearly stated. A method called 'vote-counting' tallies $P$-values indicating positive, negative, and no effects, but I am not sure that this is useful.",2011-01-23T02:18:03.647,1381,CC BY-SA 2.5,
10090,6461,0,"@onestop - just adding a further clarification to my comment above.  I think why it is *intuitively silly* for a change of sign to increase the evidence, is that the *hypothesis* we think of changes between experiments, from $effect=0$ vs $effect \neq 0$ to $effect=E_1$ vs $effect \neq E_1$ where $E_1$ is the estimate from the first study.  Shows that one needs to be *careful* when combining data, to make sure you know which hypothesis test you are doing.",2011-01-23T02:22:20.137,2392,CC BY-SA 2.5,
10092,6458,0,"@probabilityislogic Your point is on the same lines as mine: validity of the analysis is contingent on interpretation. Still, study selection is a special case on which many assumptions of meta-analysis are based. Thes assumptions are too many to review here, but the conclusions would be different if two studies are done on the same population by the same lab vs by two independent labs. The difference is not the population itself but in the investigator, and a common assumption tested by meta-analysis is the absence of an investigator effect.",2011-01-23T02:36:17.517,1381,CC BY-SA 2.5,
10093,6428,0,"An interesting extension/alternative to this is to fit a hierarchical model with a Possion sampling distribution with a *sampled* rate parameter (1 rate per year), but a *Cauchy* sampling distribution for the rate parameter (instead of normal or normal mixture).  The Cauchy distribution will allow for the *extreme* event to occur (by sampling a large rate parameter).  An intermediate case (between normal and Cauchy) is the t-distribution.  (Cauchy is easier to sample from as it can use the inverse CDF method).",2011-01-23T02:49:49.133,2392,CC BY-SA 2.5,
10094,6444,0,"@probabilityislogic: I chose $1$ and $n$, rather than (say) $1$ and $2$, for convenience of notation; otherwise the naming of the variables has no significance beyond their enumeration. I didn't posit any relationship between $X_1$ and $X_n$ with the other variables excluded. However, testing this might be a good simple way to get insight into whether we are implementing the software correctly---thanks for the suggestion!",2011-01-23T08:44:06.183,2866,CC BY-SA 2.5,
10095,6446,0,"and onestop: Thank you for your help, in particular for pointing out that the significance levels should be the same, from both a theoretical and an experimental perspective. Clearly we need to look at our implementation.",2011-01-23T08:54:47.197,2866,CC BY-SA 2.5,
10096,6465,0,"+1 I also recommend Quantum GIS from the freely available, opensource softwares.",2011-01-23T10:20:17.630,2714,CC BY-SA 2.5,
10097,6463,4,You might be better off asking on http://gis.stackexchange.com .,2011-01-23T10:38:23.423,449,CC BY-SA 2.5,
10099,6449,1,"@mpiktas: You've assumed the questioner is trying to estimate an underlying *model*, when all he said is that he's fitting a couple of linear regressions. http://www.jstor.org/stable/2676681",2011-01-23T11:02:10.390,449,CC BY-SA 2.5,
10100,6449,0,"@onestop, thanks for the article! You gave me some powerful ammunition for the discussions with my colleagues and students. Still in doing those 2 regressions I would be careful.",2011-01-23T13:42:18.257,2116,CC BY-SA 2.5,
10101,6299,0,Not so odd really. One reason for using a numerical method to find an answer to a problem that can be solved analytically is to ensure that one is using the software correctly.,2011-01-23T14:49:23.903,2617,CC BY-SA 2.5,
10102,6462,0,Thanks for clarifying that mbq! Now I can be *sure* I'm not qualified to answer the question!,2011-01-23T14:57:50.693,449,CC BY-SA 2.5,
10103,6467,0,"(+1) Nice catch! It is limited to the US, isn't it?",2011-01-23T16:10:20.780,930,CC BY-SA 2.5,
10104,6459,0,"Thanks--I am using R, but the question is a general one.  Just to be safe, I was doing the test with the LjungBox function in the portes package, as well as Box.test.",2011-01-23T16:16:58.073,2875,CC BY-SA 2.5,
10105,6298,6,"There are ""several"" algorithms that the Prediction API can choose from when training/predicting your data.  The engine chooses the one that it decides is best.  

Some users have requested a little more control over that selection, http://goo.gl/mod/5EoA, even if the algorithm is unknown.  

Redditors have speculated on the guts here, http://www.reddit.com/r/MachineLearning/comments/evdxb/what_are_your_thoughts_on_google_prediction_api/, but the stat-speak is lost on me.",2011-01-23T16:40:11.743,2884,CC BY-SA 2.5,
10106,6298,2,"@hyperslug Post it as an answer, it is quite useful so I'd like to accept it.",2011-01-23T16:48:12.283,,CC BY-SA 2.5,user88
10107,6463,2,http://en.wikipedia.org/wiki/List_of_geographic_information_systems_software,2011-01-23T17:37:45.843,919,CC BY-SA 2.5,
10108,6472,3,"I don't understand how small sample size, _by itself_, necessarily limits statistical inferences.  Sometimes the effect size is so huge that you only need enough samples to estimate variability and to check any model assumptions.",2011-01-23T17:55:08.140,919,CC BY-SA 2.5,
10109,6404,0,"If you would consider a crude little shortcut in addition to all the more sophisticated commentary appearing here, there's Dixon's Test for Outliers which you can perform on a sample as small as 4.  See http://www.cee.vt.edu/ewr/environmental/teach/smprimer/outlier/outlier.html#detect",2011-01-23T18:38:08.773,2669,CC BY-SA 2.5,
10110,6347,0,"Or, along those lines but slightly different, do you want to perform an especially selective PCA that only utilizes the most informative components?  For example many people default to extracting components with eigenvalue >=1.0 when a stricter criterion might serve their needs better and reveal more replicable results.",2011-01-23T18:47:57.913,2669,CC BY-SA 2.5,
10111,6474,0,"I've run into the same problem as the OP, and also found that dlm and the Dynamic Linear Models with R book are very useful.",2011-01-23T19:21:19.887,1764,CC BY-SA 2.5,
10112,6455,1,"@user2875, I've deleted my answer. Fact is that for large $h$ the test is not reliable. So the answer really depends for which $h$, $p<0.05$. Furthermore what is exact value of $p$? If we decrease the threshold to $0.01$, does the result of the test changes? Personally in case of conflicting hypotheses I look for other indicators whether model is good or not. How well model fits? How does the model compare to alternative models? Do the alternative model have the same problems? For what other violations the test rejects the null?",2011-01-23T19:52:36.607,2116,CC BY-SA 2.5,
10113,6475,1,"I think you will get an answer soon, but maybe you could add some background information, i.e. what are you trying to show (or equivalently, what is/are your null hypothes{i|e}s), what is the sample size, etc. In the meantime, I think @caracal's brilliant response to this question [What is the NULL hypothesis for interaction in a two-way ANOVA?](http://stats.stackexchange.com/questions/5617/what-is-the-null-hypothesis-for-interaction-in-a-two-way-anova/5622#5622) might be helpful.",2011-01-23T21:02:17.297,930,CC BY-SA 2.5,
10114,6402,0,"@probabilityislogic SNP stands for [single nucleotide polyporphisms](http://j.mp/hdK2Vr) and are widely used as DNA markers to explain phenotypic variations in [GWAS](http://j.mp/eNHd79). As I said, we usually consider the frequency of the minor allele as a numerical variable, but other coding schemes are possible depending on the model we consider. I'd recommend [Genetic association studies](http://j.mp/hFFyWJ) by Cordell and Clayton (*Lancet* 2005; 366: 1121‚Äì31) for a thorough review on that subject.",2011-01-23T21:14:38.520,930,CC BY-SA 2.5,
10116,6402,0,"@probabilityislogic (Con't) btw, I appreciate your interest -- it's always good to know what kind of data we are dealing with... I spent a lot of time figuring out what SNPs really are!",2011-01-23T21:15:29.067,930,CC BY-SA 2.5,
10117,6457,0,"(+1) Your 2nd paragraph is particularly well formulated (I initially interpreted the question as an experimental study with *a priori* hypotheses); I also like the 3rd paragraph, and I recommend this approach when finding interesting but unexpected interaction effects. Finally, I don't know about any application of cross-validation in ANOVA designs: do you have any references?",2011-01-23T21:26:07.807,930,CC BY-SA 2.5,
10118,6455,1,"@mpiktas, The Ljung-Box test is based on a statistic whose distribution is asymptotically (as h becomes large) chi-squared.  As h gets large relative to n, though, the power of the test decreases to 0.  Hence the desire to choose h large enough that the distribution is close to chi-squared but small enough to have useful power.  (I do not know what the risk of a false negative is, when h is small.)",2011-01-24T00:05:21.110,2875,CC BY-SA 2.5,
10119,6470,1,"Can I use the ""predict"" function on a new dataset, and retain the same error structure?  How does the gls command know what order the observations are in?",2011-01-24T00:23:45.313,2817,CC BY-SA 2.5,
10120,6471,1,"What is the 0.5 for in ""corr=corAR1(0.5,form=~1)?""",2011-01-24T00:29:06.647,2817,CC BY-SA 2.5,
10121,6476,1,"What is the 0.5 for in ""corr=corAR1(0.5,form=~1)?""",2011-01-24T00:30:10.610,2817,CC BY-SA 2.5,
10122,6476,1,It gives a starting value for the optimization. It makes almost no difference if it is omitted.,2011-01-24T02:11:25.510,159,CC BY-SA 2.5,
10124,6371,0,Yes. The issue is computing some confidence intervals for said entries.,2011-01-24T03:08:29.537,1144,CC BY-SA 2.5,
10125,6463,0,Thank you for the answers. I need to go out ans look at these various options.,2011-01-24T03:17:32.490,2878,CC BY-SA 2.5,
10126,767,10,"Difference can be viewed purely from mathematical standpoint -- BIC was derived as an asymptotic expansion of log P(data) where true model parameters are sampled according to arbitrary nowhere vanishing prior, AIC was similarly derived with true parameters held fixed",2011-01-24T05:57:44.100,511,CC BY-SA 2.5,
10129,6380,2,This does belong on StackOverflow. There's no statistics question here at all. Only a general programming question.,2011-01-24T08:31:08.247,29,CC BY-SA 2.5,
10130,6479,0,"I remember the interpretation of the p-value as (in this case): when assuming that the null-hypotheses (i.e., homogeneity of variances) is correct, then the probability of obtaining this or a more extreme result (i.e., contrary the null) is 57% or 95%. But whatsoever, the conclusion is the same and correct.",2011-01-24T08:52:00.350,442,CC BY-SA 2.5,
10131,6478,0,how does this question differ from http://stats.stackexchange.com/questions/5443/estimate-the-meaningful-predictors-for-a-value-in-a-cart-model-rpart ?,2011-01-24T09:14:21.530,264,CC BY-SA 2.5,
10132,6478,0,That question refers to knowing which predictor was relevant for a particular categorical value of the dependent variable.  This question is more broad (variable importance/ranking without detecting to which nominal value it effects).  Since that question wasn't answered I thought it worth to phrase it in a more general way in the hopes that someone might be able to help ...,2011-01-24T09:19:21.467,253,CC BY-SA 2.5,
10133,6484,2,@lcl23 Listwise deletion is usually for subjects with missing data on any one of the covariates. Could you indicate how you define an outlier in your particular case?,2011-01-24T09:40:40.020,930,CC BY-SA 2.5,
10134,6481,1,Would you still be interested in knowing how ordinal logistic regression work? It may have serious implications on model interpretation when you ignore the natural ordering of a variable.,2011-01-24T10:50:38.463,930,CC BY-SA 2.5,
10135,6481,2,"Just to be clear, it is the outcome variable that's ordinal? What is the alternative to ordinal logistic regression you're considering? Multinomial logistic regression, or grouping the categories into two groups and using standard logistic regression?",2011-01-24T11:13:59.033,449,CC BY-SA 2.5,
10136,6484,0,"Google leads me to believe `casewise` may be some 'feature' of SPSS. Not a package I've ever used, unfortunately(?)",2011-01-24T11:23:24.277,449,CC BY-SA 2.5,
10137,6457,0,"@chl - cross-validation is different to what I meant by ""lock up the data"".  this part would not be tested,looked at,analysed,etc.,etc. *at all* until your analysis was finished.  Cross validation would be done with the remaining data.  A good book on this (and many other things) is called *The elements of Statistical Learning*, but I don't have the full reference right now, but I can get it in a few weeks (when I go back to work).  You could try ""googling it"".",2011-01-24T12:00:38.873,2392,CC BY-SA 2.5,
10138,6457,0,"@chl - When you do cross validation, it is usually done with respect to prediction, and you need to specify a criterion for ""good"" and ""bad"" predictions, similar to a loss function in a decision theoretical framework.  You also need to specify the competing models (I don't think cross-validation can do this for you).  Cross validation then gives one way of deciding which model *out of the alternatives you gave it* was the best *according to the criterion of ""best"" that you gave it*.  In ANOVA one usually isn't interested in prediction, but inference, so it may not be so useful in this case.",2011-01-24T12:09:28.300,2392,CC BY-SA 2.5,
10139,6457,0,"...continuing... although, having said that, the model which gives the ""best"" predictions does seem like a good model to investigate further (as oppose to the one which gives the ""worst"" predictions).",2011-01-24T12:11:20.247,2392,CC BY-SA 2.5,
10140,6455,0,"@user2875, this the third time you changed the question. First you ask about the strategy of picking $h$ with smallest value, then how to interpret the test if $p<0.05$ for some values of $h$, and now what is the optimal $h$ to choose. All three questions have different answers and may even have different answers depending on the context of particular problem.",2011-01-24T12:12:31.600,2116,CC BY-SA 2.5,
10141,6457,0,"Ah, but I know what CV is (btw, the ESL book is available here, http://www-stat.stanford.edu/~tibs/ElemStatLearn/). I was just wondering whether you know references where CV is used in applied work.",2011-01-24T12:24:05.677,930,CC BY-SA 2.5,
10142,6402,0,"@chl - thanks for the clarification.  I feel much better about my ignorance, seeing as I know virtually nothing about genetics, I can't even spell the actual word for DNA (its ""deoxy-narglyic-acid"" right? lol).",2011-01-24T12:34:49.033,2392,CC BY-SA 2.5,
10144,6484,0,"Hi chl, casewise list is a table produced for analysis of residual in logistic regression, nothing related to missing data. i learnt from this link: http://www.uk.sagepub.com/burns/website%20material/Chapter%2024%20-%20Logistic%20regression.pdf. Page 17.",2011-01-24T14:22:50.427,2793,CC BY-SA 2.5,
10145,6488,0,"There is no need to merge -- this is an example of ""good duplicate""",2011-01-24T15:22:55.680,,CC BY-SA 2.5,user88
10146,6484,2,"@lcl23 Ok, I just browsed the attached doc; p. 583, it is said: ""the casewise list produces a list of cases that didn‚Äôt fit the model well."" (where outlying criterion is considered by default to be individuals outside a 2 SD band, p. 577). So why did you run so many instances of your logistic regression? Did you first look at the 44 individuals that were flagged when fitting the initial model?",2011-01-24T15:27:07.040,930,CC BY-SA 2.5,
10147,6490,2,"Well said, another thing is that a p-value tells you nothing about *what to do* if the null hypothesis is ""rejected"", but a plot of the data gives you a clue as to the problem",2011-01-24T16:13:05.220,2392,CC BY-SA 2.5,
10148,6474,2,"I would also add that I bought ""An Introduction to State Space Time Series Analysis (Practical Econometrics)"", which was a very gentle introduction to the topic, and it too, used SsfPack/Ox. Makes it much less useful, in one sense, though it does have the benefit of once you implement it in dlm (in R) yourself, you can check your answer against an independent implementation.",2011-01-24T18:00:07.060,1764,CC BY-SA 2.5,
10149,6455,0,"@mpiktas, the questions are all the same, just different ways of looking at it.  (As pointed out, if p>0.05 for all h, then we know how to interpret the smallest p; if we knew the optimal h--we don't--then we would not be concerned with choosing the smallest p.)",2011-01-24T18:43:59.767,2875,CC BY-SA 2.5,
10150,6493,1,Normals have conjugate priors: http://en.wikipedia.org/wiki/Normal-gamma_distribution .  You might find these much easier to use.,2011-01-24T19:26:47.550,919,CC BY-SA 2.5,
10151,6492,4,"A t distribution might not be a good choice, because it is symmetric whereas asset returns tend to have strong skew.  At a minimum, consider modeling the *logarithms* of the returns rather than the returns themselves.",2011-01-24T19:28:47.717,919,CC BY-SA 2.5,
10152,6484,2,"@lcl23 This procedure will likely converge, but the results might be useless.  Exactly what do they characterize?  Certainly not the population or process you are trying to understand, because you have thrown away so much of the data!  A solution developed specifically for this phenomenon is *robust regression,* which automatically downweights--but does not eliminate--unusual cases.",2011-01-24T19:31:59.443,919,CC BY-SA 2.5,
10153,6492,0,"Yeah, that's a good point, I was thinking about that in the back of my mind, but this question is still of interest to me.",2011-01-24T19:34:44.440,1146,CC BY-SA 2.5,
10154,6493,0,"Interesting. I am doing numerical stuff, is there an advantage to these distributions besides congugality?",2011-01-24T19:40:17.353,1146,CC BY-SA 2.5,
10155,6492,2,Do you have a truly *huge* amount of data? I think it's more common even in Bayesian modelling to fix the df and try different values as a sensitivity analysis.,2011-01-24T20:11:03.437,449,CC BY-SA 2.5,
10156,6493,6,"Not really my area but this 'might' be relevant?
Gelman A. Prior distributions for variance parameters in hierarchical models. Bayesian Analysis 2006; 1:515‚Äì533. http://dx.doi.org/10.1214/06-BA117A",2011-01-24T20:22:25.977,449,CC BY-SA 2.5,
10157,5327,0,"Huge mistake; I realized I meant *Tobit* regression the whole time, not *Truncated* regression. I just changed the question to reflect this error.",2011-01-24T20:25:42.133,1583,CC BY-SA 2.5,
10159,5327,0,"The Wooldridge reference is still the correct reference; i.e., it refers to Tobit regression.",2011-01-24T20:31:27.620,1583,CC BY-SA 2.5,
10160,6492,0,"I do have a pretty large quantity of data, but it may be that this is the best approach. Submit as an answer and I'll vote you up and accept if no one provides a better solution.",2011-01-24T21:07:19.150,1146,CC BY-SA 2.5,
10162,6332,0,"thanks for the suggestion, I'll start to move in that direction.  In the case where my holdout equals zero what if I introduced some kind of penalty for parameters?",2011-01-24T23:23:11.503,2817,CC BY-SA 2.5,
10163,6332,1,"As I keep saying, use out-of-sample performance on a large set of series. You can't easily compare in-sample performance of the two model classes.",2011-01-24T23:47:09.340,159,CC BY-SA 2.5,
10164,6363,0,"@ richiemorrisroe - i agree, worth keeping in mind.  

@ Andy W - Just confirmed in SPSS that, using forced entry only, missing pairwise and missing listwise give different results in every respect, including different df.",2011-01-25T00:13:52.197,2669,CC BY-SA 2.5,
10165,6484,0,"@ICI23 and whuber - I'm going to look up robust regression.  But one question is, 44 out of how many?  If out of 50,000, then I would feel less squeamish about excluding at least that first set of influential or poorly-fitting cases.  Beyond that,  normally (and forgive me if this is basic stuff you already know) you'd want to examine all your distributions and see whether some outliers are actually implausible values, like age being coded as 999 for 'missing.'  Also you'd often want to transform any very skewed distribution to pull outliers in.",2011-01-25T00:34:31.370,2669,CC BY-SA 2.5,
10166,6481,0,"@onestop, the independent variable is ordinal, but not the outcome variable, outcome variable is whether the patient gets an infection or not",2011-01-25T03:20:18.120,588,CC BY-SA 2.5,
10167,6363,0,"I still think your confused, how can SPSS return different sets of results by declaring missing pairwise unless it makes up values for the missing data? Here is an example using simulated data I have posted in a text file, http://dl.dropbox.com/u/3385251/SPSS_missing_Listwise_vs_Pairwise.txt . I have currently downvoted your answer, as all this talk about how the regression command handles missing data is confusing, has nothing to do with the OP's original question and is likely to be misleading.",2011-01-25T04:40:59.407,1036,CC BY-SA 2.5,
10168,6486,0,"Just one note on p-values (in relation to my bracketed comment), it could very well be that in the case that you have ""unusual"" data (say p-value of 0.0001).  BUT it may also be the case that it is *even more unusual* under the alternative hypothesis (say a p-value of $10^{-30}$ when you switch the null and alternative hypothesis around).  I believe this can happen when the statistic $T$ is not a sufficient statistic for the  hypothesis test.",2011-01-25T05:18:23.260,2392,CC BY-SA 2.5,
10169,6486,0,"..continuing...It may also be the case that you have very ""good"" data (say p-value of 0.5).  BUT the alternative hypothesis may be *better* (or more consistent) with this data (say p-value of 0.99999 when the null and alternative hypothesis are switched around).",2011-01-25T05:21:11.980,2392,CC BY-SA 2.5,
10170,6466,1,"So, what you are saying is that AIC doesn't sufficiently punish models for parameters so using it as a criteria may lead to overparametrization.  You recommend the use of AICc instead.  To put this back in the context of my initial question, since BIC already is more stringent than AIC is there a reason to use AICc over BIC?",2011-01-25T05:41:24.113,196,CC BY-SA 2.5,
10172,6483,1,"I would speculate that, in restricting the algorithm to combining clusters you may get a poorer result than if you started from scratch, because you disallow ""interactions"" between the geo-spatial and temporal effects.",2011-01-25T06:14:32.527,2392,CC BY-SA 2.5,
10173,6484,0,"@chl & rolando2 - 44 out of 797. All ZResid > 2.58, so i deleted all 44 cases & re-run. As mentioned, still appear 11 outliers. By the way, I thought logistic doesn't require normal distribution?",2011-01-25T07:03:51.967,2793,CC BY-SA 2.5,
10175,6507,0,"@probabilityislogic, Delta method does not give the approximation $E[g(X)]\approx g(EX)$. If $g$ is convex(concave) then you even have the inequality. See http://stats.stackexchange.com/questions/5782/variance-of-a-function-of-one-random-variable/5790#5790 and http://en.wikipedia.org/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables",2011-01-25T07:17:08.073,2116,CC BY-SA 2.5,
10176,6494,0,Hi John.  I believe he might want it on a bootstrap sample.,2011-01-25T07:48:23.630,253,CC BY-SA 2.5,
10179,6511,0,"thanks, and I just found that i can use glm(output ~ NULL, data=z, family=binomial(""logistic"")) for creating a NULL model, and so i can use the lrtest afterwards. FYI, thanks again",2011-01-25T08:04:01.237,588,CC BY-SA 2.5,
10180,6511,3,"@lokheart `anova(model1, model0)` will work too.",2011-01-25T08:05:15.037,930,CC BY-SA 2.5,
10181,6511,5,"@lokheart `glm(output ~ 1, data=z, family=binomial(""logistic""))` would be a more natural null model, which says that `output` is explained by a constant term (the intercept)/ The intercept is implied in all your models, so you are testing for the effect of `a` after accounting for the intercept.",2011-01-25T08:13:47.563,1390,CC BY-SA 2.5,
10182,6513,4,"you try very general models for specific problem. Did you any research before trying these models? Google search on ""forecasting electricity loads"" gives plenty of links.",2011-01-25T08:45:22.490,2116,CC BY-SA 2.5,
10184,6515,1,+1 It's good to know (and it seems I forgot about that package).,2011-01-25T09:28:59.057,930,CC BY-SA 2.5,
10185,6481,2,"Ah, in that case you can definitely ignore ordinal logistic regression, as that's for an ordinal *outcome* variable.",2011-01-25T09:46:37.833,449,CC BY-SA 2.5,
10187,6510,0,"for ""nested mixed random effects model"", do you mean multi-level modelling? if I don't use this for hierarchical variable (my variable are some kind of risk index that higher the number, more risk the patient has, does it suit your case?)",2011-01-25T10:29:23.133,588,CC BY-SA 2.5,
10188,6513,4,There is loads of literature on this problem. Check out http://www.forecasters.org/ijf/journal-issue/320 for a few papers published in the IJF.,2011-01-25T11:57:09.553,159,CC BY-SA 2.5,
10190,6520,0,"If you want an answer to include **R** code, it would probably help if you include your current **R** code. Just the `lmer()` call should be enough.",2011-01-25T12:45:52.203,449,CC BY-SA 2.5,
10191,6507,0,"Yes it does. expand $g(X)$ about $\mu=E(X)$.  Then we have $g(X)\approx g(\mu) + (X-\mu)g'(\mu)$  where $g'(\mu)$ is the derivative of $g(.)$ evaluated at $\mu$.  Taking expectations of both sides gives $E[g(X)]\approx E[g(\mu)] + E[(X-\mu)g'(X)]=g(\mu)$.  So my approximation is valid.  However, it is not the only way to get an approximation.  Perhaps this is not called the ""delta method"", but it is a valid approximation.  Can be improved by adding an extra term in the Taylor series $E[g(X)]\approx g[\mu + \frac{1}{2}\sigma^2 g''(\mu)]$ (which is on wiki page).",2011-01-25T12:52:56.450,2392,CC BY-SA 2.5,
10192,6507,0,"I thought the ""delta method"" only took the first term in a taylor series, but no extra terms.  And in the ""large N"" case $\sigma$ will be small, and basically negligible. so my approximation should give an adequate answer.  Only asked for the ""tail behaviour"" w.r.t $N$.",2011-01-25T12:54:55.310,2392,CC BY-SA 2.5,
10193,6521,0,"the OP observes percentages, so the dependent variable is not binomial. Can you use it in glm anyways?",2011-01-25T12:55:10.677,2116,CC BY-SA 2.5,
10194,6521,0,"The OP said the vacancy rate is computed as the ratio of vacant houses over the total number of houses, which seems to imply he has these data. Vacant houses would then be a binomial outcome, with total houses as the denominator.",2011-01-25T13:05:37.670,449,CC BY-SA 2.5,
10195,6519,0,"Whatever you end up doing, don't ever overwrite your raw data.  Just in case you change your mind, or want to compare methods.",2011-01-25T13:20:24.210,2392,CC BY-SA 2.5,
10196,6513,0,"Do you know why your linear model is not successful?  And you've put a massive amount of predictors into your model - without much thought? - I would find the best 4 or 5 predictors first, and see how they go (in the linear model).",2011-01-25T13:29:43.237,2392,CC BY-SA 2.5,
10197,6524,3,"this site supports latex, please reformat your question. I am reluctant to do it myself, since the notation is not very clear.",2011-01-25T13:41:43.693,2116,CC BY-SA 2.5,
10198,6332,0,"Ok, thanks for all the advice.  I'll use out-of-sample performance on the whole ensemble to choose a model class.",2011-01-25T13:42:48.180,2817,CC BY-SA 2.5,
10199,6461,0,@onestop - I hadn't considered the issue of different signs in the effects. Good call.,2011-01-25T14:04:36.027,364,CC BY-SA 2.5,
10200,6521,0,"@mpiktas I have the house numbers. Actually, what I need is to simulate/forecast the probability for a house in a region to be vacant...",2011-01-25T14:08:36.887,1443,CC BY-SA 2.5,
10201,6527,0,"This solution is a lot nicer than my ""hackish"" try!",2011-01-25T14:13:19.370,2714,CC BY-SA 2.5,
10202,6527,0,"+1, did not know about curve :) Always used plot directly.",2011-01-25T14:13:25.990,2116,CC BY-SA 2.5,
10203,6525,0,"the legend in your graph does not correspond to the graph of function $y=10^6/x$, is this intentional?",2011-01-25T14:15:39.640,2116,CC BY-SA 2.5,
10204,6528,1,"why use loop? df <- within(df,y<-10^6/x) is much nicer and more efficient. Also OP asks for line, so you need to supply type=""l"" in your plot function.",2011-01-25T14:17:10.200,2116,CC BY-SA 2.5,
10205,6529,0,"+1, this is what I was trying to express but was not able to :)",2011-01-25T14:20:10.853,2116,CC BY-SA 2.5,
10206,6527,0,"For completeness, perhaps add `text(2000, 30000, expression(paste(""where xy="", 10^6)))`?",2011-01-25T14:22:52.960,8,CC BY-SA 2.5,
10207,6475,1,"I suggest Maxwell & Delaney, 2004, Designing Experiments and Analyzing Data as a thorough introduction to ANOVA. Chapter 8 discusses the 3-factor case, especially the different types of interaction: http://books.google.de/books?id=h-bMhmQMifsC&lpg=PA423&ots=mMCqiVQXd-&dq=maxwell%20delaney%202004&pg=PA354#v=onepage&q&f=false.",2011-01-25T14:23:32.610,1909,CC BY-SA 2.5,
10209,6527,0,@mpiktas: Good point ;),2011-01-25T14:38:33.287,8,CC BY-SA 2.5,
10210,6525,0,"Thanks @mpiktas. My mistake. 
And my intention is not y=10^6/x. I want to make a graph with x & y values and the mark the points(curve) where the values of x & y satisfies (x*y=10^6). Is it possible? Not sure if this is mathematical correct or not.",2011-01-25T14:54:49.643,1706,CC BY-SA 2.5,
10211,6527,0,But I want to make graph with x & y values and the mark the points(curve) where the values of x & y satisfies (x*y=10^6),2011-01-25T14:56:26.380,1706,CC BY-SA 2.5,
10212,6528,0,"@mpiktas: true! It is terrible, that I fall into a habbit always using loops :( I edit my answer based on your suggestions.",2011-01-25T14:58:31.407,2714,CC BY-SA 2.5,
10213,6527,0,"@Saneef, my comment above is irrelevant now, since you fixed the graph. I will delete it. Now with @csgillespie comment it will be replicated.",2011-01-25T15:01:22.300,2116,CC BY-SA 2.5,
10214,6525,0,For reference: http://en.wikipedia.org/wiki/File:Vector_Video_Standards2.svg I want to plot the line which acts as the boundary for yellow on the lower right (>1M pixels),2011-01-25T15:02:05.527,1706,CC BY-SA 2.5,
10215,6527,0,"@Saneef: to add this curve to an existing plot, use the option `add=TRUE`. See ?curve.",2011-01-25T15:13:03.887,449,CC BY-SA 2.5,
10216,6513,0,"@mpiktas: I did some articles, however they are very implementation-scarce. One I found very instructive is http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V92-3SWYBTY-1&_user=10&_coverDate=06%2F30%2F1997&_rdoc=1&_fmt=high&_orig=search&_origin=search&_sort=d&_docanchor=&view=c&_searchStrId=1619298249&_rerunOrigin=google&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=5f5ef0511a06f6c4e324fdf1faab7b5c&searchtype=a. In this article, they model each hour by a different time series. However, I may not have enough data to do that.",2011-01-25T15:17:40.040,2889,CC BY-SA 2.5,
10217,6513,0,"Also, they got very good results with Regression with autocorrelated errors. I tried to do that with arima() function in R. A lot of the predictors are from that article.

@Rob Hyndman: Thank you for the link, I will look at those articles. Is there an article that you would suggest to start with? I would like it to be as implementation-close as possible.

@probabilityislogic: Yes, my linear model is not succesful probably because the dynamics of my system is not linear. That's why I tried to model the system with arima() - autoregression with autocorrelated errors.",2011-01-25T15:18:40.227,2889,CC BY-SA 2.5,
10218,6505,0,"@Gavin thanks for reminding me, as comparing with stackoverflow, I need to spend more time to ""digest"" the answer before deciding whether the answer is appropriate or not, anyway, thanks again.",2011-01-25T15:32:49.853,588,CC BY-SA 2.5,
10219,6532,0,"Byt the way, for more information about hierarchical agglomerative clustering:

Izenman, Modern Multivariate Statistical Techniques. Springer. 2008.",2011-01-25T16:19:25.943,2902,CC BY-SA 2.5,
10221,6412,1,"No special needs. When you say ""simply reimplementing"" .. that is work.  I want easy.",2011-01-25T17:28:39.597,2849,CC BY-SA 2.5,
10222,4684,2,"It's a little late, but I finally got time to sit down and re-create your argument. The key was ""multinomial coefficient"". I had tried figuring it out using plain old binomial coefficients and I was getting all balled up. Thanks again for a nice answer.",2011-01-25T17:29:09.907,1270,CC BY-SA 2.5,
10223,6415,0,"Does it report fit statistics well (e.g., p values, variance inflation factors, ...)?",2011-01-25T17:29:44.403,2849,CC BY-SA 2.5,
10225,6519,0,That's defenitly a valid point. We are *ver* reluctant to throw away data.,2011-01-25T18:10:01.990,2907,CC BY-SA 2.5,
10228,6538,1,what field of mathematics you received your doctorate? This might be relevant.,2011-01-25T19:42:36.740,2116,CC BY-SA 2.5,
10230,6535,0,"What software are you using? There are different types of 'balance' in ANOVA, and the precise requirements can get rather complicated. Some packages may be able to deal with designs that others can't, but any decent package should give an error, or at least a warning, if the data don't fulfil its requirements.",2011-01-25T19:50:35.697,449,CC BY-SA 2.5,
10231,6538,7,Could you share with us *why* you want to learn stats?  Curiosity?  Needed for a project or research?  Wanting to change jobs?  Need to teach some courses?  Want to collaborate with statisticians as the theoretical person?,2011-01-25T19:51:04.603,919,CC BY-SA 2.5,
10232,6535,0,"@onestop In the past I have used SAS or SPSS. Currently, I'm trying to transition to mostly using R",2011-01-25T19:54:04.313,2322,CC BY-SA 2.5,
10233,6537,0,"@B R I don't know if it will change your answer, but I have edited the question to be more specific.",2011-01-25T19:57:03.717,2322,CC BY-SA 2.5,
10234,6507,0,"@probabilityislogic Although you claim your result holds for ""any"" set of iid RVs, it is obviously incorrect for any distribution with appreciable chance of being negative (such the standard normal), for then the expectation of the minimum must be negative.  Even worse, if the left tail is heavy enough, the expectation of the minimum must diverge with $N$.",2011-01-25T19:58:55.463,919,CC BY-SA 2.5,
10235,1875,1,This is related to a past question: http://stats.stackexchange.com/q/2275/495,2011-01-25T20:19:06.560,495,CC BY-SA 2.5,
10237,6545,0,"Hi Dan, Good initiative, I'll try to contact you.",2011-01-25T21:07:24.870,223,CC BY-SA 2.5,
10239,6507,0,"I did not claim my *final results* hold for any iid RVs.  I said the *distribution of the minimum* is of the form I specified.  And the ""danger"" you speak of (minimum diverging), that clearly does not happen with my answer, it converges to 0 as $N\rightarrow\infty$.  And I am not using the ""standard normal"" I am using a normal centered at $np$ with which is where $F(np)\approx\frac{1}{2}$ comes from.  The normal approx. to the binomial is good, with poor approximations for extreme p (same as the delta method I have used).",2011-01-25T22:19:31.337,2392,CC BY-SA 2.5,
10240,6415,0,"I don't know logistic regression terribly well, so I cannot give a precise answer. However, I can see that the object implements a predict_proba method: http://scikit-learn.sourceforge.net/modules/generated/scikits.learn.linear_model.LogisticRegression.html#scikits.learn.linear_model.LogisticRegression.predict_proba . With regards to variance inflation factors, I am pretty sure that they are not implemented, though they could be computed as a post-processing step.",2011-01-25T22:19:43.877,1265,CC BY-SA 2.5,
10241,6521,0,"is binomial ""model"" not problematic, given that the economic indicators are at country level so the same for all the 100 regions?",2011-01-25T22:24:51.270,1443,CC BY-SA 2.5,
10242,6507,0,"The *only* place where the normal approximation comes in is the approximation $F(np)\approx\frac{1}{2}$, so someone dissatisfied with that, just take $Nnp[1-F(np)]^{N-1}$ instead (which uses the Taylor series linear approximation only).  The approximation ""goes"" in all the ""right"" directions for the problem - $0$ for $N\rightarrow\infty$ or $np\rightarrow 0$ and $\infty$ for $np\rightarrow \infty$ (for fixed $N$).",2011-01-25T22:34:36.630,2392,CC BY-SA 2.5,
10244,6548,0,"Nice answer, better than all the others, because you have *bounded* the expectation. +1 from me!",2011-01-25T22:44:25.963,2392,CC BY-SA 2.5,
10247,6499,0,"Although the answers below differ in their exact form, they all point to *exponential* in $N$ for fixed $n$ and $p$",2011-01-25T23:12:50.853,2392,CC BY-SA 2.5,
10250,6507,0,I would conjecture that the approximation $E[min(X)]\approx \frac{N}{2^{N-1}} E[x_1]$ works pretty well for *any non-negative* RV which *is not too skewed* and has *finite variance*.  This is because the delta method any normal approx. are pretty good in these situations.,2011-01-25T23:16:31.523,2392,CC BY-SA 2.5,
10251,6539,0,"Dear bill_080, thank you for your elaborate answer. I know that more data would help, but unfortunately I have only 11 months of data. Is it possible to construct a model on some other data and use this model for my data? Then, how should one go about tweaking the model when applying it to the new data? Is that feasible at all? Am I looking in the right direction, or am I just doomed because of too little data that I have in my specific case? Thank you.",2011-01-25T23:23:17.250,2889,CC BY-SA 2.5,
10254,6539,0,"@Grega: You're somewhat doomed in that you need more data.  However, the electrical demand data that you need may be easier to find than you think.  For instance, in Texas here's a link: http://www.ercot.com/gridinfo/load/index     What area of the world are you trying to forecast?",2011-01-26T00:12:06.707,2775,CC BY-SA 2.5,
10255,6551,0,"I am aware of te invariance property. My question refered to the fact, that the _likelihood_ at the MLEs does not change. If you for example consider a trivial transform like $x \mapsto 0.1 * x$, then this transform will _reduce_ variance because all the points move _closer_ together. Hence the likelihood will _increase_, because of the factors of the form $1/ \sqrt(2 \pi \sigma^2)$.",2011-01-26T00:16:03.593,2916,CC BY-SA 2.5,
10256,6542,3,"I would suggest It's a must read for *anybody* who wants to be a good statistician, Frequentist, Bayesian or anything else.",2011-01-26T00:16:46.337,2392,CC BY-SA 2.5,
10257,6539,0,"@bill_080: I'm trying to forecast an electricity load of a factory in central Japan. 

My question still stays. Is it possible to devise a model on some other data and apply it to my specific case? But wouldn't dynamics of the system be different then?",2011-01-26T00:24:55.000,2889,CC BY-SA 2.5,
10258,6537,0,"It won't! I don't know enough about advanced ANOVA techniques to say if that changes anything. My answer could really be summarized as ""There may be ways to overcome these difficulties, but why not just use mixed models?""",2011-01-26T00:49:43.337,2739,CC BY-SA 2.5,
10259,6526,2,"You can get this article for free (although may be a slightly different version) here  http://www.isds.duke.edu/~berger/papers/interplay.pdf .  Okay to give a ""free"" link, because its on James Berger's Website.",2011-01-26T01:05:58.543,2392,CC BY-SA 2.5,
10260,6551,2,"Nope, the likelihood will remain *exactly the same* because a jacobian factor will come out of the transformation to make them equal (dx to 0.1 dx)",2011-01-26T01:09:33.093,2392,CC BY-SA 2.5,
10262,6557,0,Could you show an example image from an other package/software or give a more detailed description what you want to plot?,2011-01-26T01:38:14.350,2714,CC BY-SA 2.5,
10263,5454,2,Just don't read the ggplot2 source. It will make your brain bleed,2011-01-26T02:01:58.727,46,CC BY-SA 2.5,
10264,5419,4,I wouldn't recommend reshape - even I don't understand how it works. Reshape2 is much much better and follows many more good development principles.,2011-01-26T02:03:14.717,46,CC BY-SA 2.5,
10265,6558,0,"Just one more thought, I don't know if it's obvious from my initial post. I am not predicting many days (or months/years) in advance. I am trying to predict maximum 16 hours in advance (with 30 minutes sampling, this means 32 points). Does the approach change with that fact?",2011-01-26T02:49:02.787,2889,CC BY-SA 2.5,
10266,6554,0,Have you read the manual and worked through the tutorial?  http://www.mrc-bsu.cam.ac.uk/bugs/winbugs/manual14.pdf,2011-01-26T03:36:38.900,919,CC BY-SA 2.5,
10267,6558,0,"@Grega, I added an answer to your comment as an edit in the above answer.",2011-01-26T04:21:53.023,2775,CC BY-SA 2.5,
10268,6559,0,"your answer indicates that your question is more about R implementation, than about statistics. So you better ask it at stackoverflow.com.",2011-01-26T05:07:11.187,2116,CC BY-SA 2.5,
10269,6508,0,Wow! Thanks for finding a viewable reference‚ÄîI hadn't thought to look on Google Books when looking for a copy of Wooldridge's book.,2011-01-26T05:15:48.880,1583,CC BY-SA 2.5,
10270,6559,0,"what is the purpose of parseCall function? In function getDepVar you call parseCall and then access formula object from parseCall result, why not accessing it directly from the call? When you have formula object you can get the response name using the all.vars function, try all.vars(formula(y~x)[[2]]).",2011-01-26T05:19:39.200,2116,CC BY-SA 2.5,
10271,6559,1,"you probably want lag(y,-1), not lag(y,1). Try cbind(y,lag(y,-1),lag(y,1)) to see the difference.",2011-01-26T05:20:29.997,2116,CC BY-SA 2.5,
10272,6559,1,"and final comment. R has a powerful formula interface, use it.  Look at the code of lm, and functions model.matrix, model.frame, model.response. Using strsplit and parse on formulas and call objects is a bit of trying to fit a square peg into the round hole.",2011-01-26T05:22:42.520,2116,CC BY-SA 2.5,
10273,6539,0,"@bill_080, +1 for detailed and insightfull answer",2011-01-26T06:51:28.617,2116,CC BY-SA 2.5,
10274,6507,0,"@probabilitiyislogic, approximating $Eg(X)$ by $g(EX)$ in general will not work. Pick zero mean variable and $g(x)=x^2$, then $Eg(X)=var(X)$, but $g(EX)=0$. Pick centered random variable without second moment and you will get that $Eg(X)$ does not exist, but $g(EX)$ will still be zero. So if you want to approximate $Eg(X)$ by $g(EX)$ you should be very careful to check properties of $g$ and $X$.",2011-01-26T06:58:05.910,2116,CC BY-SA 2.5,
10275,6565,0,"""additional binary dummy"", meaning I need to create another variable, whereby 1=with loan & 0=no loan? At the same time, still putting in the original Loan variable? If so, 80% of the cases will be treated as missing & only 20% left for analysis. 20% of total sample is too little for me to run logistic regression.  (sorry, i'm still in the learning stage)",2011-01-26T07:01:08.287,2793,CC BY-SA 2.5,
10276,6548,0,"+1 for nice proof. It should be noted however, that for $c^N$ to dominate term $(n-1)d^N$ by a factor of $m$, $N$ should be larger than $\frac{\log m(n-1)}{-\log d}$. For binomial distribution with $n=100$ and $p=0.1$, and the factor $m=10$ this works out at $N\approx 3500$. So this estimate should be used carefully for not so large $N$. On the other hand you can use estimate $c^N+(n-1)d^N$ directly in that case.",2011-01-26T07:32:05.587,2116,CC BY-SA 2.5,
10277,6561,2,"you can get predictions using predict.
dat[,""y0""]<-predict(res,newdata=data.frame(x1=dat[,""x1""],x2=0))
Saves a bit of typing.",2011-01-26T07:38:14.097,2116,CC BY-SA 2.5,
10278,6562,1,"why haven't you tried it yourself? This of course works. There is a typo in your code, capital C instead of c.",2011-01-26T07:45:15.153,2116,CC BY-SA 2.5,
10279,6548,0,"@mpiktas Thanks for your comment. It seems you forgot $c$ in your estimation of the minimal $N$. Note that the OP asked for the *scale* of $E(X_N)$, meaning (presumably) its asymptotics when $N\to+\infty$. Re an actual estimation in the binomial case and for moderately large values of $N$, I would recommend going one step further, that is, using $c^N+d^N\le E(X_N)\le c^N+d^N+(n-2)f^N$ with $f=P(x_1\ge 3)$.",2011-01-26T08:16:17.550,2592,CC BY-SA 2.5,
10280,6552,0,You need to shuffle `listfil` somewhere in your code.,2011-01-26T08:17:49.213,930,CC BY-SA 2.5,
10282,6562,1,"I tried already, but since I have no idea of how to counter-check the result, so I wonder if anyone can enlighten me here, thanks anyway.",2011-01-26T08:54:14.377,588,CC BY-SA 2.5,
10283,6548,0,"$m$ accounts for $c$, but it is not that important.",2011-01-26T09:03:21.967,2116,CC BY-SA 2.5,
10284,6565,2,"They won't be treated as missing, they'll go into estimating the value for no loan.  Maybe you've made no loan 'NA' in which case you need to recode those to 0.",2011-01-26T09:10:07.450,601,CC BY-SA 2.5,
10285,6558,0,"sorry nothing related to the original question, but do you know anything about the australian electricity market? Seams quite stable, especially with all their coal easily available?",2011-01-26T09:57:23.623,1709,CC BY-SA 2.5,
10286,6561,0,"@mpiktas: thank you, I did not know about `predict`, but seems useful.",2011-01-26T10:12:06.087,2714,CC BY-SA 2.5,
10288,6523,1,"Thanks. That's a nice article. It's a start at what I have in mind, but as you say it's very brief.",2011-01-26T13:52:40.467,319,CC BY-SA 2.5,
10289,6526,0,I look forward to reading that one. Jim Berger has deep insights.,2011-01-26T13:53:47.753,319,CC BY-SA 2.5,
10292,6507,0,"@mpiktas- For the first example, how can a *non-negative* random variable have a mean of 0?  Only if it is a constant (=0), then the approx. is exact!  and I said *finite variance* case, which rules out the other example.  I did say when the approximation was good.  Find an example which is in the class I specified in the comment above (non-negative, finite variance, low skewness).  And besides, the approximation works well *in the particular case being described*.",2011-01-26T14:07:36.507,2392,CC BY-SA 2.5,
10295,6565,4,"@John Thank you, that's exactly what I am recommending.  The point is to express the loan values ($X$) in any way appropriate (such as log(amount+1)) and set $X=0$ and $I=1$ for any case without a loan.  This is a standard technique in regression, including logistic regression.",2011-01-26T15:10:15.423,919,CC BY-SA 2.5,
10296,6570,6,"This is not a professional opinion, so I'm putting it in a comment.  As a responder to questionnaires, I always wonder about the validity of results based on questions where no answer corresponds to my opinion.  If you force me to answer arbitrarily, you will get arbitrary results.  If many respondents have the same problem with a question, the *collective* response will be arbitrary, too.  (This can lead to awful decision making if you have no independent way of checking that these results are meaningless.)",2011-01-26T15:16:31.310,919,CC BY-SA 2.5,
10297,6572,0,are you saying that four options is better for a **new product**. Can you elaborate - why do you say that?,2011-01-26T15:37:52.803,1259,CC BY-SA 2.5,
10298,6572,0,The new product is in its initial stage and so there may not be time enough to form an opinion; four options are not good for this case.,2011-01-26T15:53:08.647,2926,CC BY-SA 2.5,
10299,6551,0,"I am not talking about the likelihood of $x$ under the new distribution obtained by plugging in the transformation, but about the likelihood of $t(x)$! That _is_ something different.",2011-01-26T15:59:26.873,2916,CC BY-SA 2.5,
10300,6570,0,"As is I don't think this is a question of statistics. If the question were changed to how do you assess the reliability between the two separate options, that will be more within the domain of the site. Any statement treating one or the other as correct will be based on domain specific knowledge.",2011-01-26T16:01:32.813,1036,CC BY-SA 2.5,
10301,6558,0,"@fRed: I don't know much about Australia.   However, I do know that the power market is changing all over the world.  Coal is cheap and stable, but the CO2 guys are after them.   Combined Cycle plants (gas fired turbines with waste heat recovery) can now achieve fairly high efficiencies.  Things are changing fast.",2011-01-26T16:06:59.730,2775,CC BY-SA 2.5,
10303,6552,0,@chl: I assumed that getting the filenames into a variable was his main question.,2011-01-26T16:15:42.523,2775,CC BY-SA 2.5,
10304,6570,4,"I think OK is very different from No opinion in most situations. If you want to include ""no opinion"" then put it as a 6th option (or remove the OK part)",2011-01-26T16:19:01.680,582,CC BY-SA 2.5,
10308,6485,2,Really - deserves far more votes than it has.,2011-01-26T17:21:08.997,71,CC BY-SA 2.5,
10309,6548,0,"@mpiktas I cannot reconcile your ""$m$ accounts for $c$"" with your ""for $c^N$ to dominate term $(n-1)d^N$ by a factor of $m$"", which, for me, asks that $c^N\ge m(n-1)d^N$, that is, that $N\ge N(m)$ with $N(m)=\log(m(n-1))/\log(c/d)$. But, as you say, *it is not that important*.",2011-01-26T17:24:43.973,2592,CC BY-SA 2.5,
10310,6552,0,"Yes, I just re-read the question and it makes sense as filenames are assumed to follow no particular order.",2011-01-26T18:33:13.260,930,CC BY-SA 2.5,
10311,6580,0,"Hey @tomek, do you mean parameter $b_p$ or parameter estimate $\hat{b_p}$?",2011-01-26T18:34:17.973,1307,CC BY-SA 2.5,
10312,6582,0,What do you mean by 'follow-up comparisons'? Looking at the same three-way interaction in other data sets?,2011-01-26T19:03:15.757,449,CC BY-SA 2.5,
10313,6580,0,@suncoolsu: I menat parameter estimate $\hat{b_{p}}$,2011-01-26T19:05:01.067,1643,CC BY-SA 2.5,
10314,6573,0,"thanks for the explanation. If I understand correctly, what I was missing is that my my priors needed hyperpriors?",2011-01-26T19:30:45.493,2750,CC BY-SA 2.5,
10315,6573,0,@Abe Your BUGS code was syntactically wrong and you didn't define any priors.,2011-01-26T19:38:07.433,307,CC BY-SA 2.5,
10316,6573,0,"I see that the syntax was wrong, but is it not possible to have a single $N(\mu,\sigma)$ prior? or do $\mu$ and $\sigma$ need priors?",2011-01-26T19:59:53.930,2750,CC BY-SA 2.5,
10317,6573,0,"@Abe What do you mean by ""single $N(\mu, \sigma^2)$ prior""? Assuming that you want to estimate both parameters, $\mu$ and $\sigma^2$, one has to assign appropriate prior distributions, e.g. dnorm and dgamma.",2011-01-26T20:08:34.620,307,CC BY-SA 2.5,
10321,6582,0,"I have merged your account, but you should rather register it -- this way it won't get lost. You can do it here: http://stats.stackexchange.com/users/login",2011-01-26T20:59:35.617,,CC BY-SA 2.5,user88
10323,6573,0,sorry. I think I lost this in the translation between paper and code.,2011-01-26T22:15:08.003,2750,CC BY-SA 2.5,
10324,6593,0,"@Tal, I don't use `rpart` and I will leave the answer to more experienced members of this forum.",2011-01-26T22:19:48.603,1307,CC BY-SA 2.5,
10325,6595,0,is there another method that you would recommend?,2011-01-26T22:22:21.473,2750,CC BY-SA 2.5,
10326,6595,0,"@Abe Well, this depends on what you want to do...",2011-01-26T22:26:23.420,,CC BY-SA 2.5,user88
10327,6551,0,"Okay, here's how it goes, start with CDF $F_{T}(t_0)=Pr(T<t_0)$, then we know $T=t(X)$ sub this in to get $F_{T}(t_0)=Pr(t(X)<t_0)=Pr(X<t^{-1}(t_0))=F_{X}(t^{-1}(t_0))$  To get the pdf you have to differentiate *with respect to $t_0$, which leaves us with $f_{T}(t_0)=f_{X}(t^{-1}(t_0)) \times |\frac{\partial t^{-1}(t_0)}{\partial t_0}|$.  It is this Jacobian I was taking about, which makes the ""maximum"" of the likelihood equal, whenever $t(x)$ is a 1-to-1 function.",2011-01-26T23:29:42.133,2392,CC BY-SA 2.5,
10328,6593,0,"I think I've got the idea...
But rpart prints deviance even for regression trees O.o",2011-01-26T23:46:28.543,2902,CC BY-SA 2.5,
10329,6551,0,"...In your example we have $T=0.1X$ so $t(x)=0.1x$ and $t^{-1}(x)=10x$ with derivative 10, and $f_{T}(t)=f_{X}(10t)10$  Which, in the normal case you will have $f_{T}(t)=\frac{10}{\sigma \sqrt{2\pi}}exp(-\frac{1}{2\sigma^2}(10t-\mu)^2)$.  So it would appear you're right.  My thinking now, is that in fixing the variance, you are also fixing the Jacobian to be equal to 1 for certain functions $t(.)$ (i.e. functions who's inverse have derivative 1).  This may explain why it doesn't always work.",2011-01-26T23:50:11.723,2392,CC BY-SA 2.5,
10330,6555,0,"Its not a slow thing to do it this way.  In excel it wouldn't take more than 5 minutes (and you wouldn't need to manually work out the days, just use a date format, and it will have a ""number"" hiding behind it, which you can use in the calculations).  And you should clarify in the question precisely what it is that you want (which I believe you have done, and thank you for doing it).  Otherwise you get people like me telling you something you already know. :)",2011-01-27T00:31:30.360,2392,CC BY-SA 2.5,
10331,6538,1,It would be useful in my work which involves a heavy amount of machine learning. It would also be useful at times as my (very small) company occasionally could use a genuine statistician.,2011-01-27T00:58:41.247,2912,CC BY-SA 2.5,
10332,6565,0,"Thanks! I get your point. Anyway, is imputation work in my case?",2011-01-27T02:54:29.613,2793,CC BY-SA 2.5,
10333,6565,3,"@lcl23 If I understood the situation correctly, imputation makes no sense: your ""missing"" data aren't missing; they indicate no loan has been taken out.",2011-01-27T03:44:40.800,919,CC BY-SA 2.5,
10334,6555,1,"You *cannot* use the google function (FV (..,..,)) to update, because it requires *evenly spaced* intervals, and *equal payments*, both of which you do not have (at least not in the example).  So you could ""in theory"" apply the function individually, but it is just as quick to ""reinvent the wheel"" (especially as you don't understand the syntax of the FV function).  If you *know* how to calculate FV, then you don't *need* a special function to do it for you, unless it would save you a considerable amount of time.",2011-01-27T04:21:17.337,2392,CC BY-SA 2.5,
10335,6564,0,"Ohmigosh, you are so right!  I'm embarrassed I missed this.  Thank you so much for your help: it is much appreciated.",2011-01-27T05:46:37.403,2921,CC BY-SA 2.5,
10336,6568,1,"Measure theory for dummies - that sounds like its written at the right level for me, I'll definitely check it out. Thanks!",2011-01-27T06:19:56.390,1913,CC BY-SA 2.5,
10337,6558,0,yes very interesting topic,2011-01-27T06:27:52.947,1709,CC BY-SA 2.5,
10338,6605,4,@shabbychef: Most of the people grasp it in the _worst_ possible way i.e. probability of making Type I error.,2011-01-27T07:19:05.397,1307,CC BY-SA 2.5,
10339,6564,0,"@D.W. There's no cause for embarrassment.  In fact, your question was worded in an unusually clear manner.",2011-01-27T07:24:57.213,919,CC BY-SA 2.5,
10340,6605,2,I think that's mostly related to how p-values are explained in classes (i.e.: just by giving a quick definition and without specifying what p-values are NOT),2011-01-27T07:34:43.957,582,CC BY-SA 2.5,
10341,6602,0,"As usual great answer, I was looking exactly for this.",2011-01-27T08:43:49.560,1643,CC BY-SA 2.5,
10342,6585,0,"@onestep: This is very good answer to the problem described in the motivation, I haven't heard about it earlier and it seems to be really useful. Thanks!",2011-01-27T08:45:28.687,1643,CC BY-SA 2.5,
10343,6593,0,@deps_stats that deviance is node residual sums of squares summed over the terminal nodes of the tree.,2011-01-27T08:48:07.547,1390,CC BY-SA 2.5,
10344,6601,0,"(I don't know enough to put this as an answer, hence adding a comment.) I always thought it was strange that PI crops up in statistical equations. I mean - what's PI got to do with statistics? :)",2011-01-27T09:16:32.087,1259,CC BY-SA 2.5,
10345,6600,0,Thank you for your comments but I am not sure whether or not I understand. Perhaps the best thing to ask is how to determine the significance of my interactions in my mixed model other than just looking at the t-statistic.,2011-01-27T09:35:32.247,2934,CC BY-SA 2.5,
10346,6608,0,"(+1) Sorry, my post came later and I didn't notice yours. As I think they don't overlap too much, I will leave mine if you don't mind.",2011-01-27T10:02:42.947,930,CC BY-SA 2.5,
10347,6589,0,"Rotation is not limited to FA models: You can use it on any loadings or pattern matrix, provided it makes sense. In fact, there is a `promax()` function in base R which takes as input the loadings matrix, and other oblique rotations methods can be found in the [GPArotation](http://cran.r-project.org/web/packages/GPArotation/index.html) package. This is basically what `principal()` from the [psych](http://cran.r-project.org/web/packages/psych/index.html) package uses.",2011-01-27T10:56:41.070,930,CC BY-SA 2.5,
10348,6601,2,"I'd agree (In my surprisal) - I think its that $\pi$ pops up in many mathematical analysis.  Just a note you can write $\pi$ by with Latex commands as $\text{\pi}$ enclosed within $ signs.  I use the wiki page to get the syntax http://en.wikibooks.org/wiki/LaTeX/Mathematics . Another trick is to ""right click"" on an equation you see on this site, and select ""show source"" to get the commands that were used.",2011-01-27T10:59:43.320,2392,CC BY-SA 2.5,
10349,6605,0,"I think this is mainly to do with how it is introduced.  For me, it was an ""add-on"" to the classical hypothesis test - so it appears as though its just another way to do a hypothesis test.  The other problem is that it is usually only taught with respect to a normal distribution, where everything ""works nice"" (e.g. p-value *is* a measure of evidence in testing a normal mean).  Generalising the p-value is not easy as there is no specific principles to guide the generalisation (e.g. there is no general agreement on how a p-value should vary with the sample size & multiple comparisons)",2011-01-27T11:12:31.860,2392,CC BY-SA 2.5,
10350,6602,0,"The book that You mentioned is available for free in google books, well at least most of it. Nevertheless, have You heard about differentiation with respect to a single attribute? I'm asking because I was 'playing' with matrices lately and I have found that it's quite simple to calculate and I was courious if that is anything new...",2011-01-27T11:18:24.740,1643,CC BY-SA 2.5,
10351,6606,1,"The more I think about confidence intervals, the harder it is for me to think of what kind of question they can answer at a conceptual level that cannot be answered by asking for ""the chance a true value is within an interval, given one's state of knowledge"".  If I were to ask ""what is the chance (conditional on my information) that the average income in 2010 was between 10,000 and 50,000?"" I don't think the theory of confidence intervals can give an answer to this question.",2011-01-27T11:28:22.983,2392,CC BY-SA 2.5,
10352,6610,1,"+1 nice answer chl, which, as you say, complements mine so no problems there. Very nicely put.",2011-01-27T11:54:55.350,1390,CC BY-SA 2.5,
10353,6601,0,"@Wiki If you accept that $\pi$ crops up when you go from measuring the length of a straigh piece of line to the length of a piece of circle, I don't see why it would not appear while going from measuring a probability to fall down on a segment to measuring the probability to fall down in a piece of circle ?",2011-01-27T12:19:01.487,223,CC BY-SA 2.5,
10354,6605,0,"@shabbychef +1 though student often have difficulties with p-values (roughly because the concept in testing is a bit more subtle than a binary decision process and be cause ""inverting a function"" is not easy to aprehend). When you say  ""for some reason"" do you mean it is unclear for you why people have difficulties ? PS: If I could, I would try to make statistics on this site about the relation between ""being a top answer"" and ""talking about p-value"" :) . I also even ask myself if the hardest statistical concept to grasp can have the most upvote (if it is difficult to grasp ... :) )",2011-01-27T12:28:33.483,223,CC BY-SA 2.5,
10355,6609,2,"I don't use SPSS, so I can't help with that, but I think you want to at least explore an ordinal logistic model.  If the assumptions are met, you will have a much simpler model to deal with.",2011-01-27T13:18:58.457,686,CC BY-SA 2.5,
10356,2369,1,"The question as posed is a bit ambiguous, because it does not stated clearly what *information* the 100 people have.  Do they know the distribution in the bag? for if they do, they ""experiment"" is useless, one would just give the interval $[0.1,0.5]$ or even just the two values $0.1$ and $0.5$ (does give required $\text{100%} \geq \text{95%}$ coverage).  If we only know that there are a bag of coins to be drawn from, the Bayesian would specify the whole [0,1] interval, because false positives is *all* that matters in this question (and the *size* of the interval does not).",2011-01-27T13:24:03.727,2392,CC BY-SA 2.5,
10357,6615,0,"Looks good, except (of course) we would need to scale(x1) and scale(x2) first.",2011-01-27T13:54:31.767,196,CC BY-SA 2.5,
10359,6619,1,your are welcome. Note that right clicking on formula will present you pop-up menu with a choice to see the source.,2011-01-27T13:57:49.877,2116,CC BY-SA 2.5,
10360,6620,0,You confirm the answer is aprox. 0.735 ? Seems the problem solution might be wrong.,2011-01-27T14:03:36.170,1833,CC BY-SA 2.5,
10361,2369,0,I would have thought the above argument holds just as valid for the frequentist as well.  The argument above (as far as I can tell) does not invoke any specifically Bayesian or Frequentist principles (although it does invoke the principle of *sanity*).,2011-01-27T14:04:16.780,2392,CC BY-SA 2.5,
10362,6620,0,"@Queops, R code `pbinom(5,20,1/5)-pbinom(1,20,1/5)` gives 0.7350325, so yes.",2011-01-27T14:07:37.510,2116,CC BY-SA 2.5,
10363,6620,0,"@Queops, maybe the problem is to calculate the probability that exactly questions numbered 2 to 5 are answered correctly? Then probability should be calculated differently.",2011-01-27T14:08:55.607,2116,CC BY-SA 2.5,
10365,6620,0,"No no, it's just the number, doesn't matter which ones. And here I was trying to figure out what was wrong. Thanks so much for the help.",2011-01-27T14:10:56.393,1833,CC BY-SA 2.5,
10366,6619,0,@mpiktas: Nice to know that it is possible to see the source.,2011-01-27T14:11:58.943,1643,CC BY-SA 2.5,
10367,6613,0,"in its present form the question is unanswerable, since it is not known what failed. One advice I can give is to reduce your data set and try again. Also look for NA values.",2011-01-27T14:21:10.387,2116,CC BY-SA 2.5,
10368,6613,0,"Is there a word missing in the note at the end? ""1st column is ... and first row are names""",2011-01-27T14:26:50.437,449,CC BY-SA 2.5,
10370,6615,1,"@drknexus Yes, of course (in my initial tests, I used standardized N(0;1) variates, instead of yours).",2011-01-27T14:36:37.733,930,CC BY-SA 2.5,
10371,6619,0,"If you feel @mpiktas's answer helped you to solve your problem, then the best way to thank him is probably to upvote his response.",2011-01-27T14:41:10.147,930,CC BY-SA 2.5,
10372,6561,1,I'd alway recommend using predict instead of calculating the slopes yourself - it's much simpler especially when you have interactions or non-linear components.,2011-01-27T14:55:26.817,46,CC BY-SA 2.5,
10375,6621,0,"Ack, sorry, I totally messed up the shape of my data when constructing the fake example (the real data is HIPAA-sensitive).  My true observations aren't paired, there are a different number of x and y observations in each group.  Maybe I should kill this question and start a new one.",2011-01-27T15:11:53.430,1434,CC BY-SA 2.5,
10376,6622,1,"Since the error states `too many elements specified`, rather than `cannot allocate vector`, it seems that its an address space issue rather than a memory issue (though this would follow). This message kicks in if you attempt to allocate more than $2^31-1$ elements. While this doesn't seem to be the case here, there may be more elements in use than specified in the question above.",2011-01-27T15:42:00.453,229,CC BY-SA 2.5,
10380,2281,7,"@svadalli - the Bayesian approach does not take the view that $\theta$ *is random*.  It is not $\theta$ that is distributed ($\theta$ is fixed but unknown), it is the *uncertainty about* $\theta$ *which is distributed, conditional on a state of knowledge about* $\theta$.  The actual probability statement that $f(\theta)$ is capturing is $Pr(\theta\text{ is in the interval } (\theta,\theta+d\theta)|I)=f(\theta)d\theta$.  In fact, the exact same argument applies to $X$, it too can be considered fixed, but unknown.",2011-01-27T16:14:56.637,2392,CC BY-SA 2.5,
10381,6602,0,"@Tomek Differentiation w.r.t. a single attribute ought to give its coefficient (when there are no interactions).  Of course, since the solutions are functions of all the data, you could differentiate w.r.t any particular $x_{ij}$.  It should be straightforward to work out.  (In fact, finding the solution for OLS would be sufficiently general.)  I haven't read B,K,& W in 20 years, so my memory is dim, but I don't recall that they went down to this level of detail and they definitely don't do this computation in the section I quoted.",2011-01-27T16:22:42.387,919,CC BY-SA 2.5,
10382,2373,7,"The issue with confidence intervals in repeated experiments, is that in order for them to work, the conditions of the repeatable experiment need to stay the same (and who would believe that?), whereas the Bayesian interval (if used properly) conditions on the data observed, and thus provides allowances for changes which occur in the real world (via data).  I think it is the *conditioning rules* of Bayesian statistics which make it so hard to outperform (I think it is impossible: only equivalence can be achieved), and the automatic machinery which it achieves this that make it seem so slick.",2011-01-27T16:40:03.073,2392,CC BY-SA 2.5,
10383,2372,0,"@svadali -  confidence intervals evaluate *data* for a fixed hypothesis.  Thus when changing the ""fixed"" part of the equation, if you fail to take account of the probability of the hypothesis prior to observing your data, then you are bound to come up with inconsistencies and incoherent results.  Conditional probability is not ""constrained"" when changing the conditions (e.g. by changing the conditions you can change a conditional probability from 0 to 1). The prior probability takes account of this arbitrariness. Conditioning on X is done because we are certain X has occured-we did observe X!",2011-01-27T17:11:49.710,2392,CC BY-SA 2.5,
10385,6611,0,"This solution requires no less calculation than the one I gave (unless $e^{rt}$ is less calculation than $(1+r)^{t_0-t}$, I hardly think so).  Another thing is that the interest rate in the PV and FV calculations are not consistent in *r* for in one place you are annually compounding, and another place you are continuously compounding.  your equations imply that $(1+r)^{t}=e^{rt}$.  $r$ in the exponential should be adjusted to $log(1+r)$.",2011-01-27T17:25:36.903,2392,CC BY-SA 2.5,
10386,6618,5,I think John Tukey might disagree http://en.wikipedia.org/wiki/Exploratory_data_analysis ;o),2011-01-27T17:32:28.670,887,CC BY-SA 2.5,
10387,6611,0,"how does the continuous compounding give *proper bounds* when there are both positive and negative terms?  I understand how it does when there are only positive terms. But including negative terms means you have *minimum negative* (max absolute) plus *maximum positive* cash flows, how does this put a ""bound"" on the sum?",2011-01-27T17:36:25.693,2392,CC BY-SA 2.5,
10388,6596,0,"Thank you for your answer, but I am confused- you say 'estimate the spike using the proportion of zeros' but plot it with no bounds. does the spike have a discrete height or is it infinite, if discrete, is it $P(X=0)$?",2011-01-27T17:45:54.737,2750,CC BY-SA 2.5,
10389,6611,0,"another question, in your answer, why did you use an example which was different to the example data given in the question?  ""100EUR with 5% after 3 years"" is an odd example, given that there is *already example data in the question*.  I think if you try to use your method on the example data, and then mine (using the same degree of accuracy in time, whole years in both, decimal years in both,etc.) you will find that the take the same time to do, except that my method gives the exact FV, yours an approximate one.",2011-01-27T17:54:30.843,2392,CC BY-SA 2.5,
10390,6622,0,"Oh, good point.",2011-01-27T17:58:30.450,46,CC BY-SA 2.5,
10391,6618,3,"I would partially disagree here.  I think the caveat that people miss is that *the appropriate conditioning operations are easy to ignore* for these kinds of issues.  Each of these operations change the conditions of the inference, and hence, they change the conditions of it applicability (and therefore to its generality).  These is definitely only applicable to ""confirmatory analysis"", where a well defined model and question have been constructed.  In exploratory phase, not looking to answer definite questions - more looking to build a model and come up with hypothesis for the data.",2011-01-27T18:05:20.880,2392,CC BY-SA 2.5,
10392,6610,3,"Just one remark: For linear regression Deviance is equal to the RSS because the normality assumption of errors implies RSS is same as LR test statistic which further implies Deviance is normally distributed, _irrespective of asymptotics_. This is just an expansion of (I think) terse comment of chl.",2011-01-27T18:11:46.980,1307,CC BY-SA 2.5,
10393,6621,0,I've gone back & revised the data example here to make it unpaired.,2011-01-27T19:54:25.950,1434,CC BY-SA 2.5,
10394,2356,0,"I think, I have seen such an example in the book named ""All of Statistics"" by Larry Wasserman in which he gives an example where using Bayesian CI is _not_ the sensible thing to do. However, it is a pathological example.",2011-01-27T20:00:06.043,1307,CC BY-SA 2.5,
10395,6610,1,"@suncoolsu If it is about my comment about ML and OLS estimates, yes I meant ""assuming a gaussian distribution"" for the $\varepsilon_i$ (i.e., in linear models). Your comment is welcome.",2011-01-27T20:39:07.633,930,CC BY-SA 2.5,
10396,6642,3,"Whether $E(X_1)=\pm E(Y_1)$ or not would yield quite different results. And a hypothesis more precise than ""$X_i$ and $Y_i$ are dependent"" might be needed. (Duplicate from math.stackexchange... You might want to choose one of the two sites and stick to it.)",2011-01-27T21:38:57.457,2592,CC BY-SA 2.5,
10397,6596,0,"This is a mixture of a discrete distribution and a continuous distribution. When plotted as a density, the spike is infinite (actually a Dirac delta function). Sometimes people plot the discrete part as a probability mass function (so then the spike has height $P(X=0)$) and the continuous part as a density function. That probably makes a better visual, but it does involve two different scales.",2011-01-27T23:10:30.477,159,CC BY-SA 2.5,
10398,6622,0,I'm running on 4gb RAM MacBook. Anyways thanx for the comments.,2011-01-27T23:12:18.987,,CC BY-SA 2.5,user2895
10399,6645,11,Use the trigonometric method for this one and then perform a little algebraic simplification.  About the shortest way to express the solution is $u(z) = 2 \sin \left(\frac{1}{3} \arcsin(2 z-1)\right)$.,2011-01-27T23:28:21.850,919,CC BY-SA 2.5,
10402,6644,0,"Thanks for the quick response. All of the stores are included in this analysis and I do wonder what kind of conclusions can be made ""without probability statements or statistics."" For example, if the average sales are higher for Group A but the st. dev. seems high then I would think I might need some sort of statistics to say whether or not the promotion truly ""worked"" for that group of stores.

I've used the t-test but usually in the context of a a true sample of a population and I'm probably just confusing myself since this scenario isn't really a sample of a population?",2011-01-28T00:32:16.653,,CC BY-SA 2.5,user2951
10403,6599,1,great question. I have wondered as well about how to construct the calibration even if you do use 1 versus the rest sort of scheme. If you create k models using 1 versus the rest (and there are k classes) do you have to / should you normalize them somehow so that they sum to 1 (e.g. divide each calibrated probability by the sum of all k)?,2011-01-28T01:10:36.833,2040,CC BY-SA 2.5,
10404,6645,0,I'm afraid this is a bit overwhelming.  In your u(z) the z refers to what?  Some transformation?,2011-01-28T02:04:26.947,2808,CC BY-SA 2.5,
10405,6600,0,"@ joanna - apologies for not tying the end back to the significance side of things.  Once the procedure is finished, you have 1 model, and will act *as if this model is true*.  Hence any interactions remaining in this model can be declared ""significant"" compared to those which don't make it into model.  Because the procedure is for the whole model, this automatically adjusts for the multiple comparisons you want to do.  And you can state that the relationships in this model are the ones which are able to reproduce the data as best as you can out of the alternatives you had.",2011-01-28T03:05:17.270,2392,CC BY-SA 2.5,
10406,6652,4,"This post has some good discussion of the issue of confidence intervals http://stats.stackexchange.com/questions/2356/are-there-any-examples-where-bayesian-credible-intervals-are-obviously-inferior-t/6373#6373 .  The article referred to in the post, I think, helps shed some light one precisely why the above definitions are correct for confidence intervals.  It is often when viewing how CIs break down that one is able to understand them better.",2011-01-28T03:21:31.217,2392,CC BY-SA 2.5,
10407,6636,0,"Pretty well posed question (+1 from me), I like the background context, it makes it easier to judge which assumptions are plausible to make.  I'll have a go at answering, post my answer at some point in the near future",2011-01-28T03:32:16.670,2392,CC BY-SA 2.5,
10408,6645,0,"@rmarimon It's the inverse CDF: that's what your title asks for, right?  It is *identical* to the formula @Erik gives in the range $0 \le z \le 1$, which are the only values that matter.",2011-01-28T03:50:36.273,919,CC BY-SA 2.5,
10409,6651,0,"+1, even though I cannot fathom what the OP is looking for either.",2011-01-28T04:11:43.620,919,CC BY-SA 2.5,
10410,6623,1,"The Sumatra project is one that may be of help too: http://neuralensemble.org/trac/sumatra/wiki.  You can use it's command line interface to run your code, be in R or something else. There is a Python API to it too. There is a nice blog post on R-bloggers discussing R-centric tools for reproducible research, and it mentions using Sumatra as well.         
                                                                        http://www.r-bloggers.com/managing-a-statistical-analysis-project-‚Äì-guidelines-and-best-practices/",2011-01-28T04:59:48.927,1080,CC BY-SA 2.5,
10411,6629,0,"don't think your solution will work because most of the abnormal points have wrong $z$ values, not wrong $x,y$ values",2011-01-28T06:06:30.043,175,CC BY-SA 2.5,
10412,6632,0,`some fixed multiple of the standard error`. This is pretty ambiguous. How do I know what is the correct coefficient?,2011-01-28T06:07:23.057,175,CC BY-SA 2.5,
10413,6634,0,"I think that's the problem; I don't know what value of $\hat{\gamma}(k,k) = \frac{1}{\#\mathcal{N}(k,k)} \sum_{(i,j), (p,q) \in \mathcal{N}(k,k)} | z_{i,j} - z_{p,q} |^2$ would qualify as ""outlier""",2011-01-28T06:08:29.527,175,CC BY-SA 2.5,
10414,6656,0,But what is the question precisely? Can you give more details?,2011-01-28T08:02:57.507,,CC BY-SA 2.5,user88
10415,6658,0,"Just to be clear, you've got 4 tables, each one if of 2 by 2, with number of ""Fatality"" for each population (from the people who got and didn't get treatment).  Is that correct, or are you having it in any way different then what I described?",2011-01-28T08:16:41.187,253,CC BY-SA 2.5,
10416,6660,3,"[Odds ratio versus relative risk](http://www.childrens-mercy.org/stats/journal/oddsratio.asp) offers a good summary of those two measures. Another caveat that is frequently seen is people that apply a logistic regression model, but interpret the results with RR instead of OR.",2011-01-28T09:05:52.983,930,CC BY-SA 2.5,
10417,6632,0,"That will be a trade-off. Setting a lower value will give you more false-positives but fewer false-negatives, i.e. between flagging some points that are fine, and missing some points that aren't. In practice, a suitable value will depend on the number of points in your data set. I'd suggest maybe trying 4 to start with.",2011-01-28T09:13:38.890,449,CC BY-SA 2.5,
10418,6191,2,"You're right, whuber; nevertheless I can't resist sharing another quip I've just stumbled across: ""Those with a taste
for foundational questions are referred to measure theory, an excursion from which few return."" ‚ÄîJames Franklin http://dx.doi.org/10.1007/BF02985802",2011-01-28T10:16:51.673,449,CC BY-SA 2.5,
10419,6655,1,Quick clarification: minimising orthogonal errors rather than perpendicular ones isn't really an estimation strategy decision but a decision about the model that needs estimating - typically one that assumes the existence of measurement error in X rather than one that doesn't. (And you could estimate *its* parameters various ways too.),2011-01-28T11:49:31.727,1739,CC BY-SA 2.5,
10420,6655,1,"I like your idea of a 'best fitting' estimation strategy, but what is being fitted best?  Usually the issue is about the fit of the strategy to the state of the researcher's confidence in her knowledge of the various parametric assumptions she'd like to make, and the level of anxiety she has about them being quite wrong.  The econometrics literature is quite explicit about this as a motivation ML vs GMM vs Robust, etc.",2011-01-28T11:57:44.380,1739,CC BY-SA 2.5,
10421,6654,4,"Thank you for providing an answer to an excellent question. May I bring up the following analogy for further discussion? Suppose I flip a fair coin over and over. Then, $P(Head) = .50$. Now, I flip the coin once, but don't show you what I flipped, and I ask: ""What is the probability that heads are up?"". How would you answer that question?",2011-01-28T12:46:51.633,1934,CC BY-SA 2.5,
10422,6642,0,"@Ian Langmore, I've answered the question in math.SE (http://math.stackexchange.com/questions/19274/limiting-distribution-of-a-squared-sum-of-random-variables/19351#19351)",2011-01-28T12:53:29.073,2116,CC BY-SA 2.5,
10423,6642,0,"hm, what is the usual practice for questions duplicated in two SE sites? Should I post the answer in both sites, and then gather the feedback from two different communities?",2011-01-28T13:01:58.673,2116,CC BY-SA 2.5,
10425,6642,0,@mpiktas Just flag it for mod attention (which is what I've done),2011-01-28T13:17:47.553,930,CC BY-SA 2.5,
10426,6634,0,"@dep_stats , while this is a good answer and I have upvoted, it will be very hard for anyone not already familiar with what a variogram is to follow your notation. I would suggest either expanding upon the definition of the variogram or linking to other references that define how to estimate variograms in more detail.I think this article is a good one on introducing variograms , http://www.geog.ucsb.edu/~chris/readings/Variogram.Interpretation.Modeling.Aide.pdf .",2011-01-28T13:29:02.220,1036,CC BY-SA 2.5,
10427,6642,0,"@chl, judging from the number of votes this question is more interesting to statisticians :)",2011-01-28T13:33:19.823,2116,CC BY-SA 2.5,
10428,2647,12,"You can interpret a continuous density the same as the discrete case if $O$ is replaced by $dO$, in the sense that if we ask for $Pr(O\in(O',O'+dO') |\theta)$ (i.e. probability that the data $O$ is contained in an infinintesimal region about $O'$) and the answer is $f(O'|\theta)dO'$ (the $dO'$ makes this clear that we are calculating the area of an infinintesimaly thin ""bin"" of a histogram).",2011-01-28T13:40:08.867,2392,CC BY-SA 2.5,
10429,6654,5,"Another way to phrase it: for non-Bayesians, the only ""things"" that can have a probability are possible events - in the sense of future outcomes of a random experiment. Given that the parameter has a fixed true value, once you have an interval with specific values, it's not a possible event anymore whether or not the parameter is included in the interval. As a result, you can have confidence in the process that generates the interval, but not in two specific numbers.",2011-01-28T13:50:45.490,1909,CC BY-SA 2.5,
10430,2645,3,"Interesting answer, I actually thought that the ""likelihood school"" was basically the ""frequentists who don't design samples school"", while the ""design school"" was the rest of the frequentists.  I actually find it hard myself to say which ""school"" I am, as I have a bit of knowledge from every school.  The ""Probability as extended logic"" school is my favourite (duh), but I don't have enough practical experience in applying it to real problems to be dogmatic about it.",2011-01-28T13:53:58.340,2392,CC BY-SA 2.5,
10432,6645,2,@whuber thanks.  The u(z) you provided seemed so simple in comparison to the other that it was hard for me to understand that they are the same.  Just plotted both and indeed they are the same.,2011-01-28T14:01:10.383,2808,CC BY-SA 2.5,
10433,6654,2,"@caracal - just some food for thought, is a ""coin flip"" every truly ""random""? If you say ""yes"" then you would reject the idea that whether a coin comes up heads is a deterministic (but complicated) function of many things (say- wind,altitude,force and angle of flip,weight of coin,etc.etc.).  I think this shows the *double standard* of ""randomness"" that applies to CI-based thinking, The *data* are fixed but we are uncertain about its value (ergo **data are random**), while the *parameters* are fixed but we are uncertain about its value (ergo **parameters are not random**).",2011-01-28T14:33:23.813,2392,CC BY-SA 2.5,
10434,6654,0,Very good analogy @wolfgang (+1 from me).  This is exactly the way a CI should be interpreted.,2011-01-28T14:40:02.810,2392,CC BY-SA 2.5,
10436,6629,0,"I think it's a moot point because others have suggested better techniques, but I'm not sure I understand your objection -- I would think the fact that the $z$ coordinates are off is *essential* to why my method would work.",2011-01-28T14:47:16.020,2898,CC BY-SA 2.5,
10437,6618,0,I edited my answer a bit to take into account the comments of Dikran and probabilityislogic. Thanks.,2011-01-28T14:52:12.347,25,CC BY-SA 2.5,
10438,6642,0,"@chl, mpiktas This question is certainly in-scope here, even if it is an exact duplicate on another site.  In fact, it's probably more relevant to our community than the other one.  I'm inclined to suggest that we leave it open?",2011-01-28T14:54:23.890,5,CC BY-SA 2.5,
10439,6642,0,@Shane @mpiktas @Didier It seems more appropriate here (IMHO). Maybe @Ian Langmore could close it on math.SE so that @mpiktas can move his response there?,2011-01-28T15:03:47.230,930,CC BY-SA 2.5,
10440,6654,0,"@probabilitylogistic I agree that the waters become more murky once we are talking about empirical experiments, and not anymore about the statistical concept of random experiments and variables.",2011-01-28T15:13:43.607,1909,CC BY-SA 2.5,
10441,6644,1,"Statistics is still appropriate even if you have data on the entire population if you consider there to be a ""random"" element to the process, both from truly random factors and from other errors from unrecorded characteristics that are randomly distributed in the characteristics you are modelling.",2011-01-28T15:31:11.300,229,CC BY-SA 2.5,
10442,6654,0,"Great discussion. I am not sure though if I understand all the points being made. I brought up the analogy, because I think one could very well answer with ""The probability that heads are up on that particular flip is .5"". I guess under a frequentist framework, this statement is technically not correct (since the event has already occured, so either heads are really up or not), so the only way I can make a probability statement is by reference to the long-run relative frequency. But I just can't help thinking: Well, on \bold{that} flip, the probability is .5 (before I see the actual outcome).",2011-01-28T15:47:53.483,1934,CC BY-SA 2.5,
10443,6644,1,"I'd say it depends on what your goal is: 1)I want to describe *the population I observed* or 2)I would like to describe *a population like the one I observed*.  For 1) you have no uncertainty at all, so statistics just reduces to describing differences and similarities.  But beware: the ""standard deviation"" in this case does not describe the *accuracy* of a difference in means.  In case 1) if the $\overline{x}_A$ and $\overline{x}_B$ are different, then the means *are* different, regardless of what the standard deviation is.",2011-01-28T15:50:03.280,2392,CC BY-SA 2.5,
10444,6666,0,"I think I just had a minor heart attack. I am digesting this right now, thank you so much. I wish I could upvote more than once!",2011-01-28T15:59:50.753,2950,CC BY-SA 2.5,
10445,6644,0,"... just one point of clarification on my comment above, while the means *are* different, this alone tells us *nothing* about **why** they are different, apart from the labels of $A$ and $B$ (or equivalent synonyms of them).  Anything beyond that is not answered by the means, but by the interpretation it is given (which will depend on your prior beliefs about what the potential causes of a difference may be - the obvious stand out being the promotion).",2011-01-28T16:01:19.727,2392,CC BY-SA 2.5,
10446,6656,0,"What equation do you use to calculate the entropy?  Like a ""frequency"" entropy?  What constraints are the maximised entropy based on?  agree with @mbq, this question needs more details.",2011-01-28T16:13:52.633,2392,CC BY-SA 2.5,
10447,6654,4,"@Wolfgang I do not see how your example pertains to confidence intervals.  You do not ask for anything related to a distributional parameter.  Your situation is most closely related to *prediction intervals.*  I think this entire discussion may have some interest in that context, but it doesn't belong in a thread about confidence intervals.",2011-01-28T16:36:12.177,919,CC BY-SA 2.5,
10448,6668,2,"If you mean that you have a density function expressed as a linear combination of Epanechnikov densities and you wish to compute its quantiles directly using the inverse CDF, the answer is no in general.  (Conceivably some special configurations of data--equal spacing comes to mind--could be amenable to such an approach.)  You're best off integrating your density using the CDF of the kernels and numerically finding its quantiles.",2011-01-28T16:45:18.043,919,CC BY-SA 2.5,
10449,6654,3,"@whuber The question whether one can make a probability statement about a particular 95% CI capturing the true unknown parameter is very similar to the question whether one can make a probability statement about a specific flip where the outcome is still unknown. In the long run, 95% of the CIs will capture the parameter. In the long run, 50% of the flips are heads. Can we say that there is a 95% chance that a particular CI captures the parameter? Can we say that there is a 50% chance that heads are up before looking? I would say yes to both. But some people may disagree.",2011-01-28T16:55:44.500,1934,CC BY-SA 2.5,
10450,6635,2,"I rule I use to remember: Use probabilities to predict frequencies.  Once the frequencies have been observed, use them to evaluate the probabilities you assigned.  The unfortunately confusing thing is that, often the *probability* you assign is equal to a *frequency* you have observed.  One thing I have always found odd is why do *frequentists* even use the word probability? wouldn't it make their concepts easier to understand if the phrase ""the frequency of an event"" was used instead of ""the probability of an event""?",2011-01-28T16:58:17.103,2392,CC BY-SA 2.5,
10451,6605,0,"@robin girard - in the spirit of the last part of your comment, one quote I like, I heard it from a comedian Bill Bailey is: *I analyse things too much...or maybe not enough?*.  Another one I like (from me, not Bill Bailey): *I used to be uncertain, but now I'm not so sure...* :).  It does look like p-value is going to win",2011-01-28T17:06:49.267,2392,CC BY-SA 2.5,
10452,6672,5,"Whether you get the same result using frequentist confidence intervals and Bayesian credible intervals depends on the problem, and in particular on the prior distribution used in the Bayesian approach.  It also important in maths and science that when you are right you are right for the correct reason!",2011-01-28T17:20:56.287,887,CC BY-SA 2.5,
10453,6666,0,"let me know if I need to clarify anything.  It would be interesting to see how the results change if we assumed the rate of failure was non-constant, but sinusoidal - i.e. system is cyclical, with ""good"" and ""bad"" streaks.  You would probably need more info to make this kind of improvement work, such as the ""transition times"" - the time points when the system changed from ""up"" to ""down"" and ""down"" to ""up"".",2011-01-28T17:27:00.543,2392,CC BY-SA 2.5,
10455,6246,0,"This site supports LateX commands, you just enclose them in dollar signs.  For example to get $(-\infty,\infty)$ write ""(-\infty,\infty)"" enclosed in $ signs. see [the wiki page](http://en.wikibooks.org/wiki/LaTeX/Mathematics) for syntax, but ignore the 'begin{math}' and 'end{math}' parts, just use dollar sign instead.",2011-01-28T17:33:01.927,2392,CC BY-SA 2.5,
10458,6600,0,"Thanks again for your input. However, I feel like I'm still not being very clear. I thought the initial model would be held true and thus probing the significant interactions of the fixed effects in the model, I could calculate the significance of the multiple comparisons stemming from that significant interaction. If I understand correctly, is what you suggest something along the lines of the Monte Carlo Markov Chain sampling? I apologize for being so dense...MEM is not my strong suit!",2011-01-28T17:39:08.137,2934,CC BY-SA 2.5,
10459,6377,1,You need to state what software package you are using.,2011-01-28T17:54:53.793,8,CC BY-SA 2.5,
10460,6642,0,"My initial thought was to close this question, but as @Shane points out, it is particularly relevant for statisticians. @mpiktas do you want to answer the question (again?)",2011-01-28T17:56:50.313,8,CC BY-SA 2.5,
10461,6642,0,@mpiktas: It's usual for a question to only appear on a single site. But I think their should be some flexibility to this rule.,2011-01-28T17:57:54.073,8,CC BY-SA 2.5,
10462,6654,1,"@Wolfgang You are discussing the meaning of probability rather than the interpretation of confidence intervals.  This is interesting, but in the interest of keeping these comments relevant, I suggest opening a new thread if you want to continue.",2011-01-28T18:04:45.497,919,CC BY-SA 2.5,
10463,6672,4,"If you ""use classical statistics to calculate the 95% confidence interval for [a parameter],"" then, if you are reasoning consistently, it is *meaningless* to refer to a ""probability that the parameter lies in that interval.""  The moment you mention that probability, you have changed your statistical model of the situation.  In the new model, where the parameter is random, it is incorrect to compute a CI using frequentist methods.  Obtaining the right answer this way in some situations is interesting but does not justify the conceptual confusion that underlies it.",2011-01-28T18:13:46.527,919,CC BY-SA 2.5,
10464,6634,0,"I am downvoting primarily because the suggestion doesn't work.  Outliers in a set of ""variograms"" (I use quotes because the formula is not the usual empirical variogram), as indexed by $(k,k)$ (why the double subscript?) will identify *neighborhoods,* not points.  One truly outlying point will contribute to many *large* (not small) outlying values in this set of variograms.  A sharp fault in the slope, such as a cliff (or building, for LIDAR datasets) will also create large numbers of large outliers.",2011-01-28T18:19:34.053,919,CC BY-SA 2.5,
10465,6629,1,"The main problem is that the size of an extreme variation in the $z$ values may be--and usually is--quite a bit smaller than typical spacing between points in $(x,y)$.  Your method would tend to thin the data without necessarily finding any outlying elevations at all.",2011-01-28T18:22:36.950,919,CC BY-SA 2.5,
10466,6632,0,"The idea is good but I think it can fail where slopes are abrupt.  Such areas won't be fit well by local linear fits (which is what LOESS does) and therefore will seem to be outlying.  Rejecting those points will grossly smooth the sharpest features of the terrain.  To make it work you could apply cross-validation methods of Kriging, such as a jackknife calculation (leave-one-out) with residuals standardized by the kriging estimation error.  That's usually a big effort.",2011-01-28T18:26:46.490,919,CC BY-SA 2.5,
10467,6601,0,"@Wiki Whenever you have trigonometric funcions (sine, cosine, tangent etc.) you risk having $\pi$ pop up. And remember that whenever you derive a function you're actually finding a tangent. What is surprising is that $\pi$ doesn't appear *more* often.",2011-01-28T18:36:58.580,666,CC BY-SA 2.5,
10468,6642,0,"@csgillespie, ok, I am reposting my answer here. Maybe we should notify moderators of math.SE to close the question there?",2011-01-28T18:42:15.013,2116,CC BY-SA 2.5,
10469,6677,2,"The two formulas are essentially the same: 1.34 is 1.349 rounded down and 0.9 has been increased to 1.06.  But that change makes little difference in the appearance of smoothness, so we might just as well recognize what's going on and provide a simpler formula: **divide the smaller of two estimates of the standard deviation by the fifth root of the number of observations.**  The two estimates are the sample standard deviation, *s*, and a rescaled IQR (which agrees with *s* asymptotically for normal distributions).",2011-01-28T19:14:25.477,919,CC BY-SA 2.5,
10470,6670,0,"What is Silverman's formula, exactly?",2011-01-28T19:15:11.387,919,CC BY-SA 2.5,
10472,6677,0,"@whuber: yes indeed, the two answers are the same up to a factor of about 15% for any set of data. I am not sure the constant factor in front is irrelevant, however!",2011-01-28T19:49:46.457,795,CC BY-SA 2.5,
10473,6680,1,"table B is not a table, and probably you have the letters mixed up.",2011-01-28T19:55:28.933,2116,CC BY-SA 2.5,
10474,6680,0,"the best you can do is get a list: `tapply(tb$value,tb$name,function(x)x)`, where `tb` is the your table A with 1st and 2nd columns named `name` and `value`.",2011-01-28T20:09:30.893,2116,CC BY-SA 2.5,
10475,6680,0,"yeah its not a table but I need the answer in 2 columns, first one should have the alphabets and the second one should have the corresponding values either in comma separated or tab delimited.",2011-01-28T20:10:36.220,2725,CC BY-SA 2.5,
10476,737,3,Rolf Sundberg attributed this quote to J.M. Hammersley in a 1994 article: http://dx.doi.org/10.1016/0169-7439(93)E0041-2,2011-01-28T20:41:06.567,449,CC BY-SA 2.5,
10480,6682,0,"Thanks mpiktas! As you said the question was not correct in the way I wanted, I agree with you but you answer is what I needed.Thanks again",2011-01-28T21:06:11.307,2725,CC BY-SA 2.5,
10481,6656,0,"I use this: http://upload.wikimedia.org/math/a/2/f/a2f05485301595188046d986c8cdd705.png. It is probability-based, as it compares the frequency of a state to the probability of a state. The maximum entropy is simply the highest possible entropy for the list, and occurs when the frequency for the two states is equal. The minimum occurs when it is all one state. The specific question is whether it would be ""correct"" to set the frequencies in the initial condition to produce an entropy that is an average between the minimum and maximum, to allow for equal change in entropy in either direction.",2011-01-28T21:10:10.057,,CC BY-SA 2.5,user2976
10482,6611,0,prob.*: updated.,2011-01-28T21:15:50.940,2914,CC BY-SA 2.5,
10483,6682,0,"@Jana, you're welcome. If this is the answer you need I suggest you accept it, this is how this site works. Look at the faq how to do that. You might want to accept answers to your other questions.",2011-01-28T21:21:29.900,2116,CC BY-SA 2.5,
10484,6677,2,"Given that these formulas are approximate and have optimal properties only (a) asymptotically and (b) for Normally distributed data, a 15% difference is immaterial.  Remember, too, that they are ""optimal"" only in the sense of reducing mean squared error, which tends to impose a lot of smoothness.  They are not necessarily optimal for other purposes.  That's why the OP should feel free to depart from such recommendations: they should be considered initial values only.",2011-01-28T21:36:51.203,919,CC BY-SA 2.5,
10487,6540,0,Just emulate http://en.wikipedia.org/wiki/Net_present_value#Example,2011-01-28T22:20:53.020,919,CC BY-SA 2.5,
10488,6629,0,I see. In my mind the variation in $z$ was pretty big. Thanks for both of your comments.,2011-01-28T22:31:18.363,2898,CC BY-SA 2.5,
10489,6682,0,@Jana: click on the checkmark just below the vote up/down buttons.,2011-01-28T22:51:25.547,582,CC BY-SA 2.5,
10490,4559,0,@chl: upps- today(end of january) I came back to this question/conversation of mid-november.... Sorry I didn't check earlier - there was something with my courses and then I forgot the stat.exchange... If I've something worth I'll come back next days.,2011-01-28T23:23:04.190,1818,CC BY-SA 2.5,
10491,6686,0,"If I use names(dat) instead of c(xVar,traceVar) when renaming newdata prior to using predict, then predict doesn't do what I'd expect. I'm sorry that my previous example code was unclear.  Now, I've provided a more clear example above.  What I want is to find the predicted y value at the mean, 1 SD above of the mean, and 1 SD below the mean for each predictor.",2011-01-29T01:46:37.320,196,CC BY-SA 2.5,
10493,4560,0,"I'm working on a project that uses stepwise regression.  The reason is because I have D >> N, where D is dimensionality and N is sample size (thus ruling out using one model with all the variables), subsets of the features are highly correlated with each other, I want a statistically principled way of selecting maybe 2-3 ""best"" features, and I don't intend to report the P-values, at least without some kind of fairly conservative correction.",2011-01-29T04:16:14.197,1347,CC BY-SA 2.5,
10494,6618,1,"For me, the ""excluding outliers"" is not as clearly *wrong* as your answer implies.  For example, you may only be interested in the relationships at a certain range of responses, and excluding outliers actually helps this kind of analysis.  For example, if you want to model ""middle class"" income, then excluding the super rich and impoverished outliers is a good idea.  It is only the outliers within your frame of inference (e.g. ""strange"" middle class observations) were your comments apply",2011-01-29T06:10:08.530,2392,CC BY-SA 2.5,
10495,6682,0,"@Jana, you might also want to read the [faq](http://stats.stackexchange.com/faq). It gives a pretty good summary how this site works.",2011-01-29T06:20:50.847,2116,CC BY-SA 2.5,
10496,6690,0,"I would like to preface the question by saying, I know I should have gone to r-help for questions like these. However, I know that some of the gurus are members of this forum as well. Also, I fear total destruction on r-help and r-dlevel for asking simple questions like these.",2011-01-29T06:51:29.437,1307,CC BY-SA 2.5,
10497,6691,0,"Ok - so probably, I should have made it clearer, x is supposed to `integer`, right? Any specific reason it's double?",2011-01-29T07:03:49.603,1307,CC BY-SA 2.5,
10500,6691,0,"@suncoolsu, you are welcome.",2011-01-29T07:19:11.437,2116,CC BY-SA 2.5,
10504,6666,0,"@probabilityislogic It took me awhile, but I am pretty sure I grasped the $\hat{F}$ calculations, however, I am fairly stumped with the $F_{obs}$ data column. Let's say we are looking at $F$ failures of 1 second or less (total downtime of 500,000 and 1,000,000 ""switches""). According to the first formula, to calculate the probability, the result is $$1-\frac{31056926}{31556926}\Bigg(\frac{31056926}{(1,000,000*30)+31056926}\Bigg)^{(500,000/1,000,000)+1}$$ but that yields a probability of $0.643$ for me vs. $0.625$ found in your table. Am I missing something obvious? And also is $S = F * 30$?",2011-01-29T07:44:53.733,2950,CC BY-SA 2.5,
10506,6666,0,"To add to the comment above, I do not think I understand the $F_{obs}$ quite well. If $F_{obs}$ stands for ""number of down periods"", then for $F = 1$ the $F_{obs}$ *should* be 1 as well. Unless of course $F_{obs} = F / T_D$ in which case we match at the $F = 1$ but not $F = 1,000,000$ (my P yielding 0.87)",2011-01-29T07:59:55.753,2950,CC BY-SA 2.5,
10507,6684,0,these types of questions might get better answers in http://stackoverflow.com.,2011-01-29T08:16:27.137,2116,CC BY-SA 2.5,
10508,6689,1,Have You ever used LARS? I'm asking because I have never heard about it earlier and it sounds really interesting. The orginal article is a bit long (93 pages) so I'd like to get some opinion before I get deep into it.,2011-01-29T11:26:27.570,1643,CC BY-SA 2.5,
10509,6684,0,"If `xxA` and `xxB` are standardized before calling `lm()`, it seems to work. E.g., `dat <- cbind.data.frame(dat, xxAstd=scale(dat$xxA), xxBstd=scale(dat$xxB)); lm.res.scale <- lm(out ~ xxAstd*xxBstd,data=dat); names(newdata) <- c(""xxAstd"",""xxBstd""); newdata$Y <- predict(lm.res.scale,newdata)`. Not sure it's what you seek, though.",2011-01-29T11:50:42.633,930,CC BY-SA 2.5,
10510,6666,0,"@terry - for your first question, $S=30$ in your case, because you want the probability of $\theta$ (actual time of first failure) being between $0$ and $S$.  The $(1,000,000*30)$ should be replaced by $30$.  And the power should be raised to $F_{obs}+1=1,000,001$, not $\frac{500,000}{1,000,000}+1=\frac{3}{2}$.  So correct form is: $1-\frac{31056926}{31556926}\Bigg(\frac{31056926}{\textbf{30}+31056926}\Bigg)^{\textbf{1,000,000}+1}$.  Which equals 0.625 (I double checked just to be sure).",2011-01-29T12:44:04.710,2392,CC BY-SA 2.5,
10512,6666,0,"@terry - clarification about $\hat{F}$ and $F_{obs}$.  The difference is that $F_{obs}$ indicates you actually know the number of failures which occurred (i.e. you know that the $T_D=500,000$ seconds of down time was caused by *exactly* $F_{obs}$ failures).  When using $\hat{F}$, you are *uncertain* about the precise number of failures, but you expect it to be $\hat{F}$.  Mathematically, it is $E(F_{obs}|\hat{F})=\hat{F}$, where the expectation is taken over your *uncertainty* about $F_{obs}$ (it is the *uncertainty* which is distributed, $F_{obs}$ is a fixed unknown constant).",2011-01-29T13:07:22.333,2392,CC BY-SA 2.5,
10513,6600,0,"@joanna - you are considering different models, therefore you must be uncertain about what the true model is.  If you believe the initial model is *already true*, then by definition, everything in the initial model is significant.  I don't think this is what you believe though.  I think that the initial model specifies a *class* of models (i.e. all variables and their interactions), one of which you believe to be true (or at the very least, you are prepared to act *as if* the true model is in the class you specified)...see next comment for more...",2011-01-29T13:16:28.597,2392,CC BY-SA 2.5,
10514,6600,0,"...cont'd... Thus you are *partially* uncertain about the true model.  The procedure above gives you a coherent way to choose a model from this class (p-values give you a quick way).  And yes it is an Monte Carlo algorithm, but it is not a markov chain, because each cycle is independent of the other (more like a bootstrap than MCMC).  Note it need not be a sampling based method, you could also do a ""jacknife"" based evaluation (split data into $G$ groups, and predict the $gth$ group based on the model fit to remaining $G-1$ groups)",2011-01-29T13:28:48.913,2392,CC BY-SA 2.5,
10516,6666,0,"@terry-just to add to your comment, yes when $\hat{F}=1$ this implies $F_{obs}=1$, and my equations reflect that the probability is the same regardless of which one you use:
$$1-\frac{T_U}{T_U+T_D}\Bigg(\frac{T_U}{T_U+S}\Bigg)^{1+1}=1-\frac{\Bigg(\frac{T_U}{T_U+T_D}\Bigg)\Bigg(\frac{T_U}{T_U+S}\Bigg)^2}{1-(1-1)\Bigg(\frac{T_U}{T_U+S}\Bigg)}$$
This is a good ""common sense"" check that the calculations are sound.  Also another example of the coherence of Bayesian methodology (when things should be equal, they are equal *automatically*, a property which never ceases to amaze me).",2011-01-29T13:43:15.750,2392,CC BY-SA 2.5,
10517,6684,0,"Yeah chl, I noticed that, but was disappointed I couldn't call scale from within lm and have predict work.",2011-01-29T16:34:56.097,196,CC BY-SA 2.5,
10518,6668,1,"[This paper](http://www.uv.es/~bernardo/Kernel.pdf)  may be useful as a ""structural"" guide to what you want to do, but you will need to adapt it to the particular distribution you used.  It uses a normal mixture, rather than a Epanechnikov mixture",2011-01-29T16:41:52.833,2392,CC BY-SA 2.5,
10519,6694,0,"Thanks for the advice.  There are three problems with this, though:  

1.  I care about quantifying the uncertainty in my predictions and the contribution of each variable, not just binary prediction accuracy.

2.  Given the nature of my dataset, it's way too computationally intensive.

3.  From domain knowledge, I believe that the local optima issue is not important.",2011-01-29T17:18:40.337,1347,CC BY-SA 2.5,
10520,6656,0,"Hang on, to have maximum entropy of $50$, this means you have $n=e^{50}=gozillian$ categories? is that right? and how many times will you observe the system?",2011-01-29T17:44:45.573,2392,CC BY-SA 2.5,
10521,6672,4,"@whuber - your premise ""...if you are reasoning consistently..."" has a consequence from the good old Cox's theorem.  It says that if you are reasoning consistently, then your solution *must* be mathematically equivalent to a Bayesian one.  So, given this premise, a CI will necessarily be equivalent to a credible interval, and its interpretation as a probability is a valid one.  And in Bayes, it is not the parameter which has a distribution, it is the uncertainty about that parameter which has a distribution.",2011-01-29T17:58:00.630,2392,CC BY-SA 2.5,
10522,6672,2,"...cont'd...So one can play the silly game of I'm a Bayesian ""Prob that parameter is in the interval"", I'm a frequentist ""prob that interval covers parameter"", I'm a Bayesian..., I'm a frequentist,..., I'm a Bayesian..., I'm a frequentist,..... all the while the numbers of the actual calculation never change",2011-01-29T18:01:32.313,2392,CC BY-SA 2.5,
10523,6694,1,"How can it be computationally intensive, when you have less than $100$ observations? The drop-one-jacknife is very quick for small $n$.  And the fraction $F$ is a measure of the uncertainty of your predictions, namely the probability that you will classify correctly given your training data, and your selected model.",2011-01-29T18:16:42.813,2392,CC BY-SA 2.5,
10524,6694,0,"Actually you're right.  This procedure would be part of a larger codebase and I forgot that some of the rest of the code wouldn't need to be re-run for every jackknife iteration.  The other two points still apply, though.",2011-01-29T18:31:19.093,1347,CC BY-SA 2.5,
10525,6689,0,"@Tomek Tarczynski: I have used it a small amount. There is a package in Matlab (I am sure there is one or more in R), which I have used. It also provides a sparse PCA, which I was more interested in. I admit I only skimmed the paper. ;)",2011-01-29T18:35:01.660,795,CC BY-SA 2.5,
10526,6688,3,"Is there a reason not to go with a penalized regression approach (lasso, elasticnet, etc.)?",2011-01-29T18:36:10.113,2126,CC BY-SA 2.5,
10527,6656,0,"No, a list of length 100 with 2 states is enough. $50=-\frac{50*0.5*\log{0.5}}{\log{2}}-\frac{50*0.5*\log{0.5}}{\log{2}}$",2011-01-29T18:49:26.250,,CC BY-SA 2.5,user2976
10528,6618,2,"Ultimately the real problem with the issues raised in the initial answer is that they (at least partially) invalidate p-values.  If you are interested in quantifying an observed effect, one should be able to do any and all of the above with impunity.",2011-01-29T19:24:22.703,196,CC BY-SA 2.5,
10529,6697,1,"Hmm, the much-cited Benjamini-Hochberg JRSSB paper was published in 1995, so just *outside* the window I'm afraid! http://www.jstor.org/stable/2346101
Storey's paper that introduced $q$-values was 2002 though. http://dx.doi.org/10.1111/1467-9868.00346",2011-01-29T20:11:37.260,449,CC BY-SA 2.5,
10530,6695,1,Thanks. I was actually using Rf_dpois in a C++ program using `Rcpp` and testing it using `inline`. I have been been a user for about a week. But I very happy with the way I can test my C functions with `inline`. It turns out I am not smart enough to see this simple proof of concept. Thanks again!,2011-01-29T20:54:17.300,1307,CC BY-SA 2.5,
10534,6642,1,"Sorry for the duplicate question...I'm claiming newbie over-enthusiasm...In any case, if people agree that the question is better on stats.SE, I happy with it being closed on math.SE.  Not sure if I can do this though...I could vote to delete it.",2011-01-29T21:24:51.103,2952,CC BY-SA 2.5,
10535,6661,0,Onestop: This is certainly the spirit of the question I was aiming for. And thank you for the clarification between criteria of evaluating estimators and methods for deriving them!,2011-01-29T21:38:17.273,1118,CC BY-SA 2.5,
10536,6699,1,"So Dirk, in this example the annual standard error of stock returns would simply be: 2% times SQRT(12) = 6.93%.  Can you confirm that is correct.  It seems so straightforward, I am afraid I missed something.",2011-01-29T21:45:58.250,1329,CC BY-SA 2.5,
10537,6699,2,Correct. See the edit I just made to my initial answer.,2011-01-29T21:54:20.183,334,CC BY-SA 2.5,
10538,6601,0,"@Carlos I suspect the prevalence of $2\pi$ is mostly due to the use of the $\ell^2$ metric, leading to n-spheres. In the same vein, I would expect it's $e$ whose prevalence is due to analysis.",2011-01-29T22:00:38.143,2456,CC BY-SA 2.5,
10540,6607,3,"This was the question I found most distracting after statistics 101. I would encounter many distributions with no motivation for them beyond ""properties"" that were relevant to topics at hand. It took unacceptably long to find out what any represented.",2011-01-29T22:12:34.160,2456,CC BY-SA 2.5,
10541,6617,0,"Measure theory is neither a field of statistics nor hard. Some types of integration are hard, but, once again, that isn't statistics.",2011-01-29T22:12:59.570,2971,CC BY-SA 2.5,
10542,6701,3,I was interested in binary because it was easiest to analyze. I have found a very broad sufficient condition in works of Lauritzen -- you get closed form if a corresponding log-linear model is decomposable,2011-01-29T22:16:24.243,511,CC BY-SA 2.5,
10543,6605,0,"If the null hypothesis is ""your girlfriend hasn't been cheating on you with your best friend,"" and you catch them hugging, cuddling and holding hands in a dark place, then the p-value is **really** low. Unless she is ugly, in that case, the p-value might still be high. I'm completely sure even high-school students could understand p-values that way.",2011-01-29T22:16:33.857,2971,CC BY-SA 2.5,
10544,6699,2,"This is the right approach, but for stock returns (and for many other time series) serial correlation has an important influence.  Ignoring it (by using $\sqrt{N}$) is going to be too optimistic.",2011-01-29T22:16:51.843,919,CC BY-SA 2.5,
10545,6661,0,"Thanks. I'm a bit surprised no-one else has chipped in though - that was written pretty much off the top of my head, and i'm certainly no expert in estimation theory.",2011-01-29T22:40:44.193,449,CC BY-SA 2.5,
10546,6699,1,"Dirk, thanks much for the confirmation and the real data application.  To whuber, I know Benoit Mandelbrot argues there is a long term serial correlation of returns.  I remember looking at real data, and I saw no evidence of serial correlation using lags from - 1mth to - 12 mth.  Thus, I am not convinced that Dirk's approach is incorrect.  In any case, my main concern was understanding how to convert a monthly standard error into an annual one.  And, Dirk did a good job at giving me the straightforward answer to that.",2011-01-29T23:07:25.997,1329,CC BY-SA 2.5,
10547,6700,0,"Thank you. I got the answer. I request you for one more advise. Now I have some more statistics. I want to compare number of deaths and number of cases of a disease from three different populations Each population received a different treatment. I have year wise data from 2003 to 2010. I want to compare the effect of these two treatments for each year and also total eight years and know whether the difference is significant.
Kindly advise which is the best statistical test.",2011-01-29T23:27:57.297,2956,CC BY-SA 2.5,
10548,6694,0,"@dsimcha - if all you're doing is building a binary classifier, why would you care about anything other than the predictions it makes?  Surely this has to be the number 1 priority, with the contribution from each variable only needed to help understand *why* it is the best classifier.  A way you can get the contribution for each variable, is to re-calculate $F$ removing each variable, one at a time, and check how this compares to the $F$ for the chosen model ($F_{chosen}-F_{(-j)}$ is like a marginal effect for the *jth* variable on the predictive accuracy due to each variable)",2011-01-30T01:10:52.200,2392,CC BY-SA 2.5,
10549,6635,0,"Interestingly, cross validation can be seen as a Monte Carlo approximation to the integral of a loss function in Decision Theory.  You have an integral $\int p(x) L(\textbf{x}_{n},x) dx$ and you approximate it by $\sum_{i=1}^{i=n} L(\textbf{x}_{[n-i]},x_i)$ Where $\textbf{x}_{n}$ is data vector, and $\textbf{x}_{[n-i]}$ is the data vector with the *ith* observation $x_i$ removed",2011-01-30T01:30:56.060,2392,CC BY-SA 2.5,
10550,6618,0,"@drknexus - I wouldn't say they invalidate the p-value per say, rather that the null hypothesis that it is based on is implicitly changed",2011-01-30T01:38:37.953,2392,CC BY-SA 2.5,
10551,6605,0,"@Eduardo - a reasonable alernative hypothesis ""your girlfriend was not chased by a rapist into a dark place, and your best friend found her and got the rapist to leave"".  If you see them hugging, cuddling, and holding hands in that dark place, this is inconsistent with the above hypothesis (unless your best friend or your girlfriend are uncomfortable about touching people).  Hence it too will have a small p-value",2011-01-30T01:52:45.893,2392,CC BY-SA 2.5,
10552,6605,0,@probabilityislogic: Well. At least we both understand p-values.,2011-01-30T01:59:20.543,2971,CC BY-SA 2.5,
10553,6605,0,"@Eduardo - I don't really get the ""ugly"" argument.  ""Ugly"" people have sex, and for some people the saying ""forbidden fruit is the sweetest"" holds true.  Another thing, is that it is your best friend who needs to think she is ugly for it to apply, and you cannot be certain of this (unless you can read his/her mind).",2011-01-30T02:02:15.147,2392,CC BY-SA 2.5,
10554,6605,0,@probabilityislogic: I was just trying to use an example even the most mentally-challenged high-school student could understand.,2011-01-30T02:09:04.250,2971,CC BY-SA 2.5,
10555,6700,0,"Interesting.  And more complicated.  So now your outcome variable has 3 values:  healthy, ill, or died?  It seems as if you could run chi-square tests separately within each year and compare the residuals or the measures of association across years.  Another level of sophistication would involve, for each year, a multinomial logistic regression, and finding a way to compare the coefficients across years.  Someone out there may be able to suggest another way using something like loglinear modeling.",2011-01-30T02:09:23.617,2669,CC BY-SA 2.5,
10556,6605,0,"@eduardo - I think the problem that comes about is that the data are insufficient for the problem.  If you replaced observing ""hugging, cuddling, and holding hands"" with ""walking in on your best friend and girlfriend having sex"", then it would be much harder to come up with alternatives (although having thought about it, an alternative could be assault or blackmail)",2011-01-30T02:10:00.007,2392,CC BY-SA 2.5,
10557,6605,0,@probabilityislogic: This is rapidly going off-topic. We might argue elsewhere.,2011-01-30T02:11:02.530,2971,CC BY-SA 2.5,
10558,6605,0,"@eduardo - I'd say the example is off topic, but the analogy isn't.  This is because you can always come up with the hypothesis of ""honesty"" - i.e. you can get a small p-value by saying the hypothesis is ""these data were not changed to be consistent with a different hypothesis"".  Because the probability of observing the particular data you observed is always small, the p-value will also be small.  hence it is only by using your *prior information* about the integrity of the data, that you can eliminate these kinds of hypothesis.",2011-01-30T02:18:22.710,2392,CC BY-SA 2.5,
10559,6605,0,"@probabilityislogic: I know. I never claimed that the p-value would be zero. But a small enough p-value is sufficient to claim ""Bullcrap!"" when you are (1 minus p-value) sure they lying to you. Of course, how small does ""small"" mean to you is a matter of policy, and statistics doesn't deal with that.",2011-01-30T02:32:42.013,2971,CC BY-SA 2.5,
10560,6656,0,"I would have thought that if a list can only have 2 states, then its maximum entropy (in nits) is $log(2)=0.693$ (or 1 bit).  Or is it that each element in the list can have 2 states for a total of $2^{100}$ possible lists?  This has maximum entropy of $100 log(2)$, which is not equal to $50$ (in bits or nits).  Where does the $50$ come from in your equation, and why did you divide by $log(2)$?  neither of the $50$ or the division by $log(2)$ is in the formula in your link.",2011-01-30T02:47:23.767,2392,CC BY-SA 2.5,
10561,6708,0,"+1 Thank you very much. That does the trick. Sorry about not specifying the `fitdistr` package. I'm quite new to R so I will keep that in mind for the next time. I have accepted this as the answer but as a final note, can you please tell me if there is a numerical measure that tells me how well the data could be fitted using the distribution?",2011-01-30T03:12:38.407,2164,CC BY-SA 2.5,
10562,6708,0,"In addition, is there a way to limit the upper limit of the sampling to the maximum value in the data samples? Otherwise, the Q-Q plot seems to be showing some numbers way out of the data sample range.",2011-01-30T03:31:27.780,2164,CC BY-SA 2.5,
10564,6700,0,Thank you. There is some communication gap. I will clarify. I am getting more data from 3 to 4 regions. Now I have survived and died cases from different regions. One region reported more deaths because of not giving treatment thoroughly. one region reported zero deaths. some regions had only 1 case and no deaths. I have data from 2003 to 2010 year wise and region wise. I am a medical doctor with limited knowledge of statistics.,2011-01-30T04:40:04.043,2956,CC BY-SA 2.5,
10565,6607,1,"Maximum entropy ""thinking"" is one method which helps understand what a distribution is, namely a state of knowledge (or a description of uncertainty about something).  This is the only definition that has made sense to me in all situations",2011-01-30T04:57:47.983,2392,CC BY-SA 2.5,
10567,6605,1,"@eduardo - yes a small enough p-value is sufficient to cast doubt on the null hypothesis: but it is calculated *in complete isolation* to an alternative.  Using p-values alone, you can never formally ""reject"" $H_0$, because *no alternative has been specified*.  If you formally reject $H_0$, then you must also reject the calculations which was based on the assumption of $H_0$ being true, which means you must reject the calculation of the p-value that was derived under this assumption (it messes with your head, but it is the only way to reason *consistently*).",2011-01-30T05:14:36.433,2392,CC BY-SA 2.5,
10568,6605,0,"@eduardo - using p-values in hypothesis testing is similar to the logical case of requiring a set of axioms to prove their own consistency.  It just can't be done.  However you can assess a given *theorem*, and work out which sets of axioms are consistent with the theorem. (axiom=hypothesis, theorem=data)",2011-01-30T05:19:48.720,2392,CC BY-SA 2.5,
10570,6708,1,"@Legend: As to your first comment, it should really be a separate question.  I can point you towards the Kolmogorov-Smirnov test (https://secure.wikimedia.org/wikipedia/en/wiki/Kolmogorov_Smirnov_Test), but I encourage you to ask this question, both because you may get better advice and because the answer will be more findable by others).",2011-01-30T06:06:31.890,2975,CC BY-SA 2.5,
10571,6708,0,"@Legend: As to the second point, I'm not sure what you're really asking.  (This usage of) a QQ plot compares the distribution of some data to a mathematical function -- so there's no ""sampling"" involved.  The fact that the high end of the plot has values higher than those in the data shows you exactly what you are using the QQ-plot to test for: that the shape of the tail of the distribution of the observed data does not match the tail of the theoretical distribution.  (Also, I don't see any ""out of range"" values with this code and your data from the question...how did you generate that data?)",2011-01-30T06:09:48.770,2975,CC BY-SA 2.5,
10572,6605,0,"@probabilityislogic: Again, I know. If the null hypothesis is wrong, then the test statistic is meaningless.",2011-01-30T06:18:25.767,2971,CC BY-SA 2.5,
10573,6715,0,"Nice question, its a good example to expose to different procedures, because we basically know without any maths or formality, that Environment 4 and 6 are different to the rest (and Environment 1 is a little different from 2, 3, and 5).  Thus any good procedure should be able to produce the obvious result, only difference coming from quantifying *how different* in a mathematical sense.  The obvious question is ""is there any other way the experiment could have actually produce these results, besides an error?""",2011-01-30T06:28:48.883,2392,CC BY-SA 2.5,
10574,6708,0,"Thank You. I will look into the Kolmogorov-Smirnov test and post another question if it is not clear. Regarding my second question, the highest data point in my sample is 170 and there are 89 points. Now, if I use the function you specified, it is producing 89 points but some points are above 170 (181.38, 207.32...). While this can happen, I was just wondering if there is a way to have some upper limit so that it will sample only within a range (0,170) so that the Q-Q plot will contain equal limits on the X and Y axis.",2011-01-30T06:31:42.337,2164,CC BY-SA 2.5,
10575,6715,0,"@probabilityislogic: Thank You. What you say is useful: if I can somehow quantify the effectiveness of the experiment in each environment, then may be I can say something but am still not sure what to say or how to say. Ah.. (...feeling stupid typing in puzzles) :) Regarding your question: the experiment was quite controlled in the sense that, it was made sure that the environment did not change. However, the procedure could have gone wrong. May be the procedure was not executed properly according to the guidelines (perhaps?)",2011-01-30T06:35:04.597,2164,CC BY-SA 2.5,
10576,6716,0,"girad: Actually, this question came up from someone over the testing team. Properly means that they were given a set of instructions to execute to get a final value. The experiment will complete even if one of the instructions is skipped but will result in an incorrect observation. I will check out the `student test` that you mentioned. But if the test relies on mean, isn't mean supposed to be a bad measure due to its sensitivity to change in the data values? Thank you for your time.",2011-01-30T06:52:45.907,2164,CC BY-SA 2.5,
10577,6694,0,"The other issue is that my answer is generic in the criterion, because I don't know what the consequences of using this classifier are.  If you know more background information, then $F$ should be changed to reflect what is important *when you will actually be using the classifier* and **not** what's important when you are fitting the model.  I believe I made this clear in my answer.",2011-01-30T07:10:34.043,2392,CC BY-SA 2.5,
10578,6605,0,"@Eduardo - but if you think about it, Say we reject the null hypothesis, due to a small p-value.  Once we have rejected the null; this means that the decision to reject the null hypothesis was based upon a *meaningless statistic*.  Therefore, this invalidates the rejection we just made!",2011-01-30T07:20:37.633,2392,CC BY-SA 2.5,
10579,6605,0,"@probabilityislogic: No it doesn't invalidate the rejection we just made. We have already concluded that the null hypothesis can't be true. The p-value is meaningless because it is defined in terms of something that isn't true. From a strictly logical point of view, the rejection of the null hypothesis (on the premise that p-values shouldn't be that low) just forces you to come up with another null hypothesis or stay content with being ignorant about what's going on.",2011-01-30T07:31:41.147,2971,CC BY-SA 2.5,
10580,6605,0,"I think the above discussion between @eduardo and myself shows quite clearly why p-values are a conceptual nightmare.  even in a seemingly obvious example, they do not appear to be what we think they are.",2011-01-30T07:43:26.077,2392,CC BY-SA 2.5,
10581,6676,0,"@Ian Langmore, you are welcome, I had some fun in answering it. If this answer was useful, you might want to accept it, this is the way this site works. To cite the [faq](http://stats.stackexchange.com/faq) you can accept the answer by clicking on the check box outline to the left of the answer.",2011-01-30T07:53:42.223,2116,CC BY-SA 2.5,
10583,6667,0,"Thank you for the response. I had initially used linear regression to examine this, with which I tested the change in mean between tow time points. However, the challenge comes when testing for the general trend over all the time periods. That means having one significant (or non-significant) value. I believe there are several assumptions that have to be incorporated. I have also seen that non-parametric tests (Mann-Kendall) can be used to test for trend. I am not quite sure how feasible this will be with the data I have maybe for non-normal data.Will look at the packages suggested. Thanks.",2011-01-30T09:57:15.220,2961,CC BY-SA 2.5,
10584,6719,0,Don't the $c_t$ need to form an absolutely convergent sum? i.e. $\sum_{t=0}^{\infty}|c_t| < \infty$ as well,2011-01-30T10:03:11.050,2392,CC BY-SA 2.5,
10585,6702,4,"Did you read the accompagnying vignette, [Estimation of multinomial logit models in R : The mlogit Packages](http://cran.r-project.org/web/packages/mlogit/vignettes/mlogit.pdf)? It seems to me you just have to apply the fitted coefficients on new data, isn't it?",2011-01-30T10:14:24.220,930,CC BY-SA 2.5,
10586,823,2,"(-1) also (concur with @walkytalky), and @lese - note that ""...consult the consensus of experts in that field of research than to misinterpret the data first-hand""  can be cheekily rebutted by ""I consult the experts so that I can mis-interpret the data second-hand"".",2011-01-30T10:38:08.183,2392,CC BY-SA 2.5,
10587,1064,0,If I was a betting man I'd say Richard Feynman was an agnostic,2011-01-30T10:48:28.700,2392,CC BY-SA 2.5,
10588,1406,8,"I think this quote means that the results of the experiment should be ""obvious"", and statistics just lets one put a precise figure as to ""just how obvious"".  The word *needs* is the key.",2011-01-30T10:56:58.613,2392,CC BY-SA 2.5,
10589,4302,2,"I think its because physics has a similar level of pedantry required for statistics, and the physicist has the huge benefit of wanting to get rid of uncertainty, the statistician just wants to describe it.",2011-01-30T11:04:59.273,2392,CC BY-SA 2.5,
10590,1360,8,"I would go one step further, within statistics.  Those who ignore Bayesian statistics are condemned to reinvent it.",2011-01-30T11:08:03.870,2392,CC BY-SA 2.5,
10591,6605,0,"@eduardo - So you would consider second order logic to be fallicious reasoning?  because that is what your answer implies.  If you reject the *conditions* ($H_0$ is a condition of the p-value) then you must reject the conclusions which were based on those conditions.  The same thing goes for axioms: you cannot simultaneously reject an axiom, but keep the theorem that depends on it for its proof.",2011-01-30T11:29:12.037,2392,CC BY-SA 2.5,
10592,6719,0,"@probabilityislogic, I intentionaly wrote non-rigorous mathematical text, since strict definition of singular and regular processes involves Hilbert spaces, which I do not think are really necessary to get the point accros. I will update the answer to reflect that. Concerning $c_t$, it does not need to be summable. The equation must hold in $L^2$ sense, hence summability of $c_t^2$ is sufficient.",2011-01-30T11:58:42.623,2116,CC BY-SA 2.5,
10593,2365,2,"I think something which is perhaps been overlooked in the discussion above (including by me) is that the ML solution is exactly equal to the maximum of the joint posterior density using a uniform prior (so $p(\theta|X)\propto p(X|\theta)$ ($\theta$ is the vector of parameters).  So you *cannot* claim that ML is good and Bayes is not, because ML is mathematically equivalent to a Bayesian solution (flat prior, and 0-1 loss function).  You need to find a solution which *cannot* be produced using Bayesian methods.",2011-01-30T12:55:53.090,2392,CC BY-SA 2.5,
10594,2356,4,"pathology or not, it would probably be the first of its kind.  I am very keen to see this example, for these ""pathologies"" usually have a good learning element to them",2011-01-30T13:18:52.187,2392,CC BY-SA 2.5,
10595,6720,1,"if there are zeroes in your data, exponential regression might not be appropriate, since the model as you stated it cannot allow zero values to be observed.",2011-01-30T14:00:34.440,2116,CC BY-SA 2.5,
10596,2356,0,"@suncoolsu - is the example in Wasserman where the supposed ""coverage"" drops to zero?",2011-01-30T14:21:44.513,2392,CC BY-SA 2.5,
10597,6715,0,"I'm talking more along the lines of ""is $32.1$ a physically meaningful quantity?  What would happen in the real world if this were correct.""  It may also be useful to speak to someone who actually did the experiment 4 or 6 (preferably the person who recorded the data).",2011-01-30T14:31:06.483,2392,CC BY-SA 2.5,
10598,6723,1,"you have 6 independent variables, is there a particular reason you need only a subset of them in your model? Why not including all of them?",2011-01-30T15:40:35.273,2116,CC BY-SA 2.5,
10599,6618,0,"@probablityislogic: Agreed.  In practice, since we assume a straightforward null and the underlying probabilities governing various choices the experimenter made are probably undefined, do we have any realistic hope of converting a mangled p value into our standard frame of reference?  I guess the difficulty of this task is what prompted me to call them invaldated.",2011-01-30T16:22:30.383,196,CC BY-SA 2.5,
10600,6718,0,"mean(newdata[,""Y""]); mean(dat[,""out:).  If predict was working as expected these values should be near one another.  They aren't.",2011-01-30T16:24:25.437,196,CC BY-SA 2.5,
10601,2356,0,"@probabilityislogic. I think so, but I need to check. Will get back to you soon on this soon!",2011-01-30T16:24:40.333,1307,CC BY-SA 2.5,
10602,6605,0,"@probabilityislogic: That's where you're wrong. The p-value isn't a _conclusion_ of the null hypothesis, but a _means to determine its believability_. Thus, if the p-value is too low, I can say, ""Hahaha! Bullcrap!""",2011-01-30T16:46:49.277,2971,CC BY-SA 2.5,
10603,6700,0,"Ok, so each row in the table is a different region--not 3 or 4, but 16?  also, have you assigned just 2 levels of the treatment variable (thorough and not thorough)?",2011-01-30T17:43:08.037,2669,CC BY-SA 2.5,
10604,6725,4,"Generate a random float _x_ in the interval (0,1) and perform a binary search of the cumulative fitness array, finding the first entry that equals or exceeds _x_.  Its index denotes the chosen fitness.",2011-01-30T18:02:24.730,919,CC BY-SA 2.5,
10606,6716,0,"@Legend A test of difference of means may be inappropriate, but that is not the fault of @robin, as pointed out in the second half of his response, which is apt: the test to use is determined by which characteristic of a suite of results signals an ""improper"" experiment.  You could conduct an F-test for a difference of standard deviations; you could conduct multiple-outlier tests; you could conduct a Kruskal-Wallis test; etc., depending on what kind of differences you're looking for.",2011-01-30T18:08:32.130,919,CC BY-SA 2.5,
10609,6723,0,isn't this a homework assignment?,2011-01-30T20:14:25.390,2116,CC BY-SA 2.5,
10610,6371,0,I'm asking the moderators to close this as no longer relevant.,2011-01-30T22:39:08.547,1144,CC BY-SA 2.5,
10611,6657,0,"it is possible to estimate the probability of absorption in a given state given any initial distribution if you have the transition matrix. The entries of the transition matrix are estimated as you say, but you forget that in the presence of $m$ absorbing states you don't have $n(n-1)$ parameters but $(n-1)(n-m)$ (at least by my rough count), so for 4 states and 2 absorbing states, you have exactly enough parameters to estimate the transition matrix (and in my case there are further constraints which cut down the number of parameters needed - this is immaterial)",2011-01-30T22:56:48.470,1144,CC BY-SA 2.5,
10612,6657,0,See e.g. http://math.furman.edu/~tlewis/math110/maki/chap8/sec4handout.pdf for a quick discussion of calculating absorption probabilities.,2011-01-30T23:04:05.967,1144,CC BY-SA 2.5,
10613,2356,0,"@suncoolsu - this example of Wasserman is not an example of ""defective Bayes"".  Because $\theta\sim N(0,1)$, and the coverage  for $\theta<2$ is good, note that $Pr(|\theta|<2)\approx 0.977$, so the supposed ""poor coverage"" is obtained only in a small fraction of possibilities, if the prior is true.  If you were to average this coverage taken with respect to the posterior of $\theta$, it would be about 95%, because most of the posterior probability would be in the $\theta<2$ range.  (more later)",2011-01-30T23:15:37.087,2392,CC BY-SA 2.5,
10614,2356,1,"@suncoolsu - I would say that it is a good example of the non-robust properties of conjugate priors though.  Because if the true value of $\theta$ is say $4$, but your prior says $\theta\sim N(0,1)$, then almost surely, the prior and the data will come into conflict.  If the prior is conjugate, then what you are basically saying is that the prior information is *just as cogent as the data*.  If the prior was instead $\theta\sim St(0,1,10)$ (T with 10 df), then because the likelihood is normal, you are saying that the data is more cogent than the prior...(more still)",2011-01-30T23:20:09.093,2392,CC BY-SA 2.5,
10615,2356,1,"...cont'd... and that in a case of conflict, you want the data to ""win"".  If the situation was reversed (Student likelihood and normal prior), then if the data conflicts with the prior, the prior would ""win"".  See [This post](http://stats.stackexchange.com/questions/6493/mildly-informative-prior-distributions-for-scale-parameters/6506#6506) for some links to how this works.  I would suspect that the coverage would be better for large $\theta$ (but possibly worse for small $\theta$) if a student-t distribution was used as prior instead of normal.",2011-01-30T23:25:45.700,2392,CC BY-SA 2.5,
10616,6723,0,"@mpiktas It is not a homework assignment, but the R code does indeed come from a course I'm following. The course text is very short on how to interpret the derived results and I posted here trying to get a better grasp on the material while stuyding.",2011-01-30T23:49:54.920,2980,CC BY-SA 2.5,
10617,720,5,"@whuber: if the data are continuously distributed (which they might be, depending on whether the original poster really means ""general"" or ""generalized"" LMM) then the probability density has an implicit ""delta-x"" term in it, which is indeed affected by changing units. See also <http://emdbolker.wikidot.com/faq>",2011-01-31T00:03:02.937,2126,CC BY-SA 2.5,
10618,2356,1,"I think I will put this in more detail, as an answer, because it is a prime example of what Jaynes talks about in his paper.  Wasserman shows a problem, shows that the Bayesian way gives an apparently counter-intuitive result, warns about the ""danger of Bayesian methods"", without any investigation as to *why* the Bayesian solution gives the result it does.  Secondly the *Frequentist Confidence Interval is not given in the equivalent problem!* I will show in my answer, that the example can be formulated in equivalent frequentist terms *which give exactly the same answer as the Bayesian one!*",2011-01-31T00:24:05.393,2392,CC BY-SA 2.5,
10621,6718,0,"What does, ""this happens since R is forced to forget the original scaling using formula"", mean?  What is meant by saying that R knows or does not know the original scaling?",2011-01-31T01:05:28.740,196,CC BY-SA 2.5,
10622,6718,0,Ah... I see - you mean that the predictors in new data themselves are subjected to Z scaling prior to the generation of the prediction?,2011-01-31T01:29:49.560,196,CC BY-SA 2.5,
10624,6718,0,... so I think the theme here that there is no way to use the scale command within the call to lm and pass predict the Z values I want and have everything turn out smoothy in one step the way I might hope.,2011-01-31T01:37:22.963,196,CC BY-SA 2.5,
10627,6700,0,"Thank you for the prompt reply. Yes. Each row is a different region. There are 16 regions with different populations. The 2 levels of treatment variable are thoroughly implemented and not so thoroughly implemented. Some regions implemented the treatment thoroughly and so the deaths were less. Other regions did not implement the treatment thoroughly and so the deaths were more. We have to show statistically that in regions where treatment was inadequate had significantly higher number of deaths. If the table is too big, we can use 3 or 4 regions data from 2006 to 2010 (5 years) for analysis.",2011-01-31T03:12:38.863,2956,CC BY-SA 2.5,
10628,6735,0,"Thank you for your answer. I see what you mean (kind of). Maybe you can comment on this: I did not mean that the ""ordering"" of the list changes -- of course it does. Imagine that you sort the list anyway after you did all changes. But would the sorted list then look (kind of) the same , i.e. have the same distribution of values, in the second scheme compared to the first one?",2011-01-31T03:31:08.170,2440,CC BY-SA 2.5,
10630,6735,1,"@Felix Dombek, the answer is ""yes"". In each of the three schemes, if you sorted each of the three vectors at the end, into ascending or descending order, then the resulting three vectors would all have the same joint distribution. These would be the order statistics of a multinomial random vector. But, the ordering at the end is important. Otherwise, each of the vectors have different distributions (though I think first and last would still be the same).",2011-01-31T03:57:05.713,2970,CC BY-SA 2.5,
10631,6732,0,"@Will, good. Glad it worked. You might consider accepting the answer then. Regards.",2011-01-31T03:58:58.553,2970,CC BY-SA 2.5,
10634,2369,0,A confidence interval does not bound the rate of false positives - see my answer below for a counter-example to back up my claim.,2011-01-31T07:11:53.130,2392,CC BY-SA 2.5,
10635,6716,0,"@Legend There is also another difficulty that is shadowed by your question because here you guessed that 4,6 was the different samples. But what if you don't know in advance... you will have to test all configuration and probably introduce a multiple hypothesis criterion. In this case this looks like outliers detection and many question have already dealt with that here.",2011-01-31T07:35:54.183,223,CC BY-SA 2.5,
10636,6714,2,"I do not see why the probability of a proposition according to definition 1 should be a rational number. *Long run proportion* seems to refer to the limit of the proportions of times such that the proposition is observed to be true. Each proportion is a rational number but their limit might not be. (Fortunately, this parenthesis of yours seems tangential at best to the rest of your answer.)",2011-01-31T07:55:30.563,2592,CC BY-SA 2.5,
10638,6657,0,"David: I was not aware that the situations you were interested in involved such small values of $n$ (and you are right, the number of parameters is $(n-1)(n-m)$). May I ask why you now want this question closed? Did you solve the question? I note that there is nothing in the handout you refer to about the *estimation* problem you asked.",2011-01-31T08:18:40.743,2592,CC BY-SA 2.5,
10639,6737,1,Thanks for the analysis.  AFAICS this is just an example of a problem caused by an incorrect (informative) prior assumption and says nothing about the internal consistency of the Bayesian approach?,2011-01-31T08:25:36.813,887,CC BY-SA 2.5,
10640,6727,1,"@Ralph, +1 for looking up the reference. I should note however that if this is truly how the authors are dividing stochastic part it is a bit misleading. Suppose you have autoregressive model $Y_t=\rho Y_{t-1}+\varepsilon_t$. The division is that $\rho Y_{t-1}$ is systematic and $\varepsilon_t$ is non-systematic part. But if we substitute $Y_{t-1}=\rho Y_{t-2}+\varepsilon_{t-1}$ into the first equation and repeat the process indefinitely  we will get that $Y_t=\sum_{s=0}^{\infty} \rho^{s}\varepsilon_{t-s}$, so there is actually no systematic part according to the original definition.",2011-01-31T08:28:03.037,2116,CC BY-SA 2.5,
10643,6721,0,"Shame I havent been taught about it at the university :/ It looks that is will be helpful in this case, but I need some time to get deep into details. Thanks!",2011-01-31T09:53:05.337,1643,CC BY-SA 2.5,
10645,6722,0,What about initial values of the parameters? What is good way to choose them? As I stated in an update it may be assumed that there are no continuous variables.,2011-01-31T10:02:52.487,1643,CC BY-SA 2.5,
10647,6738,0,"@Didier, I feel rephrasing this part of the question will be the hardes part of the job, maybe the question is asking for pairs of (purposes,best distance). But you are right, maybe @Felix could tell us more about why he is asking his question.",2011-01-31T10:34:06.403,223,CC BY-SA 2.5,
10648,6371,1,@David Are you sure you want to remove it? I think it should stay because it still may be useful for other people.,2011-01-31T10:38:24.713,,CC BY-SA 2.5,user88
10649,6736,0,"What is this ""given purpose""?",2011-01-31T10:40:43.727,,CC BY-SA 2.5,user88
10650,6736,0,"@mbq: This here, for example: http://www.lscp.net/persons/peperkamp/Peperkamp_Le_Calvez_Nadal_Dupoux_%282006%29_The_acquisition_of_allophonic_rules.pdf (i.e. probability distributions for speech sounds in context of other adjacent sounds) ... but my question aims for a general answer of what I have to take into account when I rate each measure's applicability for any use case.",2011-01-31T10:44:16.913,2440,CC BY-SA 2.5,
10651,6738,0,"Actually, the first and foremost reason was to get some reputation to be able to answer a question on meta: http://meta.stats.stackexchange.com/questions/707/could-r-tag-be-displayed-as-a-capital-r . Also, I don't understand much of the wikipedia pages and I have of course read them first. I'm extremely lousy in statistics and maths.",2011-01-31T10:47:22.030,2440,CC BY-SA 2.5,
10652,6738,3,"Dumbfounded by this comment. This is priceless and should be kept for posterity: ""Actually, the first and foremost reason [for me to ask this  question] was to get some reputation to be able to answer a question on meta"".--Felix Dombek, January 31st, 2011.",2011-01-31T10:53:17.487,2592,CC BY-SA 2.5,
10653,6738,0,Good that only five are needed ... I don't understand why they don't use the total rep on all SE sites for some actions =),2011-01-31T10:57:48.137,2440,CC BY-SA 2.5,
10654,6721,0,"Note that $y_i$ can always be rescaled to integer values when it is rational, eg measure pence/cents rather than pounds/dollars. Though you may want to round to the nearest pound/dollar anyway since the distribution of the pence/cents part of the price of goods will likely be very uneven (ie mostly 99).",2011-01-31T11:13:22.703,229,CC BY-SA 2.5,
10655,6738,2,"I understand that if more points were needed, you would have asked several other fake questions. Is this what you mean?",2011-01-31T11:38:47.180,2592,CC BY-SA 2.5,
10656,4712,10,"This is particularly important. I remember reading a study about whether Ramadan was bad for babies whose mothers were fasting. It looked plausible (less food, lower birth weight), but then I looked at the appendix. Thousands of hypotheses, and a few percent of them were in the ""significant"" range. You get weird ""conclusions"" like ""it's bad for the kid if Ramadan is the 2nd, 4th or 6th month"".",2011-01-31T13:06:21.333,2813,CC BY-SA 2.5,
10657,6722,0,"@Tomek, I think there is no one good way of choosing them. Usually it depends on the data. I suggest mean for the intercept and zero for other coefficients.",2011-01-31T13:12:07.333,2116,CC BY-SA 2.5,
10658,6739,0,"I found this link helpful for deciphering how lm.ridge is calculating the coefficients: http://www.mail-archive.com/r-help@r-project.org/msg81115.html  But still puzzled how different the results are from the text I referenced and SAS, given that each is supposedly back to the original scale.",2011-01-31T13:13:45.020,2040,CC BY-SA 2.5,
10659,6739,0,"@user2040, check that the same data is used by SAS and R. If it is the same, then the only conclusion is that the algorithms are different. What SAS help page is saying?",2011-01-31T13:22:21.210,2116,CC BY-SA 2.5,
10661,6426,0,EP does seem cool (although it still hurts my head).  Does it still lack general convergence guarantees?,2011-01-31T13:27:49.333,1739,CC BY-SA 2.5,
10662,6739,0,"@user2040, I've replicated [SAS ridge regression](http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_reg_sect046.htm) on R with your data. So we know for sure that the algorithms are different.",2011-01-31T13:59:21.810,2116,CC BY-SA 2.5,
10663,6738,0,"This is not a fake question. I do indeed not understand what the different formulas have as implications, and from my comment for the question, it should be evident that I encountered them in real life. If more points were needed, I would either have just dropped it, or possibly just told someone who answered on a previous question of mine what I wanted to answer on meta and asked him to do it for me. Also, I intended to get the points only from accepting an answer (I already had 3 previously) and not from upvotes, but I guess my question isn't bad after all with 3 upvotes so far.",2011-01-31T14:07:06.327,2440,CC BY-SA 2.5,
10664,6738,0,"Also, you must admit that my answer on meta is pretty useful.",2011-01-31T14:11:47.197,2440,CC BY-SA 2.5,
10666,6730,0,"I'ts also in Knuth's Graphbase book, p. 392. For a Python recipe, see http://code.activestate.com/recipes/576564-walkers-alias-method-for-random-objects-with-diffe/",2011-01-31T14:35:04.663,557,CC BY-SA 2.5,
10668,6704,0,"+1; could you add an example or two, favourite or current ?",2011-01-31T14:53:20.263,557,CC BY-SA 2.5,
10669,720,4,"@Ben Thank you.  When I wrote this I was confused between the AIC and the difference of AICs, thinking the latter was the former.  It is correct that the choice of units introduces a multiplicative constant into the likelihood.  Thence the *log* likelihood has an *additive* constant which contributes (after doubling) to the AIC.  The difference of AICs unchanged.",2011-01-31T14:57:32.243,919,CC BY-SA 2.5,
10670,6714,3,"@probability This reply seems to be taking us off on a tangent in a not very constructive way.  Equating probability and proportion is a form of ontological confusion, akin to equating a temperature with the level of mercury in a thermometer: one is a theoretical construct and the other is a physical phenomenon used to measure it.  There is some discussion of this at http://stats.stackexchange.com/questions/1525/whats-the-difference-between-a-probability-and-a-proportion/4850#4850 .",2011-01-31T15:02:49.777,919,CC BY-SA 2.5,
10671,4083,1,A note on why this was downvoted would be appreciated.,2011-01-31T15:10:19.283,1036,CC BY-SA 2.5,
10673,6738,0,"Felix: I simply read what you wrote, nothing else. Now if you'll excuse me, I will take my leave from here.",2011-01-31T15:33:32.163,2592,CC BY-SA 2.5,
10674,6739,1,"You just beat me to that! :) I was looking at the SAS help page you referenced. I compared the RMSE (in-sample data only, did not validate with a CV or bootstrap yet) and the R result was superior.So, do you think ridge regression is best suited for prediction and not interpretting the coefficients (since the results can be so different by algorithm)? I know already that regular linear model theory (CI's, contrasts etc.) are out for ridge regression (bootstrapping gets closer but still can be misleading due to bias).",2011-01-31T16:06:28.597,2040,CC BY-SA 2.5,
10675,6256,0,"Thank you very much for the answer. What can i do, if i have higly screwed numerical(!) data? Does weighting work there, too?",2011-01-31T16:14:22.427,,CC BY-SA 2.5,user2781
10676,6753,0,I really wonder how an equal frequency histogram might look like. I have the intuition that it is really flat. Can you give an example?,2011-01-31T16:58:57.840,442,CC BY-SA 2.5,
10677,6753,0,"@Henrik: See e.g. this question (http://stats.stackexchange.com/questions/5573/how-to-build-an-equilibrated-histogram). Yes it is flat, so it clearly cannot be used for density estimation ;). However, since the equal-width approach is so generic, it seems that it can be applied in every situation equal-freq can be applied. So when to favor equal-freq ?",2011-01-31T17:09:40.383,264,CC BY-SA 2.5,
10682,6754,1,"Could you clarify what you mean by a ""common entity"". It's not clear to me because it seems you are describing two features (for me, a feature or attribute = a variable) with different units (meters and seconds) rather than a single one?",2011-01-31T18:25:33.280,930,CC BY-SA 2.5,
10683,6750,2,"Thanks a lot for such a descriptive answer. Thanks to StackExchange, I think it's time I'm not ashamed of asking stupid questions because they trigger such beautiful answers.",2011-01-31T18:39:53.290,2989,CC BY-SA 2.5,
10684,6751,1,"Thanks, I think I got it. You can have infinite different normal distribution curves with same maximum, right?",2011-01-31T18:41:08.810,2989,CC BY-SA 2.5,
10685,6751,2,"@gaearon There may be a potential for confusion here due to terminology: ""maximum"" can refer to a maximum value attained by the distribution function (which, for the normal distribution, occurs at the *average* value of the distribution!) or it can refer to the largest possible number that has any chance of occurring (which for any normal distribution is infinitely large).  @Aniko's response appears to interpret your ""worst/best"" cases as specifying extreme values in the *second* sense, whereas your comment appears to interpret the maximum in the *first* sense.",2011-01-31T19:19:28.687,919,CC BY-SA 2.5,
10687,6754,0,"@chl with common entity, I mean an ""instance"". As I tried to explain above, there are several measurements with different criteria(s), I need to normalize(?) them to further use in regression algorithms. But, I am not sure how to make these numerical measurements to carry same meaning for each instance.",2011-01-31T19:27:41.217,2170,CC BY-SA 2.5,
10688,6630,0,You question is rather tricky... I'm thinking in it.,2011-01-31T19:27:54.120,2902,CC BY-SA 2.5,
10689,6753,4,"@Henrik No, an equal frequency histogram generally is *not* flat. Histograms are commonly confused with bar charts, which display values by means of the *heights* of bars. However, by definition, a histogram displays frequencies by means of *areas*. Consider (*e.g.*) the data {0,1,2,4,8,16,32,64}, to be shown in the range [0,100] with two bins. The break for an equal-frequency histogram has to be between 4 and 8.  If we put it at 6, the height of the left bar *multiplied* by (6-0) = 6 equals 4, whence the height is 4/6. The height of the right bar equals 4/(100-6) = 4/94. Not flat at all!",2011-01-31T19:29:30.327,919,CC BY-SA 2.5,
10691,6753,1,"(Continued) See a Wikipedia example of a variable-width histogram at http://en.wikipedia.org/wiki/File:Travel_time_histogram_total_n_Stata.png , which is an illustration for its article on ""Histogram.""",2011-01-31T19:31:33.787,919,CC BY-SA 2.5,
10692,6753,2,@steffen Your second question has already been asked and answered at http://stats.stackexchange.com/q/798/919 .  More formulas appear at http://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width .,2011-01-31T19:32:46.297,919,CC BY-SA 2.5,
10695,6760,1,Thanks Bernd. I thought unique can be applied only for a specific column. I didn't know that it can be used for the entire data frame as well. thanks again,2011-01-31T20:25:13.270,2725,CC BY-SA 2.5,
10696,6753,0,@whuber wow. It is weird to see that one still mixes up pretty basic things. Thanks a lot for pointing my mistake out!,2011-01-31T20:50:40.490,442,CC BY-SA 2.5,
10697,6714,0,"@Didier - you're right, in fact the sequence of $x_n=\frac{r}{2x_{n-1}}+\frac{x_{n-1}}{2} \rightarrow \sqrt{r}$, which is rational terms with irrational limit.  I have removed this remark.  Thanks for bringing this up.",2011-01-31T21:03:04.197,2392,CC BY-SA 2.5,
10698,6559,0,"@mpiktas: I posted my second draft, what do you think?",2011-01-31T21:08:22.670,2817,CC BY-SA 2.5,
10699,6714,6,"@whuber - The point is relevant to bring up because it is precisely this misunderstanding that leads people to interpret CIs in the wrong way.  Confusing probability with ""rational degree of belief"" is not consistent with the frequentist paradigm. This is what happens when you take CIs to mean ""probability of true value being in interval"", which is what @dsimcha is doing in the question.",2011-01-31T21:10:33.077,2392,CC BY-SA 2.5,
10700,6737,1,"Nope, the prior is not necessarily incorrect, unless one didn't actually observe a value of $0$ prior to conducting the experiment (or obtain some equivalent knowledge).  It basically means that, as the true $\theta$ becomes arbitrarily large, the probability of observing these implicit observations becomes arbitrarily small (like getting a ""unlucky sample"").",2011-01-31T21:22:33.277,2392,CC BY-SA 2.5,
10701,6737,0,"you can see by noting that the sample consists of an observation at $0$ and another one at $X$.  $0$ is fixed (because it has been observed), but $X$ will be ""close"" to $\theta$ in most cases.  So as $\theta$ becomes large, the sample average gets further and further away from both $X$ and $0$, and because the variance is fixed, the width of the CI is fixed, so it will eventually not contain either $X$ or $0$, and hence not be near either of the two likely values of $\theta$ (for one of them is an outlier when they become far apart, for fixed $\theta$)",2011-01-31T21:27:17.950,2392,CC BY-SA 2.5,
10702,6714,1,"@probability Thank you for the explanation.  I had understood your reply as being in accord with a definition of ""probability = proportion.""  In fact, a close rereading still suggests this is what you are saying in the third paragraph, even though your comment now characterizes this as a misunderstanding.  You might want to clarify this point.",2011-01-31T21:42:44.163,919,CC BY-SA 2.5,
10703,6754,1,Could you give an example of sth that is recorded as a single entity in meters *and* seconds?,2011-01-31T21:48:48.033,930,CC BY-SA 2.5,
10704,6605,0,"if the p-value isn't a conclusion of the null hypothesis, they why does it appear as one of the conditions required when calculating the null hypothesis.  presumably, your response implies that the p-value could be constructed without the null hypothesis.  And you cannot determine the *believability of the null* without referring to the prior probability of the null.  Otherwise, you are to believe incompatible null hypothesis simulatneously (the data is fake vs the data is not fake can give the same p-value)",2011-01-31T22:11:23.977,2392,CC BY-SA 2.5,
10705,6605,0,"@eduardo - suppose *every* p-value for every null you could think of (including faking the data) was smaller than $10^{-5}$, what are you to believe?",2011-01-31T22:14:55.220,2392,CC BY-SA 2.5,
10706,6758,0,"Using only `dynlm` package will not provide forecasts for your dependent variables. Providing forecasts for your dependent variables will require a model to explain them and probably additional data. I suggest you to read something about multivariate regression such as ""Applied Multivariate Statistical Analysis"" by Johnson and Wichern. or a course on forecasting: http://www.duke.edu/~rnau/411home.htm",2011-01-31T22:32:43.857,2902,CC BY-SA 2.5,
10707,6371,0,"I don't want to delete it, but close it (on MathOverflow there's a difference, but they don't run SE2.0). I'm happy to leave it as is for the sake of posterity, but I've realised my problem is not what I was asking in this question.",2011-01-31T22:42:41.093,1144,CC BY-SA 2.5,
10708,6757,1,Yes I found it interesting that there was a write function but no read function.  What would be ideal is if there was a read function and then you could write it as a sparse matrix.,2011-01-31T23:01:12.367,2310,CC BY-SA 2.5,
10709,6657,0,"I've realised that what the paper cited shows is not really what I need. The paper deals with absorption probabilities calculated from samples, whereas I have complete information, and the uncertainty is not in how well I can guess the true values of the model given incomplete information, but the fit of the model to reality.",2011-01-31T23:24:00.790,1144,CC BY-SA 2.5,
10711,6758,2,"@deps_stats The dependent variable is what I want to forecast.  I'm assuming that I already have forecasts for my independent variables.  In my example code, y is the dependent variable I am trying to forecast, and A,B,C are the independent variables, which I already have forecasts for. If you run the example code I posted, you will understand the nature of my problem.",2011-01-31T23:31:52.407,2817,CC BY-SA 2.5,
10712,6715,0,"@probabilityislogic: I see. I get your point. The data is question is a response time variable. My take on your question would be that the value does make sense in a physical world but its just too unusual enough to be called a rare case. The person I talked to said he did not do anything differently. Actually, the data that I put here is just a sample from the entire data and there are some cases like this spread here and there.",2011-02-01T00:17:27.080,2164,CC BY-SA 2.5,
10713,6716,0,@whuber: I did not intend to see it is anyone's fault. I am a novice here so I apologize if sounded so. @robin girard: That is a very interesting take. Thanks. I was just thinking about outlier detection. Will you be able to point me to some relevant material for this particular case? All I have used before are simple ones like k-means etc.,2011-02-01T00:20:07.857,2164,CC BY-SA 2.5,
10715,6763,0,Which software are you working with?,2011-02-01T00:36:40.853,71,CC BY-SA 2.5,
10716,6652,2,"Part of me applauds the question (+1).  A competing part wants to point out that 1. The vast majority of statistics consumers, people who use statistics pragmatically but not philosophically in order to make their point in chemistry or market research, will never grasp the niceties of the issues, and we will often be at a loss to explain results.  2. Even some purist statisticians can fall into the trap of making supposedly probabilistic statements like those involving confidence intervals when they are not working with random samples.  A much bigger issue.",2011-02-01T01:33:49.450,2669,CC BY-SA 2.5,
10718,6715,0,"so it would appear that the most likely result is an error, but interesting discoveries can be made if you ""dig deeper"" so to speak.  Could possibly be a new finding of some sort!  but don't get too excited, it probably is nothing, but it may be worthwhile to entertain the possibility, and see where it leads you.",2011-02-01T02:29:35.833,2392,CC BY-SA 2.5,
10720,6770,0,1/4 1/8 1/16 ....,2011-02-01T02:39:49.593,1585,CC BY-SA 2.5,
10721,6770,0,so... you're saying you don't see any consistent relationship between the amounts at the various lengths?,2011-02-01T02:44:26.813,601,CC BY-SA 2.5,
10722,6770,0,each one is 1/2 of the one before and  1/2 + 1/4 +1/8 ... =1,2011-02-01T02:45:54.597,1585,CC BY-SA 2.5,
10724,6770,0,"Well yes, that is why I said that the pattern is obvious. But I need a formula, preferably with an explanation as to WHY each one is 1/2 of the one before, and how do I get the first value. I need to calculate this independently of the simulation, and then compare these two results. Don't get me wrong, I'm not looking for someone to solve my homework for me, I just don't know what to do anymore.",2011-02-01T03:57:08.850,2996,CC BY-SA 2.5,
10726,6371,0,"So you could accept my answer if it fits your question as you asked it, then open a new question on stats.SE with what you really are interested in.",2011-02-01T06:24:32.800,2592,CC BY-SA 2.5,
10727,6605,0,"@probabilityislogic: Setting the alpha of a test is a matter of ""policy,"" if you want to give it a name. Given that the null hypothesis is true, the p-value could be _anything_ from 0 to 1, even a small value. So, from a strictly logical point of view, you cannot _decide_ (in a deductive way) whether the null hypothesis is true or not. So, at the risk of being wrong, you establish that the p-value shouldn't be lower than alpha, otherwise, you _don't believe_ that the null hypothesis is true, regardless of whether it is actually true, which you will never know. There is no paradox.",2011-02-01T07:11:33.463,2971,CC BY-SA 2.5,
10728,6753,0,"@whuber: Thank you for your explanation. Your answer to the second question indicates, that all this rules are not necessarily require an equal-width-histogram. I did not know that.",2011-02-01T07:13:23.617,264,CC BY-SA 2.5,
10729,6748,0,Thanks for the insight and references! The Gary King piece is indeed very interesting though not directly on topic.,2011-02-01T07:31:20.270,1283,CC BY-SA 2.5,
10730,6770,0,"@Davor, [this](http://mathforum.org/library/drmath/view/56637.html) might give you some ideas.",2011-02-01T08:11:40.273,2116,CC BY-SA 2.5,
10731,6761,0,"@Zach, did you look at the results? Your function still does not predict the future values. In the loop you are basicaly repeating the same thing you done before the loop. Also note, that your function will fail if the dependent variable is not in the first column of `newdata`.",2011-02-01T08:22:43.217,2116,CC BY-SA 2.5,
10732,6763,2,transpose your data. You sample 8 measurements and the each batch is an element in your sample. In this case PCA expects batches in rows and measurements in the columns.,2011-02-01T08:27:46.303,2116,CC BY-SA 2.5,
10733,6753,0,"@whuber: Although it seems ""strange"" that one can use FD to calculate bin-width, then the number of bins and then the equal-freq-bin-width.",2011-02-01T08:37:48.587,264,CC BY-SA 2.5,
10734,6653,0,Tough question! My guess is that it would help to know more than I do about counting processes (and more than Wikipedia does: http://en.wikipedia.org/wiki/Counting_process ),2011-02-01T09:30:44.077,449,CC BY-SA 2.5,
10735,6749,0,"Thanks. So let's say that the $K$ functions for processes $x$ and $y$ are given by $K_x(t)$ and $K_y(t)$. How do I test the hypothesis that $x$ is independent of $y$? Under the assumption of stationarity I can simply look at $L_{xy}(t) = \sqrt{K_{xy}(t)/\pi}$ and look for departures from $L_{xy}(t)-t=0$. For non-stationary distributions I see that $K_{xy}(t)$ will in general be different from $K_{yx}(t)$, but I don't see what the impact of this on my hypothesis testing should be.",2011-02-01T10:07:20.120,2425,CC BY-SA 2.5,
10738,6605,0,"@Eduardo - but if you don't believe the null is true, then you are acting *as if* $P(H_0)=0$.  And when this occurs, the conditional probability based on $H_0$ is *undefined* $P(T>T_{obs}|H_0)=\frac{P(T>T_{obs},H_0)}{0}$.  By putting your *prior probability* of $H_0$ in addition to the p-value $P(H_0)\times P(T>T_{obs}|H_0)$ the $P(H_0)$ in the denominator cancels out, and you are no longer dividing by zero, and you are just left with the joint probability.  This is a more sound way to reason because you are reject the null AND the data as a combination.",2011-02-01T10:49:04.697,2392,CC BY-SA 2.5,
10739,6754,0,"@chl assume there is a runner, who runs 100 m in 15 secs, and 200 m in 32 secs, other runner runs 500 m 40 secs and 1000 m 83 secs, who will run 1 m (or a common) distance better? is this process related with standardization or normalization? It may seem simple but we can also assume that running 1000 m is harder than 100 m.",2011-02-01T11:39:46.643,2170,CC BY-SA 2.5,
10740,6780,0,"according to [wikipedia page](http://en.wikipedia.org/wiki/Zipf's_law) Zipf's law has two parameters. Number of the elements $N$ and $s$ the exponent. What is $N$ in your case, 10? And frequencies can be calculated by dividing your supplied values by the sum of all the supplied values?",2011-02-01T13:44:00.337,2116,CC BY-SA 2.5,
10741,6780,0,"let it's ten, and frequencies can be calculated by dividing your supplied values by the sum of all the supplied values.. how can I estimate?",2011-02-01T13:51:02.840,2998,CC BY-SA 2.5,
10742,6761,0,"@mpiktas: what I am trying to do is this: 1. Starting with the first NA value, use all previous data to construct a 1-step forecast using dyn.  Replace the NA value with this forecast, and repeat.  I thought this would iteratively step through all future NA values... what am I missing?",2011-02-01T14:21:24.843,2817,CC BY-SA 2.5,
10743,6761,0,"@Zach, the problem is with one-step forecast. Apparently it does not work.",2011-02-01T14:30:42.963,2116,CC BY-SA 2.5,
10744,6749,0,"You have to redo the calculations for 1D and your particular assumptions, Chris: the formulas you quote are for 2D and a homogeneous process.  Dixon's references provide the theory.",2011-02-01T14:37:16.527,919,CC BY-SA 2.5,
10745,6778,0,This is the point of departure in Dixon's survey I referenced.,2011-02-01T14:38:11.220,919,CC BY-SA 2.5,
10746,6753,0,"@Steffen Sorry, I was mistaken.  Those rules are for equal-width histograms.  For equal-frequency histograms the theory is different, because you determine the *area* of each bar in advance.  Thus, variation in the area is proportional to the square root of the (common) bin population.  Choosing that population is therefore a tradeoff between horizontal precision (number of bars) and areal precision; where to come down in that tradeoff is your decision.",2011-02-01T14:45:23.627,919,CC BY-SA 2.5,
10747,6781,1,"There's also an R package zipfR http://cran.r-project.org/web/packages/zipfR/index.html
I haven't tried it though.",2011-02-01T14:51:30.560,449,CC BY-SA 2.5,
10748,6766,1,"+1 You can obtain the distribution with an (efficient) dynamic program.  Let $p(n,j,k,m)$ be the probability of $j$ isolated actors and $k$ non-isolated actors choosing from $m$ actions.  Adding one more actor gives the recursion $p(n+1,j,k,m)$ = $p(n,j,k,m)(k/m)$ + $p(n,j+1,k-1)((j+1)/m)$ + $p(n,j-1,k)(m-j-k+1)/m$.  Of course $p(n,1,0,m) = 1$, letting us start the recursion.  Computations simplify when you base them on $m^{n-1}/(m-1)^{[j+k-1]}p(n,j,k,m)$: the values are integers independent of $m$.  ($x^{[j]}$ is the factorial power $x(x-1)\cdots(x-j+1)$.)",2011-02-01T15:03:25.000,919,CC BY-SA 2.5,
10749,6781,0,"@onestop, thanks for the link. It would be nice if someone would answer this question using this package. My solution definitely lacks depth, though it gives some kind of answer.",2011-02-01T15:09:56.830,2116,CC BY-SA 2.5,
10750,6770,0,Well you said a formula and 1/2 1/4 1/8 etc is certainly a formula. It seem to me it indicates a rather simple proof.  Suppose you start throwing the coin and record the length of each run.   the first throw is either H or T.  the second throw can be H or T and the probability is  0.5 that they are it is different from the first throw. thus the probability is 0.5 that the length of the string is 1.  Similarly it is 0.25 that the length of the string is 2 and so on.,2011-02-01T15:53:50.503,1585,CC BY-SA 2.5,
10751,6761,0,"@It seems to work with the dyn function, rather than the dynlm function.",2011-02-01T16:17:50.203,2817,CC BY-SA 2.5,
10752,6772,2,"Assuming A and B are uncorrelated, use the variance propagation formula on Wikipedia at http://en.wikipedia.org/wiki/Fieller%27s_theorem#Case_1 .  Because your CIs are so small, we know the sampling distributions are approximately normal and the sample sizes are fairly large.  Therefore you can back-calculate Var(A) and Var(B) from your CIs in the usual way, for input in the variance propagation formula.  (BTW, your edit is mysterious.  Perhaps you are confusing the coverage of a CI with the interval itself?)",2011-02-01T16:23:52.373,919,CC BY-SA 2.5,
10753,6778,0,"Yes, I think I understand now. I didn't grasp the full range of your answer when I read it the first time!",2011-02-01T16:54:59.887,2425,CC BY-SA 2.5,
10755,6605,0,"@probabilityislogic: No. Not believing it is plausible that the null hypothesis be null doesn't mean acting as if the probability of the null hypothesis were zero. It means ""it might be true, but I won't _bother_ taking that possibility into consideration anymore."" Remember that, in real life, we use statistical inference to make decisions when we don't have enough information to use the strictly deductive approach.",2011-02-01T17:39:19.397,2971,CC BY-SA 2.5,
10756,6785,0,"Thank you very much for your answer. All the preidictor you mentioned are meaningful. To simply the problem, lets suppose I am trying to find out the number of library visit and how many book borrowed, how long each book kept etc realted to a student final mark of this course.",2011-02-01T17:46:35.207,2454,CC BY-SA 2.5,
10759,6785,0,"if I use predictor a1, b1, c1 to build network(a1 is avearge number of library visit per week, b1 is average book borrowed weekly etc).  and I build another network with a2, b2, c2 (a2 is total number of libary visit, b2 is total book borrowed). ...  what if I want to find out combination (a1, b2, c1)??   Does that mean I have to include all these variation in the network? if yes, which one is parents? a1 or a2 ... and I think the network will be very compliate.  there must be smart way to do it I think?",2011-02-01T17:53:55.960,2454,CC BY-SA 2.5,
10760,6786,2,"@whuber, I might humbly suggest a little caution with the formulation given above. Zipf's law is usually stated as a relative-frequency result. It is not (normally considered) the distribution from which an iid sample is drawn. An iid framework is probably not the best idea for these data. Perhaps I'll post more on this later.",2011-02-01T18:09:15.460,2970,CC BY-SA 2.5,
10761,6785,0,"My main goal is to find something like:  if a student go to libary total number >50,and  average weekly borrowed books >3, or last week borrow book.2 ....  then the probability of the student get good mark is 80%.      as you can see, not only the structure of these rules will be different combination of different predictor,  I also need to find the threshold for each one, such as 50 for total book borrowed in the example.   as Bayesian network normally do not take contiunous values,  thus I possibly could try different thresholds to discretize these continuous value.",2011-02-01T18:09:37.830,2454,CC BY-SA 2.5,
10763,6786,3,"@cardinal I look forward to what you have to say.  If you don't have time for a thorough response, even a sketch of what you think might be the ""best idea for these data"" would be most welcome.  I can guess where you're going with this: the data have been ranked, a process which creates dependencies and should require me to defend a likelihood derived without recognizing the potential effects of the ranking.  It would be nice to see an estimation procedure with sounder justification.  I am hopeful, though, that my analysis can be rescued by the sheer size of the dataset.",2011-02-01T19:20:06.183,919,CC BY-SA 2.5,
10766,6605,0,"@Eduardo - but rejecting the null hypothesis *does* mean you are acting *as if it is false*.  If you are using p-values as a heuristic guide to your inference, then there is no quams with that.  But a low p-value on its own is *not a direct measure of the evidence* against the null( [Bernardo and Rueda](http://www.stat.duke.edu/research/conferences/valencia/IntStatRev.pdf) ).  [This paper](http://predictive.files.wordpress.com/2010/03/binder1.pdf)  also shows some of the logical flaws of p-values",2011-02-01T20:02:08.143,2392,CC BY-SA 2.5,
10767,6605,0,"@probabilityislogic: Who said statistical inference was a logical process? Basically, the situation is this: Someone would like to know something about a distribution (usually to make a decision), however, all he has is a limited subset of data drawn from that distribution. **From a strictly logical point of view, he simply does not have enough information to do anything at all. But he must do something anyway.** Thus, he introduces ""premises"" (such as, if a p-value were calculated, it shouldn't be lower than alpha) in order to make up for the information he doesn't have.",2011-02-01T21:32:25.937,2971,CC BY-SA 2.5,
10769,6781,1,(+1) You're really impressive. So many good contributions in so many different statistical fields!,2011-02-01T22:22:30.587,930,CC BY-SA 2.5,
10770,6786,0,"@whuber +1 for this detailed explanation of the modeling approach, and ensuing comments.",2011-02-01T22:26:38.070,930,CC BY-SA 2.5,
10772,6786,0,"@whuber, unfortunately, I fear the margins of this comment are a bit small, yet I don't think my thoughts on this would qualify as an ""answer"". Can you think of a sensible ""generative"" model from which your likelihood would arise and which would match the structure of this data?",2011-02-02T01:17:07.677,2970,CC BY-SA 2.5,
10775,6371,0,"Yes, I know what my options are :) Unfortunately the real problem is not a statistical one....",2011-02-02T02:28:30.380,1144,CC BY-SA 2.5,
10776,6653,0,"Do you have access to the ASA journal Chance?  Seems to me there was a relevant article appearing in the last year or so, whether about hockey or another sport.",2011-02-02T04:33:14.670,2669,CC BY-SA 2.5,
10777,2369,0,"Hi -- yes, a confidence interval's coverage probability is bounded below by the confidence parameter. So a 95% confidence interval will have coverage of at least 95%, irrespective of the true value of the parameter. A credibility interval does not make this guarantee, and can have coverage lower than its probability -- it can even have 0% coverage for some values of the parameter, as in the ""king"" example. See http://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval for a fuller explanation.",2011-02-02T05:37:39.203,1122,CC BY-SA 2.5,
10778,6781,1,"@chl, thanks! I certainly feel that I am not the only one with such characteristics in this site though ;)",2011-02-02T07:37:26.307,2116,CC BY-SA 2.5,
10779,6786,1,"@cardinal, do not do Fermat on us :) If you have some different insight than other answerers feel free to express it in the separate answer, even if it will not constitute a valid answer per se. In [math.SE](http://math.stackexchange.com) for example such situations arise quite frequently.",2011-02-02T07:43:41.933,2116,CC BY-SA 2.5,
10780,6761,0,"@Zach, I thought dyn$lm was a typo of dynlm. I did not know about package dyn. With it everything works. For the code, my other comment about first column of `newdata` still holds. Also you might want to look into using `call` objects. They make the passing of default arguments much more easier. You can ask about it in stackoverflow.",2011-02-02T07:52:27.290,2116,CC BY-SA 2.5,
10781,6761,0,"@Zach, as this answer fully solves the problem, I suggest deleting your other answer (with `parseCall`), and rephrasing this answer to stress the usage of function `dyn$lm`. I will delete my answer also, since `dyn$lm` solves the problem I was refering in it. This way it will be clearer for people who will stumble upon this question (using Google search or other ways), how to deal with dynamic regression in `R`.",2011-02-02T07:55:54.760,2116,CC BY-SA 2.5,
10782,6761,0,"@Zach, +1 for the `dyn$lm` function.",2011-02-02T07:57:10.517,2116,CC BY-SA 2.5,
10784,6796,0,"Yes, I realize I'm not ""supposed"" to be looking at some of these things for ordinal data, but really, it's the only tool I could think of to compare the two years. Really, I was looking at things that could compare the distributions. But, I guess testing means could be plausible - but a confidence interval may not necessarily include my mean as there have been many structural changes to the industry for which this question reviews YoY.",2011-02-02T09:11:28.210,776,CC BY-SA 2.5,
10785,6797,0,"As a note to your second comment. The anchors are the same as they were in the previous year of the survey. Essentially, the granularity of the scale was reduced.",2011-02-02T09:16:59.027,776,CC BY-SA 2.5,
10787,6785,0,"You may think a decision tree will work best for this kinda problem. however, there are a lot unknown factors will affect the result, thus I think probability based model like Bayesian network might be more suitable",2011-02-02T10:18:57.480,2454,CC BY-SA 2.5,
10788,6791,0,why do you say Likert scale and then Categorial/Ordinal? Likert means interval scaled. Can you clarify this a little?,2011-02-02T10:36:48.803,442,CC BY-SA 2.5,
10789,6785,0,"@Zhang: 1. decision trees are also based ""on counting"" and calculating approximate probabilities (similar to bayes), so this is no argument. The question is whether you want to have rules at the end (as stated in one of your comments) or not. 2. If you do not want to have rules, I'd try a naive bayes first and if the results are not satisfying, switch to bayesian network 3. a) Bayesian Networks do take continuous variables, it is just more difficult b) finding the correct/best network architecture is an NP-problem. I suggest to learn/read more how to build BN.",2011-02-02T10:53:21.430,264,CC BY-SA 2.5,
10790,6791,0,"To be more specific, the title should be changed to Likert ""item"". On your second point, I think a lot of people would disagree as to whether or not a Likert item presents interval or ordinal data. For my question, it's an agreement scale, from strongly disagree to strongly agree. Each level of agreement being a ""category"" and the distance between being ""ordinal"". But let's not get tied up in semantics!",2011-02-02T11:04:50.117,776,CC BY-SA 2.5,
10791,6653,0,"I try to reformulate the problem (to stimulate discussion ?): Let's say we have a set of discrete states in a game (e.g. in tic-tac-toe). Now it is reasonable to create one model per state (maybe using logistic regression) to predict the outcome. Now HERE we have also a game, but with continuous states (i.e. game time). The question now of the OP is: How to a) discretize the time into finite set states or b) how to build a model whose parameters vary depending(!) on the current game-time. There must be someone who has already solved this ""general"" problem.",2011-02-02T11:08:53.867,264,CC BY-SA 2.5,
10792,6788,0,"@user2376, shouldn't $P(Y_i=1)$ be $P(X_{i-1}=0,X_{i}=1,X_{i-1}=0)$?",2011-02-02T13:08:44.360,2116,CC BY-SA 2.5,
10793,6801,3,"Note that $Y>a$ almost surely, hence the interval $[y_l,y_u]$  contains the parameter $a$ with probability zero. In fact your argument works if what you are estimating is $\theta=a+\frac12$.",2011-02-02T13:58:17.127,2592,CC BY-SA 2.5,
10794,6795,2,"@Belmont, what is $u(\alpha)$? Could you provide more context about your problem? Link to article with standard properties of LAR for example would help a lot.",2011-02-02T14:00:46.373,2116,CC BY-SA 2.5,
10795,6806,1,"May be I am doing something wrong, but your I _don't_ get any error if df$pos is a character vector. Can you pls `dput` your data.frame?",2011-02-02T14:29:09.213,1307,CC BY-SA 2.5,
10796,6785,0,"Yes, I do want the rules.  I am quite new to Bayesian network, I plan to build a network,get all possible combination of predictors then calcuate probability.then rank them. However, I do not know how to include those predictor with time related values. one solution is like u said, build many networks with different possible preidictor, the problem is there are quite a few similar predictors, thus total number will be big. ...  another solution is to include all predictors in one network, but I do not know how to define their relationship, which one is parents which one is child etc",2011-02-02T14:36:01.527,2454,CC BY-SA 2.5,
10797,6806,1,@suncoolsu : The transform call uses the as.data.frame convention of stringsAsFactors=TRUE so the goal of getting a character column was defeated. It was another factor column. Better would have been to use as.character around the argument passed to strsplit. Full code below.,2011-02-02T14:57:16.033,2129,CC BY-SA 2.5,
10799,6788,0,"@mpiktas  If $b_i$ is the first bit in a run of ones,  why should be $b_{i+1}=0$? ( I think you intended to write $(Y_i=1)=(X_{i-1}=0, X_i=1, X_{i+1}=0)$)",2011-02-02T15:51:09.540,2376,CC BY-SA 2.5,
10800,6808,0,"Thanks for your attention on the matter. It's nice to see that I'm not the only one getting confused over the stuff. That being said, I think you've overestimated my situation; while I have taken a number of courses and am familiar with the existence of a number of different ways of statistical analysis; they never stick with me after the courses. A couple of months after the exams, I keep finding myself wondering; ""I have seen/heard this somewhere, but how did it work really?"" This to me suggests that I need to tear it all down and start building it over with a stronger foundation.",2011-02-02T16:08:45.790,3014,CC BY-SA 2.5,
10801,6814,0,"I don't have the answer... but I am not sure that, in this context, it does really make sense to consider an inverse gaussian distribution with mean 1 and variance 3 or 5...",2011-02-02T16:14:12.183,,CC BY-SA 2.5,user3016
10803,6786,1,"@cardinal Easily.  For example, you collect frequencies and identify and rank the ten highest.  You hypothesize Zipf's Law.  You collect a new set of frequencies and report them according to the *previous* ranking.  That's the iid situation, to which my analysis is perfectly suited, *contingent* upon the new ranks agreeing with the old ones.",2011-02-02T16:27:10.130,919,CC BY-SA 2.5,
10804,6817,0,this seems more like a question for the or-exchange.,2011-02-02T16:29:42.843,795,CC BY-SA 2.5,
10805,6814,6,"Careful!  The KL divergence is not a true ""distance"" because it is *asymmetric*.  In each case, which of the two possible values have you computed?",2011-02-02T17:21:27.687,919,CC BY-SA 2.5,
10806,6761,0,"@mpiktas Thanks for the suggestion. I re-wrote the version I'm using to make use of ""call"" objects.",2011-02-02T17:26:41.907,2817,CC BY-SA 2.5,
10807,6805,1,"+1 Good advice and good references.  It's difficult to think of what use a ""random"" model would have: no method is universal, so one needs to test it on data that are characteristic of the intended areas of application.",2011-02-02T17:28:08.750,919,CC BY-SA 2.5,
10808,6819,7,"+1.  Maybe the ranking depends on the order in which H was computed.  E.g., one could be KL(green, red) and the other could be KL(red, blue).  Morever, tail behavior can have a profound effect on the value: what we really need to see are plots of the *logarithms* of the densities, not the densities themselves.",2011-02-02T17:32:59.087,919,CC BY-SA 2.5,
10809,6814,1,What distributions are these?  Both the Gamma and the Inverse Gaussian take two parameters.  The red one clearly is *not* a Gamma with a shape parameter of 0.85.  Through trial and error it looks like the Gamma has a scale of 1 and shape of 1/0.85 while the Inverse Gaussians have means of 1 and scale parameters as given.  Is this correct?,2011-02-02T17:39:11.183,919,CC BY-SA 2.5,
10810,6786,0,"@whuber, good example. I think that last sentence is key. There is actually some recent work in this vein where it is shown that the number of words in the correct order for a model very similar to the one you posit grows quite slowly. For example, for the entire British National Corpus the number of correctly ranked words is about 75. Bruce Hill's estimator seems tailor made for similar and more general models. More to follow (hopefully).",2011-02-02T18:04:18.707,2970,CC BY-SA 2.5,
10811,6786,0,"@whuber, also, if there is any dependence in how words are generated, then the frequencies are not iid. For the simplest example, consider multinomial sampling. Also, if with very high probability the frequency counts were in a consistent order, how could the associated random variables be iid?",2011-02-02T18:16:20.470,2970,CC BY-SA 2.5,
10812,6786,0,"@cardinal These potential problems seem easy to overcome.  The model I used is appropriate for frequencies observed by randomly sampling the text with replacement.  That's an appropriate model for testing Zipf's law.  If you view the data (hypothetically) as an exhaustive census of a work, then statistical testing is irrelevant: the frequencies (in this example) manifestly do not follow the law.  If the ranks of the words in the dataset differ from their anticipated ranks, then it looks like sorting them makes the distribution look *more*, not *less*, like Zipf: thus my test is generous.",2011-02-02T18:25:19.670,919,CC BY-SA 2.5,
10813,6817,1,"Yes.  It's also vague: whose ""standard method"" are we talking about?  Whose ""third equation""?  Moreover, these issues are discussed extremely well in Numerical Recipes, www.nr.com , chapter 10.",2011-02-02T18:29:31.677,919,CC BY-SA 2.5,
10814,6786,1,"@whuber, thanks for your patience. Now I'm fully clear on your line of reasoning. Under the sampling model you've now fully fleshed out, I agree with your analysis. Perhaps your very last statement is still a bit slippery. If the sorting does not induce strong dependence than your method would be conservative. If the induced dependence were moderately strong, it might become anticonservative. Thx for you patience in the face of my pedantry.",2011-02-02T19:16:56.853,2970,CC BY-SA 2.5,
10815,6794,1,"This question has hardly anything to do with applied or theoretical statistics, and I would have voted to close if no (thorough) answers were already given.",2011-02-02T19:21:07.703,930,CC BY-SA 2.5,
10816,6791,0,"@Henrik @Brandon There were already some discussions, headed under the [scales](http://stats.stackexchange.com/questions/tagged/scales) tag, about the nature and the way to treat Likert scale/item.",2011-02-02T19:23:47.953,930,CC BY-SA 2.5,
10817,6794,1,"@ chl where would these questions go though? I found it quite hard to find references on setting up R, sweave and latex when i started, so thats why i answered.",2011-02-02T19:30:31.113,656,CC BY-SA 2.5,
10818,6786,1,"@cardinal Not pedantry; correctness.  My analysis blithely overlooked the important issues that you raise.  I agree that the last argument is a bit of hand-waving, too.  It would still be interesting (hint) to see an approach to the original question that is directly suited to how such frequency data are typically collected.",2011-02-02T19:48:48.037,919,CC BY-SA 2.5,
10819,6794,1,"@richiemorrisroe Well, at least a quick check on SO and Google would be helpful (IMHO). I'm not criticizing the question itself, just the fact that such questions are not really in line with CV [FAQ](http://stats.stackexchange.com/faq) and the other questions here, but I may be wrong. I've upvoted your response as well as @PaulHurleyuk's one, though. I guess this question will be kept alive because of your answers (although there's already a vote to close, which was also the reason of my warning).",2011-02-02T20:25:24.303,930,CC BY-SA 2.5,
10820,6822,0,"thanks for the links, I will try and go through them as much as I can in the coming weeks... I have been exposed to R once before, in survival analysis course in which we did a lot of multivariate regression (cox and aelen models) and a bunch of other stuff I can't really recall. My impression of R, as a person who's very used to MATLAB was quite negative, but I did it had a lot to do with the fact that we were more or less thrown to the deep end of the pool, and then expected to learn to swim on our own, which of course led to me hating the software from then on :) Time to change that perhaps",2011-02-02T20:37:28.497,3014,CC BY-SA 2.5,
10821,6817,1,"The second equation makes no sense and appears to contradict the first.  What are these equations supposed to be doing?  If we take $x_k$ to be a vector then the first equation looks like an update step.  The second and third equations are not.  The third equation is part of the usual termination criteria.  Another part of those criteria is that $|F(x_{k+1})-F(x_k)|$ should be small, where $F$ is the objective function (of which $f$ is its (scaled) gradient).",2011-02-02T21:45:22.893,919,CC BY-SA 2.5,
10822,6817,0,"I think the OP meant the second equation to be $k = k+1$. We should also not take $f(x)$ to be the function to be minimized, I believe.",2011-02-02T21:58:21.003,795,CC BY-SA 2.5,
10823,6808,0,"I would add a resounding ""agree"" for Harrell's (note spelling) text. It is excellent as is the two-package combination of the R code that accompanies it. I also think ""Modern Applied Statistic with S"" by Venables and Ripley would be a good acquisition. I had a masters level background (with an undergrad degree in physics)  before using MASS to learn R. There is a wealth of application-wisdom in that text.",2011-02-02T22:44:04.500,2129,CC BY-SA 2.5,
10824,6817,0,"I agree that this is off-topic, but possible relevant sites are in production... It could work on SO, but I'm eager to let it stay while it was answered.",2011-02-02T23:21:26.660,,CC BY-SA 2.5,user88
10825,6820,0,"It is also a pretty good topic for machine learning, especially if linear methods would fail/be too general.",2011-02-02T23:26:22.083,,CC BY-SA 2.5,user88
10827,6827,3,"What is the final product supposed to look like? Have you already tried something that doesn't work or takes too long? Where does your ""table"" above come from?",2011-02-02T23:52:57.627,696,CC BY-SA 2.5,
10828,6795,0,"@Belmont, This looks like a problem from Hastie, et al., *Elements of Statistical Learning*, 2nd. ed. Is this homework? If so, you might add that tag.",2011-02-03T01:50:41.550,2970,CC BY-SA 2.5,
10829,6605,0,"@Eduardo - I think you would have a hard time convincing someone that abandoning logic is the best thing to do.  You simply move from *deductive* logic to *inductive* logic when you have insufficient information to do *deductive* logic.  And of course he must do something - but the something that he does should be consistent, based on a proper test statistic.  A proper test statistic has to be defined when the null is true and when it is false. A p-value is not, as my previous comments show.",2011-02-03T02:56:56.893,2392,CC BY-SA 2.5,
10830,6814,1,Information-theoretic interpretation of KL divergence is in answer here -- http://stats.stackexchange.com/questions/1028/questions-about-kl-divergence/1569#1569,2011-02-03T03:21:25.423,511,CC BY-SA 2.5,
10831,6821,0,can you give link to Mathematica source?,2011-02-03T03:24:34.983,511,CC BY-SA 2.5,
10832,6814,2,"Also, KL-divergence is not symmetric, so to remove ambiguity it is better to say KL(A,B) or KL(B,A) instead of ""distance between A and B""",2011-02-03T03:28:55.180,511,CC BY-SA 2.5,
10833,6605,0,"@probabilityislogic: Inductive ""logic"" is not logic at all; it is just (sometimes educated) guessing. Fortunately for us, some forms of induction (such as statistical inference) are _reliable enough_, so the risk associated to the possibility that the induction be wrong is overshadowed by the benefit of getting more information, even if it is somewhat uncertain. We don't seek to have perfect information, just enough information to make good decisions. Remember that both deduction and induction are just tools.",2011-02-03T05:02:54.053,2971,CC BY-SA 2.5,
10834,6835,5,"You will need much more explanation. What do you mean by ""trend detection"" and how are you defining ""trend""? It would help if you explained your specific problem rather than ask a general and vague question.",2011-02-03T05:04:47.763,159,CC BY-SA 2.5,
10835,6821,0,@Yaroslav I added it to the end.,2011-02-03T05:05:03.457,919,CC BY-SA 2.5,
10836,6817,0,"@shabbychef Thank you.  Yes, $f$ clearly is some multiple of the gradient of the objective function.",2011-02-03T05:07:01.270,919,CC BY-SA 2.5,
10838,6821,1,@Whuber : Wahou ! Thank you very much. I am going to prepare a coffee and then I focus on your answer !,2011-02-03T06:59:27.540,,CC BY-SA 2.5,user3016
10839,6785,0,"@zhang: I can only repeat myself. If you want to have rules, I'd try a decision tree approach first. BN does not give you rules without application of an additional algorithm (don't know which one, but I am pretty sure one exists). See additionally the field of feature subset selection. E.g. the tool rapidminer has a lot of functions to search the predictor space efficiently.",2011-02-03T07:25:31.487,264,CC BY-SA 2.5,
10840,672,3,I also want to suggest this link as some kind of low-level-explanation: http://yudkowsky.net/rational/bayes,2011-02-03T07:52:40.477,264,CC BY-SA 2.5,
10841,6821,0,"@whuber +1, excellent and detailed answer as always :)",2011-02-03T07:56:36.260,2116,CC BY-SA 2.5,
10842,6821,0,"@Whuber: I have read carefully your answer. As far as I understand, the blue curve is indeed closer to the red curve than the green one even if my eyes suggested me the opposite. The way to realize this is to plot the logarithms instead of the densities themselves. Right? Thank you!",2011-02-03T08:09:41.937,,CC BY-SA 2.5,user3016
10843,6812,2,"+1, did not know that you can pass `[[` as a function.",2011-02-03T09:00:07.717,2116,CC BY-SA 2.5,
10844,6808,0,"The Gelman regression book is wonderful, he explains it all very well, and provides R code which is really useful to check your understanding of the material.",2011-02-03T10:04:07.910,656,CC BY-SA 2.5,
10845,2742,0,I asked a question that illustrates the need of factor rotation after PCA since PCA gives the biased result. See http://stats.stackexchange.com/questions/6575/how-to-do-primary-component-analysis-on-multi-mode-data-with-non-orthogonal-prima,2011-02-03T10:08:29.517,2820,CC BY-SA 2.5,
10846,6841,0,"@teucer, what do you mean by simulate? You want to get the forecasts?",2011-02-03T10:28:11.450,2116,CC BY-SA 2.5,
10847,6812,0,"@mpiktas: Yes, and one can also pass ""["" as a function, including sometimes adding an extra comma with an empty argument followed by an second index expression in order to work on columns in dataframes or to get to parts of other lists. Probably ""[<-"" as well, but I don't have a ready made example in mind, and I don't know if there might be some sort of environment issue.",2011-02-03T12:19:36.640,2129,CC BY-SA 2.5,
10848,6841,0,"@mpiktas I want  to plug in new values for x1, x2 and  get the probability distribution for trvr (which should be the sum of two normals, right?). So it is a simulation and not a  forecast...",2011-02-03T12:53:19.227,1443,CC BY-SA 2.5,
10849,6841,0,"@teucer, in general Arrelano-Bond estimator does not assume normality. Furthermore it is used to estimate conditional expectation $E(y|x)$, so it is not a good idea to use it for simulating the probability distribution of $y$.",2011-02-03T13:10:21.887,2116,CC BY-SA 2.5,
10851,6844,0,"Both books seem to have good reviews on amazon, can anyone add any opinions (perhaps in a bit more detail) on these? btw; by casella & berger do you mean ""Statistical Inference""?",2011-02-03T13:11:59.253,3014,CC BY-SA 2.5,
10852,6449,0,"@onestop, @probabilityislogic, I've updated my answer to take your comments into account.",2011-02-03T13:36:12.130,2116,CC BY-SA 2.5,
10853,6841,0,"@mpiktas thx for the explanation. I tried a mixed effect model (lme4), the problem is somehow the fitted values (lme4::fitted) are strangely close to trvr(-1) instead of trvr. Is that normal?what can be the problem?",2011-02-03T13:54:47.077,1443,CC BY-SA 2.5,
10855,6841,0,"@teucer, mixed effect models are not appropriate when lagged dependent variables are used. This is a standard result which is discussed in all books which describe dynamic panel regression. The problem is that lagged variable correlates with the random effect (in mixed effect model parlance, it is individual effect in panel data terminology).",2011-02-03T14:00:33.497,2116,CC BY-SA 2.5,
10856,6841,0,"@teucer, you will need to specify the error distribution also. Even then it will not be trivial if normality is not assumed. For normality assumption you will need to test whether your residuals are normal.",2011-02-03T14:02:27.910,2116,CC BY-SA 2.5,
10857,6841,0,"@mpiktas how can I model such systems then? The goal is to simulate next year's values. (I have mistakenly deleted my previous comment: can't I assume distributions for x1,x2 and derive a distribution for trvr in Arellano-Bond (AB) setup? Btw, the AB fitted values are also close to trvr(-1), is that normal?)",2011-02-03T14:05:19.763,1443,CC BY-SA 2.5,
10858,6841,1,"@teucer, why do you insist on simulating? If you want to get the feel how model works for different future values of `x1` and `x2`, simply plug them into model and calculate `trvr`. What is the purpose of your simulation?",2011-02-03T14:09:43.037,2116,CC BY-SA 2.5,
10859,6821,3,"@Marco Yes.  It's a good idea also to plot the pdf for the density against which you are integrating (which you had already done for this question).  You might find it helpful to read the section on ""Graphical Moments"" at http://www.quantdec.com/envstats/notes/class_06/properties.htm .",2011-02-03T14:09:52.123,919,CC BY-SA 2.5,
10860,6839,0,"Great!  Thanks for the help.  I was going to use R, and was going to setup the data similar to how you suggested, interaction effects and all.  Glad to see I was on the right track, and I do really appreicate your time.",2011-02-03T14:53:53.487,569,CC BY-SA 2.5,
10861,6841,0,"@mpiktas OK, I can do that. But why are the AB fitted values close to trvr(-1)?",2011-02-03T15:15:04.043,1443,CC BY-SA 2.5,
10863,6841,1,"@teucer, look at the coefficients, lag coefficient is 0.9, this is very large. This means that almost everything is explained by the lagged value. If the values of x1 and x2 are small they contribute little, since the corresponding coefficients for the them and the constant are small.",2011-02-03T16:59:01.347,2116,CC BY-SA 2.5,
10866,6850,0,"Understood ! Thank you very much, again !
I have computed the KL divergence using the integrate() function of R and I have obtained the same result as yours.",2011-02-03T18:45:43.383,3019,CC BY-SA 2.5,
10867,6853,0,"@user862, hint: use chracteristic function of bivariate normal.",2011-02-03T18:52:59.743,2116,CC BY-SA 2.5,
10870,6856,6,"Sounds a bit like data dredging to me. Shouldn't the focus be on what you plausibly think is an appropriate model, what covariates, transformations etc *before* you start modelling. R doesn't know you did all that model fitting to find a good model.",2011-02-03T19:08:48.297,1390,CC BY-SA 2.5,
10872,6836,0,"Whoa. That posted prematurely‚Ä¶ the table structure didn't account for year since I was assuming that I'd have a separate table for each year‚Ä¶ so the year was inherent. I think, like you suggested, I'll have separate tables while I input the data (which I have to mangle from existing Excel files), but I'll combine them into a wide-structured final analytical database.",2011-02-03T19:18:04.903,3025,CC BY-SA 2.5,
10873,6836,0,Now I just need to figure out how JMP works with long and wide structures‚Ä¶,2011-02-03T19:18:42.297,3025,CC BY-SA 2.5,
10874,6856,3,"@Gavin - I can see this getting horrendously off-topic very quickly, but the short answer is no, I am not advocating data dredging or finding spurious relationships between random variables in a dataset. Consider a regression model that includes income. Is it not reasonable to test transformations on income to see their impact on the model? Log of income, log of income in 10s of dollars, log of income in 100s...?Even if this is data dredging - a function / summary tool that can aggregate the output from many model runs would still be very helpful, no?",2011-02-03T19:32:15.963,696,CC BY-SA 2.5,
10875,6859,1,"@Eduardo, +1, nice graph. It should be used with care though when the different transformation of dependent variable is used in different regressions.",2011-02-03T19:39:58.247,2116,CC BY-SA 2.5,
10876,6859,0,"mpiktas, that¬¥s true in a table as well. Graphs just make it more compact, at the expense of precision.",2011-02-03T19:44:02.530,375,CC BY-SA 2.5,
10877,6853,2,See equation (11) in http://www.stuart.iit.edu/shared/shared_stuartfaculty/whitepapers/thomopoulos_some.pdf (but watch out for the awful typesetting).,2011-02-03T19:44:59.237,919,CC BY-SA 2.5,
10879,6825,1,"I agree, this question needs another exchange site :)",2011-02-03T19:54:28.580,2902,CC BY-SA 2.5,
10885,6855,1,"This is quite strange since those are implementation of the same algorithm... Are you sure you are not comparing OOB error estimation with error on train (this one is usually near 0 for RF, and this does not mean overfitting)?",2011-02-03T22:44:49.153,,CC BY-SA 2.5,user88
10886,6855,1,"RFs are generally not prone to overfitting issues... With default settings, 500 trees will be grown, considering $\sqrt{9}=3$ variables each time. As @mbq said, the `cparty` package relies on the `randomForest` package, but add some convenient way to assess ""conditional importance"". It's hard to tell anything without knowing how data are structured (No. classes, sample size/class, etc.). Of note, if you're trying all parameters combinations for your model without using a cross-validation scheme, then you're likely to break the control exerted by bagging etc.",2011-02-03T22:54:17.707,930,CC BY-SA 2.5,
10887,6855,0,"No, I compare both train errors and for RF it is usually equal to zero, but for cforest (party) it is much bigger and not expected... I know RF errors on additional test data are typically about 10-20%.",2011-02-03T23:12:31.627,3041,CC BY-SA 2.5,
10888,6855,1,"If so, I'm clueless... As @chl wrote, some code (or, ideally, a reproducible example) would be helpful. It may be also a bug/strange feature of party itself, so it can be a good idea to consult package maintainer.",2011-02-03T23:53:55.543,,CC BY-SA 2.5,user88
10889,6855,0,@mbq I found that randomForest is based on CART trees and cforest - on unbiased conditional inference trees.,2011-02-04T00:50:59.023,3041,CC BY-SA 2.5,
10890,6844,0,Yes 'statistical inference'. For me a big step was going from understanding probability models to understand how to use data to test models and estimate parameters of models. Especially the Davison book really focuses on this point.,2011-02-04T01:02:32.227,3033,CC BY-SA 2.5,
10891,6865,0,"Hey @caracal, I think the way you are using ""Split-Plot Design"" is not the way as Casella, George defines it in his book, Statistical Design. Split Plot definitely talks about nesting, but is a special way of imposing the correlation structure. And most of the time you will end up using `lme4` package to fit the model AND NOT `lm`. But this may be a very specific book-based view. I will let other's comment on it. I can give an example based on how I interpret it which is different from yours.",2011-02-04T01:31:24.907,1307,CC BY-SA 2.5,
10892,1639,4,"F-test is the generalization of t test. t-test is for 2 treatment comparison and the F test is for multiple treatments. The derivation is in Casella's Statistical Design, Chapter 3 and 4. However, as Prof. Hyndman points out, with unequal variances, it is not a t-test anymore. It is the Fisher Behren's problem. We generally don't use the Fisher's solution, instead use Welch's Test or a Bayesian approach.",2011-02-04T01:35:03.200,1307,CC BY-SA 2.5,
10893,6859,0,@Eduardo can you please share the code for graphs?,2011-02-04T01:40:00.670,1307,CC BY-SA 2.5,
10896,6870,3,"To truncate, use T(-,-), but read the users manual for info on censuring and truncationq",2011-02-04T04:06:11.787,1381,CC BY-SA 2.5,
10898,6799,0,"Thank you for the thorough respose. KPSS is having trouble assessing my output, but I think that this is the sort of analysis that I was trying to do. I will keep the question open a bit longer, in case there are some other suggestions.",2011-02-04T04:25:38.940,3010,CC BY-SA 2.5,
10899,6799,0,"@Marshall Ward, KPSS has a problem if the non-stationary part is only a small part of the all series. If there are new observations coming up all the time, I suggest using `monitor` from the same **strucchange** package.",2011-02-04T06:48:17.210,2116,CC BY-SA 2.5,
10900,4280,0,"+1, I can confirm that this works and it is very simple. I used it in my pretty complicated project: jQuery+PHP+R.",2011-02-04T06:53:41.290,2116,CC BY-SA 2.5,
10901,6859,2,"@suncoolsu R code is available on the first link given in @Eduardo's response. He he, it's `grid`, not `lattice` :)",2011-02-04T07:58:48.097,930,CC BY-SA 2.5,
10902,5257,1,"I found a potential bug in fitted.pgmm and left an ""issue"" in github...",2011-02-04T08:33:31.233,1443,CC BY-SA 2.5,
10904,6838,0,"this is a very interesting question. Is it possible to play at least with a subset of anonymized data? And how were the forecasts generated, what kind of model was used?",2011-02-04T08:49:02.957,2116,CC BY-SA 2.5,
10905,6838,1,"@mpiktas thanks, you can concider it has been generated with a suitable AR modeling (one for each wind farm), it won't change the problem very much. Sorry, there are too much confidenciality issues with these data, can't provide you anything, even anonymized...",2011-02-04T09:57:43.237,223,CC BY-SA 2.5,
10906,6865,2,"@suncoolsu The terminology in the social sciences might be different, but both Kirk (1995, p512) and Maxwell & Delaney (2004, p592) call models with one between- and one within-factor ""split-plot"". The between-factor provides the ""plots"" (analogous to the agricultural origin).",2011-02-04T10:46:45.363,1909,CC BY-SA 2.5,
10907,6869,0,"I appreciate the intro to analyzing split-splot designs with mixed-effects models and further background info! It certainly is the preferred way to carry out the analysis. I've updated my question to emphasize that I'd still like to know how to do this ""the old way"".",2011-02-04T10:51:53.373,1909,CC BY-SA 2.5,
10908,6605,0,"@Eduardo - The problem with most forms of statistical inference is that they are hard to generalise, becuase they are often based on intuition rather than solid foundations.  The p-value is such an example (how to adjust for multiple comparisons, how to deal with nuisance parameters, arbitrary choice of the statistic on which the p-value is based).  Without a set of more broad axioms or desiderata to guide the generalisation, p-values are basically restricted to the realm of 1 parameter problems, or where pivotal quantities exist.",2011-02-04T10:59:36.213,2392,CC BY-SA 2.5,
10909,6839,1,Be careful with the non-independence generated by including multiple time-slices. A random effects (multi-level) model could help.,2011-02-04T11:52:17.223,375,CC BY-SA 2.5,
10910,6714,0,"Is (1) really the commonly accepted definition of classical probability? I find it circular, and use a different one myself. Specifically, the long-run proportion is a random variable, and only approaches the probability in probability. The definition I use is a many-worlds interpretation, in which probability is the proportion of relevant possible futures. My understanding is that quantum physics isn't at war with this interpretation, but that's difficult to ascertain with coevolutionary fields.",2011-02-04T12:48:01.187,2456,CC BY-SA 2.5,
10912,6877,0,"I've edited with your suggestion to clarify, if you'd like to add anything.",2011-02-04T13:51:35.503,3048,CC BY-SA 2.5,
10913,6877,0,"It's interesting that you don't feel any specific courses are good indicators. In comparison, I've found that when interviewing programmers, great programmers have often studied any combination of Advanced Algorithms II/III, Cryptography, Paradigms of Programming languages, or Compiler Construction. If their grades in those are good they seem more likely to be good programmers.",2011-02-04T13:55:55.797,3048,CC BY-SA 2.5,
10914,6876,0,If someone wants to re-tag this question I'd be happy. I couldn't with my low rep.,2011-02-04T14:00:01.423,3048,CC BY-SA 2.5,
10915,6605,0,"@probabilityislogic: I don't think it's desirable to generalize p-values to n-parameter problems. I find it conceptually simpler to reduce those problems to 1-parameter goodness of fit problems. It might not fit your definition of beautiful (it certainly doesn't fit mine), but it works.",2011-02-04T14:17:08.843,2971,CC BY-SA 2.5,
10916,6839,1,"@ Eduardo: I agree that the dependence is not modelled and that this is somewhat problematic, thanks for pointing it out. I'm not sure how random effects would help -- since the binary outcome `win_home` is constant at the level of grouping (i.e. for all time slices for any given match it's either 0 or 1), including, e.g. a random intercept, for the matches will just result in huge problems with separation in this context.",2011-02-04T14:44:05.380,1979,CC BY-SA 2.5,
10918,6839,0,"You might also want to consider including a parameter for total goals scored, as leads tend to be given away more easily in high scoring games.",2011-02-04T14:51:14.607,229,CC BY-SA 2.5,
10919,6340,1,"You maybe meant to say the gamma, beta, etc. functions are ""transcendental.""",2011-02-04T15:15:15.643,2971,CC BY-SA 2.5,
10920,6867,1,See http://stats.stackexchange.com/q/5504/919 .,2011-02-04T15:41:18.847,919,CC BY-SA 2.5,
10922,6859,0,"@Eduardo - Thanks for the detailed answer, I wasn't aware of `memisc` previously, looks like a very handy package to have in one's quiver!",2011-02-04T16:36:40.470,696,CC BY-SA 2.5,
10923,6873,0,"+1 Thanks Fabians, You are totally right, the problem is not that I don't have enough data. Note that my question is especially about the interdependence structure. Scatterplots, heatmaps and hexbin plot are good tools if they are used for the good purpose.  I think general additive model can also be very powerfull there is a wonderfull paper of Brillinger providing good hints on how to use GAM.",2011-02-04T16:45:06.010,223,CC BY-SA 2.5,
10924,6879,0,"This is very helpful, thanks. Your answer will help me get further. I understand the need for more specific questions, which I shall do in the future. Now that I have a starting point, I can get into more specific areas. Thanks again.",2011-02-04T17:07:03.633,3040,CC BY-SA 2.5,
10925,5581,1,"Very, very important!",2011-02-04T17:18:08.080,2902,CC BY-SA 2.5,
10927,2952,1,"Hello there, 

Just to let you know that Gephi cannot handle weighted undirected graph to identify community through modularity. 
thanks. 

-Gautam",2011-02-04T18:24:21.047,,CC BY-SA 2.5,user3038
10928,2952,1,"@Gautam Good to know, thanks. I am not really familiar with Gephi, but I thought it was in active development.",2011-02-04T19:11:29.053,930,CC BY-SA 2.5,
10929,6876,1,"this question is off-topic, so I will comment instead of answering. Ask about M-estimators. Or pick any problem from Assymptotic Statistics by van der Vaart. The only one problem with this question is that you cannot say that the interviewee is not excelent if he(she) did not answer. If on the other hand he (she) does answer, excellency is pretty much guaranteed. This is of course IMHO.",2011-02-04T19:15:14.277,2116,CC BY-SA 2.5,
10930,6859,0,Memisc has a bunch of not very much related but nonetheless very useful functions. I don't use it as often as I should.,2011-02-04T19:23:45.687,375,CC BY-SA 2.5,
10931,6891,0,"To be sure, your design is really 3x2x3 (3 factors, all with 3 levels except the second one), with three replicates, that is 54 values for each statistical unit?",2011-02-04T19:34:19.207,930,CC BY-SA 2.5,
10932,6891,0,"there are 3 factors, each of which have 3 more factors, each of which has 2 more factors (so like DrugA/B/C + DrugX/Y + ConditionA/B), for a total of 18 measurement groups",2011-02-04T19:38:03.073,1327,CC BY-SA 2.5,
10933,6855,0,"@42n4 My fault! You're right: `cparty` does not depend on the `randomforest` package. The authors have implemented their own stuff on RFs (I just browse the R and C code). This does not solve your problem, though. Could you add example data or more context about your analysis?",2011-02-04T19:42:11.553,930,CC BY-SA 2.5,
10934,6876,4,"@mpiktas Are you saying that knowledge of asymptotics will assure that one is excellent at visualizing data?  There seems to be little to connect the two.  Indeed, that's the problem with this entire question: its premise is that excellence in data mining, stats modeling, and data visualization requires ""great mathematical statisticians"" who have taken lots of courses.  Neither one of those criteria--being mathematical or taking courses--seems to be closely related to succeeding in such positions.",2011-02-04T21:08:13.470,919,CC BY-SA 2.5,
10935,6877,0,"@John Things have changed.  When I was hiring programmers in the late '80s, two tests emerged as the most reliable indicators of actual performance.  The first was typing ability(!) and the second was any ability at all to do trigonometry.  The former was related (quite crudely but effectively) to experience and the latter was related to capacity for abstract thinking.  I suspect your list of courses might be measuring something similar to the latter.",2011-02-04T21:16:28.763,919,CC BY-SA 2.5,
10936,6896,0,"what is your motive behind ""determining the closeness"" of these data sets? What you are trying to do may be Ok.., but may inference can be improved if you are interested in other specifics like `mean configuration` or equivalent.",2011-02-04T21:58:55.417,1307,CC BY-SA 2.5,
10937,6896,4,"As a side note: I don't think you mean $|\theta_1 - \theta_2| \mod \pi$. Rather something like $\min\{|\theta_1 - \theta_2|, 2 \pi - |\theta_1 - \theta_2|\}$.",2011-02-04T23:16:03.117,2898,CC BY-SA 2.5,
10938,6896,4,"Rather than working with angles, I suggest converting to (x, y)-coordinates on the unit-circle first. You can then calculate normally (distances and the like), and averaging isn't a problem like with angles.",2011-02-04T23:19:48.547,1909,CC BY-SA 2.5,
10939,6896,1,"@suncoolsu The motive is to determine if my training set has enough similar examples to infer the poses of my test set. I am using a learning-based approach to pose estimation, and I am beginning to suspect that the data I am using is too different across data sets. I want to ensure that for each pose in a test set, there is at least a ""reasonably similar"" pose in the training set. Visually (reanimating the poses) it appears that my suspicion is well-founded, but I want to analyse it quantitatively.",2011-02-05T01:36:53.777,3052,CC BY-SA 2.5,
10941,6896,0,"@caracal Can you please explain what problem exists when averaging angles? Is it specific to a particular representation, or a general problem regardless of representation? Thanks.",2011-02-05T02:06:38.133,3052,CC BY-SA 2.5,
10942,6896,2,"@Josh Erik P.'s suggestion is a good one.  Alternatively, consider each angle $\theta$ to be a point $\left(\cos(\theta), \sin(\theta)\right)$ on the unit circle and compute the Euclidean distances between them using the usual (Pythagorean) formula.  The difference between these distances and the angular distances shouldn't matter.  (I believe this may be what Caracal suggested, too.)",2011-02-05T02:34:02.427,919,CC BY-SA 2.5,
10943,6896,0,"@Erik, I just corrected what @Josh had and I think @whuber is right.",2011-02-05T03:38:46.297,1307,CC BY-SA 2.5,
10944,6891,0,"Lets see: you are saying -- for each unit, you have the treatment like (., ., .) and the dots represent one of ABC, one of XY, on of AB? and you have 18 such units? For eg: a treatment may look like (A, Y, A) for _a_ particular unit?",2011-02-05T04:06:46.697,1307,CC BY-SA 2.5,
10945,6859,0,"@Chl .. Thanks .. for the humor. Eduardo, your website is great! Thank you for sharing awesome code :-) I hope I can do it in `lattice`. :D",2011-02-05T04:37:30.403,1307,CC BY-SA 2.5,
10946,6889,0,What you say makes sence. We use the information on courses and grades as one of many data points to make sense of a greater picture.,2011-02-05T07:50:34.517,3048,CC BY-SA 2.5,
10947,6876,0,"@whuber - I'm not claiming it to be the only way, nor the best way. I'm hoping that it'll be a nice addition to an already extensive and exhaustive recruiting process that hopefully is more effective than asking the man on the street.",2011-02-05T07:54:10.473,3048,CC BY-SA 2.5,
10948,6896,2,"@Josh The average of, e.g.,  $\pi/4$ and $7\pi/4$ is $\pi$. In many circumstances, this doesn't make sense, and it should be $0$ instead. In your specific situation, this might not be an issue since maybe human joints don't have a range of motion past $\pi$. Also, in your case, maybe you want the aforementioned average to be $\pi$ since the joint motion is uni-directional. @whuber's suggestion is exactly what I meant.",2011-02-05T10:45:18.023,1909,CC BY-SA 2.5,
10949,6714,0,"@sesqu- but a ""many worlds"" interpretation is useless, because we only ever observe one of them!  How do know that the ""other worlds"" obey the same set of probabilities that this particular world does? (perhaps logic does not exist in these other worlds - how could we ever know?)  It is completely arbitrary (at least to us in ""this world""), and based entirely on assumption.  (obviously, I am not a subscriber to the ""many worlds"" - I think it is just as demonstrable as the existence or non existence of a God).",2011-02-05T10:46:27.257,2392,CC BY-SA 2.5,
10950,6714,0,"@sesqu - in reply to the rest of your comment, you may have a point about circularity.  I don't like the definition of probability as frequency anyway, but this is not the point of the answer to the question.  If you replaced my definition with yours, I don't think it would change the conclusions, because it is something which can be confused with ""rational degree of belief"" (which I think is the best way to define probabilities - they are only known to exist in your mind, so why not define them as such?)",2011-02-05T10:55:46.883,2392,CC BY-SA 2.5,
10951,2419,2,"Rpy is a really nasty dependency. R has a huge set of features, and thus it is nice to be able to dig in them using Rpy, but if you have to share that work, you might be in trouble, even if it is across different computers of a same lab, if your lab is in a heterogeneous computing environment. This is due to the fact that Rpy depends on having the right minor versions of Python, numpy and R. For instance, it keeps being broken in the major Linux distributions.",2011-02-05T10:59:47.673,1265,CC BY-SA 2.5,
10954,6859,0,@suncoolsu Let's go with a `lattice` implementation then :) It should not be too hard.,2011-02-05T11:14:19.347,930,CC BY-SA 2.5,
10955,2369,0,"@Keith - if what you say is true, then you should point out the mistake I have made in my answer (relating to Wasserman's example).  Because the CI in that case does not have the 95% coverage for all values of the parameter.  So if you are correct, then logically, I must have made a mistake somewhere in the calculations.",2011-02-05T11:25:18.147,2392,CC BY-SA 2.5,
10956,2287,30,"Good answer - just one minor point, you say ""....Instead of saying the parameter has one true value, a Bayesian method says the value is chosen from some probability distribution.....""  This is not true.  A Bayesian fits the probability distribution to express the uncertainty about the true, unknown, fixed value.  This says which values are plausible, given what was known before observing the data.  The actual probability statement is $Pr[\theta_0\in (\theta,\theta+d\theta)|I]$, where $\theta_0$ is the true value, and $\theta$ the hypothesised one, based on information $I$.",2011-02-05T11:34:13.310,2392,CC BY-SA 2.5,
10957,2287,1,"...cont'd... but it is much more convenient to just write $p(\theta)$, with the understanding of what it means ""in the background"".  Clearly this can cause much confusion.",2011-02-05T11:38:26.533,2392,CC BY-SA 2.5,
10958,6868,1,"Sounds right to me.  But remember that this is basically a prior probability, because it is based on expectations (aka beliefs) about what will happen, rather than a description of what will actually happen to the price.  The price in reality is not random, because it will be set by the managers of the various companies.  Probability provides a way to describe the uncertainty due to not knowing what the other managers will actually do when setting their prices.",2011-02-05T12:59:50.673,2392,CC BY-SA 2.5,
10959,744,1,"This reminds me of a quote made by Edwin Jaynes.  It roughly goes ""...a mathematician came to me and said 'I found a brilliant solution, all I need now is the problem'...""",2011-02-05T13:09:13.720,2392,CC BY-SA 2.5,
10960,6714,0,"@probabilityislogic - if probability is defined as state of mind, we lose derived results (except maybe as conjugate posteriors from a description length perspective). Consider the binomial distribution - if the probability $P(K=k|p,N)$ is defined as belief, what is the $p$ parameter of that belief? Not a probability. The binomial would have to be seen as the limiting case of a beta-binomial. While such complications may prove manageable, I don't think the *rational belief* can be. As for the other worlds behaving similarly: that is explicit in the conditional iid assumption always made.",2011-02-05T13:09:19.883,2456,CC BY-SA 2.5,
10961,732,3,"I would say that the key to cracking the meaning of this quote is to recognise that the word ""strange"" is relative to what your model of ""normal"" is.",2011-02-05T13:11:45.797,2392,CC BY-SA 2.5,
10962,1280,1,And 45.8% of people don't believe that statistic,2011-02-05T13:12:23.133,2392,CC BY-SA 2.5,
10965,6891,0,"yes, but maybe that was a bad choice of letters.  It would be more like ABC/XY/NM for an example of A,X,M",2011-02-05T13:37:09.930,1327,CC BY-SA 2.5,
10966,6896,3,"Your problem will probably become much easier to solve if you can specify the consequences of ""getting it wrong"".  So if you say the data sets are the same or similar, but they in fact aren't, what will happen to you?  Will it depend on ""how wrong"" your decision was?  What will happen if you declare the data/poses different, but they are in fact the same or similar?  What is lost?  answering these questions will help determine *what matters* for the comparison *you* want to make.  This ensures that you are answering the right question.",2011-02-05T13:39:26.290,2392,CC BY-SA 2.5,
10967,6714,0,"@sesqu - It is a belief, conditional on *knowing* $p$ and *knowing* $N$, and *knowing* that they are parameters of a binomial distribution.  But this probability is either $0$ or $1$ once we actually observe the value $K$ (it either is $k$ or it isn't).  What changed? nothing but our state of mind",2011-02-05T13:49:26.667,2392,CC BY-SA 2.5,
10968,6714,0,"@sesqu - on the ""iid"" comment: this may be true, but it is unverifiable if you consider ""alternate worlds"" view.  In the ""single world"" view, you can compare observations and verify the consistency of the iid assumption, but in ""alternate worlds"" how can you do this?  You have to be able to look into the other worlds, to see what results they give.  How is one supposed to do that?",2011-02-05T13:55:41.537,2392,CC BY-SA 2.5,
10969,6889,0,"Thanks! Of course, I figured you would not base your selection process on grades, I merely tried to present an alternative perspective. I have a number of friends that are devoted programming geeks, that never cherished under the rigidity of university education. Anyways, as long as you stay open minded about the strengths and weaknesses of the candidates, I am sure you'll get great team members eventually. :)",2011-02-05T13:56:31.897,3014,CC BY-SA 2.5,
10970,6714,0,"@sesqu - a better way to think of iid assumption is that it is one of the least restrictive assumptions you can make about data.  Each data point is ""free"" to roam without being restricted to what other data points are.",2011-02-05T13:59:39.487,2392,CC BY-SA 2.5,
10971,6714,0,@probabilityislogic - the question I attempted to pose is the nature of $p$. What is this known thing your faith in $k$ is so specifically conditional on? Why is it conditional on $p$ in that particular manner?,2011-02-05T14:17:52.250,2456,CC BY-SA 2.5,
10972,6714,0,"@sesqu - because that is the information that was put into the probability (i.e. the conditions on the right hand side of the $|$, plus the statement you made about it being a binomial distribution).  Probability only exists conditionally, you must specify what conditions the probability is to be calculated (i.e. you must specify your state of mind)",2011-02-05T15:25:50.440,2392,CC BY-SA 2.5,
10973,6714,0,"@sesqu - it seems like you are asking something along the lines of ""how could one come to hold such a particular kind of belief/knowledge""?  would I be correct in this assumption?",2011-02-05T15:31:33.223,2392,CC BY-SA 2.5,
10974,6909,1,"Even the ""intrinsic discrepancy"" will be infinite when $P$ is zero with positive probability for $Q$ and *vice versa,* even if $P$ and $Q$ are otherwise identical.",2011-02-05T15:56:01.810,919,CC BY-SA 2.5,
10975,6907,1,"In what ""natural sense"" are these curves ""not that distinct""?  How is this intuitive closeness related to any statistical property?  (I can think of several answers but am wondering what you have in mind.)",2011-02-05T15:57:36.690,919,CC BY-SA 2.5,
10976,6913,3,Why would having a 5 star rating be fairer? There are 6/16 reviews that gave a lower rating...,2011-02-05T17:00:43.903,582,CC BY-SA 2.5,
10977,6913,0,"Ok, than you think Mean is the right average? the majority said its 5. 60% more the rest 6/16 said so.",2011-02-05T17:08:31.137,778,CC BY-SA 2.5,
10978,6915,0,because just one person in your example can change the result dramatically. if the person believed the book has a different topic his fault will change the rating,2011-02-05T17:11:31.337,778,CC BY-SA 2.5,
10979,6714,0,"@probabilityislogic - yes. If probability is redefined, one can no longer use the inductive and abductive justifications for various distributions. Distributions like the binomial need to be re-derived within this belief system, and it's my understanding that this is done merely by grafting the necessary axioms onto the system, under the pseudonym ""rationality"". These axioms are intrinsic to the nondeterministic interpretation.",2011-02-05T17:30:24.293,2456,CC BY-SA 2.5,
10980,6896,0,"@whuber Using the unit circle seems like the best approach. I can then convert each distance between two angles back into an angle ( theta = cos^-1((2-dist^2)/2) ), given the properties the unit circle and the law of cosines, right?",2011-02-05T17:40:33.870,3052,CC BY-SA 2.5,
10981,6896,0,@caracal Thanks for the example and also the suggestion of the unit circle representation; I understand what you mean now.,2011-02-05T17:42:03.260,3052,CC BY-SA 2.5,
10983,6907,1,"Well... they are pretty close to each other in the sense that both are defined on positive values; they both increase and then decrease; both have actually the same expectation; and the Kullback Leibler distance is ""small"" if we restrict to a portion of the x-axis... But in order to link these intuitive notions to any statistical property, I would need some rigourous definition for these features...",2011-02-05T17:47:28.210,3019,CC BY-SA 2.5,
10984,6909,1,Yes... I am afraid that the intrinsic discrepancy does not fulfil the requirement. But thank you for the suggestion. Any other suggestion would be appreciated.,2011-02-05T17:49:24.697,3019,CC BY-SA 2.5,
10985,6914,2,"Hi onestop.  I imagined this was the case, but couldn't quite see that from the wikipage (http://en.wikipedia.org/wiki/Tukey's_range_test), any suggestions on references for how this is performed?",2011-02-05T17:54:34.067,253,CC BY-SA 2.5,
10986,6896,1,"@probabilityislogic The consequence is simply a pose that is not representative of the ground truth pose. It would depend on the estimation approach used as to what would occur. If knn was used then the nearest neighbour would be too distant. What is considered ""too distant""? It depends on the application, but as a general rule, it should at least be indicative of the action being performed (e.g. in a walking sequence, you would always want to have the person in an upright position). However, in ridge regression, the learnt mapping is wrong and likely to result in output that is rubbish.",2011-02-05T17:56:49.283,3052,CC BY-SA 2.5,
10987,6896,0,"@probabilityislogic Adding to my previous comment, the learnt mapping isn't necessarily wrong (of course you have to be careful of overfitting, and other considerations... perhaps you can't even use a linear regressor given the problem statement), but applying a learnt mapping for doing push-ups to a walking sequence is going to give very wrong output poses in every case. Whereas, a jogging sequence mapping, whilst still not perfect, would yield much better results, even though it is still not the best mapping to use.",2011-02-05T18:02:28.843,3052,CC BY-SA 2.5,
10988,6913,2,"If I had to give a discrete evaluation, seeing those 16 reviews I would give 4, not 5, as -to me- 5 would mean that all (or the great majority) of the votes are 5. 6/16 is ~40%, which is not exactly negligible.",2011-02-05T18:22:04.600,582,CC BY-SA 2.5,
10989,6913,8,"So, in essence, I think neither mean or median are good. Showing (as Amazon does) a bargraph with the different votes is the best option. Also, it is interesting to point out that online 1-5 ratings are not always so fair... http://youtube-global.blogspot.com/2009/09/five-stars-dominate-ratings.html",2011-02-05T18:27:05.797,582,CC BY-SA 2.5,
10990,6922,0,"maybe I'm confused but this appears to refer to a time series model? my model is actually simpler in that the samples are not a time series (effectively they are independent (x->y) samples, they are just accumulated in large volumes over time)",2011-02-05T19:26:36.793,2942,CC BY-SA 2.5,
10991,6913,0,"Median is ""less sensitive"" to data. It is less affected by outliers, but also it contains less information about the original distribution",2011-02-05T19:43:03.487,511,CC BY-SA 2.5,
10992,6922,1,"Yes, in the general case this is used for time series with non independent observations; but you can always assume incorrelation between successive observations, which gives the special case of interest to you.",2011-02-05T19:46:18.147,892,CC BY-SA 2.5,
10993,6913,1,"@nico: scoring is full of traps, what you point out is one of the arguments of my article here: http://objektorient.blogspot.com/2010/09/scoring-you-doing-it-wrong.html",2011-02-05T19:56:45.703,778,CC BY-SA 2.5,
10994,5966,3,"@venkasub, you might also look at Muller's method for root-finding. It's simpler than Brent's method and ""almost"" as efficient as Newton's method, but without needing to know the derivatives of the target function.",2011-02-05T20:24:19.533,2970,CC BY-SA 2.5,
10996,6923,2,Is the method Maindonald describes related to or the same as Gentleman's algorithm? http://www.jstor.org/stable/2347147,2011-02-05T21:46:26.153,449,CC BY-SA 2.5,
10997,6923,0,"@onestop It must be related.  Maindonald uses Gentleman & Wilk (*Biometrics* **31**, 1975) as a reference.",2011-02-05T21:49:58.503,919,CC BY-SA 2.5,
10998,6909,1,"It does fulfil the requirement, if you restrict the support of the blue density to be where it has strictly positive support, just as you have for the red one (>0)",2011-02-05T22:01:53.117,2392,CC BY-SA 2.5,
10999,6927,1,@BR whats the bottle neck here? Basically what's taking time in `Rprof`.,2011-02-05T22:14:50.443,1307,CC BY-SA 2.5,
11000,6714,0,"@sesqu - One example is as a limiting form of urn sampling $N$ balls without replacement, where I know the number of ""red"" and ""blue"" balls; and they both tend to infinity in such a way that $\frac{R}{R+B}=p$ stays fixed in the limit. I then let $K$ be the number of Red balls drawn in $N$ draws.  But you always need to be a bit careful about how the limit is carried out, because a different limiting process may give different results.  This discussion has become sidetracked of course, so I suggest you start a new post if you wish to continue your line of thinking down this path.",2011-02-05T22:17:08.980,2392,CC BY-SA 2.5,
11001,6927,1,"One way is to just ignore the ""mixed"" part of the GLMM.  Just fit an ordinary GLM, the estimates won't change much, but their standard errors probably will",2011-02-05T22:40:01.813,2392,CC BY-SA 2.5,
11002,6923,6,"In that case see also the extensions by Alan Miller http://www.jstor.org/stable/2347583.
An archive of his Fortran software site is now at http://jblevins.org/mirror/amiller/",2011-02-05T22:44:25.940,449,CC BY-SA 2.5,
11003,6923,0,@onestop Your first reference obviously solves the entire problem: at the bottom of the first page he even states it can be used for weighted least squares.  And Alan Miller's very first Fortran link (lsq.f90) contains the code.  Why don't you combine your comments into a reply so they are more prominently available?,2011-02-05T22:51:38.457,919,CC BY-SA 2.5,
11004,6314,8,"Beyer et al. seem to be addressing a little bit different aspect of the NN problem. But, for (binary) classification purposes, under mild conditions, it is a classical result that 1-NN classification has, *in the worst case*, twice the probability of error of the Bayes (i.e., optimal) classifier asymptotically. In other words, the first nearest neighbor contains ""at least half the information"" about the label of the target as the best classifier does. In this sense, the 1-NN seems quite relevant. (See Cover & Hart (1967) for more. I'm surprised Beyer et al. doesn't cite it.)",2011-02-05T23:13:04.333,2970,CC BY-SA 2.5,
11005,6923,5,"An explicit algorithm appears at the bottom of p. 4 of http://saba.kntu.ac.ir/eecd/people/aliyari/NN%20%20files/rls.pdf .  This can be found by Googling ""recursive least squares.""  It does not look like an improvement on the Gentleman/Maindonald approach, but at least it is clearly and explicitly described.",2011-02-05T23:20:49.330,919,CC BY-SA 2.5,
11006,6295,0,"Additionally, since the interaction has the additional parameter, technically rather than a simple Likelihood-ratio test, shouldn't some information criterion (AIC, BIC, etc.) be used?",2011-02-05T23:25:27.153,2800,CC BY-SA 2.5,
11007,6927,0,"@probabilityislogic. In addition to your remark, I also think, whether answer would differ much depends on the group size and individual behavior in group. As Gelman and Hill say: mixed effects model results would be between pooling and no pooling. (Obv. this is for Bayesian hierarchical models, but mixed models are a classical way of doing the same.)",2011-02-05T23:29:22.513,1307,CC BY-SA 2.5,
11009,6927,0,"@probabilityislogic: That works for LMM, but it seems to fail for GLMM (meaning that I ran models with and without the extra M on the same data and ended up with significantly different results). Unless, of course, there is an error in the implementation of glmer.",2011-02-05T23:47:59.693,2739,CC BY-SA 2.5,
11010,6927,0,"@suncoolsu: the bottleneck is the estimation of the GLMM, which may a couple of seconds (especially with several random effects). But do that 1000*1000 times, and that's 280 hours of computation. Fitting a GLM takes about 1/100 of the time.",2011-02-05T23:55:39.670,2739,CC BY-SA 2.5,
11011,6927,0,"In my specific situation, there are about 50 individuals and each is measured about 10 times.",2011-02-05T23:57:26.767,2739,CC BY-SA 2.5,
11012,6929,0,"Thanks for the answer, it'll take me a bit to digest it (and I already have plans for my Saturday night). It is different enough that it is not clear to me if it gives the same answer as the GLMM approach, but I need to think about it more.",2011-02-06T00:16:51.413,2739,CC BY-SA 2.5,
11013,6923,2,"The last link looks like the method I was going to suggest. The matrix identity they use is known in other places as the Sherman--Morrison--Woodbury identity. It is also quite numerically efficient to implement, but may not be as stable as a Givens rotation.",2011-02-06T00:28:25.313,2970,CC BY-SA 2.5,
11014,5671,1,"You would use Gibbs sampling when you know $p(y|x)$ and $p(x|y)$, but not if you know the marginal $p(x)$",2011-02-06T01:05:06.040,2392,CC BY-SA 2.5,
11015,6909,0,"Yes indeed, restricting the support is the simplest idea but I guess that the resulting KL distance should be reduced in some way in order to take that trick into account...",2011-02-06T08:10:04.337,3019,CC BY-SA 2.5,
11016,6919,1,"Very nice! I am going to try to find ""A Probabilistic Theory of Pattern Recognition"" and to understand its chapter 3!",2011-02-06T08:11:21.543,3019,CC BY-SA 2.5,
11017,6924,1,Thank you for your suggestion about the Kolmogorov distance. Can you make your comment about the monotonic transformation a little bit more explicit? Thx,2011-02-06T08:13:12.147,3019,CC BY-SA 2.5,
11018,4551,1,"@whuber There was some good answers, so I've merged them both.",2011-02-06T11:02:41.973,,CC BY-SA 2.5,user88
11019,4095,1,Comments for downvote are always welcome!,2011-02-06T11:08:55.717,930,CC BY-SA 2.5,
11020,4083,2,"I'll second @Andy and suggest leaving a comment when downvoting: it's always helpful to learn what's wrong, if any. Especially in this case: Andy W came back with CW extended comments which, in my opinion, add further support to the original question. There's no need to downvote anything here!",2011-02-06T11:10:14.870,930,CC BY-SA 2.5,
11021,3917,0,Comments for downvote are always welcome!,2011-02-06T11:14:15.290,930,CC BY-SA 2.5,
11022,6916,12,"No. If you lived in Glasgow, Scotland, then most people who wear a ""blue t-shirt"", would probably be a [Rangers](http://en.wikipedia.org/wiki/Rangers_F.C.) supporter. You would be missing out on [Celtic](http://en.wikipedia.org/wiki/Celtic_F.C.) supporters. In Glasgow the [football](http://en.wikipedia.org/wiki/Soccer) team would be a proxy for religion.",2011-02-06T13:28:29.933,8,CC BY-SA 2.5,
11023,6909,0,"Actually now that I think of it, you don't need to restrict the support, because the $0$ in the log gets canceled out by the $0$ in the pdf multiplying it.  So you actually get $Lim_{P\rightarrow 0} P log(\frac{P}{Q})= Lim_{P\rightarrow 0} P log(P)=0$, and the intrinsic discrepancy is defined after all!",2011-02-06T14:19:13.673,2392,CC BY-SA 2.5,
11024,6877,2,"The problem with looking at courses studied is that courses with the same name at different universities can vary widely. Knowing that someone has studied 'Advanced Algorithms' tells me very little about what they actually know, even less about whether they can apply that knowledge in their job, and less still about whether they can effectively explain what they're doing to someone with a non-technical background.",2011-02-06T15:12:29.203,2425,CC BY-SA 2.5,
11025,6909,3,"@probabilityislogic: I do not unerstand your last remarks. First, let us give their proper names to the notions involved and say that $P$ is absolutely continuous with respect to $Q$ (denoted $P\ll Q$) if, for every measurable $A$, $Q(A)=0$ implies $P(A)=0$. Now, notwithstanding your somewhat mysterious (to me) limit considerations, your $\delta(P,Q)$ is finite iff $P\ll Q$ or $Q\ll P$. .../...",2011-02-06T15:25:29.493,2592,CC BY-SA 2.5,
11026,6909,2,".../... A way out of the conundrum you seem to be dug into might be to introduce the mid-point measure $P+Q$. Since $P\ll P+Q$ and $Q\ll P+Q$, the quantity $\eta(P,Q):=\kappa(P|P+Q)+\kappa(Q|P+Q)$ is always finite. Furthermore $\eta(P,Q)=0$ iff $P=Q$ and $\eta$ is symmetric. Hence $\eta(P,Q)$ indeed measures a kind of ""distance"" between $P$ and $Q$.",2011-02-06T15:26:42.720,2592,CC BY-SA 2.5,
11027,6909,0,"Edit: the midpoint is $\frac12(P+Q)$ and $\eta(P,Q):=\kappa(P|P+Q)+\kappa(Q|P+Q)+2\log2$.",2011-02-06T15:33:39.387,2592,CC BY-SA 2.5,
11028,6909,0,"@probabilityislogic, @Didier Piau, A simple example to show what Didier is saying is to take $p(x) = 1_{(x \in (0,1))}$ and $q(x) = 1_{(x \in (1,2))}$. Then $\delta(p,q)$ is infinite. If you want a more extreme example, for any $\epsilon > 0$, let $q(x) \equiv q_{\epsilon}(x) = 1_{(x \in (\epsilon, 1+\epsilon))}$.",2011-02-06T15:43:53.430,2970,CC BY-SA 2.5,
11029,6909,0,"@cardinal: Indeed. I now wrote an answer to the OP explaining this ""midpoint+KL"" stuff.",2011-02-06T15:50:04.820,2592,CC BY-SA 2.5,
11030,6909,0,"@probabilityislogic, A simple example which I believe shows where your limiting argument fails is to consider $p(x) = 1_{(x \in (0,1/2)\cap\mathbf{Q})} + 2 1_{(x \in (1/2,1))}$. Then $p(x)$ is zero on a subset of $(0,1/2)$ with positive measure, but $p(x) \not\to 0$ for any $x \in (0,1/2)$. So your limit argument needs further conditions, even if it were to otherwise work. A sort of work-around is not to focus on the limits of $p$, but instead to define $x \log x$ on $[0,\infty]$ via an extension by continuity. The other problems with your development would still remain, however.",2011-02-06T15:50:39.413,2970,CC BY-SA 2.5,
11031,4553,0,"This one really bugs me.  My clients do this all the time.  And when I point out the error, they understand, but insist on doing it anyway",2011-02-06T15:54:50.273,686,CC BY-SA 2.5,
11032,6926,4,"This is correct, but how would you know if it was orthogonal unless you had a truly random sample?",2011-02-06T15:57:20.923,686,CC BY-SA 2.5,
11034,6937,2,"@Marco, @Didier Piau, it might be noted that @Didier's suggestion is another special case of an $f$-divergence where $f(x) = x \log x - (1+x) \log( \frac{1+x}{2} )$.",2011-02-06T17:26:26.103,2970,CC BY-SA 2.5,
11035,6937,1,"@Marco, @Didier Piau, an alternative formulation which has some evocative nature is $\eta(P, Q) = \int P \log P + \int Q \log Q - 2 \int R \log R = 2 H(R) - (H(P) + H(Q))$ and so $\eta(P,Q) = 2 ( H(\mu(P,Q)) - \mu(H(P), H(Q))$ where $\mu(x,y) = \frac{x+y}{2}$. In other words, $\frac{1}{2} \eta(P,Q)$ is ""difference between the entropy of the average measure and the average entropy of the measures"".",2011-02-06T17:30:17.130,2970,CC BY-SA 2.5,
11036,6941,1,I must remark -- this is one the reasons why p-values are so popular and easy to be misunderstood. (IMHO),2011-02-06T18:47:30.633,1307,CC BY-SA 2.5,
11037,6941,0,"Ok, this makes sense. I do have what I believe is a good estimation of the null distribution. Any hints on how to implement this in R? Thanks!",2011-02-06T19:07:01.243,52,CC BY-SA 2.5,
11039,6924,1,"@Marco I don't understand how one could be any more explicit.  Do you mean restating what I wrote in terms of a formula such as $\arctan(KL(P,Q))$ or $f(KL(P,Q))$ for $f:\mathbb{R_+} \to [0,C]$ with $x \ge y$ implies $f(x) \ge f(y)$ for all $x,y \ge 0$?",2011-02-06T19:12:37.300,919,CC BY-SA 2.5,
11040,6923,1,"@cardinal Thank you for the reference and for the advice.  I was mainly concerned about stability.  In a long sequence of updates, there may be circumstances where the data are nearly collinear and there could also be a lengthy accumulation of numerical errors, so stability seems to be of paramount concern.  That is the justification for the Givens rotations (instead of Householder reflections, which take less computation).",2011-02-06T19:16:27.013,919,CC BY-SA 2.5,
11041,6941,1,"@Alan - Do you know how to generate random values from your Null Distribution? If yes, suppose - T= c(T1, ..., TN) are draws from the null distribution - p-value = sum(T > T_obs)/N. If you don't know how to generate, may you need to use Metropolis Sampling or Gibbs Sampling to get T1...TN, but it is very do-able.",2011-02-06T19:30:44.183,1307,CC BY-SA 2.5,
11044,6923,0,"@whuber. Please don't take it otherwise - but the reference(s) that you cite are classic, like - Freedman, Maindonald, etc.. I haven't seen a lot of people cite these books. (It's my personal observation, I may be wrong. In my graduate studies, I have seen a lot of references to Freedman (and Diaconis), but never his books.)",2011-02-06T22:18:28.643,1307,CC BY-SA 2.5,
11045,6938,5,"Your point about a measure of spread is a key here. That's one of the issues that keeps coming up in this discussion, under other names, and it also ties in with Erik P's discussion of weighting schemes.",2011-02-06T22:47:21.017,1764,CC BY-SA 2.5,
11046,6935,1,"Thanks for the answer. Somehow, I've overlooked the fact that I have two cores (my computer isn't very new). I should've looked at multicore a long time ago.",2011-02-06T23:16:04.723,2739,CC BY-SA 2.5,
11048,6946,0,"@Queops, are the three consecutive pages assumed to be predetermined or is it the probability of finding errors on any set of three consecutive pages out of a total of $n$?",2011-02-07T02:00:13.397,2970,CC BY-SA 2.5,
11049,6947,0,Very detailed. I loved it. Thank you and I'll consider it as an answer unless someone comes up with something else.,2011-02-07T02:58:03.247,1833,CC BY-SA 2.5,
11050,6946,2,"I realize this question will be answered by someone wanting to rack up points, but I want to point out that answering trivial homework question dilutes the usefulness of this forum to professionals.",2011-02-07T03:00:23.033,3080,CC BY-SA 2.5,
11051,6946,2,"@Leo, I, for one, am not looking to ""rack up"" points. I am interested in learning from others and also good pedagogy. My understanding is that this forum's target audience is fairly broad and students are welcome. Probability and statistics can both seem daunting at first. That said, when looking at homework questions, it's always nice to see that the OPs have already spent some time on their own thinking about it.",2011-02-07T03:19:00.770,2970,CC BY-SA 2.5,
11052,6946,0,"@cardinal, I don't know what you mean by predetermined, but the problem indicates it's not some specific set of them, just the probability of finding 5 errors in 3 consecutive pages. @Leo, are you suggesting there isn't space for everyone? Sure most professionals probably can figure out this by themselves but we have to start somewhere right? I for one, am very new to this and I find it hard to follow most articles on the internet and I find this community most helpful and friendly.",2011-02-07T03:23:44.097,1833,CC BY-SA 2.5,
11053,6946,0,"@Queops, I think the problem probably means given a set of three prespecified pages, e.g., pages 13-15, what's the probability of five errors on these three pages. This is very different from the asking, e.g., what's the probability of finding 5 errors on some set of three consecutive pages between, say, pages 1--50. See the difference?",2011-02-07T03:27:50.203,2970,CC BY-SA 2.5,
11054,6946,0,"@cardinal, yes completely. And no, it's just some set, doesn't matter which one. (Which I believe it's essential in a Poisson process?). The problem doesn't specify how many pages are in total, though one could assume there's more than 3.",2011-02-07T03:32:05.213,1833,CC BY-SA 2.5,
11055,6948,0,"I understand now! You were most helpful. Much appreciated. I believe I'm ready for some real ""Poissonic"" action hehe.",2011-02-07T03:38:02.883,1833,CC BY-SA 2.5,
11056,6923,0,"@whuber, I agree that stability is the main concern. That's why it was very nice to see the cute Givens rotation approach. I can't imagine that a full update wouldn't be necessary every so often, too. @mikera, for an exponential-decay-like approach, you might look at the LMS algorithm from signal processing. It's normally geared toward time-series, but I suspect can be adapted to a more general setting.",2011-02-07T03:49:17.157,2970,CC BY-SA 2.5,
11057,6924,1,"Yes, that's what I meant :-) I was not sure on what to apply the transformation. Now, it is clear, thx",2011-02-07T07:17:45.943,3019,CC BY-SA 2.5,
11058,6950,1,You may find helpful those related posts http://stats.stackexchange.com/questions/6856/aggregating-results-from-linear-model-runs-r http://stats.stackexchange.com/questions/1812/fa-choosing-rotation-matrix-based-on-simple-structure-criteria,2011-02-07T07:21:00.930,339,CC BY-SA 2.5,
11059,6924,1,"@Marco: I am lost. Do you settle for the Kolmogorov distance (which is always finite but has nothing in common with KL divergence)? Or for a bounded monotone transform of KL divergence (such as $\arctan$)? In the example of your post (and in any other *not absolutely continuous* example), the latter produces the supremum of the transform ($\pi/2$ if you settle for $\arctan$). In effect, this abandons any idea of estimating a *distance* between such probability measures more precisely than saying they are *far far away* (whether you encode this by $\pi/2$ or by $+\infty$ is irrelevant).",2011-02-07T08:47:27.167,2592,CC BY-SA 2.5,
11062,6949,0,"I can't actually remember if such a specific application is covered in *Biometrics: Theory, Methods, and Applications*, by Boulgouris et al. (Wiley, 2009), but it may be worth checking on [Google Books](http://j.mp/gSKX7K).",2011-02-07T09:51:07.683,930,CC BY-SA 2.5,
11065,4144,0,"Am I correct in thinking that if the variances did vary significantly, you should weight inversely by the variances? In the case of equal variance this reduces to weighting by the number of observations since the variance of n IID observations is (variance of one observation / n).",2011-02-07T12:20:08.753,2425,CC BY-SA 2.5,
11067,6924,0,"@Didier Yes, the transformed KL divergence (when symmetrized, as you describe) might not satisfy the triangle inequality and therefore would not be a distance, but it would still define a topology (which would likely be metrizable).  You would thereby give up little or nothing.  I remain agnostic about the merits of doing any of this: it seems to me this is just a way of papering over the difficulties associated with infinite values of the KL divergence in the first place.",2011-02-07T13:22:54.080,919,CC BY-SA 2.5,
11069,6923,0,"@suncoolsu Is there a problem with classic references?  (BTW, it's likely that all the stats software people respect--SAS, SPSS, S-Plus/R, Systat, Stata, etc.--still uses code based on the algorithmic references from the 60's through the 80's, because the core parts of all these programs were written by the early 90's.)",2011-02-07T13:45:13.873,919,CC BY-SA 2.5,
11070,6952,0,I would entirely agree with Andrej. This is one of those rare cases where principal components will probably do a better job than factor analysis.,2011-02-07T13:46:21.580,656,CC BY-SA 2.5,
11071,6924,0,"@whuber Topology and the (defect of) triangle inequality are not my point here. I simply wondered why Marco suddenly seemed happy with the $\arctan$ artefact you suggested, although this artefact leaves every pair of not absolutely continuous probability measures as infinitely *far far away* from each other as they were with respect to KL divergence. Note also that $\eta$ is not only (and not mainly) a *symmetrized* version of KL, more importantly $\eta$ is *finitized*. .../...",2011-02-07T13:52:20.507,2592,CC BY-SA 2.5,
11072,6924,0,".../... Example: for every $t>0$, let $P_t$ denote the uniform probability distribution on the interval $(0,t)$. For every $t\ne s$, $P_t$ and $P_s$ are at infinite distance for KL but $\eta(P_t,P_s)$ is positive and finite and depends non trivially on $(t,s)$. Inter alia, $\eta$ *quantifies* the fact that, if $t<s<u$, then $P_t$ is closer to $P_s$ than to $P_u$. I understood this is what Marco was after but maybe I am mistaken.",2011-02-07T13:53:12.597,2592,CC BY-SA 2.5,
11073,6924,0,"@Didier Could you clarify what you mean by ""finitized""?  After all, two distributions can be infinitely far apart w.r.t. $\eta$, even when their supports overlap.  The family of uniforms you discuss is special in that in each pair, one of them is absolutely continuous w.r.t the other.  (Even that doesn't in general guarantee $\eta$ is finite, but it suffices in this case.)  However, we are moving off onto a tangential topic here: what we are missing is any sense of *why* there is a concern about the possibility of an infinite divergence (or infinite distance).",2011-02-07T13:58:02.307,919,CC BY-SA 2.5,
11074,6924,0,"@whuber Why is there a concern about the possibility of infinite distance? Dunno... maybe because infinite distances is the feature the OP asked a remedy for in the first place... Re $\eta$, let me stress once again that $\eta(P,Q)$ is finite for **every** probability measures $P$ and $Q$.",2011-02-07T14:09:52.570,2592,CC BY-SA 2.5,
11075,6951,0,Small correction: 1.6^5 should be (3*1.6) ^5. Thanks!,2011-02-07T14:18:46.470,1833,CC BY-SA 2.5,
11076,6924,0,"@whuber Here is another example to let you better understand the behaviour of $\eta$. Let $U_t$ denote the uniform distribution on the interval $(t,1+t)$. Then $\eta(U_t,U_s)=\min(1,|t-s|)2\log(2)$. No $U_t$ and $U_s$ are absolutely continuous with respect to each other unless $s=t$. Finally, note that $\eta\le2\log(2)$ uniformly, and that $\eta(P,Q)=2\log(2)$ iff $P$ and $Q$ are mutually singular.",2011-02-07T14:23:27.553,2592,CC BY-SA 2.5,
11078,6924,0,@Didier Thank you: I misread the formula for $\eta$ until just this moment; I had confused it with the $\delta$ defined in another answer.  Too hasty.  I appreciate your patient explanations as well as the clarifications provided by @cardinal in comments to your answer.,2011-02-07T14:59:18.543,919,CC BY-SA 2.5,
11080,4144,0,"@Chris (This replaces an earlier reply, which mistakenly referenced a different question.)  Yes, that's correct.  One must always weight in direct proportion to the number of observations, though (assuming the observational errors in each group are independent).  I don't think there's an easy or general rule concerning whether or not one should use variance-weighted regression when the variances don't differ by much.  (The range of SD's of 6:2 in this case, with only 2 - 8 degrees of freedom, is not wide enough to provide convincing evidence of heteroscedasticity.)",2011-02-07T15:09:02.317,919,CC BY-SA 2.5,
11081,6939,0,"@Tom, how did you uncover the obvious pattern? For 4 time periods and 4 different values for each period there are $4^4=256$ possible combinations, why did you post only 4 of them? I am asking because I suspect that there might be another variable which indicates how to group the values.",2011-02-07T15:11:16.157,2116,CC BY-SA 2.5,
11082,6961,0,"I agree it's a bit inept to recreate the ADF-test by hand. It was more an exercise for me to get used to the language of ADF-tests. Thanks for the help, very much appreciated.",2011-02-07T15:14:00.810,3086,CC BY-SA 2.5,
11083,6962,1,This is spooky.,2011-02-07T15:16:41.523,,CC BY-SA 2.5,user88
11084,6962,0,"@mbq, seconded :)",2011-02-07T15:19:55.777,2116,CC BY-SA 2.5,
11085,6915,2,"Is someone's opinion a fault? I'd argue that the failure is trying to draw meaningful conclusions based on a single statistic from only a few data points. As noted by @nico above, Amazon does show a bar graph of all the ratings.",2011-02-07T16:14:18.357,597,CC BY-SA 2.5,
11086,5150,0,Have a look at the answers to [this question](http://stackoverflow.com/questions/2018178/finding-the-best-trade-off-point-on-a-curve) on Stackoverflow to find the 'elbow' in the ROC.,2011-02-07T17:32:48.593,198,CC BY-SA 2.5,
11087,6966,0,[Duplicate?](http://stats.stackexchange.com/questions/1164/why-havent-robust-and-resistant-statistics-replaced-classical-techniques/1185),2011-02-07T18:09:35.373,8,CC BY-SA 2.5,
11090,6939,0,"@mpiktas, I probably missed a lot of patterns too in the above example. I just gave a simple example (it is not my actual data). I made them up while writing this question. I am trying to find patterns between individual fields in a dataset.",2011-02-07T18:44:49.883,3040,CC BY-SA 2.5,
11091,6942,0,It is clear to me I should learn R. Do you recommend any books that I could read to learn about pattern recognition in R? I am not just looking for a pattern in my distribution. I'm looking for all patterns that multiple fields of my distribution make up together.,2011-02-07T18:47:44.037,3040,CC BY-SA 2.5,
11092,6942,0,"@Tom: you definitely should :) R has a really great potential if you plan to make complex computations in the future. To learn R, you can find a lot of great books and blogs also in the issue, just to pick up a free one, look for *Statistics with R* by Vincent Zoonekynd (http://zoonek2.free.fr/UNIX/48_R/all.html) and look for  Ch. 11-12 for different kind of models. I have also made up a small collection of useful links in the subject, check it if you wish: http://r.snowl.net/interesting-urls-dealing-with-r/",2011-02-07T18:55:04.693,2714,CC BY-SA 2.5,
11093,6942,0,@Tom: a better approach could be reading through the answers of *Books for learning the R language* question on SO: http://stackoverflow.com/q/192369/564164,2011-02-07T18:57:11.417,2714,CC BY-SA 2.5,
11094,6968,1,"+1, Thanks. Well argued. But in introductory courses, there is no model selection, in the strict sense. You could cite other contexts that are appropriate for the introduction of hypothesis testing? It is acceptable to report the outcome of a test without an estimate of power?",2011-02-07T19:09:45.527,523,CC BY-SA 2.5,
11095,6939,0,"@Tom, so what is your real data look like then?",2011-02-07T19:20:56.477,2116,CC BY-SA 2.5,
11096,6950,0,Mallows Cp http://en.wikipedia.org/wiki/Mallows'_Cp  Here's a paper on Cp http://mlrv.ua.edu/2008/vol34_1/Lieberman-Morris.pdf,2011-02-07T19:23:07.853,2775,CC BY-SA 2.5,
11097,6967,0,why do the approaches described in the question you mention not work for you?,2011-02-07T19:26:17.403,2116,CC BY-SA 2.5,
11098,6967,0,@mpiktas - it looks like the OP tried the methods mentioned previously has has copied the corresponding errors in the code above.,2011-02-07T20:08:58.877,696,CC BY-SA 2.5,
11099,6967,0,"@Chase, when I commented, there was no R code example.",2011-02-07T20:25:45.713,2116,CC BY-SA 2.5,
11100,6967,0,@mpiktas - I blame the [R gremlins](http://theshoegame.com/wp-content/uploads/2009/01/gizmo-gremlins-picture.jpg),2011-02-07T20:26:59.533,696,CC BY-SA 2.5,
11103,6939,0,"@mpiktas it's stock data by date, so basically multiple stocks with certain daily dates with stock prices of each date",2011-02-07T21:08:41.693,3040,CC BY-SA 2.5,
11104,6967,0,@mpiktas - I added a code example that lists what I tried to do and the errors I got.  I hit the 3 methods discussed in the previous question.,2011-02-07T21:57:55.457,2817,CC BY-SA 2.5,
11105,6923,0,@whuber. Oh no! I am impressed actually. That's why I said don't take it otherwise. It was in praise that you can actually site literature that old. I agree with you on the algorithm part as well -- we still use Prof. Maindonald's code for optimization in R (if I am not mistaken).,2011-02-07T22:04:13.567,1307,CC BY-SA 2.5,
11106,6953,0,"I think your method is way easier than what I suggested, especially if there are no restriction on the function that you are integrating. I am not aware of the numerical tech. in R.",2011-02-07T22:12:08.190,1307,CC BY-SA 2.5,
11107,538,6,@Craig That's the point; it's not possible.,2011-02-07T22:16:51.993,3093,CC BY-SA 2.5,
11108,6859,0,How would I plot just the intercept and unemployed?,2011-02-07T23:12:21.323,2817,CC BY-SA 2.5,
11109,6953,0,"Yes, I think this is more in line with my current abilities. Thanks!",2011-02-07T23:19:14.810,52,CC BY-SA 2.5,
11110,6972,2,"@Queops, maybe you can edit the question and add in your thoughts and approach so far and where you are getting stuck.",2011-02-07T23:35:36.700,2970,CC BY-SA 2.5,
11111,6972,0,"More likely is that 575 is the standard deviation. You should first ask yourself what would be the probability in a sample of size 1. (Your prof didn't make it a nice even multiple of sd's away from the mean, so I imagine he expected you to use a statistical system or calculator of some sort.)",2011-02-07T23:36:41.580,2129,CC BY-SA 2.5,
11112,6972,0,"I suspect that 575 is the population standard deviation, both from the rest of the question and the real Norwegian data in figures 1 and 2 of http://eb.niehs.nih.gov/bwt/subcfreq.htm",2011-02-07T23:38:36.620,2958,CC BY-SA 2.5,
11113,6968,2,Having no model selection in introductory courses isn't a necessity.  If you're considering changing a course consider that as a good place to start.,2011-02-07T23:42:16.570,601,CC BY-SA 2.5,
11114,6972,1,"While we are happy to help with homework questions, this site isn't really aimed at *doing* your work. Please see the relevant [meta](http://meta.stats.stackexchange.com/questions/12/how-should-we-deal-with-obvious-homework-questions) question. In particular, how homework questions should be formatted.",2011-02-08T00:01:04.760,8,CC BY-SA 2.5,
11115,6972,0,"The answer is of minimal importance to me, I'm studying and it's the how to I'm interested in. @DWin and Henry, I think it really is the standard deviation. Henry, I'll look into it, thanks. @Cardinal, I tried applying the probability density function but that was probably silly right?",2011-02-08T00:30:24.287,1833,CC BY-SA 2.5,
11116,6972,0,"You might want to think, not about the density function, but about using the probability I suggested , coming from the CDF, as input into a binomial expansion.",2011-02-08T00:37:22.573,2129,CC BY-SA 2.5,
11117,6952,0,"Sorry I have one doubt, should I include in PCA the variable I want to predict?",2011-02-08T01:58:40.290,1808,CC BY-SA 2.5,
11118,6974,0,"My logic so far: From stage 1 to stage 2, every person has 5 options to choose from for their stage 2 group (A, B, C, D, E). So, the chance that he/she will retain his/her own group is 20%. So, the expected number of people who will retain their own group is 20%. Comparing 65% with 20% using a 1-sample proportions test with continuity correction (in R: prop.test(34, 52, 0.2)), I get: Chi^2 = 54.14, df = 1, p < 0.001, meaning that my claim 1 is statistically significant.",2011-02-08T02:25:32.487,3097,CC BY-SA 2.5,
11119,6976,0,"@pchalasani, Does LTCM count? ;)",2011-02-08T03:00:42.670,2970,CC BY-SA 2.5,
11120,6976,0,What about discrete choice modeling? I'd think most people would consider DCM to be a branch of econometrics and the entire travel demand forecasting industry relies on DCM to generate inputs for their forecasting models...that may not be what you had in mind though.,2011-02-08T03:06:25.833,696,CC BY-SA 2.5,
11121,6976,0,"@pchalasani, yes, how broadly *are* you interpreting the word ""econometric""? Does, for example, Netflix's ""Cinematch"" system count (or any of the other successful ""recommender"" engines out there)?",2011-02-08T03:23:51.387,2970,CC BY-SA 2.5,
11122,6975,3,Can you please be more specific about what kind of data set do you have? Answers may vary accordingly. `rpart` may be Ok - but may be something better is available.,2011-02-08T03:28:20.627,1307,CC BY-SA 2.5,
11123,6939,0,"@Tom, so you want to see if stock price has any obvious time trend?",2011-02-08T03:59:10.637,2116,CC BY-SA 2.5,
11124,6510,0,"Yes, nested models are hierarchical in nature. In the case of a variable representing degree (i.e. degree of risk), you do not need a hierarchical model. Depending on how the risk index you're using is divided, you may even want to consider grouping the levels on the index (i.e. if it's 1-9 consider grouping as high [7-9], medium [4-6], and low [1-3]) as this may allow you to better represent the dominance effect where those in the reference group represent a substantially different outcome than those in the comparisons.",2011-02-08T04:42:59.380,2166,CC BY-SA 2.5,
11125,6979,2,"@JMS, ""how many intro stats students know the real definition of a confidence interval?"" Or, PhD stat graduates, for that matter.",2011-02-08T05:11:26.377,2970,CC BY-SA 2.5,
11126,6978,0,"I think the answer is ""yes"". Of course, there is a bit of an issue of definitions here. What one person considers ""large-scale"" is sometimes very different from other's. My impression is that, e.g., many academic researchers consider the Netflix dataset ""large-scale"",  while in many industrial settings it would be considered ""puny"". As regards estimation techniques, usually with very large data, computational efficiency trumps statistical efficiency. For example, method of moments will, in many cases, perform (nearly) as well as MLE in these settings and can be *much* easier to compute.",2011-02-08T05:25:35.443,2970,CC BY-SA 2.5,
11127,6978,2,"you might also look up the Workshop on Algorithms for Modern Massive Data Sets (MMDS). It's young, but draws a pretty impressive set of speakers at the interfaces of statistics, engineering and computer science as well as between academia and industry.",2011-02-08T05:27:17.487,2970,CC BY-SA 2.5,
11128,6979,0,"Quite! Incidentally, I meant no dig at students or practitioners of any stripes. But it's a little crazy to expect the mental gymnastics from someone who didn't sign up for advanced work in statistics.",2011-02-08T05:32:30.547,26,CC BY-SA 2.5,
11129,6700,0,"@rolando2: Thank you for the prompt reply. Yes. Each row is a different region. There are 16 regions with different populations. The 2 levels of treatment variable are thoroughly implemented and not so thoroughly implemented. Some regions implemented the treatment thoroughly and so the deaths were less. Other regions did not implement the treatment thoroughly and so the deaths were more. We have to show statistically that in regions where treatment was inadequate had significantly higher number of deaths. If the table is too big, we can use 3 or 4 regions data from 2006 to 2010 for analysis.",2011-02-08T06:00:50.797,2956,CC BY-SA 2.5,
11130,6923,2,@suncoolsu Hmm... Maindonald's book was newly published when I started using it :-).,2011-02-08T06:02:11.280,919,CC BY-SA 2.5,
11131,6972,2,"@Queops I added tags to provide links to useful threads.  As a hint, consider that $Y$ is a random variable that counts a certain number of things out of 25.  What distribution do you suppose it might have?  What do you need to know about that distribution?  The assumptions about the distribution of $X$ are there so you can compute the chance that a *single* baby has low birth weight.",2011-02-08T06:09:16.963,919,CC BY-SA 2.5,
11132,6963,1,"@≈Åukasz Can you say anything more about the parameters $n$, $\alpha_i$, and $\beta_i$?  It's possible to obtain exact expressions for $\sum_j{X_j}$ and thereby approximate the expectations of the ratios, but for certain combinations of the parameters one could exploit Normal or saddlepoint approximations with less work.  I don't think there will be a universal approximation method, which is why additional restrictions would be welcome.",2011-02-08T06:17:59.163,919,CC BY-SA 2.5,
11133,6909,0,"@cardinal - in the example you give, the distance *should* be infinite, because the supports are *disjoint* (and thus not comparable).",2011-02-08T06:55:59.350,2392,CC BY-SA 2.5,
11134,6974,0,"I'm sorry, but when 65% stayed in their group it isn't very probable, that the chance for staying in your group is 20%.",2011-02-08T06:59:10.953,3100,CC BY-SA 2.5,
11135,6909,0,"@cardinal: I think the measure I made requires one measure to have a support of a subset of the other.  If there is a ""zero"" region in P, and not Q, and also a ""zero"" region in Q, and not in P, then it is infinite.  If the supports are nested, then it is always finite",2011-02-08T07:33:06.217,2392,CC BY-SA 2.5,
11136,6976,0,Arnold Zellner isn't a bad name to look up.  larry bretthorst's phd thesis on spectral estimation is also good,2011-02-08T07:36:30.097,2392,CC BY-SA 2.5,
11137,6979,2,"How many *people* can say the real definition of CIs?  And how many people use them consistently with this definition?  Its just too hard not to think ""the parameter is likely to be in said interval"" - even if you *know* its not what a CI is.",2011-02-08T07:51:38.433,2392,CC BY-SA 2.5,
11138,6933,2,"@cardinal, +1. The answer is magnitudes better than the question.",2011-02-08T07:56:51.240,2116,CC BY-SA 2.5,
11139,6795,0,"@Belmont, now that @cardinal gave a complete answer, can you specify what LAR really is, for future reference? Judging from the answer this is standard manipulation of products of least squares regressions given some initial constraints. There should not be a special name for it without serious reason.",2011-02-08T07:59:02.083,2116,CC BY-SA 2.5,
11140,6984,0,thanks: it really didn't occur to me that a transpose could make such a large difference.,2011-02-08T08:46:00.980,603,CC BY-SA 2.5,
11141,6984,0,"@user603, you might want to file a bug for Matrix package developers. It seems that this is a simple matter of forgetting to add appropriate method definitions.",2011-02-08T08:49:16.033,2116,CC BY-SA 2.5,
11142,6605,0,"@Eduardo - but then how do you adjust the p-value to account for the multiple 1-parameter comparisons you are making?  there is no principles to guide you, only your intuition.",2011-02-08T08:50:12.800,2392,CC BY-SA 2.5,
11143,6952,0,"No, leave your dependent variable (Y/N variable in your case) out of PCA dimension reduction. You can read my article about same topic at the http://goo.gl/lJh5s",2011-02-08T08:57:14.050,609,CC BY-SA 2.5,
11144,6966,4,"[These quotes](http://en.wikiquote.org/wiki/George_Box) are very appropriate. All models are wrong, but some are useful.",2011-02-08T09:15:20.720,2116,CC BY-SA 2.5,
11146,6939,0,"@mpiktas not in the whole dataset. I want it to combine all possible combinations of each field and test for patterns in the slope for consecutive days (so eg. x=1,2,3 or x=5,6,7), if there is one (or with a slight variation in pattern I can set) I want it to return that combination.",2011-02-08T11:02:25.683,3040,CC BY-SA 2.5,
11147,6984,0,"@mpiktas (+1) Nicely done. (oh, and good to know that you're on a Mac :-)",2011-02-08T11:15:18.117,930,CC BY-SA 2.5,
11148,6978,0,"It's only a few decades since most datasets were too large to fit in main memory, and the choice of algorithms used in early statistical programs reflected that. Such programs didn't have facilities for mixed-effects models though.",2011-02-08T11:38:48.657,449,CC BY-SA 2.5,
11149,6984,0,"@chl, only because of hardware :) Mac OS X's woeful swap treatment is killing me. Better than Windows, but way worse than Linux.",2011-02-08T11:41:32.743,2116,CC BY-SA 2.5,
11150,6976,0,"@chase and @cardinal: yes Travel demand forecasting and Netflix Cinematch ( and other recommendation engines) would be great examples, if there were any documentation about how they work, or at least a description of the underlying techniques. Other examples could be from non-profit domains such as education or government.",2011-02-08T11:47:11.913,2544,CC BY-SA 2.5,
11151,6976,0,@probabilistic logic: thanks for the refs I will look them up.,2011-02-08T11:47:40.117,2544,CC BY-SA 2.5,
11152,6314,0,"@cardinal, the Cover-Hart bound seems not to depend on dimension at all, as you say a different aspect ?",2011-02-08T11:53:41.620,557,CC BY-SA 2.5,
11154,6984,0,"@chl, if your swap partition is larger than 5GB sure :) If you know that you will need more swap than your current swap partition size, you can mount additional space as swap. This needs manual intervention of some kind, but on other hand it is doable via R.",2011-02-08T12:08:01.913,2116,CC BY-SA 2.5,
11155,6984,0,"@mpiktas Oups, I deleted my previous comment while editing... My point was about loading or processing large amount of (genetic) data in R which I found convenient under the Mac thx to its ""unlimited"" swapping facilities (I must admit I don't use Linux for some years now, and all that is certainly of marginal interest to the OP). Thx for sharing your thoughts anyway.",2011-02-08T12:13:54.437,930,CC BY-SA 2.5,
11156,6909,0,"@probabilityislogic, the second remark you make is essentially the concept of absolute continuity that @Didier was bringing to your attention. As regards your first comment, look again. I gave you the ""extreme"" example for a very good reason: the difference in support can be made *arbitrarily* small and yet your measure $\delta$ remains infinite.",2011-02-08T12:52:26.833,2970,CC BY-SA 2.5,
11157,6795,0,"@mpiktas, LAR stands for *least angle regression* as coined by Efron et al. In their 2004 paper. It is closely related to both forward-stagewise regression and the lasso. Indeed it allows the *full* regularization path of each of these to be calculated with the same computational complexity of a single least-squares fit. (Note that both methods reduce to least-squares when the shrinkage goes to zero.) LAR was discovered and developed after the similarities in the regularization paths of the coefficients in forward-stagewise and the lasso were noted.",2011-02-08T13:02:09.860,2970,CC BY-SA 2.5,
11158,6314,0,"yes I believe this is true and this was, in large part, my point in bringing it up. 1-NN seems pretty relevant in that sense, i.e., the fact that it works (so) well (theoretically) uniformly in the dimension of the feature space seems to help it stand on it's own, regardless of what the behavior of the nearest and farthest neighbors is in a large dimensional space. It makes me wonder if Beyer was aware at all of this (classical) result.",2011-02-08T13:08:30.630,2970,CC BY-SA 2.5,
11159,6795,0,"@cardinal, but if we regularize $\beta$ why $u(\alpha)=\alpha X\hat{\beta}$, with $\hat{\beta}$ LS estimate?",2011-02-08T13:13:35.087,2116,CC BY-SA 2.5,
11161,6977,0,"Can you please give a little more information. For example, what are the hypothesis being tested with this model? Are there any R implementations using @Mikhil's data?",2011-02-08T13:19:01.753,2040,CC BY-SA 2.5,
11162,6795,1,"@mpiktas, it's a stagewise algorithm, so each time a variable enters or leaves the model on the regularization path, the size (i.e., cardinality/dimension) of $\beta$ grows or shrinks respectively and a ""new"" LS estimate is used based on the currently ""active"" variables. In the case of the lasso, which is a convex optimization problem, the procedure is is essentially exploiting special structure in the KKT conditions to obtain a *very* efficient solution. There are also generalizations to, e.g., logistic regression based on IRLS and Heine-Borel (to prove convergence in finite no. of steps.)",2011-02-08T13:19:41.953,2970,CC BY-SA 2.5,
11163,6990,8,Please write something more about your problem -- this way this question will be closed.,2011-02-08T13:21:37.067,,CC BY-SA 2.5,user88
11164,6977,0,"@user2040, ok. I'll try to post something more in a bit. I noticed the Lindsey book can be viewed on Google Books. A standard **glm** call using family=""poisson"" can be used for the fitting and testing. All it requires is to get the symmetry covariates coded properly.",2011-02-08T13:27:59.757,2970,CC BY-SA 2.5,
11165,6795,0,"@cardinal, thanks for trying to explain. I understand the idea, but some of the details are still not clear, I see though that I can figure them out myself using the references you gave.",2011-02-08T13:37:38.657,2116,CC BY-SA 2.5,
11166,6795,0,"@mpiktas, unfortunately I'm not aware of a good overview treatment of the algorithm. The Hastie et al. text has a handful of pages on it, but as with most of the methods discussed in the book, many significant details are missing. The original Efron et al. paper is at least 45 pages long, if I recall, which requires some determination to get through. Perhaps there is a gap in the literature to be filled.",2011-02-08T14:05:55.340,2970,CC BY-SA 2.5,
11168,6605,0,"@probabilityislogic: I'm more often interested in the whole model than in individual 1-parameters. If the model works (i.e., helps me understand how a physical system works, and predict how it would behave given certain conditions), then each parameter has a good enough value.",2011-02-08T14:52:20.317,2971,CC BY-SA 2.5,
11169,6795,0,"@cardinal, ok I got the Efron paper, will spend few evenings reading it instead of answering questions here :)",2011-02-08T14:52:36.140,2116,CC BY-SA 2.5,
11170,6975,2,"Before you do any of this stuff, perform univariate exploration of each of your 40 attributes.  Many of them should be re-expressed as square roots or started logarithms before you go any further.  In practice this will improve subsequent results *immensely.*  You also want to identify outliers and make a plan for assessing their influence in your analyses, at least if you're going to use least-squares techniques (including PCA, which can be extremely sensitive to geometric [multivariate] outliers).",2011-02-08T16:25:16.140,919,CC BY-SA 2.5,
11172,6987,0,"thanks for the details @Chase, DCM is an area I'm not familiar with, but looks interesting.",2011-02-08T16:43:45.047,2544,CC BY-SA 2.5,
11174,6975,0,"My preliminary visual survey of the data was plotting all parameters against each other, so I have a plot of every relationship, and can observe a few patterns and outliers. Should I apply the same transformations to every variable, and include them, or should I examine what the most likely transformations should be first.",2011-02-08T16:53:46.873,2635,CC BY-SA 2.5,
11176,6987,0,"@pchalasani - for a more general overview of discrete choice modeling, [Kenneth Train's book](http://www.econ.berkeley.edu/books/choice2.html) is a very good place to start for both theory and practical applications. It's a fairly dense read, but well worth the effort if you are interested.",2011-02-08T16:58:48.317,696,CC BY-SA 2.5,
11177,6983,0,"My goal with the issue was to collect expert opinion in order to enrich the debate on the revision of courses in statistics that is ongoing at the institute where I work in Brazil. The objective is being achieved, with opinions as well placed as of @cardinal, @Andrew Robinson, @probabilityislogic and @JMS. Clearly, hypothesis testing (via N-P, DT or Byes) should be very well taught, but the challenges to build courses as appropriate, given the universality of the teaching of statistics, is equally or more complex than the technique itself.
Thank you for your contribution.",2011-02-08T17:19:24.877,523,CC BY-SA 2.5,
11180,6980,2,"Thanks for the interesting list of examples. Given the objective of the question: To contribute to the debate on the review of our statistics courses, we will try to get more details on the implementation of testing in modern devices, can be a great motivation for our engineering students.",2011-02-08T17:28:41.140,523,CC BY-SA 2.5,
11181,6979,0,E sobre a pr√°tica usual de n√£o reportar-se estimativas do,2011-02-08T17:30:20.187,523,CC BY-SA 2.5,
11182,6979,0,Sorry for the earlier comment in Portuguese. What about the usual practice of not reporting to estimate the power of a test and the difficulty in obtaining such an estimate? When is it (Is it?) acceptable to the absence of this estimate? Is it acceptable?,2011-02-08T17:35:29.497,523,CC BY-SA 2.5,
11184,6923,0,"@whuber: Wow! Thanks for sharing years of experiences with us! I don't know, but I think, citing Maindonald's book may not be citing `Statistical Methods for Research Workers` - by `Fisher` but it comes pretty close it.",2011-02-08T18:56:13.607,1307,CC BY-SA 2.5,
11187,6979,0,I'm afraid I don't quite understand your question. Are you asking when/if it is appropriate to report the results of a test without an estimate of its power?,2011-02-08T21:09:35.753,26,CC BY-SA 2.5,
11188,6310,0,"@Simon, why is the correction to minimize?

Doesn't value of P found maximize the ratio?",2011-02-08T21:28:56.113,,CC BY-SA 2.5,user3115
11189,6865,0,I have a lot of things on my plate at the moment. I will expand my answer to be more specific to your question. I see you have invested a lot of effort in framing your question. Thanks for that.,2011-02-08T22:01:05.867,1307,CC BY-SA 2.5,
11196,6997,0,"When you say that ez was buggy for mixed ANOVA designs, are you basing this on my recent announcement of version 3.0 in which I note that there was a bug in the ezMixed() code? If so, I think you misinterpreted that note. ezMixed() isn't related to ezANOVA(). ezMixed() serves to help assess the influence of fixed effects in a mixed effects modelling context.",2011-02-08T23:24:33.910,364,CC BY-SA 2.5,
11198,6979,0,"JMS, sorry my English. I'll try again. Yes, I tried to challenge that hypothesis testing results without reporting estimates of power is highly questionable and that interval estimators do not have this additional source of sins, even agreeing that they are sin prone, but less sin prone.",2011-02-08T23:55:58.010,523,CC BY-SA 2.5,
11199,6996,0,"Formally speaking, *any* hypothesis test with alpha bound on the rate of Type I error can be turned into a confidence interval with coverage parameter (1-alpha) and vice versa, no? I don't think you have to be a hardcore frequentist to believe that this is entailed by the definitions. :-)",2011-02-09T00:08:45.013,1122,CC BY-SA 2.5,
11200,6979,0,"We will ask for a wage increase, in addition to having to teach relatively complex topics for almost all courses in fields as diverse as engineering and management, we have to try to teach the approaches of different schools of thought. Construct good courses in statistics should be a line of research very well funded, it is very difficult to achieve an equilibrium between working hours and topics to be taught to students of courses that increasingly demanding the ability to analyze data",2011-02-09T00:14:49.787,523,CC BY-SA 2.5,
11201,3790,0,"That should be ""mean is unique maximizer""",2011-02-09T00:53:06.340,,CC BY-SA 2.5,user3116
11202,6977,0,"@B_Miner, @Mikhil, added model description and code. Hope this helps.",2011-02-09T04:24:57.703,2970,CC BY-SA 2.5,
11203,7010,1,"Ya, realized what I was doing.  Missed that whole exponential thing.  Thanks.",2011-02-09T04:28:43.993,2387,CC BY-SA 2.5,
11204,6979,0,"An interval estimate isn't necessarily (or usually) accompanied by a power calculation, so I'm not sure I agree with your point as I understand it. And as others have pointed out, classical hypothesis tests and confidence intervals are really two sides of the same coin (and as such share the same flaws). Certainly a CI is more informative than just a reject/fail to reject sort of statement, but I'm not sure that goes to the original point.",2011-02-09T04:35:33.357,26,CC BY-SA 2.5,
11205,7008,0,"this is a very nice answer to my question. However, although I follow the calculation that you make for $n|\rho$, it is not exactly clear to me if the units for $rho$ is percent in the solution $n=65$ for $\rho<1$; does this mean ""$\rho$ is less than $1\times s^2$"" or ""$\rho$ less than $1\%$ of $s^2$?",2011-02-09T05:58:50.607,2750,CC BY-SA 2.5,
11206,595,0,are you talking about MODWT (Maximum Overlap Discrete Wavelet Transform) ?,2011-02-09T06:59:17.013,1709,CC BY-SA 2.5,
11207,595,0,"@fRed: nop, here is the paper, Coifman and Donoho: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.125.3682&rep=rep1&type=pdf",2011-02-09T07:17:09.490,223,CC BY-SA 2.5,
11208,3790,2,"Various points: $E|X-k|$ is minimised when $k$ is the median of $X$.  Von Hippel's article (linked above by chl) discuses exceptions and http://www.btinternet.com/~se16/hgb/median.htm shows the possible relationship between mean, median, mode and standard deviation, both for continuous and for discrete distributions. The 3 can in fact take any value: positive, negative, zero or infinite.",2011-02-09T08:18:13.503,2958,CC BY-SA 2.5,
11209,6996,3,"@Keith  No argument over the definitions, but you do have to be a Frequentist to consider them to be more than interesting and perhaps handy bits of mathematics.  That is, *if* you think sampling theoretic properties are vital for statistical inference then you will (or should) be *equally* keen on confidence intervals and hypothesis tests since, as we agree, they have this symmetry.  Mine was a response to the questioners contrast between 'good' CIs and 'bad' HTs. By lumping them together I wanted to refocus on the contrasts brought up in other answers.",2011-02-09T08:29:39.480,1739,CC BY-SA 2.5,
11210,7007,0,"I guess you mean ""Strongly disagree"". You can do all kinds of analysis on it, yet it depends on what type of arguments you want to make.",2011-02-09T08:39:09.160,2904,CC BY-SA 2.5,
11211,6994,0,Thanks!! I only recently started looking into bayesian analysis and i still find it a bit difficult to grasp. I guess this is an opportunity to learn a bit more about it.,2011-02-09T09:16:38.460,1871,CC BY-SA 2.5,
11212,6992,0,"Hi, thanks very much. I was searching for nearly a week for a definitive answer and didn't find any.",2011-02-09T10:17:47.863,3104,CC BY-SA 2.5,
11213,6991,0,"I'm no statistician but a physicist trying to analyse some data, so thank you for this nice explanation.",2011-02-09T10:29:45.667,3104,CC BY-SA 2.5,
11216,6979,1,What I tried to express is that hypothesis tests not accompanied by estimates of power are very questionable and that interval estimates do not have this additional source of complications.,2011-02-09T12:33:12.177,523,CC BY-SA 2.5,
11217,6605,0,"@Eduardo: I think we're talking at different levels here.  p-values usually give good indications in models (such as OLS regression) of the size of parameters.  This is because they are based on a sufficient statistic, and no nuisance parameters (T-test).  But once you leave this ""safe"" area (e.g. non-linearity), p-values do not necessarily have good properties. They can be useful as a guide, simply because the tend to be easy to calculate.  It makes more sense to calculate the prob of the null, given the data, because we know what data we saw, but do not know what hypothesis to accept",2011-02-09T12:53:29.303,2392,CC BY-SA 2.5,
11218,7008,0,"@Abe, updated and hopefully clarified in the process. There was one particularly bad typo in the previous version. Sorry about that.",2011-02-09T12:55:32.890,2970,CC BY-SA 2.5,
11219,6962,0,"@ Glen: I did it, but the warning keeps on coming as follows: ""your current security settings do not allow this file to be downloaded."" Your help will be appreciated. Thanks a lot.",2011-02-09T12:58:18.860,3085,CC BY-SA 2.5,
11220,6605,0,"...cont'd... why do we calculate the probability of something which is certain (the data)?  And why do we condition on something which is uncertain (the null)?  This has always seemed backwards to me (although, if by prob of ""data"" we actually mean ""future data"", then it makes a bit more sense).  To me, it makes more sense to condition on what you observe, because you cannot ""unobserve"" anything once it has been observed.",2011-02-09T12:59:57.000,2392,CC BY-SA 2.5,
11221,6909,0,"@cardinal - I take your point, but my answer does apply reasonably well to the particular densities being compared, as the supports are nested in this case.",2011-02-09T13:15:26.367,2392,CC BY-SA 2.5,
11222,6909,0,"@probabilityaslogic, I guess I viewed the exercise as an attempt to find a good distance measure in a general setting. If the objective is to find a good measure that matches one's intuition for a single particular example, you might as well pick your favorite number and assign that value to be the distance.",2011-02-09T13:31:47.880,2970,CC BY-SA 2.5,
11223,6963,0,$X_1$ and $\sum_j X_j$ are correlated so we have to approximate the integral itself. $\alpha_i$ is often a small number like 1 or 2 and sometimes as big as 10000. Similarly wih $\beta_i$ but it is usually 10 times bigger than $\alpha_i$.,2011-02-09T13:50:07.193,217,CC BY-SA 2.5,
11224,6963,0,The problem is with small $\alpha_i$. If all $\alpha_i$ are big then the good approxmiation of the whole integral is: $\frac{\alpha_1/\beta_1}{\sum_j \alpha_j/\beta_j}$,2011-02-09T13:51:45.430,217,CC BY-SA 2.5,
11225,6933,0,"@cardinal, you might want to change the link to amazon or some other site. I think that linking to the full book might raise some copyright issues.",2011-02-09T14:09:41.390,2116,CC BY-SA 2.5,
11226,6795,1,"@Belmont -1, as I recently bought the book of Hastie, I can confirm, that this is an exercise from it. So I am giving you a big -1, since you do not even manage to give all the definitions, I am not even talking about giving the reference.",2011-02-09T14:14:57.847,2116,CC BY-SA 2.5,
11227,6933,3,"@mpiktas, nope. No copyright issues. That is the official website for the book. The authors obtained permission from Springer to make the PDF freely available online. (See the note to this effect on the site.) I think they got the idea from Stephen Boyd and his *Convex Optimization* text. Hopefully such a trend will pick up steam over the next few years. Enjoy!",2011-02-09T14:30:10.093,2970,CC BY-SA 2.5,
11228,6933,0,"@cardinal, ooh massive thanks! That is mighty generous from the authors.",2011-02-09T14:42:42.190,2116,CC BY-SA 2.5,
11229,6933,0,"@mpiktas, it's by far the most popular book in the Springer Series in Statistics. It looks good on an iPad. Which reminds me---I should download Boyd's text onto it as well. Cheers.",2011-02-09T14:47:54.227,2970,CC BY-SA 2.5,
11230,7021,0,"I can be more modeling specific, but I don't want to guess here. If you chose to modify your question, I will add more details.",2011-02-09T14:52:17.317,1307,CC BY-SA 2.5,
11231,6933,0,"@cardinal, it looks good on Kindle too, also Boyd's too :)",2011-02-09T14:52:20.573,2116,CC BY-SA 2.5,
11232,6933,0,"@mpiktas, the Kindle must have gotten better at displaying books with math. When it first came out, I heard that it was useless for such a purpose.",2011-02-09T15:09:08.703,2970,CC BY-SA 2.5,
11233,6310,0,@statnovice: The original version of the answer had the numerator and denominator switched.,2011-02-09T16:27:24.650,495,CC BY-SA 2.5,
11234,6979,0,"I don't agree. If you're using a confidence interval to test a hypothesis (does it contain zero, for example) you have exactly the same problem, no? OTOH, if you're not worried about testing a hypothesis then the concept of statistical power, as it is traditionally developed, is meaningless.",2011-02-09T17:07:09.897,26,CC BY-SA 2.5,
11235,7022,0,"you should mention java environment in the title of your question this will maximize the chance to get an answer. Also is there a tag ""java"" ?",2011-02-09T17:09:25.053,223,CC BY-SA 2.5,
11236,7022,0,@robin Yes it is.,2011-02-09T17:14:08.283,,CC BY-SA 2.5,user88
11237,7022,0,"@mbq: cheers! I wasnt too sure about tagging/mentioning java too much; my interest is primarily if it can be done relatively easily, then a Java implementation. But this works as well.",2011-02-09T17:20:14.547,3014,CC BY-SA 2.5,
11238,6933,0,"@cardinal, the new kindle displays pdf files just fine. There are some corner cases, but if pdf is nice, it looks pretty good.",2011-02-09T17:50:41.237,2116,CC BY-SA 2.5,
11244,7022,1,"@posdef, if it is normal, then simulation is [easy](http://acs.lbl.gov/software/colt/api/cern/jet/random/Normal.html), just calculate [mean](http://acs.lbl.gov/software/colt/api/cern/jet/stat/Descriptive.html#mean(cern.colt.list.DoubleArrayList)) and standard deviation. For motivation you will need to [test](http://www.vni.com/products/imsl/jmsl/v40/api/com/imsl/stat/NormalityTest.html) whether the sample is normal.",2011-02-09T20:01:00.353,2116,CC BY-SA 2.5,
11245,7005,0,do you have a textbook reference for the unbiased estimator of variance? I don't know where to go from Wikipedia for more context.,2011-02-09T20:02:27.690,2750,CC BY-SA 2.5,
11246,7005,0,"I don't have my standard text [Rice](http://www.amazon.com/Mathematical-Statistics-Data-Analysis-John/dp/0534209343) with me here, so I can't check the page number for you, but I'm sure it's in there. Wikipedia suggests it should also be mentioned in: Montgomery, D.C. and Runger, G.C.: _Applied statistics and probability for engineers_, page 201. John Wiley & Sons New York, 1994.",2011-02-09T20:21:25.713,2898,CC BY-SA 2.5,
11248,7032,3,"Context, details, or a real or hypothetical example would help focus this discussion greatly.",2011-02-09T20:44:48.550,696,CC BY-SA 2.5,
11249,7032,0,"@user3125. Pls be more specific. Also why did you tag it regression? One of the regression methods is related to least sqaure, but not necessarily the same.",2011-02-09T21:12:17.380,1307,CC BY-SA 2.5,
11251,6977,0,"This is great, thanks so much. I need to study it and look at the preceding chapters in Agresti to truly understand.",2011-02-09T23:53:06.527,2040,CC BY-SA 2.5,
11253,6997,0,HI Mike - you are quite right - it was your note on ezMixed() I read and misinterpreted this for ezANOVA().,2011-02-10T01:15:19.780,,CC BY-SA 2.5,user3112
11254,7039,5,"@whuber, note that $-2\log(\sin(\mathrm{arctan}(x))) = \log(1+x^{-2})$, which relates the form of the cdf closer to that of the pdf. It's also interesting to note that this pdf is asymptotic to one-half the pdf of a standard Cauchy. So, the main reason for its use would seem to have to be for its behavior around 0.",2011-02-10T03:41:27.290,2970,CC BY-SA 2.5,
11255,7039,1,"@whuber, though I think I see where you are coming from with regard to your statement about cdfs having closed forms (hint: Louiville), I would urge caution with that remark. The Cauchy distribution itself is a ""counterexample"" in that respect.",2011-02-10T03:54:04.143,2970,CC BY-SA 2.5,
11256,6977,0,"@B_Miner, I'm not sure what your familiarity with GLMs is, so forgive the suggestion if it is too elementary. But, if you want a good quick overview, look up the notes for log-linear models by G. Rodriguez---at Princeton, I believe. I came across them once and they seemed like a good way to get quickly up to speed for someone who hadn't had much previous exposure.",2011-02-10T04:12:00.307,2970,CC BY-SA 2.5,
11258,7039,0,"@cardinal I do not understand the point of your remark about the Cauchy distribution.  I am only using the form of the CDF as a heuristic for narrowing searches and as a target for searches.  The CDF is a little more convenient than the PDF because it is easier to see how it will change when the variable is transformed.  And yes, the relationship you noted is clear, but I chose to write the CDF in this form because of the presence of the arctangent in the other term (which suggests the substitution x = tan(u)).",2011-02-10T05:47:02.240,919,CC BY-SA 2.5,
11259,7026,0,"I want to measure the overall quality of service using the attributes like safety, convenience, comfort, etc. per age group, income level, occupation.Is it possible to sum the score for each attribute to obtain the overall quality of service? Is it possible to know how important these attributes to people when considering to ride the jeepney? Thanks.",2011-02-10T07:58:01.450,3118,CC BY-SA 2.5,
11260,7022,0,"@mpiktas: thanks for your reply, although I am not quite sure why you chose to comment instead of answering :) As you have mentioned, sampling is no problem but I need to test normality. It seems like the library you've mentioned is commercial though. Could I just do a $\chi^2$ test, or would I need to implement a Shapirov-Wilk, alternatively Lilliefors, test on my own?",2011-02-10T09:28:15.467,3014,CC BY-SA 2.5,
11261,7030,0,"unfortunately I need to create samples as large as the original dataset, plus I need to do this process a large number of times... So picking random values doesn't seem like a viable option in my case.",2011-02-10T09:30:28.863,3014,CC BY-SA 2.5,
11264,7042,0,"Thank you for the prompt, clear and very useful answer. I have spent almost a day trying to search internet.",2011-02-10T10:29:55.097,2956,CC BY-SA 2.5,
11266,7042,0,@DrWho So accept it by clicking tick mark.,2011-02-10T10:31:57.530,,CC BY-SA 2.5,user88
11268,7022,1,"@posdef, answers require better quality than comments, this is why I commented. As for commercial use, try google to find non comercial. [This seems free](http://www.numericalmethod.info/javadoc/suanshu/basic/com/numericalmethod/suanshu/stats/test/distribution/normality/ShapiroWilk.html). In worst case scenario you will need to reimplement this test, or another normality test. I suggest asking here or on stackoverflow , which of the normality tests is easiest to implement.",2011-02-10T10:48:46.137,2116,CC BY-SA 2.5,
11269,3497,0,"Joshep, if I understand your question, the objective of your work would be to demostrate that running regression analysis on random samples you obtained simular results to those from the entire data set if the exchangeable assumption holds. My question is if someone knows any reference in which this method has been used.",2011-02-10T10:54:47.067,221,CC BY-SA 2.5,
11270,1539,1,"the difficult part actually is to show weak convergence. The convergence of supremums then follows directly from continuous mapping theorem. This result can be found in Billingsley's ""Convergence of Probability Measures"". Van der Vaart and Wellner give more general result and their book is really, really tough :)",2011-02-10T10:58:16.300,2116,CC BY-SA 2.5,
11272,358,0,"Remember, the normal distribution is the most ""cautious"" one for known variance (because is has maximum entropy among all densities with fixed variance).  It leaves the most to be said by the data.  Or put another way, for ""large"" data sets with the same variance, ""you"" have to ""try"" *incredibly* hard to get a distribution which is different from a normal.",2011-02-10T12:16:32.693,2392,CC BY-SA 2.5,
11274,7039,1,"@whuber, well perhaps I would have been better off asking for clarification rather than assuming. What was your point regarding your comment that a closed form cdf severely limits the possibilities?",2011-02-10T12:27:05.630,2970,CC BY-SA 2.5,
11275,7046,0,I will try to rectify the table. I am new to this. I am trying,2011-02-10T12:35:01.837,2956,CC BY-SA 2.5,
11276,7045,0,"@mariana, I don't understand. Both models you listed are linear in the coefficients. Are you looking for something that automatically does both feature extraction *and* model selection? Regards.",2011-02-10T12:37:36.823,2970,CC BY-SA 2.5,
11277,2501,24,"+1 for a good and informative answer. I find it useful to see a good explanation for a common misunderstanding (which I have incidentally been experiencing myself: http://stats.stackexchange.com/questions/7022/parameter-estimation-for-normal-distribution-in-java). What I miss though, is an alternative solution to this common misunderstanding. I mean, if normality tests are the wrong way to go, how does one go about checking if a normal approximation is acceptable/justified?",2011-02-10T12:45:49.370,3014,CC BY-SA 2.5,
11278,42,2,@shabbychef: Java programs doesn't have to be a slow monsters. I admit that well written C-code will almost always be faster than a similar implementation in Java but well written Java code will most likely not be super slow. Plus developing in Java has many significant advantages.,2011-02-10T12:52:21.620,3014,CC BY-SA 2.5,
11279,7045,0,"Yes, but mostly for model selection.",2011-02-10T12:54:24.423,1808,CC BY-SA 2.5,
11280,7046,0,"Thank you for your help. The 1-tailed p-value was recommended by the software Comprehensive Meta Analysis v2.2. The Data are
Year Region 1  Region 2  Region 3
Cases Dead     Cases Dead Cases Dead Total cases Total dead cases
2006 2320 528 1484 108 73 3 3877 639 2007 3024 645 1592 75 32 1 4648 721 2008 3012 537 1920 53 3 0 4935 590 2009 3073 556 1477 40 246 8 4796 604 2010 3540 494 1460 26 138 1 5138 521 Total 14969 2760 7933 302 492 13 23394 3075 I will try to rectify the table.‚Äì DrWho",2011-02-10T12:54:29.153,2956,CC BY-SA 2.5,
11281,2498,1,"+1: great answer, very intuitive. Perhaps a bit off-topic but how would one go about implement the second method without qq-plots (due to lack of visualization)? What logical steps are taken here to get the p-values?",2011-02-10T13:04:34.003,3014,CC BY-SA 2.5,
11282,2498,0,"@posdef : those are just the p-values of the shapiro-wilks test, to indicate that they contradict the qq-plots.",2011-02-10T13:31:37.780,1124,CC BY-SA 2.5,
11283,7046,0,"The data of a particular disease is

Year Region 1  Region 2  Region 3   
 Cases Dead Cases Dead Cases Dead Total cases Total dead cases
2006 2320 528 1484 108 73 3 3877         639
2007 3024 645 1592 75 32 1 4648         721
2008 3012 537 1920 53 3 0 4935         590
2009 3073 556 1477 40 246 8 4796         604
2010 3540 494 1460 26 138 1 5138         521
Total 14969 2760 7933 302 492 13 23394        3075",2011-02-10T13:41:06.337,2956,CC BY-SA 2.5,
11285,7039,1,"@cardinal I am performing a *wide* search in the sense of finding a named (or heretofore studied) distribution $G$ and a relatively simple re-expression $y$ (such as a power or logarithm etc.) such that $y(X)$ has cdf $G$ iff $X$ has pdf $f$.  If a distribution has been studied before, then it's highly likely its CDF has been obtained and, if it can be written in closed form, that form has also been published.  Therefore we need only look for functional forms $G$ that look like $u - \tan(u)\log(\sin(u))$ with $u = u(x)$.  Know of any?",2011-02-10T13:54:02.410,919,CC BY-SA 2.5,
11287,7045,2,"@Mariana Do you mean ""general linear model"" (http://en.wikipedia.org/wiki/General_linear_model ) or ""generalized linear model"" (http://en.wikipedia.org/wiki/Generalized_linear_model )?  The second one handles binomial dependent variables.",2011-02-10T14:00:10.183,919,CC BY-SA 2.5,
11289,7039,0,"@cardinal Exactly! Good analysis. That was the thought behind describing this as a ""perturbed Cauchy"" distribution. (There aren't too many algebraic manipulations to try. If the general-purpose substitution u = arctan(x/2) (which is essentially what we started with) does not work we're probably out of luck, because if an expression in terms of elementary functions is possible, this substitution should do it. Otherwise, start looking at elliptic functions...but at that point you know this is an unusual distribution anyway.) [It seems that the comment to which this is responding was deleted.]",2011-02-10T14:43:42.233,919,CC BY-SA 2.5,
11290,7039,0,"@whuber, not off-hand. The distribution can be viewed as an additive mixture of a standard Cauchy with something else, though. That ""something else"" has thin tails, but doesn't seem to be obvious. It seems pretty implausible, but maybe finding the ch.f. and looking for some relationship with the Cauchy would yield something.",2011-02-10T14:44:28.367,2970,CC BY-SA 2.5,
11291,7039,0,"@Cardinal The ""something else"" is not a distribution because it does not increase monotonically (its ""pdf"" would have some negative values).  It basically shifts some probability out of the region $|x| \gt$ 0.504976 into its complement (a neighborhood of 0).",2011-02-10T14:52:21.847,919,CC BY-SA 2.5,
11292,7039,0,"@whuber, sorry, yes. My attempt to clarify something I was saying mucked things up. My apologies. And, yes, I noticed the fact that it couldn't be a mixture right before your most recent comment. But, you beat me to it!",2011-02-10T14:56:27.903,2970,CC BY-SA 2.5,
11293,2498,1,@joris: I think there might have been a misunderstanding; Shapiro-Wilks give p_{n5000} = 0.87 while the second calculation yields p_{n5000} = 0.007. Or have I misunderstood something?,2011-02-10T14:58:06.390,3014,CC BY-SA 2.5,
11294,2498,1,"Indeed. 0.87 is the proportion of datasets that give a deviation from normality, meaning that in 87% of the datasets from an almost normal distribution, Shapiro-Wilks will have a p-value smaller than 0.05. The second part is just an example of some datasets that illustrate this.",2011-02-10T15:02:03.170,1124,CC BY-SA 2.5,
11295,7049,0,"How do your empirical distributions differ from Gamma variates, then?",2011-02-10T15:15:40.280,919,CC BY-SA 2.5,
11296,7050,0,"Hello Whuber.  Your example of ESDA is a great example, thank you!  If you (or others) can suggest other examples of when formal procedures are less relevant - this would be most helpful.",2011-02-10T15:23:13.793,253,CC BY-SA 2.5,
11297,7040,0,"Comparison of Region 1 and 2 on what metric? e.g. log odds ratio, log risk ratio, risk difference ...?",2011-02-10T15:27:57.543,449,CC BY-SA 2.5,
11298,7056,3,Move to StackOverflow?,2011-02-10T16:23:20.497,597,CC BY-SA 2.5,
11299,7047,0,Do you strictly have to do it with Matlab? Otherwise I suggest using Hugin http://hugin.sourceforge.net/ . The software is released under GPL so you may also peek into the source code and see how the magic happens.,2011-02-10T16:28:32.257,582,CC BY-SA 2.5,
11300,7049,2,"I have used the gamma distribution for these data. If you use the gamma distribution with a log link you get almost the exact same result you get from an over-dispersed poisson model.However, in most of the statistical packages I am familiar with poisson regression is simpler and much more flexible.",2011-02-10T16:50:09.343,3136,CC BY-SA 2.5,
11301,7005,0,"thanks for your help with this. This answer has been very useful and it has been informative to quantify variance uncertainty - I have applied the equation about 10 times in the last day. calculating $kappa$ is easy with the `moments` library: 
`library(moments); k <- kurtosis(x);
 n <- length(x); var(x)^2*(2/(n-1) + k/n)`",2011-02-10T16:50:56.623,2750,CC BY-SA 2.5,
11302,6963,0,"@≈Åukasz If you need to evaluate the expression of the expectation, why do you require an algebraic formula?
I am thinking in applying some numerical trick to get the expectation but I need some feedback :)",2011-02-10T16:58:26.490,2902,CC BY-SA 2.5,
11303,7056,1,I think it is Ok to answer this question here. We do have a `R` tag.,2011-02-10T16:58:42.963,1307,CC BY-SA 2.5,
11304,7054,1,What is the aim of the analysis? What question(s) are you trying to answer?,2011-02-10T16:59:27.957,449,CC BY-SA 2.5,
11305,7057,1,please provide an example illustrating your problem. Also how do you define power of algorithm?,2011-02-10T17:11:11.277,2116,CC BY-SA 2.5,
11306,7048,3,"At least in my case, I'm somewhat in the same boat. I appreciate Mondrian and keep it up to date, but when I actually explore a new dataset it tends to be in R, which is less interactive but more flexible overall. I started writing you a full answer and realized that I was speaking in the theoretical and not from actual experience.",2011-02-10T17:14:08.807,1764,CC BY-SA 2.5,
11307,7058,0,"it would help if you provided example of how do you define redundancy in the boolean case, and what kind of results you would expect in continuous case",2011-02-10T17:16:49.940,2116,CC BY-SA 2.5,
11308,7058,0,@mpiktas: Edit my question in response to your comment.,2011-02-10T17:35:05.957,977,CC BY-SA 2.5,
11310,7026,0,"You could revise your question to clarify this, or set up a new question. Of course you can build a ""scale"" by combining the three measures. But it's also good to look at the correlations (using a correlation matrix) between the three measures first. This way you can get an idea, whetter people differentiate between those measures, or if they all answer these items in a similar way.",2011-02-10T19:27:24.610,1435,CC BY-SA 2.5,
11311,7057,1,"I've used glmnet and lars packages in a couple of projects. In my limited experience I've had A LOT of problems implementing glmnet. I think that glmnet needs some bug fixes regarding the type of variables used in the data frame. Besides, glmnet has confusing documentation. I ended up using lars, and I was very satisfied with the results. Nevermind the size of your problem, I think lars can handle it.",2011-02-10T19:44:45.207,2902,CC BY-SA 2.5,
11312,7061,1,The answer depends upon what prior knowledge you have.  You might get a different answer if you know John is the captain of the basketball team.,2011-02-10T20:37:06.950,2485,CC BY-SA 2.5,
11313,7059,1,"@David this is not just a visualization problem, you may be interested in the discussion of split-apply-combine strategies facilitated by Hadley's plyr package http://had.co.nz/plyr/plyr-intro-090510.pdf",2011-02-10T20:37:50.013,2750,CC BY-SA 2.5,
11314,7059,0,"@David how do you compute the ""amount""? Is it the sum of actions? We would need the duration of each single action if you want the amount of time...",2011-02-10T20:38:25.917,1443,CC BY-SA 2.5,
11315,7059,0,"@teucer I was thinking about the number of create/update/check actions entered per unit time for each table between the first login and last action. Some tables might have 1-2 creates/week, while others may have as many as 100 creates in a day or even more checks/day.",2011-02-10T20:42:36.470,1381,CC BY-SA 2.5,
11316,7059,0,@David could you provide the duration of each logging?,2011-02-10T20:49:38.747,1443,CC BY-SA 2.5,
11317,7029,0,"out of curiousity, have you got a citation for the source where you saw this originally?",2011-02-10T20:49:48.430,26,CC BY-SA 2.5,
11318,7059,0,"@David I guess when you say first login and last action, these are per day(?). I can easily compute the number of actions per user per week, but, with the given data, it is difficult to translate this in amount of time...",2011-02-10T20:56:35.747,1443,CC BY-SA 2.5,
11319,7059,0,"@teucer, each logging event is instantaneous, but the employees are assumed to be working on the next logging event between one logging and the next. To a first approximation, for each day x user combination, the amount of total time worked could be estimated as the max(time) - min(time) + 30 min, and a continuous block of time could be considered any stretch of time during which $max(\Delta t)<30\text{min}$.",2011-02-10T21:01:28.247,1381,CC BY-SA 2.5,
11320,7059,0,"@teucer I do have a separate time sheet, so that would probably be a good estimate of the total time spent, so really the number of actions per day or per week are more important.",2011-02-10T21:06:26.993,1381,CC BY-SA 2.5,
11321,7029,1,"JMS: ""The horseshoe estimator for sparse signals"" by Carvalho, Polson, and Scott. I saw it as a preprint, but it may have been published in Biometrika by now. They don't exactly use this prior, but the density above is an approximation to a special case of their prior.",2011-02-10T21:11:56.840,319,CC BY-SA 2.5,
11322,7017,0,"Thanks! I now have been able to perform the preacher and hayes test for mediation. I am not sure about the output though..I understand that ¬¥boot¬¥ is the estimation of the indirect effect, but is there also a significance p of this?",2011-02-10T21:18:37.187,,CC BY-SA 2.5,user3119
11323,6963,0,"I need to evaluate it many times in my program. It has to be very fast, i.e. no loops and preferably not too many divisions.",2011-02-10T21:23:56.917,217,CC BY-SA 2.5,
11324,7068,0,"thanks for walking me through the data manipulation, but how can I retain the variables 'action' and 'table' so that I can use something like `facet_grid(. ~ action)` or `facet_grid(. ~ action*table)`?",2011-02-10T22:01:20.787,1381,CC BY-SA 2.5,
11325,7061,0,assume zero prior knowledge,2011-02-10T22:07:00.787,3143,CC BY-SA 2.5,
11326,7066,0,"Aren't the probabilities for John and Mike to make their next shot simply respectively 0.76 and 0.8? Either way, that's not what I'm trying to determine. What I'd like to calculate is the probability (given the limited observations) that Mike has a higher free throw percentage than John.",2011-02-10T22:21:56.110,3143,CC BY-SA 2.5,
11327,7066,0,"+1.  The posterior probability that Mike, with distribution $B(\alpha,\beta)$, is better than John, with distribution $B(\gamma,\delta)$, is proportional to a generalized hypergeometric distribution.  This can be computed either as a polynomial or as a rapidly converging power series, giving far greater computational accuracy than any simulation.",2011-02-10T22:26:16.383,919,CC BY-SA 2.5,
11328,7066,0,"@HawkEgg There is uncertainty about how good either John or Mike are.  E.g., if Mike makes 2/3 shots and John makes 8/11 shots, the estimated probability for each is 3/4 (assuming uniform priors for each), but the chance that John is better than Mike is only 165/364 = 45.3% because there's a good chance Mike is a superb shooter (and got unlucky once) whereas we have a fairly good idea how good John is (because he has taken more shots).",2011-02-10T22:32:33.033,919,CC BY-SA 2.5,
11329,7061,1,"Assuming a uniform prior, the answer is 848437653385740282130263/1167635946155025951609137 = about 72.66%.",2011-02-10T22:35:52.450,919,CC BY-SA 2.5,
11330,7071,0,When you say correlation between values at different times... do you think all possible lag combinations or only t vs t-1?,2011-02-10T22:37:28.063,333,CC BY-SA 2.5,
11331,7061,0,"Michael McGowan's point is that you *must* make some such assumption in order to obtain an answer.  It's like asking whether a cannonball or a feather will hit the ground first without specifying how high each one was when dropped: if you are ignorant of that essential information, *you simply cannot provide a justified answer,* even though the problem makes sense and physical theory exists to solve it for any given pair of heights.",2011-02-10T22:38:08.127,919,CC BY-SA 2.5,
11332,7071,0,@user333 *All* nonzero lags: that's the first equation in the Wikipedia link @onestop gave.,2011-02-10T22:40:27.200,919,CC BY-SA 2.5,
11333,7072,0,In what sense is logistic regression nonparametric?  Just wondering...,2011-02-10T22:42:27.073,919,CC BY-SA 2.5,
11335,7072,1,"I was trying to make the point that it isn't, or isn't normally thought of as such, but it's late and i'm too tired to word it well...",2011-02-10T22:47:01.933,449,CC BY-SA 2.5,
11336,7049,0,"Wouldn't there be other distributions that are better, e.g. whuber's suggestion of gamma?",2011-02-10T22:49:12.943,686,CC BY-SA 2.5,
11339,7041,0,"Isn't that the definition of statistics....I'm 99% sure that I don't know what 99% sure means, but I still have to eat....?  I've been chewing on your answer and I ran your code (there's a typo where t is calc'd, but I got it).  I follow the concept on the first 3 equations, but I ran into trouble on the last equation.  My main problem is where N is applied to both the dependent and independent variables. My 55% dependent wins number applies only when the independent variable is ""true"".  That made me realize just how screwed up my original post really is.  I'll post more info later.",2011-02-11T00:08:22.350,2775,CC BY-SA 2.5,
11341,7038,1,"Wow, that's a surprise. Here I was molesting some Newtony-Raphsony thing and you were over there finding a solution.  It will be used in the future.  Thanks.",2011-02-11T00:11:57.357,2775,CC BY-SA 2.5,
11342,7062,3,"@dsmcha, sorry to say this, but I don't think I like that example too much. The response is identical to two of the predictors? That's beyond pathological, in my view.",2011-02-11T00:31:11.457,2970,CC BY-SA 2.5,
11343,7041,0,"From your calcs, doesn't the last equation come from aligning the alpha 1% tail of the 52.8% ""dumb luck"" distribution with the beta 99% tail of the 55% alternative?",2011-02-11T00:54:24.200,2775,CC BY-SA 2.5,
11344,7066,0,"@whuber Are you saying that Mike, who made a lower % of shots than John, has a 54.7% chance of being better than John? That doesn't pass my sniff test. Could you give me some more details on how you calculated the 3/4 and 45.3% numbers? A reference?",2011-02-11T01:15:47.893,3143,CC BY-SA 2.5,
11345,7017,0,"Hi Renee, you might want to ask a separate question about interpretation of bootstrapping for mediation. However, as a simple rule that is often applied, if the bootstrapped 95% confidence interval of the indirect effect does not include zero, then the indirect effect is deemed to be statistically significant. Preacher and Hayes (2004) explains this in great detail: http://www.comm.ohio-state.edu/ahayes/BRMIC2004.pdf",2011-02-11T01:20:28.920,183,CC BY-SA 2.5,
11346,7066,0,"@HawkEgg if you'd like to know who has the higher free throw percentage than you can compare the posterior distributions instead of the predictive. That is, you can compare a Beta(a0 + shots made, b0 + shots missed) for each player.",2011-02-11T02:17:35.343,1913,CC BY-SA 2.5,
11347,7066,0,@HawkEgg: Mike: 80/100 = 80%; John: 38/50 = 76%.  Sniff again :-).,2011-02-11T03:39:15.810,919,CC BY-SA 2.5,
11348,7041,0,"@bill_080 I think you're right, but conventionally we would say we are matching the *99%* (i.e., upper) percentile of the 52.8% distribution to the *1%* (i.e., lower) percentile of the 55% alternative.",2011-02-11T03:40:56.010,919,CC BY-SA 2.5,
11349,7051,0,"I think, in addition to your point, even if @user3136 is not willing to make the assumption of mean = variance, he/she can use the `quasipoisson` family in `glm` .",2011-02-11T03:45:28.997,1307,CC BY-SA 2.5,
11350,7051,2,"But my problem is why would you want to transform continous data to discrete. It is loosing information essentially. Also when a simple `log` transform would have worked, why discretize your data? Using `glm` works, but every result is asymptotics based (which may or may not hold)",2011-02-11T03:47:56.763,1307,CC BY-SA 2.5,
11352,7072,0,Thanks for clearing that up for me!  I was worried that I had missed something.,2011-02-11T04:12:27.127,919,CC BY-SA 2.5,
11353,7066,0,"@whuber I was referring to _""Mike makes 2/3 shots and John makes 8/11 shots""_ Not realizing that you were adding that to the previous totals (38/50 & 80/100)",2011-02-11T04:12:43.250,3143,CC BY-SA 2.5,
11354,7078,0,This is correct!  The denominator of course is the product of two Beta functions.  The numerator is equivalent to the generalized hypergeometric function I referred to.,2011-02-11T04:16:51.940,919,CC BY-SA 2.5,
11355,7078,0,@whuber Thank you for getting me started. I'm still new to stats so I had to scour wikipedia and my long forgotten calculus for help.,2011-02-11T04:20:15.880,3143,CC BY-SA 2.5,
11356,6240,1,this question on equivalence testing also has some good suggestions http://stats.stackexchange.com/questions/3038/testing-hypothesis-of-no-group-differences,2011-02-11T04:46:23.047,183,CC BY-SA 2.5,
11357,7054,0,@onestop: Thank you for your time. The same treatment was given in all 3 regions but with different levels of dedication to the suffering patient due to various administrative reasons. The aim is to show that there is a significant increase in the number of deaths in 1 region due to poor implementation of the treatment guidelines.,2011-02-11T05:04:53.667,2956,CC BY-SA 2.5,
11359,7071,2,"you forgot the constant variation. If variation varies, then the process is not white noise.",2011-02-11T07:37:03.927,2116,CC BY-SA 2.5,
11360,7054,0,@DrWho: I'm not sure if meta-analysis is the correct approach to analyze this dataset. It seems more like classic longitudinal design. I think you should reconsider your methodology.,2011-02-11T07:56:39.277,609,CC BY-SA 2.5,
11362,7017,0,"The last thing I am wondering about is what macro or test I can perform to assess moderation for non normal distributions. You mentioned bootstrapping, but I don¬¥t know where to find it in SPSS or find a specific macro for that. It is very straightforward moderation I am looking for: 1 independent variable, 1 moderator and 1 dependent variable",2011-02-11T08:00:24.533,,CC BY-SA 2.5,user3119
11364,7068,0,"@David, `rr <-records[records$action!=""Login"",]`
`ddply(rr,~user+table+action+week,nrow)`",2011-02-11T08:41:53.260,2116,CC BY-SA 2.5,
11365,2498,0,"@joris: I see, thanks for straightening it out for me :)",2011-02-11T08:46:10.490,3014,CC BY-SA 2.5,
11366,7071,0,"@mpiktas: you're right, good point.",2011-02-11T08:51:55.613,449,CC BY-SA 2.5,
11367,7086,0,"true; and I do have permutation based statistics as well. Essentially I am implementing two different ways of analyzing the data, and I need comparable scores (in this case it'll be p-values). The motivation behind my approach is that I have reasons to believe: a) the underlying distribution essentially is very close to normal and, b) the measured values are not large enough to resample as many times, or will add bias to my analysis. But as I said above, there will be permutation based statistics as well.",2011-02-11T10:35:14.710,3014,CC BY-SA 2.5,
11368,7017,0,"re moderation, check out this discussion on Preacher and Hayes' facebook page: http://www.facebook.com/topic.php?uid=44574520333&topic=15855 ; otherwise SPSS does have some bootstrapping support in the more recent versions, but I haven't tried it.",2011-02-11T10:53:54.813,183,CC BY-SA 2.5,
11369,7051,1,"@suncoolsu: 1) quasipoisson makes the assumption of mean proportional to variance. 2) I didn't mean transform to discrete, i meant transform (maintaining continuity) so you could use a different model.",2011-02-11T10:58:21.940,495,CC BY-SA 2.5,
11370,7082,2,"Lamentably, in that document Crawley perpetuates the myth that ANCOVA can be applied to scenarios when the covariate is correlated with an IV, even when this explicitly violates an assumption of ANCOVA.",2011-02-11T13:20:47.967,364,CC BY-SA 2.5,
11371,7051,0,"yeah -- I understood agree with you. Sorry, I was talking about the question. Quasi-poisson, takes into account of overdisperson right? (if I remember correctly, cf Faraway 2006)",2011-02-11T13:51:15.453,1307,CC BY-SA 2.5,
11372,7086,2,"@posdef, without a little more information on your analysis, it's a little difficult to give advice. **But**, unless you are interested in some unusual quantities or have an unusual design, then using a parametric bootstrap in a Gaussian setting is suboptimal. The core of statistical inference is centered around assumptions related to normality (e.g., exponential families) and there is some very powerful optimality theory in these cases. Without good reason, one shouldn't try to avoid it.",2011-02-11T13:51:23.877,2970,CC BY-SA 2.5,
11373,7096,1,and `spline` does not work why?,2011-02-11T13:53:04.667,2116,CC BY-SA 2.5,
11374,7096,0,"please clarify. Are you looking for packages that fit so-called *free-knot spline* models, i.e., regression splines where the number and location of knots must also be determined using the observed data?",2011-02-11T14:06:03.027,2970,CC BY-SA 2.5,
11376,7054,0,@Andrej: can you kindly say a few more words about the design?,2011-02-11T14:32:27.543,2956,CC BY-SA 2.5,
11377,7074,1,against what alternative ?,2011-02-11T15:03:45.980,603,CC BY-SA 2.5,
11378,7098,2,"+1.  The remark about final presentations brings to mind, as a notable counterexample, Hans Rosling's famous 2006 TED talk (http://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen.html ).  Re: the point about examining ""far more,"" I am reminded of how a lawyer in a deposition asked me about how I had examined the data that backed up my testimony and how her face fell when she learned the work had been done interactively and therefore nothing was printed or saved (which she could then subpoena, examine, and try to impugn). ;-)",2011-02-11T15:12:13.457,919,CC BY-SA 2.5,
11379,7090,4,There are no computational issues.  Choose a time unit that is readily interpretable.,2011-02-11T15:15:02.050,919,CC BY-SA 2.5,
11380,7094,0,"This formula has notational problems: there are $p$'s on the left and the right.  The right side makes no reference at all to the subscript $\mathbf{i}$.  Furthermore, still interpreting the $p_i$ as probabilities (as in the original question), the rhs clearly is positive whereas the lhs cannot be positive.",2011-02-11T15:21:22.070,919,CC BY-SA 2.5,
11381,7086,0,"@cardinal: can't really say I follow your reasoning, or rather, I am not sure I fully understand what you are saying. :) But I can try and explain my analysis in further detail, if necessary.",2011-02-11T15:31:54.003,3014,CC BY-SA 2.5,
11383,7086,0,"@posdef, any additional detail you could provide would be both helpful and interesting. My point was that **if** you make the leap to assuming normality, it's unlikely that a parametric bootstrap would be the desired approach to go about conducting your inference. But, maybe your situation is complex/unique enough that it does have a role to play.",2011-02-11T16:10:06.367,2970,CC BY-SA 2.5,
11384,7051,0,"In this particular instance I was not satisfied that any transformation I tried (log, sqrt, box-cox) gave a good approximation to normality. Incidentally, if I use the normal score transformation method then I can transform most data to almost beautiful normality, but I have not seen this transformation widely used so I assume there's a catch (it is hard to back-transform).",2011-02-11T17:10:37.680,3136,CC BY-SA 2.5,
11385,7096,0,"Hi - yes, you are perfectly right - I'm looking for an estimator of splines where the number and location of knots must be estimated and using the observed data (time serie data).",2011-02-11T17:43:00.707,,CC BY-SA 2.5,user3154
11386,7095,6,remove 99% of your phones!,2011-02-11T17:46:49.147,795,CC BY-SA 2.5,
11388,7103,4,Some thoughts: (i) Gaussians won't do it.  You're interested in the right tail of the distributions (winds below a few km/h won't generate any power).  (ii) You don't need 100 quantiles; you can interpolate.  Probably 5-16 quantiles will do well.  (iii) You only need a few bits per quantile (8 will do) because economically useful wind speeds lie in a small range.  (iv) 1000 distributions / 15 min * 15 numbers/distribution * 5 E5 min/yr = 5 E8 values = 0.5 GB per year.  That's worth about $0.50 today: you can afford it!,2011-02-11T17:55:00.930,919,CC BY-SA 2.5,
11389,7095,6,Stop paying the phone bill.,2011-02-11T18:30:36.690,2310,CC BY-SA 2.5,
11390,7102,1,"""fails to converge"".. ""do converge eventually"". ""a fit that looks good""...""the visual fit is terrible"". Sorry, but I'm confused!",2011-02-11T18:48:16.257,449,CC BY-SA 2.5,
11391,7090,1,I believe it's advantageous to avoid ties when dealing with time as a continuous variable.  Therefore I think you'd want to base your results on the finest-grained measurements you can get (if that is part of what your question implies).,2011-02-11T18:53:28.233,2669,CC BY-SA 2.5,
11392,7104,0,"Thanks - that seems to make some sense. Embarrassingly my maths isn't good enough to immediately see how you got to teh reparameterisation, but I'll try things out and report back.",2011-02-11T20:06:02.160,3158,CC BY-SA 2.5,
11393,7102,0,"By tweaking convergence criteria I can get the model to converge, but when this happens the plotted function appears to be a much worse fit than the unconverged results. Presumably due to very large convergence criteria.",2011-02-11T20:08:14.873,3158,CC BY-SA 2.5,
11394,7104,0,"Hmmm. After much fiddling the solution seemed to be a slight tweak of my y parameter from -0.06 to -0.061 to get convergence for all...However, fewer datasets failed to get convergence using your RHS simplification of the first equation, and all but one converged using the c2=0 simplification. Many thanks.",2011-02-11T20:43:59.087,3158,CC BY-SA 2.5,
11396,7104,0,@R_usr Sorry; I was lazy.  I edited the penultimate line to show the new parameters in terms of the original ones.,2011-02-11T21:11:37.647,919,CC BY-SA 2.5,
11397,7103,1,"That's not a comment @whuber, that's an answer!",2011-02-11T21:35:59.353,449,CC BY-SA 2.5,
11398,7103,0,"@whuber, thanks good remark ""Probably 5-16 quantiles will do well"" I think that by this remark you enter the problem of compression (even if you deny it afterward).  The remark about gaussian measure in my question is not to illustrate what I would do with wind power but rather to illustrate simple fact about compression of time series of distribution (rather more from a theoretical point of view).",2011-02-11T21:39:01.547,223,CC BY-SA 2.5,
11399,7045,0,"I mean the R function called glm, which is actually the http://en.wikipedia.org/wiki/General_linear_model (general linear_model)",2011-02-11T21:42:38.767,1808,CC BY-SA 2.5,
11400,7103,1,"Sorry, I didn't mean to appear to deny this is a compression problem. *Any* method of storing an empirical distribution is implicitly a compression method.  The issue concerns how best to go about it while not wasting computing resources.  I have established that storage is not the bottleneck, implying you should focus on methods that make subsequent analysis sufficiently efficient and accurate.",2011-02-11T21:44:08.093,919,CC BY-SA 2.5,
11401,7104,0,no need to apologise. If I'm fitting equations I should know how to manipulate them efficiently.,2011-02-11T22:09:58.200,3158,CC BY-SA 2.5,
11403,7112,3,If your software is well written it automatically standardizes internally to avoid numerical precision problems.  You shouldn't have to do anything special.,2011-02-12T01:14:30.760,919,CC BY-SA 2.5,
11404,7047,0,Voting to close.  The expertise to answer this question (without the MatLab restriction) resides at http://photo.stackexchange.com/ .,2011-02-12T01:22:59.530,919,CC BY-SA 2.5,
11405,7113,0,Appreciably simple and clear answer. You must be a great teacher. People like you must write a small book on Introduction to Statistics in 200 pages,2011-02-12T02:54:48.633,2956,CC BY-SA 2.5,
11407,7075,0,wow thanks - that was it - i hadn't even considered the possibility :),2011-02-12T04:23:03.273,1463,CC BY-SA 2.5,
11408,6746,1,"I think part of the problem lies in the fact that the process you've laid out is not fully defined by the characteristics you've listed. For a sample of size $n$, you've given $n\choose 2$ linear constraints for $2^n$ parameters. Many processes could satisfy the constraints and yet lead to different achievable classification rates. Your $R$ code *does* uniquely define a process, but it seemed you intended that as a concrete example instead of as the main object of interest.",2011-02-12T04:43:32.727,2970,CC BY-SA 2.5,
11409,7088,0,Could you elaborate more on what you mean by format?,2011-02-12T04:44:07.097,183,CC BY-SA 2.5,
11410,7088,0,By format i mean the expected flow of how you start and end your presentation.,2011-02-12T06:53:15.217,3150,CC BY-SA 2.5,
11411,7090,0,"@Whuber: Ok Ok, thanks!
@rolanda2: Yes, you are right. But if I generate survival times in years I keep all decimal points. Thx",2011-02-12T07:08:19.700,3019,CC BY-SA 2.5,
11413,7118,4,"+1, but why would you not use unstandardised regression coefficients for prediction?",2011-02-12T09:43:56.407,449,CC BY-SA 2.5,
11414,7054,0,"@DrWho, i think it would be more helpful if *you* said a few more words about the design! Are the numbers under 'cases' incident cases or prevalent cases? Are they always different people in each year? If someone became a case in 2006 and died in 2007, how would their data be entered in the above table?",2011-02-12T09:52:34.397,449,CC BY-SA 2.5,
11415,7118,1,"(+1) About assessing variable importance, I think the [relaimpo](http://cran.r-project.org/web/packages/relaimpo/index.html) R package does a good job (but see [Getting Started with a Modern Approach to Regression](http://j.mp/dE338Q)). There was also a nice paper by [David V. Budescu](http://j.mp/enzteH) on dominance analysis (freely available on request).",2011-02-12T10:09:30.400,930,CC BY-SA 2.5,
11416,7118,0,@onestep oops. typo. It's changed now.,2011-02-12T11:13:32.843,183,CC BY-SA 2.5,
11417,7054,0,"@onestop: Thank you for your help. Cases are new cases that  occurred every year (incidence). Deaths are within 6 months from onset of diasease. Every year new cases and deaths from that group are only counted. Population is the same but cases are new and different every year. If a person suffered from the disease in 2006 and dies in 2007, he is classified under deaths for the year 2006 if that death occurred within 6 months of onset of disease and attributable to that disease.Kindly advise me which is the best design- historical controls or time-series analysis or  Meta-analysis.",2011-02-12T11:54:17.423,2956,CC BY-SA 2.5,
11418,7126,0,"@sesqu ""your distribution is parametric"" can you explain why you say that?",2011-02-12T12:14:07.177,223,CC BY-SA 2.5,
11419,7123,0,Thank you for the additional information. Can you please educate me why we must use different terms if it is just differing numbers of n and k. Is there a practical significance?,2011-02-12T12:18:59.027,2956,CC BY-SA 2.5,
11420,7118,0,Hi Jeromy - great answer.  Please have a look at my question here:  http://stats.stackexchange.com/questions/6478/how-to-measure-rank-variable-importance-when-using-cart-specifically-using-r  and see if you might add something there.  Thanks!,2011-02-12T12:45:45.173,253,CC BY-SA 2.5,
11422,7126,0,"@robin You said you have a forecast, which implies a model for a random variable. That model comprises a family of distributions for the variable and a function to identify the distribution you predict from the data. The parameter is the processed data that identifies the predictive distribution - without one, there would be no variability in your predictions.",2011-02-12T13:00:23.560,2456,CC BY-SA 2.5,
11423,7126,0,could be non parametric density forecast ?,2011-02-12T13:02:06.287,223,CC BY-SA 2.5,
11424,7126,0,"Sure, if your model family is, say, an equal mix of multinomials $N(\vec {data}, \vec 1)$, you will have to store all data, since that is your parameter. In this case, you will have to compress the data according to some prior. However, unless your model assumes subsequent time points are independent (and it doesn't, or you wouldn't have specified time series), you can still take advantage of the dependence to refine the prior for $X_t|data_t,X_{t-1}$.",2011-02-12T13:35:22.567,2456,CC BY-SA 2.5,
11425,7134,3,"the comment on a reviewer is extremely sad, one would hope that a person in that position would at least not openly display their ignorance, and by doing so, further support the misinterpretation of the statistical method.",2011-02-12T17:29:04.407,656,CC BY-SA 2.5,
11426,7134,8,"Correct me if I'm wrong, but the randomness of sampling simply affects the degree to which you can generalize findings. In contrast, random assignment is the more critical feature for causal inference.",2011-02-12T17:29:25.460,364,CC BY-SA 2.5,
11428,7054,0,@DrWho: please look here: http://goo.gl/HPAy4,2011-02-12T17:48:05.170,609,CC BY-SA 2.5,
11430,7133,0,"ok, thanks a lot. Is there anyone who can suggest a different analysis?",2011-02-12T18:50:57.583,4701,CC BY-SA 2.5,
11432,7134,3,"Mike, I agree with you.  Do you make this point to extend the discussion or to indicate disagreement with something I've said?",2011-02-12T23:08:05.613,2669,CC BY-SA 2.5,
11433,6953,0,"Actually, I'm not sure I quite follow how these functions work. The examples give results for a normal distribution, but where do I plug in my probability density function?",2011-02-12T23:19:44.133,52,CC BY-SA 2.5,
11434,7136,1,"Wolfgang - interesting and helpful points.  I should have made clear, though, that much of my work is on surveys.",2011-02-12T23:28:22.950,2669,CC BY-SA 2.5,
11435,6953,0,"(I've done tests, and my data doesn't seem to be remotely normal.)",2011-02-12T23:40:22.673,52,CC BY-SA 2.5,
11436,7133,0,Could you please state your research question(s) and how many ratings each subject provided.,2011-02-12T23:48:29.860,2669,CC BY-SA 2.5,
11437,7133,0,"Hi, thanks a lot for your reply. Actually there were 36 stimuli, and each stimulus was evaluated only once by each of the 12 participants. So each subject provided 36 ratings on the 9 point Likert scale, one for each stimulus. Any suggestion?",2011-02-13T01:59:36.657,4701,CC BY-SA 2.5,
11438,6953,0,"@Alan H., plug your density function in to `dF`. That is `dF` should return density function value at given argument.",2011-02-13T04:39:26.233,2116,CC BY-SA 2.5,
11439,7123,0,@drwho I've updated my answer with a few thoughts.,2011-02-13T05:08:50.003,183,CC BY-SA 2.5,
11440,7141,1,"This doesn't qualify as an answer, but I think something along the lines of a maximum entropy distribution for $\mu_{i}$, subject to your constraints $\left| \mu_i - \mu_{i+1}\right| \le \gamma$.  Set this distribution as the prior, and then set jeffreys prior for $\sigma$.  Then update using Bayes theorem to a posterior for $\mu_n$, and calculate the probability $Pr(\mu_{n}>0)$, and use it like a p-value.  Calculating the MaxEnt distribution may be difficult to do though.",2011-02-13T05:27:40.750,2392,CC BY-SA 2.5,
11441,7141,0,"@probabilityislogic: We can assume that $\sigma$ is known, which should simplify this computation.",2011-02-13T05:50:42.823,795,CC BY-SA 2.5,
11442,7141,2,"@shabbychef, if you set up a likelihood ratio test, then under both $H_0$ and $H_1$, you have a convex program to solve for $\hat{\mu}_i$. If you look at the KKT conditions, you might even be able to find an explicit solution. Maybe try ""walking backward"" from $i = n$, e.g., under $H_0$, it seems that if $|X_{n-1}| > \gamma$, then $\hat{\mu}_{n-1}$ is likely equal to $\mathrm{sgn}(X_{n-1}) \gamma$. You might also look at the *fused lasso*, which has a ""similar"" (but not identical) form as your problem. I'm not sure that much is known about distributions of likelihood ratios in that setting.",2011-02-13T05:59:11.170,2970,CC BY-SA 2.5,
11447,7146,0,"About the picture, post to some image hosting and put a link -- I'll convert it into a pasted-in picture.",2011-02-13T08:35:32.897,,CC BY-SA 2.5,user88
11448,7146,0,"+1, interesting question, you might find that soon you will have enough reputation :)",2011-02-13T08:46:04.783,2116,CC BY-SA 2.5,
11450,7141,0,"@cardinal, we have the same amount of $\mu_i$ as there are data points. Will this not pose a problem for estimation?",2011-02-13T08:54:41.863,2116,CC BY-SA 2.5,
11451,7146,2,"judging from the graph, the problem is not the lag. What you have plotted is roughly goodness of fit. So it seems that Weibull distribution is not apropriate for your data. I see that there is a bunch up near zero, do you have zero values in your data? In that case you will need to model zero values separately. So first suggestion would be to try Weibull for non zero values. Also why Weibull, is there particular reason, some reference from similar work perhaps?",2011-02-13T09:02:59.097,2116,CC BY-SA 2.5,
11452,7146,0,well yes mainly from literature and other works on wind for the purpose of wind farms,2011-02-13T09:08:17.023,3178,CC BY-SA 2.5,
11453,7110,0,Connected question: http://stats.stackexchange.com/questions/812/what-is-the-difference-between-functional-data-analysis-and-high-dimensional-data,2011-02-13T09:11:20.500,223,CC BY-SA 2.5,
11454,7146,0,i tried ploting it with out zero's but still there is a lag,2011-02-13T09:12:24.227,3178,CC BY-SA 2.5,
11456,7146,1,"note that 'lag' is a term used mainly in analysis of data in time, referring to one thing occurring after another. This isn't a lag - it's perhaps more accurately called a shift - or maybe an offset - but shift is probably more common for distributions, they shift and scale.",2011-02-13T10:28:50.960,1549,CC BY-SA 2.5,
11458,7148,0,do you tink you can help me with my second question: calculating wind energy based on the density function?,2011-02-13T11:05:45.210,3178,CC BY-SA 2.5,
11459,7128,1,"What is your research question? Also, what are you calling a stimulus - one type of audio, or the pair?",2011-02-13T11:45:36.057,183,CC BY-SA 2.5,
11460,7151,0,+1 MDS is the main kind of analysis that I've seen for this type of study.,2011-02-13T11:48:54.010,183,CC BY-SA 2.5,
11462,7148,2,"I'm not a physicist and I don't know the necessary equations, but I imagine it will involve a numerical integral over the density. R's `integrate()` function may be useful for that.",2011-02-13T12:48:42.123,449,CC BY-SA 2.5,
11463,7131,0,"Thank you for the detailed and very useful advice. If I use only Region 1 and Region 2, and apply Pearson's chi-squared test to the data for each year separately, would it be better to do a Meta-analysis also for these 2 regions? There is heterogeneity in the sample data and a Random model looked more appropriate and gave a significance at a p value of 0.000. Egger‚Äôs linear regression method intercept (B0) 10.34631, 95% confidence interval (1.05905, 19.63357), with t=3.54535, df=3. The 1-tailed p-value is 0.01911, and the 2-tailed p-value is 0.03822 (significant funnel asymmetry).",2011-02-13T13:11:28.253,2956,CC BY-SA 2.5,
11464,7148,0,i know the equation my problam is i want to compute the percent of time the wind is at each speed,2011-02-13T14:18:22.400,3178,CC BY-SA 2.5,
11465,7090,0,"@whuber, @rolando2 Make answers of that.",2011-02-13T14:36:10.930,,CC BY-SA 2.5,user88
11466,7152,2,"More info is needed here, by zero do you mean missing?  Why do you think you have to do something special because the value is zero?",2011-02-13T14:39:53.670,2310,CC BY-SA 2.5,
11467,7148,0,what i meAn is can you help me with the integration --> thank you,2011-02-13T14:45:02.663,3178,CC BY-SA 2.5,
11468,7141,1,"@mpiktas, not in general since the constraints can force the ""perfect fit"" to be infeasible. Of course, if the perfect fit is feasible for $H_0$ then the LRT will be 1. But, I think that's the desired answer in that case anyway!",2011-02-13T15:33:02.537,2970,CC BY-SA 2.5,
11469,7155,0,If you want to know for a specific distribution then ask about your example.  It will be different for different situations.,2011-02-13T15:55:47.737,601,CC BY-SA 2.5,
11470,7155,8,"Well, I would expect that you will have a `rigorous definition of an outlier` when you'll be able to define `unreasonable amounts of subjectivity` objective manner ;-), Thanks",2011-02-13T16:00:42.393,3170,CC BY-SA 2.5,
11471,7156,2,"@Alexander, are you interested in the *change* in the level of corruption or are you interested in detecting a change in the *rate* of increase or decrease of corruption? They're quite different questions.",2011-02-13T16:06:57.503,2970,CC BY-SA 2.5,
11472,7155,1,"But the definition can vary by underlying distribution and situation.  I could say ¬± 1.5 IQR, or 3 SD, or some such.  But I could take a totally different approach if I have two kinds of measures, say reaction time and accuracy. I can say RT's conditioned on a level of accuracy.  They can all be good and mathematically rigorous and have different applications and meanings.",2011-02-13T16:08:15.643,601,CC BY-SA 2.5,
11473,7128,0,"ok, thanks a lot, I will have a look to MDS and try to use the R functions.",2011-02-13T16:15:03.657,4701,CC BY-SA 2.5,
11474,7153,0,"@onestop, @Marco, CW?",2011-02-13T16:15:11.750,2970,CC BY-SA 2.5,
11475,7128,0,@Luca Please register your account -- this way you won't lose your account again. You can do this here: http://stats.stackexchange.com/users/login,2011-02-13T16:17:21.400,,CC BY-SA 2.5,user88
11480,7140,1,"Thanks, that's great. The paper also stated that the first set of post-scan questionnaires (5 mins after) for the 97 particpants who did return the second set of questionnaires were compared to those of the dropouts. No mention was made of any indication of representativeness. The paper then asks WHY they did this- I'm assuming that is to check that scores weren't significantly higher for the dropouts, as particularly high anxiety could be the reason for dropping out? Wow, talk about a trick question!",2011-02-13T19:01:53.850,,CC BY-SA 2.5,user3171
11481,7153,0,@cardinal: CW? What does that mean???,2011-02-13T19:01:55.557,3019,CC BY-SA 2.5,
11482,7152,1,I would close it as duplicate http://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros/1445#1445,2011-02-13T19:04:24.487,223,CC BY-SA 2.5,
11484,7153,0,this may be more appropriate if marked as Community Wiki.,2011-02-13T19:14:31.267,2970,CC BY-SA 2.5,
11485,7158,4,"Not all outliers are generated from an experiment, however. I worked with a large dataset that involved the collection of real-estate information in a region (sale price, number of bedrooms, square footage, etc), and every now and then, there would be data entry mistakes and I'd have a 400,000 bedroom house go for 4 dollars, or something nonsensical like that. I would think that part of the goal of determining an outlier is to see whether it's possible to be generated from the data, or if it was just an entry error.",2011-02-13T19:25:01.540,1118,CC BY-SA 2.5,
11486,7158,2,"@Christopher Aden: I'd consider that part of the experimental process. Basically, in order to be able to remove outliers, you have to understand how the data were generated, i.e. no removing outliers without a good reason. Otherwise you're just stylizing your data. I've edited my answer to reflect this a bit better.",2011-02-13T19:40:45.703,198,CC BY-SA 2.5,
11487,7152,2,That question asked about transformations rather than about inferential issues.,2011-02-13T19:41:15.313,2129,CC BY-SA 2.5,
11489,7141,1,"I fully agree with cardinal's answer, there is not a very big problem here and hence certainly no litterature (however I like the question :) +1 for that). 

However this is certainly a simple particular case of what is called [""Adaptive tests of linear hypotheses by model selection by "" Y. Baraud, S. Huet, and B. Laurent][1]


  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1046294463",2011-02-13T19:49:37.567,223,CC BY-SA 2.5,
11490,2806,0,Can you tell us why you want to do PCA ?,2011-02-13T19:54:13.940,223,CC BY-SA 2.5,
11492,7141,0,"@robin girard: The case where the $\mu_i$ are affine with slope $\gamma$ does seem to be amenable to the test of Baraud _et. al._ However, I am looking for a test which performs reasonably well under the more general case of the Lipschitz bound.",2011-02-13T20:12:56.580,795,CC BY-SA 2.5,
11495,7141,0,@shabbychef I was pretty sure you knew Baraud's paper (from past questions and comments...). I though it contained alternatives similar to yours I'll take a look once.,2011-02-13T20:29:53.813,223,CC BY-SA 2.5,
11496,7141,0,"@robin, @shabbychef, maybe I was misunderstood. I think this problem is interesting. I also find disappointing the dearth of good quality work on inference for the many variants of constrained least-squares estimation that have become so popular over the last fifteen years. If @shabbychef can crack that nut, even for this particular example, it might even be publishable.",2011-02-13T20:58:55.900,2970,CC BY-SA 2.5,
11497,7164,2,divide the domain of your non-bijective function into parts where the function is bijective and then apply change of variables. This might work.,2011-02-13T21:05:43.913,2116,CC BY-SA 2.5,
11498,7041,0,Sorry I couldn't get back to this sooner...scheduling issues. I edited my original post based on your answer (see above).  I think I now get it.  Just a few questions.  1) shouldn't your code for t be t<-floor(N*(p + z.alpha*s.p/sqrt(N)))?  2) Isn't size where you're back-checking the 99% value?   3) Isn't sureness where you're back-checking the 1% value?,2011-02-13T21:05:45.020,2775,CC BY-SA 2.5,
11499,7156,0,"@cardinal The rate, thanks for asking. I should have been more clear on that.",2011-02-13T21:08:14.703,3182,CC BY-SA 2.5,
11500,7156,0,"@Alexander, that seems unintuitive on the surface---at least to me. Is there reason to believe that levels of corruption generally trend rather than stay constant at some level? Just curious, as I have no domain knowledge there at all. :)",2011-02-13T21:15:23.493,2970,CC BY-SA 2.5,
11501,7156,0,"Corruption is an area where there is little data, since it's hard to measure. You may be right, in that there isn't any good reason to think it's trending. However, changes could be caused by things other change a new president, though that isn't the same as trending I guess. Also, I've edited the question a bit, since my first comment. I realized I'm not sure if it's best to look at the rate or the change.",2011-02-13T21:22:02.423,3182,CC BY-SA 2.5,
11502,7156,0,"@Alexander, interesting. I can imagine it's not an easy thing to measure. My guess is that the more corrupt, and less transparent, the government, the harder a precise measurement becomes. You should also be careful of trying to conclude causality as well, as, e.g., the current situation in Egypt could end up demonstrating.",2011-02-13T21:25:13.607,2970,CC BY-SA 2.5,
11503,7156,0,"Good point! Thanks for helping me to clarify the question. Do you have any thoughts on how to set up the test? Yes, there was an interesting take on that in a recent issue of The Economist, where they introduced 'The Shoe-Thrower's Index'. http://www.economist.com/node/18114401?story_id=18114401",2011-02-13T21:29:38.903,3182,CC BY-SA 2.5,
11504,7166,0,What exactly are you trying to model/predict here?,2011-02-13T21:52:47.287,1913,CC BY-SA 2.5,
11505,7146,0,"Regarding your question about energy, it's going to be difficult to answer. Do you know anything about your device? Do you know anything of the relationship between wind speed/direction and power? Do you know anything about the layout of the turbines at this particular site since that has a huge impact on the power as you no doubt are aware.",2011-02-13T21:58:04.423,2110,CC BY-SA 2.5,
11506,7168,0,"(+1) Would you mind expanding on your opinion on the RMS textbook (of course, it's purely off-topic)?",2011-02-13T22:26:11.500,930,CC BY-SA 2.5,
11507,7168,0,"@chl, I'll see if I can dig out the notes I jotted down on it and will post a couple (if I can find them). As I was reading the book, I recall coming across several remarks and recommendations that struck me as incorrect or very questionable. This colored my opinion of it. But, as I stated above, my overall impression is generally positive.",2011-02-13T22:36:28.010,2970,CC BY-SA 2.5,
11508,7141,0,"@cardinal, could you provide a link to some kind of work illustrating inference for constrained least-squares estimation. This seems really interesting.",2011-02-13T22:39:03.267,2116,CC BY-SA 2.5,
11509,7141,0,"@mpiktas, sorry, could you clarify? A link to some kind of work illustrating what?",2011-02-13T22:40:54.277,2970,CC BY-SA 2.5,
11510,7141,0,"@cardinal, you mention dearth of good quality work, so I wondered that there is plenty of low quality work :) Any links would be fine, I just want to get an idea.",2011-02-13T22:46:20.603,2116,CC BY-SA 2.5,
11511,7141,0,"@mpiktas, I didn't quite mean to suggest that. To me, there just doesn't seem to be much work at all, and certainly very little that is compelling. Knight and Fu (2000) and some of the follow-ons by others are the exception. But, very little (that I am aware of, or can remember being aware of) exists regarding constructing valid confidence intervals and performing hypothesis tests. Chatterjee & Lahiri have a paper on a modified bootstrap for the lasso (standard one is bad). Meinshausen also seems to be doing work in this area. But, the results seem sparse so far.",2011-02-13T23:01:26.323,2970,CC BY-SA 2.5,
11512,7141,0,"@cardinal, thanks, I understand that you did not mean to suggest that. It seems interesting that nobody does any work on this, there might some underlying reason for that.",2011-02-13T23:10:43.460,2116,CC BY-SA 2.5,
11513,7141,0,"@mpiktas, for one, I think it's generally difficult. And, it's much easier just to dream up some minor iteration on the lasso or $k$-means or PCA, run a few simulations, find one ""real"" example where your method works better than ""competing"" ones, and publish without saying (virtually) anything about uncertainty. It's become a veritable cottage industry. You wanted my unvarnished opinion, right? :)",2011-02-13T23:15:41.173,2970,CC BY-SA 2.5,
11514,7141,0,"@cardinal, yep, I treasure those :)",2011-02-13T23:18:24.323,2116,CC BY-SA 2.5,
11515,7174,0,"Thank you. I did sample on the outcome. I edited my question above to be more clear. The database included two different strata: enrolled companies and unenrolled companies. I chose 65 from each stratum. In this case, what would you suggest I use for the model? Thank you!",2011-02-14T01:27:26.560,834,CC BY-SA 2.5,
11516,7176,0,I assume that *combining errors in quadrature* means calculating the *square root of the sum of the squares*,2011-02-14T02:06:47.983,2958,CC BY-SA 2.5,
11517,7176,0,"re-reading this question the answer feels like ""ignore the tiny error in A"" so let's go with the more general question: what if the errors in A and/or B are on the order of 10-30%...",2011-02-14T02:07:32.070,629,CC BY-SA 2.5,
11518,7176,0,@Henry yes. ...,2011-02-14T02:09:58.010,629,CC BY-SA 2.5,
11520,7158,0,"This is perfectly reasonable, but assumes you already have a decent amount of prior knowledge about what the true distribution is.  I was thinking more in terms of scenarios where you don't and it could be very heavy tailed or bimodal.",2011-02-14T03:06:38.110,1347,CC BY-SA 2.5,
11521,7158,1,@dsimcha: I don't think you can identify outliers in that case (see also my edit).,2011-02-14T04:08:07.380,198,CC BY-SA 2.5,
11522,7082,0,thanks for the document. i will take a close look at it.,2011-02-14T05:12:08.403,3107,CC BY-SA 2.5,
11523,7166,0,Thank you. I've edited the question. Hope it is now more clear,2011-02-14T05:12:41.430,3184,CC BY-SA 2.5,
11524,7166,0,Sounds like a Poisson Process problem to me. Let me see what do others think about it.,2011-02-14T05:18:25.827,1307,CC BY-SA 2.5,
11525,7181,0,The response is very much negatively skewed. With the majoirty of projects around 2500 hour mark and a tail stretching out to a couple of 10000-14000 hrs projects. The continous (scale) predictors are a mix of distributions while some predictors are categorical (nominal). What types of distributions are required for LASSO (or what else do you need to know from me)?  -- btw thanks for the response!,2011-02-14T06:21:09.760,3189,CC BY-SA 2.5,
11526,7181,0,"@Shane, the LASSO is a general concept of penalizing with $||$ (modulus or absolute value) function. It is independent of any distribution. If you check the `glmnet` package (see function: `glmnet`), it gives you options to fit the `glm` (linear regression is a special case) with $L_1$ penalty for a variety of distributions. It is pretty fast and amazing at the same time.",2011-02-14T06:38:00.487,1307,CC BY-SA 2.5,
11527,7181,0,"Checking SPSS help it talks about a feature called ""Categorical Regression Regularization"" or CATREG.  It seems to address Lasso and Ridge methods.  For some reason it's not enabled in my version.  If anyone knows why I'd be appreciative.",2011-02-14T06:39:27.553,3189,CC BY-SA 2.5,
11528,7181,0,"@Shane If my memory doesn't fail me, I have seen @AndyW post *fancy* SPSS code. It (code) impresses me all the time!",2011-02-14T06:48:33.813,1307,CC BY-SA 2.5,
11530,7153,0,"@cardinal: Oh, OK! But I still cannot create a new tag...",2011-02-14T07:52:19.440,3019,CC BY-SA 2.5,
11533,7173,0,Could you clarify how you sample from the four industry strata? Did your sampling scheme specify that you picked a certain number from each stratum? Also do you have data at the employee level or only at the company level?,2011-02-14T07:57:47.893,449,CC BY-SA 2.5,
11534,7190,0,"Try increasing the  `workspace` argument of `fisher.test`. That may avoid the error message, if your computer has enough memory (and you have enough patience). See `?fisher.test`. See my previous answer http://stats.stackexchange.com/questions/4023/chi-square-test-for-equality-of-distributions-how-many-zeroes-does-it-tolerate/4029#4029",2011-02-14T08:27:53.113,449,CC BY-SA 2.5,
11535,7179,0,"In the jargon of web marketing, this probability is called the 'conversion rate'. Not my field, but the Wikipedia articles may give you some leads: http://en.wikipedia.org/wiki/Conversion_rate",2011-02-14T08:39:49.773,449,CC BY-SA 2.5,
11536,7146,0,"well untill 4 m/s there is no wind generation and the max is 15 m/s, wind stronger than 15 m/s will make the same amount of energy",2011-02-14T08:48:50.363,3178,CC BY-SA 2.5,
11537,7146,0,"i thought of integrating seperetly until 4 m/s, between 4 and 15m/s and over 15m/s, and comparing it to a situation of 100% of the time a turbine working in optimal speed",2011-02-14T08:50:44.123,3178,CC BY-SA 2.5,
11538,7170,0,"Thank you for the response, however, I see no value in it: 1. As far as I see, Quantization is not focused (primarily or solely) on equal-freq-histograms 2. Determining the number of bins per hand or per automatic optimization (via sum-of-squared-errors) is an approach which can be applied anywhere.",2011-02-14T09:10:25.587,264,CC BY-SA 2.5,
11539,7086,0,"@cardinal: just added some details, regarding the nature of the data, and analysis, followed with my motivations for the analysis I proposed.",2011-02-14T09:23:18.847,3014,CC BY-SA 2.5,
11540,7169,2,(+1) for mutual information. Additional remark: a) I suggest Information Gain as special case of mutual information. b) Automatic feature selection will not only remove the redundant but also all features which have a negative impact on class discrimination.,2011-02-14T10:14:15.683,264,CC BY-SA 2.5,
11541,7071,4,"@mpiktas, i usually explain the white noise to students through the spectral density concept, at least it gives light to (through the analogy with white color) why the noise is ""white"", and why the $AR(1)$ process could be could called ""red"" and there is no ""black noise"" :)",2011-02-14T10:34:57.387,2645,CC BY-SA 2.5,
11542,7169,0,"Thanks! This sounds very promising, I will look into it.",2011-02-14T10:54:12.057,977,CC BY-SA 2.5,
11543,7170,0,"""No value"" was a little bit too harsh, I meant: ""no value"" for the specific nature of my question which is focused on equal-freq-histograms (and rules of the thumb for it).",2011-02-14T11:19:37.507,264,CC BY-SA 2.5,
11544,7141,0,"I misread the question, so I retract my answer, since I do not see for now how can I rewrite it to be useful.",2011-02-14T12:02:52.280,2116,CC BY-SA 2.5,
11545,6746,0,"@cardinal, the problem should have to have known solution, which is probably found in W.Palma Long Memory time series: Theory and Methods. The point is that autocorrelation function may be used to obtain by Yule Walker system of equations the parameters of $AR(\infty)$ representation of the process, the point is when such representation exists (invertability) and what truncation is acceptable by the means of say MSE. For $R$ code in my PhD I used `fracdiff` package.",2011-02-14T12:21:05.287,2645,CC BY-SA 2.5,
11546,7094,0,"@whuber Quite right! I stick by the model I set out in the first para, but my equation was screwed up in several ways... Goes to show I haven't actually used log-linear modelling of contingency tables since my MSc, and I haven't got the notes or books to hand. I believe I've fixed it now though. Let me know if you agree! Apols for the delay. Some days my brain just doesn't do algebra.",2011-02-14T12:51:09.890,449,CC BY-SA 2.5,
11547,7195,2,"Birth-death process attempts to analyze the throughput of the process (here, the parking garage clerk, or similar). I have a feeling that DavidD is looking for a method to predict the amount of cars that will try to check in the garage (not the queue, but the demand).",2011-02-14T12:52:52.837,1496,CC BY-SA 2.5,
11548,7166,0,See my comment to Dmitrij Celov's answer and clarify your question.,2011-02-14T12:53:31.913,1496,CC BY-SA 2.5,
11549,7181,0,"@Shane, it appears the CATREG command has been around for quite a few versions of SPSS, but you probably need some advanced regression module/licenses to use it. In the current edition you need the ""premium"" stat suite to get this functionality. I would just suggest checking out the R packages suncoolsu mentions (its free!).",2011-02-14T13:36:43.700,1036,CC BY-SA 2.5,
11550,598,3,"As a note GeoDa and SatScan aren't open source, they are freeware (not that it makes much difference to me though!)",2011-02-14T14:06:01.543,1036,CC BY-SA 2.5,
11552,7173,0,I first divided each stratum (insured and uninsured) by industry (4 groups). Within these industry groups I further divided the sample into 4 groups that correspond to the size of the company. This was to ensure that I collected similar numbers within each industry and that i didn't up with only small firms. I then numbered all observations and used quota sampling with a random number generator to select firms from each of the 32 groups. I used sampling weights in the analysis to ensure that each observation was weighted by the inverse of the probability of being selected.,2011-02-14T15:58:10.950,834,CC BY-SA 2.5,
11553,7173,0,"No, I don't look at any employee level data within the sample. I realize that my sampling is probably not the best but I was mainly interested in getting some descriptive results. However, I'd also like to be able to use the logistic regression analysis to understand the factors affecting whether or not a company enrolls in health insurance. Thank you!",2011-02-14T15:58:47.360,834,CC BY-SA 2.5,
11555,6746,0,"@Dmitrij, @Chris, the OP specifically states he is interested in binary-valued processes (I've got a pretty good guess at what he's likely interested in), for which an AR formulation via Yule-Walker would strike me as ad-hoc at the least. Perhaps you could throw a logistic around it to estimate a conditional probability, but it's still important to recognize the assumptions one is making in that case. Also, for long-memory processes, the choice of truncation can be important and induce nontrivial artifacts.",2011-02-14T16:16:27.097,2970,CC BY-SA 2.5,
11557,7182,0,Thank you for your very comprehensive answer - it's very helpful.,2011-02-14T16:39:12.473,2635,CC BY-SA 2.5,
11558,7188,0,"I assume you mean http://wordnet.princeton.edu/?  I was not aware or this, but it might be useful.",2011-02-14T16:40:09.727,1026,CC BY-SA 2.5,
11560,7193,0,"Thanks! Great info, and definitely puts me on the right path. With 2 more rep points, I'd happily give an arrow-up.",2011-02-14T17:44:09.860,3187,CC BY-SA 2.5,
11561,7193,0,"Also, I did a search for book recommendations, but I'm not sure what to look for. Books on ""binary classification""?",2011-02-14T17:58:04.900,3187,CC BY-SA 2.5,
11562,7210,13,that was a fantastic explanation!,2011-02-14T18:18:05.460,2798,CC BY-SA 2.5,
11564,7173,0,Clarification: I wrote that I used quota sampling but it was in fact stratified sampling.,2011-02-14T19:32:49.493,834,CC BY-SA 2.5,
11565,7194,0,"I like the book too, but the OP didn't really ask about Econometrics.",2011-02-14T20:56:33.440,334,CC BY-SA 2.5,
11566,7194,0,"Just going by the Contents, he will find the linear regression model and violations of its assumptions, bayesian approach, Logit, Probit, Tobit models, time series analysis, forecasting and robust estimation. So, even if the title is econometrics, I suppose it covers a large amount of statistical tools that are usefull outside of econometrics.",2011-02-14T21:23:32.950,1766,CC BY-SA 2.5,
11568,7219,13,Could this be (overly) simplified as something like the difference between generative and discriminative models?,2011-02-14T22:19:53.037,1764,CC BY-SA 2.5,
11569,7219,0,"@Wayne:  Yeah, I think this is actually a pretty fair way of putting it.",2011-02-14T22:33:48.593,1347,CC BY-SA 2.5,
11570,7219,5,"""One should solve the [classification] problem directly and never solve a more general problem as an intermediate step..."" - Vapnik",2011-02-14T22:42:03.673,1764,CC BY-SA 2.5,
11571,7144,1,"This sounds like the ""kernel trick"" applied to PCA. http://en.wikipedia.org/wiki/Kernel_PCA
It's a very good way of handling certain large matrices.",2011-02-14T23:11:24.123,2833,CC BY-SA 2.5,
11572,7194,0,"On further reflection, I would not quote the book as the definitve, most thorough source on concepts, as the OP requires.",2011-02-14T23:18:20.760,1766,CC BY-SA 2.5,
11573,7219,0,I disagree that no inference can be done with ML; one can squeeze quite a lot from importance scores some methods produce and/or some feature selection schemes.,2011-02-14T23:20:01.057,,CC BY-SA 2.5,user88
11574,7207,12,This paper just must appear in a context: http://www.biostat.wisc.edu/~page/rocpr.pdf,2011-02-14T23:42:18.417,,CC BY-SA 2.5,user88
11576,7224,19,"What's your development platform? This can easily be done in .NET because it superior over all other programming environments. Simply call the Page.EradicateBieber() function. Microsoft foresaw this need and graciously provided it for us out-of-the-box in .NET 4.5. (Those of you on older versions will have to wait.) (That is, of course, all tongue-in-cheek.)",2011-02-14T22:46:18.400,,CC BY-SA 2.5,David Stratton
11577,7224,35,I think I can safely assert that SO doesn't need a `[justin-bieber]` tag.,2011-02-14T22:47:31.833,2345,CC BY-SA 2.5,
11578,7224,2,I can safely assert that people spend more upvotes on comments and *this question* than the close option (the one that deserves the votes).,2011-02-14T22:50:11.093,,CC BY-SA 2.5,Linus Kleen
11579,7224,20,A Justin Bieber audio filter would be good too,2011-02-14T22:58:09.340,,CC BY-SA 2.5,Paul R
11580,7224,1,:( This question wasn't insincere or a joke.,2011-02-14T23:42:14.173,,CC BY-SA 2.5, û…îƒ±u
11581,7224,0,"@ û…îƒ±u: Well, the way it was tagged obviously made it look like one.",2011-02-14T23:44:58.853,,CC BY-SA 2.5,0xA3
11583,7208,0,"For inter-rater reliability with binary outcomes, I think usually one uses the tetrachoric coefficient.",2011-02-15T00:48:50.230,795,CC BY-SA 2.5,
11584,7224,6,http://stackoverflow.com/questions/953714/face-recognition-library,2011-02-15T00:54:36.333,68,CC BY-SA 2.5,
11585,7219,3,"@mbq:  I didn't mean to imply that no inference can be done, just that it's not the main goal and that usually p >> n in ML, making it a lot harder.",2011-02-15T01:00:27.687,1347,CC BY-SA 2.5,
11586,7224,1,You could try the tineye API: http://www.tineye.com/commercial_api,2011-02-15T01:06:49.380,364,CC BY-SA 2.5,
11588,7227,3,"Yeah, maybe set a threshold of 2-4 possible duplications (to handle the new-baby case) before you reject a photo. Depends on what you're going to do with the photos, I guess.",2011-02-15T01:49:38.233,3206,CC BY-SA 2.5,
11589,7199,0,"Thanks @Wolfgang; your explanation helps a lot. A follow up question that I have then is this. I am indeed analyzing a repeated-measures model with a single treatment factor. Each subject is randomly assigned to either treatment A or B. Then they are measured at 0 mins, 15 mins, 30 mins, 60 mins, 120 mins and 180 mins. From my understanding, time should be a random factor because it is just samples from time 0 to 180 mins. So, should I do: lme(UOP.kg~time*treat, random=~time|id, raw3.42)?",2011-02-15T01:59:12.990,1663,CC BY-SA 2.5,
11590,7181,0,Ok will do.  What other techniques are 'good'?  Are there any good references I can use for defence of techniques on use of regression on small samples?,2011-02-15T02:52:08.407,3189,CC BY-SA 2.5,
11592,7225,1,Of course one way I can quickly get an estimate of R-squared is by fitting a linear model predicting the fitted values from the original data and taking the R-squared from that. But this seems like it would be a massively-overfit and biased estimate of R-squared.,2011-02-15T03:13:15.947,36,CC BY-SA 2.5,
11593,7181,0,"@Shane, you can use `Ridge Regression` in your case when $n$ is close to $p$, but as you mention, your aim is to select predictors, $LASSO$ is the recommended way to go!. You can choose not to use _penalized regression_ ie simple linear regression, but that would be dangerous, as your small sample size will result in elevated standard error for your parameters.",2011-02-15T03:13:46.790,1307,CC BY-SA 2.5,
11594,7181,0,Just a note on CATREG - it seems like my version *should* have it as part of the PASW Regression module.  It's PASW Version 18 - student edition.  Do people know if it's an installer thing?,2011-02-15T03:27:39.813,3189,CC BY-SA 2.5,
11595,7227,3,"Simple, elegant solution. +1.",2011-02-15T03:40:48.733,2227,CC BY-SA 2.5,
11596,7141,0,"@shabbychef, off the top of my head, I don't think the power will necessarily be small. Intuitively, it seems it should be roughly a monotonic function of $|\mu_n|$. I say this because under both the null and alternative, the ""left-hand tail"" of the sequence is allowed to float, while under the null, you've pinned the ""right-hand tail"" to zero.",2011-02-15T03:43:56.547,2970,CC BY-SA 2.5,
11597,7235,0,"Note: For the bootstrap to be an effective solution, you'd have to bootstrap the entire procedure, starting before any screening occurs, screen the bootstrapped sample, then calculate coefficients. But now you have different sets in predictors in each regression and it's no longer clear how to calculate the distribution for any one of them. Bootstrapping confidence intervals for predicted values of the outcome may be effective, however.",2011-02-15T03:44:09.170,401,CC BY-SA 2.5,
11598,7227,14,People could use different pictures of the same person.,2011-02-15T03:47:37.447,1142,CC BY-SA 2.5,
11600,7233,1,"it would help us if you can tell us which software you used and some sample commands. At the moment, I think, we can only guess because outputs are _sometimes_ software dependent.",2011-02-15T05:14:40.353,1307,CC BY-SA 2.5,
11601,7223,1,"People sometimes do this - unknowingly (ie misuse Statistics, because they get the desired result) and knowingly (they did bootstrap and it did not affect the result substantially). Your point is valid, and Professor Harrell points this out in the Preface of his book that bootstrap is beneficial.",2011-02-15T05:19:44.670,1307,CC BY-SA 2.5,
11602,7193,0,"The task classification (with special case binary classification) is part of every basic book on machine learning and data mining (my favorite: ""Data Mining: Practical Machine Learning Tools and Techniques"" by Witten & Frank)",2011-02-15T07:10:01.843,264,CC BY-SA 2.5,
11603,7227,0,(+1) at Rebecca and (-1) @ PPPPPP: This just shifts the problem.,2011-02-15T07:15:20.810,264,CC BY-SA 2.5,
11604,7232,2,"Unfortunately, the feature generation step is both the hardest and most important one :(.",2011-02-15T07:19:01.197,264,CC BY-SA 2.5,
11606,6746,1,"@cardinal, @Chris. oh, I as usually missed the part of the task ^__^ In the case of binary-valued process it seems to be a very well known (studied) problem of traffic measurement that comes from communication networks or so called ON/OFF process that exhibits long range dependence (long memory) property. As for the particular example, I'm a bit confused, since in ""one way to predict"" Chris actually takes the previous value, not using the ACF only (or I'm even more confused by the term ""classification rate"").",2011-02-15T08:44:28.367,2645,CC BY-SA 2.5,
11607,6835,0,"In addition to Rob's comment, try to add a bit more context and what do you put into the term ""best"". Best for what and for whom? Since trend is a part that is not observable, and obtained by the decomposition, using either low/high pass filters or model based approaches.",2011-02-15T09:01:27.993,2645,CC BY-SA 2.5,
11608,7195,0,"@bgbg, the problem with parking is that cars in the garage are not staying for the whole time, when you drive into the city, you see how many free places are in a particular garage, so you decide either to occupy it or search for free of charge place somewhere else (I assume that it is a kind of the parking lot, but just underground, here I agree that more details would be useful).Since cars in the garage are not staying fro the whole day(s) so you DO need a Birth-death process to describe if a particular place is free or occupied,this time of a day and of a week. Waiting for David's comments.",2011-02-15T09:11:19.673,2645,CC BY-SA 2.5,
11609,7240,4,"Good question, but I don't have (a reference for) a good answer. There's more than one level of variation in mixed models, so there's more than one component of variance to explain, plus it's debateable whether random effects can really be said to 'explain' variance. I think the whole concept of 'proportion of variance explained' is less useful in mixed models.",2011-02-15T09:18:48.010,449,CC BY-SA 2.5,
11610,7222,0,"I will use logistic-regression-related methods like Generalized additive models and Kernel Logistic Regression, so these are due to the large sample size nearly not affected by unequal classes.",2011-02-15T09:39:33.807,2549,CC BY-SA 2.5,
11611,7199,0,"Yes, but I would think of it this way: You are essentially allowing the intercept and slope of the regression line (of UOP.kg on time) to differ (randomly) between subjects within the same treatment group. This is what random=~time|id will do. What the model will then tell you is the estimated amount of variability in the intercepts and the slopes. Moreover, the time:treat interaction term indicates whether the *average* slope is different for A and B.",2011-02-15T10:13:11.910,1934,CC BY-SA 2.5,
11612,7208,0,Could you elaborate on that? I am definitely no expert when it comes to statistics and I can't seem to find a straight forward approach to calculating a tetrachoric coefficient.,2011-02-15T10:23:43.457,1205,CC BY-SA 2.5,
11613,7136,7,"If the primary goal is to make some kind of inference to the population and the sampling mechanism is of such a nature that the representativeness of the sample is questionable, then indeed, any inference will also be rather questionable. Essentially, you can only make an inference to that part of the population that the sampling mechanism provides a representation of. In principle, the inferences you make will be appropriate for that part of the population. Whether that part of the population is of any interest to you (or the readers) is another issue.",2011-02-15T10:24:45.160,1934,CC BY-SA 2.5,
11614,7218,4,"onestop provided an answer that will hopefully give you enough to go on. If you *really* want to know about the distribution of the sample correlation coefficient itself, then the definite reference is: Hotelling, H. (1953). New light on the correlation coefficient and its transforms. Journal of the Royal Statistical Society, Series B, 15, 193-232. Note that this isn't light reading.",2011-02-15T10:30:54.950,1934,CC BY-SA 2.5,
11616,7232,0,"@steffen There is some suggestion that the OP is messing with the faces, so have some descriptor generator.",2011-02-15T12:15:24.160,,CC BY-SA 2.5,user88
11617,7151,0,"Hi Jeromy,
I am convinced that likely the MDS is not the right analysis, since my table
does not contain the distances. Indeed if you have a look to the diagonal of the
table you can see that the values are not 0 (which should express zero distance
between the same stimuli).
The values I have in my table are not distances but average scores from a 9 points
Likert scale, assessing the degree of coherence between two modalities (audio and
haptic) simulating different pairs of materials.

So my question now is: which analysis should I perform in your opinion?


Thanks in advance",2011-02-15T12:40:40.313,4701,CC BY-SA 2.5,
11619,7232,0,"@mpq: I did not doubt that, however, if the OP does not have one feature per pixel, then he has to find a meaningful aggregation level. I did not downvote, I just wanted to point to the complexity which lies behind this answer (which is, of course, correct).",2011-02-15T13:00:05.663,264,CC BY-SA 2.5,
11620,7225,0,"I add this as a comment since I am asking a ""similar"" question in a nearby post (so I dont know if I qualify as giving an *answer*), but for your question specifically it seems like you can calculate R-squared without requiring any distributional assumptions (they are needed for hypothesis tests in the ordinary way though). Can't you use a hold out set to calculate r-squared or use a k-fold validation if you don't have enough data (at each fold run your full penalized process and average the r-squares from each of the folds not used in the fitting)?",2011-02-15T13:00:13.933,2040,CC BY-SA 2.5,
11621,7232,1,"Right, the feature generation step is the hard part.  I was assuming OP could do this since he has some mechanism for processing the images already.  Even if he does though they might only be useful features for detecting face/not face instead of Bieber/not Bieber...it really depends on the features.",2011-02-15T13:17:17.507,2485,CC BY-SA 2.5,
11622,5369,0,I suspect SPSS does exactly this (which is why I think it's a common procedure).,2011-02-15T13:24:08.073,144,CC BY-SA 2.5,
11623,5423,3,"Not true. Factor Analysis differs from PCA in the variance-covariance matrix used. That only the rotation would make the difference, is a common misconception.",2011-02-15T13:29:58.540,1124,CC BY-SA 2.5,
11624,7225,1,"@B_Miner, $k$-fold cross validation tends to give quite biased estimates of $R^2$, as it generally isn't estimating the true quantity of interest. Many (most?) similar procedures have the same problem.",2011-02-15T14:20:48.723,2970,CC BY-SA 2.5,
11625,7225,1,"@Stephen, is $R^2$ *really* the quantity you are interested in? Because of the bias induced by penalization, looking at only the variance explained is probably not desirable unless you already have a very good estimate of the bias. The whole idea of using $R^2$ as a basis for inference is predicated on the unbiasedness of the estimates. Even major textbooks on regression seem to ""forget"" this. (See, for example, Seber and Lee's somewhat faulty treatment of $R^2$ in the multiple regression case.)",2011-02-15T14:26:48.323,2970,CC BY-SA 2.5,
11629,7233,0,@suncoolsu: Thank you for your help. I have used Comprehensive meta analysis v2. Run analysis was the command used,2011-02-15T15:39:22.227,2956,CC BY-SA 2.5,
11630,7029,1,It's been published: http://dx.doi.org/10.1093/biomet/asq017.,2011-02-15T15:39:44.747,1979,CC BY-SA 2.5,
11631,7029,0,"Which special case are you approximating? I've read it, but can't really relate your expression to the expressions given in the paper...?",2011-02-15T15:44:16.367,1979,CC BY-SA 2.5,
11632,7243,0,"Thank you for your help. I have included the other results herewith. Q-value-28.056, df (Q) 4.000 ,  P-value 0.000 , I-squared 85.743",2011-02-15T15:47:30.877,2956,CC BY-SA 2.5,
11633,7250,0,"apologies if I've asked too many questions about clustering over the last few days. I'm trying to become more familiar with this field quickly (also I did post this question on SO, http://stackoverflow.com/questions/4997870/using-the-cluster-package-in-r-for-kmeans-clustering but it was suggested to move it here)",2011-02-15T16:03:18.097,2635,CC BY-SA 2.5,
11634,7245,5,"+1  Your discussion in point 1 leads to the crux of the question: how is one to test a *post hoc* hypothesis that was developed (essentially on the spur of the moment, it seems) from an examination of the data?  It's unclear what the hypothesis is even supposed to be.  This is the old data-snooping problem looming large.  A good case can be made that the proper answer is that trying to apply the mechanics of hypothesis testing to such one-off observations just makes nonsense of statistical significance.",2011-02-15T16:14:42.503,919,CC BY-SA 2.5,
11636,7250,0,"That's ok, this is not TCS.SE (-;",2011-02-15T16:47:36.483,,CC BY-SA 2.5,user88
11640,6702,0,"@chl that's what I need to do, yes, but I was hoping I wouldn't have to re-invent the wheel.",2011-02-15T17:50:21.400,2817,CC BY-SA 2.5,
11641,7255,0,"it is good that you mention it! I can't figure out the intuition, do you?",2011-02-15T17:51:21.010,223,CC BY-SA 2.5,
11642,6999,0,"Thanks for the answer. I had thought that the simple methods had inherent weaknesses, but was also concerned that stochastic methods may suffer from lack of consistency between analyses. Need to look into it further.",2011-02-15T17:54:12.467,229,CC BY-SA 2.5,
11643,7255,0,"@robin This is what makes this conjecture so special: a completely elementary statement, some obvious approaches which fail miserably (characteristic functions), and one is left with nothing to grasp... By the way, should one bet on the conjecture being true or false? Even that is not obvious (to me).",2011-02-15T18:13:33.830,2592,CC BY-SA 2.5,
11645,7243,0,"@DrWho I would only report the results based on the random effects model (I mean, I would always report the REM results, regardless of the actual test statistics...).",2011-02-15T18:56:50.270,307,CC BY-SA 2.5,
11647,7259,0,"what you're asking for shouldn't be that difficult, but it's difficult to wrap my head around what you're after without some sample data. Can you update your question with some sample data? Maybe try `dput()` on the first 100 or so rows and paste it into your question?",2011-02-15T19:26:44.970,696,CC BY-SA 2.5,
11648,7239,0,could you at a minimum provide the input data that you are using?,2011-02-15T20:24:53.497,1381,CC BY-SA 2.5,
11649,7268,0,I'm guessing you get those errors from `xts()` because the `dates` column is a factor.,2011-02-15T21:26:13.440,1657,CC BY-SA 2.5,
11650,7268,0,I'm really new to R ... I created the dates column from the strptime function. The original data is from read.csv.,2011-02-15T21:30:42.920,2770,CC BY-SA 2.5,
11651,7262,0,"Thank you for your reply! I am glad to hear your answer, as that was my original assumption. I was told otherwise by a professor at my institution, so I really had to go digging. I am using SPSS and I now see that the parameter estimates are indeed identical (e.g., B values). I see now that what had confused me originally were the different p values. The ANCOVA statistic is based on a standard F test, while the GZLM is based on the Wald Chi-Square, correct? I have read that the Wald Chi-Square is used when you use parameters of the sample (such as in the GZLM).",2011-02-15T21:31:22.117,3262,CC BY-SA 2.5,
11654,7269,5,"+1. On Google: ""the univariate distribution of"" has 25,600 hits.  ""the joint distribution of"": 1,080,000. ""the multivariate distribution of"": 85,100.  ""the bivariate distribution of"": 89,800.  This sounds like the ""joint"" version is popular with ""univariate,"" ""bivariate,"" and ""multivariate"" occasionally used, each with similar frequencies.  These likely are used in circumstances requiring clarification.  (I have often seen ""the univariate distribution of"" used in this sense.)",2011-02-15T23:30:04.340,919,CC BY-SA 2.5,
11655,7225,0,@cardinal Can you please explain why k-fold will not give reliable results? Cross validation and bootstrapping are mainstays in statistical learning - no?? Or are you stating that for the case of biased estimates resulting from penalized fitting?,2011-02-16T01:46:29.510,2040,CC BY-SA 2.5,
11656,7235,0,"@charlie: [Do I read you correctly that you you are only speaking to I.(model selection) not II. (penalized)] Are you saying that for prediction intervals, it is valid to use model selection and then bootstrap the predictions from that model, but for anything else you need to bootstrap the entire process?",2011-02-16T01:53:28.600,2040,CC BY-SA 2.5,
11657,7235,0,"@charlie Regarding the suggested solution of screening on a sample. Would that be along the lines of partitioning the data, (ab)using one set (model selection etc) and then applying that model to the remaining data - and on that data with the model that was fit using traditional theory for hypothesis tests, CIs etc?",2011-02-16T02:03:32.337,2040,CC BY-SA 2.5,
11658,7274,1,"I dimly remember the world map of the facebook network, which was done in R. I think the author described his process in some detail in his blog. I suppose using that approach would generate a map that is informative even with 4 million nodes.",2011-02-16T02:53:38.077,1766,CC BY-SA 2.5,
11659,7270,0,"for some of these measures it whether R can handle it or note will depend on how many separate people (nodes) the network has. R may not necessarily be the best tool for the computational aspects. There's a guy with the last name of Leskovec who used to be at Carnegie Mellon---I think as a student---that did lots of stuff with descriptive statistics on large graphs. There are lots of utilities out there to ""visualize"" graphs, but mostly I've found they're pretty hard to interpret or make much sense out of. Graphing just the degree distributions might be a first start.",2011-02-16T03:03:54.343,2970,CC BY-SA 2.5,
11660,7277,3,"Typically one uses a fixed scale only with models like logistic regression or Poisson regression, where the response is a count or indicator/frequency variable. In this case there is no analogue to the scale parameter in normal regression.",2011-02-16T04:07:18.290,1569,CC BY-SA 2.5,
11661,7278,3,What's wrong with the formula given by http://en.wikipedia.org/wiki/Chi_squared ? _i.e._ $(1-2t)^{-m/2}$,2011-02-16T04:15:03.350,795,CC BY-SA 2.5,
11665,7238,0,"`xcorr2` is applied to two matrices (if only one given, it is used in both contexts). How are you using it? That is, how do the x,y,z direction readings become matrices?",2011-02-16T05:46:32.767,795,CC BY-SA 2.5,
11667,1903,0,"Hmm - my question is not very well defined. The only thing I can do is pick *some* model for q() that permits setting parameters, and maximise the goodness of fit by fiddling with those parameters. That is - no matter what I do I will have to make some assumptions about what q() basically looks like.",2011-02-16T06:35:44.297,997,CC BY-SA 2.5,
11668,7256,0,"Applying PCA to reduce dimension in a binary classification problem is nonsence. The directions of highest variablility are not necessarily the direction of highest separation, could be the opposite or anything. Do you need examples to be convinced?",2011-02-16T07:14:39.813,223,CC BY-SA 2.5,
11669,7264,0,@David pvr and pri are user supplied data: they are the predicted regressors...,2011-02-16T08:10:29.770,1443,CC BY-SA 2.5,
11671,7256,0,"@robin I'm not convinced it's nonsense; it depends on your aims. It's not going to identify the linear combinations of covariates most strongly associated with the binary outcome, but that may not be the aim. E.g. PCA is popular in analysing diet-disease studies as a step in reducing the huge amount of info collected from dietary diaries. One advantage is that as the principal components are chosen *before* their association with the binary outcome is examined, the analysis is free of charges of 'data snooping'.",2011-02-16T09:06:59.263,449,CC BY-SA 2.5,
11673,7256,0,"@onestop I said it is nonsence in a binary classification problem. I assume that a binary classification problem is a problem where you need to predict a class that can be zero or one (binary) according to explanatory variables. If the aim is not to identify the relation (not necessarily linear) between covariates and binary outcome, I am wrong about my definition of binary classification. It is not because an algorithm is ""popular"" that it is a good idea to use it without thinking if it suites to your problem.",2011-02-16T09:19:35.697,223,CC BY-SA 2.5,
11674,7285,1,"A very interesting problem. Do you have a sample of ""typical population of email addresses"" at hand ? Additionally it is not guaranteed, that the email-addresses of the visitors do have the same another/different structure, but since you are only looking for an approximation.... Second question: Are you able to set the seed of the RNG ?",2011-02-16T09:26:32.693,264,CC BY-SA 2.5,
11675,7218,0,I don't think your graphs are right. I've just drawn some graphs of the distribution derived from the Fisher formula which show it is correctly centered. In fact it's pretty obvious from the formula that it must be asympototically unbiased for $N \rightarrow \infty$. Could you post the mathematical core of your code?,2011-02-16T09:51:14.483,449,CC BY-SA 2.5,
11676,7218,0,@onestop Sure.  Added Mathematica code.,2011-02-16T10:04:23.663,2665,CC BY-SA 2.5,
11677,7256,0,"@robin Fair enough, I assume you wouldn't class the type of diet-disease study i'm thinking of as a binary classification problem then, and you're probably correct.",2011-02-16T10:04:32.960,449,CC BY-SA 2.5,
11678,7218,0,That's not how pdfs transform -- it's a bit more complicated. See http://en.wikipedia.org/wiki/Probability_density_function#Dependent_variables_and_change_of_variables,2011-02-16T10:11:20.487,449,CC BY-SA 2.5,
11679,7285,6,"Sounds like you want a 'hash function': http://en.wikipedia.org/wiki/Hash_function 
This is in the realms of computer science rather than statistics though, so I'm not sure it belongs on CrossValidated.",2011-02-16T10:20:45.863,449,CC BY-SA 2.5,
11682,7218,0,"@onestop Of course.  Thank you.  I realized there was a problem after I posted the code, but it would have taken me a while to figure out how to fix it.",2011-02-16T10:29:34.203,2665,CC BY-SA 2.5,
11683,7285,1,hmpf ;) ... I intended to write the same. @Jeromy: Especially this part of the site (http://en.wikipedia.org/wiki/Hash_function#Hashing_data_with_other_distributions) could be interesting for you.,2011-02-16T10:51:07.410,264,CC BY-SA 2.5,
11684,7286,0,How come you don't have +100 bonus from your SO record?,2011-02-16T10:57:45.363,,CC BY-SA 2.5,user88
11685,7286,0,"@mbq, probably because I created this account long ago when I was 0 on OS as well...pity, 100 on CW looks like a big deal, since you must be a real chap to answer stuff in here :)",2011-02-16T11:11:17.223,1542,CC BY-SA 2.5,
11686,7268,1,Let's see `str()` of the data.frame.,2011-02-16T11:30:26.227,144,CC BY-SA 2.5,
11687,7284,0,Thanks for the answer it confirmed my supposition that this method shouldn't be used.,2011-02-16T12:10:42.757,1643,CC BY-SA 2.5,
11688,7158,3,"@dsimcha - you **always** have prior knowledge!  for how were the data given to you?  you *always* *always* know that much.  data doesn't magically just show up.  and you can always make tentative assumptions.  ""outliers"" based on these assumptions basically give you a clue that something in your assumptions is wrong.  by studying the ""outlier"" (which is always relative) you can improve your model.",2011-02-16T12:50:41.443,2392,CC BY-SA 2.5,
11689,6071,2,"from this model, $g_i$ and $\delta_i$ are not identifiable.  For $g_i + \delta_i=(g_i+c) + (\delta_i-c)$ and the adjustment leaves the variances unchanged.  This model would be identifiable if $g_i=g$ (i.e. same for all i) or if it was the same across clusters (as in multilevel models).",2011-02-16T13:11:46.017,2392,CC BY-SA 2.5,
11690,2173,0,"(+1)@radek. I am also a rapidminer fan, but in my opinion it is not flexible enough for sophisticated visualizations.",2011-02-16T13:39:45.040,264,CC BY-SA 2.5,
11691,7289,0,"I have seen this paper - very interesting. Two questions. 1) Lets take logistic regression. It sounds like the only way to conduct CI or hypothesis tests is to build a model in the style of hosmer and lemeshow (precluding any data sets with big p)? So you are left with ""using"" the model for only point estimates? 2) Your paper discusses the lasso among other alternatives. Are you of the mind that this allows later hypothesis testing or is ""simply"" given as a better option of model selection?",2011-02-16T13:42:54.473,2040,CC BY-SA 2.5,
11692,7029,0,@fabians: The case I had in mind was sigma^2 = tau^2 = 1 in Theorem 1. It says the horseshoe density is bounded above and below by multiples of log(1 + c/x^2). So maybe the distribution I mentioned above is more of a simplification of the horseshoe density than an approximation.,2011-02-16T13:54:50.223,319,CC BY-SA 2.5,
11693,7285,0,"@onestop Thanks for the tip about hashtags. With regards to whether the question is on topic for the site, I think random allocation of participants to groups is inherently related to study design, which in turn is related to inferences from data.",2011-02-16T14:20:18.543,183,CC BY-SA 2.5,
11694,7268,0,"@Roman Thanks for the str() function, I wasn't aware of that. So, getting rid of the Factor column, I can generate an xts object like this, x<-xts(d[,3:5],order.by=d[,1]).  I was then able to apply to.hourly, which shortens the data from 19720 objects down to 480. I'm not sure if this will get me where I want, but I'm closer now, I think.",2011-02-16T14:22:35.663,2770,CC BY-SA 2.5,
11695,7285,1,@Jeremy A hash function is not the same thing at all as a hashtag! I see your point about study design though. I admit to not reading the whole of your question properly.,2011-02-16T14:27:06.200,449,CC BY-SA 2.5,
11697,7225,0,"@B_Miner, cross validation tends to be biased for the generalization error of the model. However, if the bias remains relatively constant with respect to a parameter over which an optimization is being done for model selection purposes, than the ""right"" decision will still be made in the end. My experience is that the generalization-error bias tends to be quite large in many situations, but it also tends to be quite constant, e.g., as a function of the regularization parameter in ridge regression. So, for model selection it still works quite well.",2011-02-16T14:50:50.813,2970,CC BY-SA 2.5,
11698,7287,0,A minor note about the above method is that there are a bunch of valid characters in email addresses - punctuation in particular - that you would want to consider if you were doing this.,2011-02-16T14:51:43.760,527,CC BY-SA 2.5,
11699,7287,0,"@dsol: I agree. You could easily be caught out with a ""+"" in an email address. For a quick and dirty solution, I would probably just skip any punctuation characters that I hadn't specified in my look-up table.",2011-02-16T14:59:54.803,8,CC BY-SA 2.5,
11700,7270,0,Even plotting 4 million points might take a while...,2011-02-16T15:00:57.640,1351,CC BY-SA 2.5,
11701,7290,3,"all you've appeared to do above is describe leave-one-out cross validation and $k$-fold cross validation. The former is rarely used these days due to high variance and usually large computational costs (some regression settings being the exception). As for your remarks on influence, if $p > n$ there are no unique least-squares estimates, which is a complication. Also, the signs of the parameter estimates can be different as well. I'm not positive, but even when the OLS estimates exist, there may still be situations where your ratio could be $> 1$ for some parameters.",2011-02-16T15:02:18.663,2970,CC BY-SA 2.5,
11702,7040,0,@onestop: I am sorry i missed your question. The comparison is on log odds ratio. Thank you for your interest.,2011-02-16T15:02:35.950,2956,CC BY-SA 2.5,
11703,7293,1,"can I assume you know the eigenvalues of $B$? If so, no approximation is needed.",2011-02-16T15:05:49.300,2970,CC BY-SA 2.5,
11704,7282,0,"Thanks for mentioning SNAP. I am looking into it. Have you used it? The centrality sample that comes with it seems close to what I want. I tried modifying it so it works with my multi directed graph data but it failed to compile. I am not sure if it is appropriate to ask a question about it here, so I might create a new Q.",2011-02-16T15:13:15.643,1762,CC BY-SA 2.5,
11705,7282,1,"@andresmh, you might try reducing your graph to have a single observation per directed pair first. For the eigenvalue stuff, your data is likely similar or equivalent to a weighted random walk on the graph. I'm not sure if SNAP supports that, but it's likely to. If all else fails, you might send a very specific email to Jure. He's a very nice guy, so I wouldn't be surprised if he provided some quick guidance.",2011-02-16T15:32:19.637,2970,CC BY-SA 2.5,
11707,7270,0,"@wok, nah. Piece of cake on today's computers. Anyway, you could always dump to a PNG first and that's likely to be good enough for the degree distribution. The OP's graph really isn't all that big.",2011-02-16T15:36:05.080,2970,CC BY-SA 2.5,
11708,7286,0,You can get the bonus by clearing all associations and then associating accounts again.,2011-02-16T15:37:22.867,1351,CC BY-SA 2.5,
11710,7235,0,"I was thinking only of model selection, but that's largely because I don't know all that much about penalized regression. I would say that you need to bootstrap the entire process in order to get inference on predictions from the model. The whole issue is that, in any one sample, you are likely to find spurious correlations that get magnified when you include some variables and leave others out. The only way to get around this is to look at multiple samples---i.e., bootstrap. Of course, no one actually does this.",2011-02-16T15:55:23.943,401,CC BY-SA 2.5,
11711,7235,0,"Right, you use one partition of your sample to come up with your model using model selection procedures, then do your inference on either the other partition or the full sample.",2011-02-16T15:59:23.873,401,CC BY-SA 2.5,
11712,7264,0,"@teucer are they predicted from user-supplied values of `ir` and `vr`? if so, how? and if so, why not calculate `pri` and `pvr` yourself inside the JAGS model?",2011-02-16T16:06:29.410,1381,CC BY-SA 2.5,
11713,7294,0,"thanks robertsy, for good reference. indeed the process is not markov. Even if acceptance probability is independent of the past, the transition kernel of the process is a function of the proposal density and thus depends on the whole chain.",2011-02-16T16:07:17.533,1542,CC BY-SA 2.5,
11714,7264,0,"@teucer, this is a shot in the dark since I still don't understand your model, and this might be the same as what you already have, but would it make sense to append the `pir` and `pvr` to the `ir` and `vr` columns, get rid of the second `for` loop, and then consider the values of `mu[]` estimated using `pir` and `pvr` to be the posterior predictive estimates of mu?",2011-02-16T16:07:39.610,1381,CC BY-SA 2.5,
11716,7297,0,"Could you please define your initialisms, or avoid using initialisms?",2011-02-16T17:10:02.407,449,CC BY-SA 2.5,
11717,7291,2,"Multiplying is not the best idea, I think. Especially if your initial overflow is the regular one - modulo some power of 2. You will get a lot of factors that are even, so most of your lower bits will be 0. Adding the numbers together instead would already be a lot better. If you need even better randomness, use [some sort of hash function](http://en.wikipedia.org/wiki/Hash_function) and use any bits of the result. If you want it to be difficult to guess anything about the outcome for people other than you, use a salted strong cryptographic hash function.",2011-02-16T17:17:52.810,2898,CC BY-SA 2.5,
11720,7301,0,I am not very familiar with ggplot2. Thank you for the suggestion.,2011-02-16T17:44:01.927,3263,CC BY-SA 2.5,
11722,7301,0,Sure - I really recommend trying it out as it has some wonderful visualization features. I've also updated my answer to include Deducer (which is an alternative GUI for ggplot2),2011-02-16T17:58:31.337,2635,CC BY-SA 2.5,
11723,7235,0,"@charlie - Thanks! In the world of big N, using a split sample for selection and then applying the model to the other split or full data, then using that to do all the normal inference, is certainly attractive.",2011-02-16T18:01:33.097,2040,CC BY-SA 2.5,
11724,7297,0,"Can you post an example plot? -- this seems like a computational error, but it is hard to tell from the description only.",2011-02-16T18:05:05.283,,CC BY-SA 2.5,user88
11725,7297,0,"FT=Fourier transform, FFT = fast Fourier transform, DFT = discrete Fourier transform",2011-02-16T18:05:29.943,3272,CC BY-SA 2.5,
11726,7305,0,"The trick applies to non-integer powers, but the proof is a little different.",2011-02-16T18:07:08.927,795,CC BY-SA 2.5,
11727,7301,0,"thank you, I will try out those things too.  I think I am getting somewhere.  I shall update the question as soon as I find a solution.",2011-02-16T18:26:13.823,3263,CC BY-SA 2.5,
11729,7174,0,"I am still hoping someone can answer my question, based on further details in response to comments from ""onestop"", above. Thanks!",2011-02-16T18:41:42.927,834,CC BY-SA 2.5,
11730,7259,0,"@chase, I hope that the example is enough now.  Can you point to a solution.  As mentioned by @celenius, I am looking at ggplot2, but I am still very much looking for a solution.",2011-02-16T18:50:45.817,3263,CC BY-SA 2.5,
11731,7297,0,"@Roman, I second @mbq's remarks. A figure would be helpful, especially in terms of understanding what you mean by ""noise"". Just because the time domain signal is smooth does not mean the Fourier transform will be. Just think of a sinusoid to convince yourself. in terms of plotting, the first thing that comes to mind is: are you plotting the magnitudes and not, say, accidentally, just the real part. Also, do you know about windowing?",2011-02-16T19:21:43.463,2970,CC BY-SA 2.5,
11732,7301,0,"I put an example of some code up there - this shows a box plot, and faceting, and is somewhat illegible based on the data that I chose, but it should illustrate the basics.",2011-02-16T19:51:49.617,2635,CC BY-SA 2.5,
11733,7308,1,"@Jyotirmoy, what if the minimum happens at the boundary of your parameter space?",2011-02-16T20:18:44.990,2970,CC BY-SA 2.5,
11734,7282,0,"@cardinal: I found a sample code in SNAP that does exactly what I want but for a undirected graph. I **think** my graph is what the SNAP docs calls ""directed multi-graph"". So I changed just one line in `centrality.cpp` from `TUNGraph` to `TNEGraph` (see http://pastebin.com/GHUquJvT line 24). It is not compiling anymore. I suspect it requires a different type of node? The error I get is: `centrality.cpp:24: error: conversion from ‚ÄòTUNGraph::TNodeI‚Äô to non-scalar type ‚ÄòTNEGraph::TNodeI‚Äô requested`
(see full error at http://pastebin.com/86mCbByG)",2011-02-16T20:20:46.393,1762,CC BY-SA 2.5,
11735,7274,0,"Apologies for the naive question, but how do I convert a table into what you have as `src` and `dst`. This is what I typically do to load the file (now a tab-delimited file):  `el <- read.csv(""comment-net/comments-ouids.tsv"",header=T,sep=""\t"")`",2011-02-16T20:45:59.817,1762,CC BY-SA 2.5,
11737,7312,0,"Well, as an experienced statistician lacking fundamental maths education I still know about Mean and Median differences and applications a lot. What I need here is somebody to DRAW - logically or mathematically - either (a) from (b) or (b) from (a), for me. I feel I can't harmonize (a) with (b) rationally myself. Marco, I find very difficult understanding your notation. If your formulas is the deduction I need please could you ""chew over"" the idea less technically for me?",2011-02-16T20:59:04.770,3277,CC BY-SA 2.5,
11738,7297,0,Some idea of the source of the sampled curve could be helpful too.,2011-02-16T21:05:32.527,449,CC BY-SA 2.5,
11739,7274,0,read.csv() should produce a data.frame. as.network() may read that directly or you may need to do as.matrix(el).,2011-02-16T21:10:12.393,3265,CC BY-SA 2.5,
11740,7188,0,You might be interested in my next question: http://stats.stackexchange.com/q/7313/1026,2011-02-16T21:19:21.600,1026,CC BY-SA 2.5,
11741,7307,0,"So what you're after for the first pair is a proof that the median, as usually defined as the middle-rank value (for an odd number of values anyway, to start with the simplest case) is also the value that minimizes the sum of absolute deviations? Preferably a proof that also gives some intuitive insight? I don't know of any proof myself, so it seems a good question, and one i'd like to know the answer to as well.",2011-02-16T21:24:07.917,449,CC BY-SA 2.5,
11742,7174,0,"I'm no expert on analysis of survey data so I'm not certain, but I tend to agree with DWin that conditional logistic regression is not necessary here. Ordinarly logistic regression with survey weights should be ok. I do have a question though: If you already have all the data available in a database, why sample at all? Why not use all the firms in the database? Or are you then going on to collect additional data on the sampled firms?",2011-02-16T22:18:19.417,449,CC BY-SA 2.5,
11743,7264,0,"@David I want to have some credible interval for the predictions (`pri`), this is why having `mu` is not enough. If I append `pvr` and `pir` I would get `alpha`, `b.vr` and `b.ir` for the whole `ri`, right? Or can I have `ri` with missing values e.g. (1,2,3,NA,NA) and append the regressors?",2011-02-16T22:18:47.860,1443,CC BY-SA 2.5,
11744,7312,0,"P.S. As long as your two inequalities got finally displayed correctly on my screen I see it's merely my (b) statements. You write, ""it can be shown that..."". So do show me that. I need a kind of mathematical proof put in terms that are intelligible for data analyst who is not a professional mathematician.",2011-02-16T22:36:11.227,3277,CC BY-SA 2.5,
11746,7297,0,"@cardinal: No, these are magnitudes, I've checked it. I've heard of windowing, but I haven't used it before.",2011-02-16T22:48:58.647,3272,CC BY-SA 2.5,
11747,7297,0,"@Roman, now you should be able to post images. Think of windowing as applying a low-pass LTI filter to the Fourier coefficients. That essentially takes local weighted averages of the coefficients, which might make peak detection in your case more feasible as long as the peaks are spaced ""far enough"" apart.",2011-02-16T22:53:31.727,2970,CC BY-SA 2.5,
11748,7307,0,"You feel me correctly. (a) and (b) are currently separate aspects/properties in my mind, for both statistics; but intuition suggests the two aspects are tied. I want to know - _how_ they are tied, to understand it all deeply.",2011-02-16T23:08:03.883,3277,CC BY-SA 2.5,
11749,7297,0,"@Roman, was the constant that was subtracted very close in value to the last data point? Otherwise, any zero padding done by the FFT to get the length out to a power of 2 would induce an artifact due to the step down at around time 100.",2011-02-16T23:15:45.187,2970,CC BY-SA 2.5,
11750,7312,2,"@ttnphns:  your request for a mathematical, rather than an intuitive, answer seems incompatible with your request for something less technical than what people have offered.",2011-02-16T23:23:19.470,2669,CC BY-SA 2.5,
11752,7315,1,"This sweeps some stuff under the rug concerning when you hit the next $x_i$ value, but is close enough to the handwavey proof desired, I think, at least for the median.",2011-02-17T00:56:58.787,795,CC BY-SA 2.5,
11753,7312,0,Can we simplify the situation to 2 or three points and ask whether the median in the double summation non-strict inequality above has a unique value? With two points it would seem to be satisfied by any point between the 2.,2011-02-17T01:03:28.183,2129,CC BY-SA 2.5,
11755,7228,0,"Dear Jeromy,
thanks a lot for your feedback. Anova seems correct to me. Anyways I am just having a look to a ""conjoint measurement analysis"" and apparently it seems to fit my problem.

All the best",2011-02-17T01:18:50.910,4701,CC BY-SA 2.5,
11756,6795,0,@mpiktas Apologies...wasn't trying to confuse; clearly didn't ask the question effectively.  Thanks for the help in clarifying it!,2011-02-17T02:40:02.790,988,CC BY-SA 2.5,
11757,7316,3,"are you asking about how to specify the knots in $R$ (i.e., via arguments to **ns**) or are you asking about strategies for deciding on where to place the knots?",2011-02-17T03:47:01.367,2970,CC BY-SA 2.5,
11758,7308,0,"@cardinal. Your are right, my argument won't work in that case. But Wooldridge is considering the case where the minimum is in the interior. Isn't he wrong in that case?",2011-02-17T04:02:14.543,1393,CC BY-SA 2.5,
11759,7308,0,"@Jyotirmoy, it can certainly be only positive semidefinite. Think of linear functions or a function where the set of minimum points forms a convex polytope. For a simpler example, consider any polynomial $f(x)=x^{2n}$ at $x = 0$.",2011-02-17T04:13:30.943,2970,CC BY-SA 2.5,
11760,7181,0,Actually SPSS do charge for this separately in SPSS Categories.  Ugh... suncoolsu you were right.  Anyone know how to get an eval of SPSS version 18 so I can check if this helps me?,2011-02-17T04:49:23.160,3189,CC BY-SA 2.5,
11761,7308,1,"@cardinal. True. What is troubling me is the phrase ""even positive semidefinite"" in the quoted statement.",2011-02-17T05:04:19.623,1393,CC BY-SA 2.5,
11762,7322,2,"You are welcome to ask questions like these, but adding details about what are the variables (not only variable names and tittle details) help. It will help others with similar questions in the future.",2011-02-17T07:01:17.170,1307,CC BY-SA 2.5,
11764,7321,0,"I can think of a simple solution for the case of an infinite 'axis', but i assume yours is finite, and I think boundary effects would completely mess up my idea... A bit more info might help: Are the 'fields' all the same length, or similar lengths, or not? How long is the 'axis' compared to a (typical) 'field'?",2011-02-17T07:25:48.943,449,CC BY-SA 2.5,
11766,7323,0,"Well, actually this is the study of ‚ÄúPhysical Growth of California Boys and Girls from Birth to Eighteen Years‚Äù by Read D. Tuddenham and Margaret M. Snyder back from 1954. The idea was that people in this study misused statistics and hence, made false inferences about what the dataset was supposed to show. Plotting the residuals was one way of showing it. Now, let me define some of the variables: regression soma.WT9 represents the regression of so called somatotype on weight at nine years of age. and regression of HT9.WT9 represents the regression of height at 9 years of age onto weight at 9yr",2011-02-17T07:33:36.467,3008,CC BY-SA 2.5,
11767,7323,0,"further, i then take the residuals from both of those regressions and plot them against each other.",2011-02-17T07:35:02.487,3008,CC BY-SA 2.5,
11768,7323,0,Unfortunately that's not available online... Could you just tell us what 'soma' is?,2011-02-17T07:35:25.860,449,CC BY-SA 2.5,
11769,7323,0,"@onestop: the data is in the R package alr3 as BGSall BGSgirls BGSboys (see http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=alr3:BGSall) and ""soma"" seems to be ""Somatotype, a 1 to 7 scale of body type"" developed by William Herbert Sheldon (described in http://en.wikipedia.org/wiki/Somatotype_and_constitutional_psychology)",2011-02-17T08:17:34.727,2958,CC BY-SA 2.5,
11770,7321,4,"In a very simple manner, this can be done as a control chart (just in space not time) -- namely by converting it to number of counts over discrete segments and then calculating deviation form Poissson distribution.",2011-02-17T09:19:13.353,,CC BY-SA 2.5,user88
11771,7297,0,"@Roman, What do you want to do with this FFT ? Using DWT (discrete wavelet transform) is often more suitable when you have time varying frequency.",2011-02-17T10:20:54.453,223,CC BY-SA 2.5,
11772,7084,0,"Could you elaborate on ""using a mathematical model evaluates functional meaning of the measured data"" ? How does the mathematical model evaluate the functional meaning? What do you mean by 'evaluate'? What do you mean by 'functional meaning'?",2011-02-17T10:49:48.657,449,CC BY-SA 2.5,
11773,6919,1,"good answer, note that most often $D_{TV}$ is defined another way which makes it half the $L_1$ distance.",2011-02-17T10:52:33.757,223,CC BY-SA 2.5,
11774,7326,0,"@Mortimer, good question, can you tell us what you want to do with this sample  ? I guess this info will help to get better answer about the size. From a cost loss perspective, the loss of small size is the loss of information which may be different depending on the use of the sample, and the cost of a large size is the computation cost that higly depends on what you do with the sample.",2011-02-17T12:25:22.053,223,CC BY-SA 2.5,
11775,7084,0,If I understood you want to build up a test of normality that will generate data from a normal random variable (with suitable mean and variance) and compare it to your orginial sample ? Why do you think this can be a better test than usual test of normality ?,2011-02-17T12:50:25.090,223,CC BY-SA 2.5,
11776,1432,2,"One really easy way to check model fit is a plot of the observed vs the predicted proportions.  But this won't work if you have bernoulli regression (i.e. all of your observations have unique combinations of the independent variables, so that $n_i=1$), because you will just see a line of zeros and ones.",2011-02-17T13:17:44.417,2392,CC BY-SA 2.5,
11777,7326,0,"I am not an expert on this stuff, so someone please correct me if i am wrong. I agree with robin that it really depends on what you need to do with the data. In your case a small sample will lead to a loss of information from the original sample (missing tags), which means your sample will not be truly representative. For the more popular tags you can use a simple significance test (like a chi-square test), but for the rarer tags that are missing from your sample, their absence will be significant. It is up to you if this is acceptable.",2011-02-17T13:19:12.707,2965,CC BY-SA 2.5,
11778,7327,0,Thank you for the response. It will take me at least a few days to process the materials you presented. I would also like to say your dissertation is incredibly nice and after reading your introduction I look forward to reading the rest.,2011-02-17T13:50:26.007,1036,CC BY-SA 2.5,
11779,7315,0,"thank you for the elegant explication. However, it sounds to me so: ""That number _y_ which small change doesn't change function _Sum|x_i-y|_ does not depend on each _x_i_ and is called median"". It's an interesting note on median of an even _n_ data. But I was asking to prove this: ""That number _y_ which minimizes function _Sum|x_i-y|_ does not depend on each _x_i_ and is called median"". And similarly: ""That number _y_ which minimizes function _Sum(x_i-y)^2_ depends equally on each _x_i_ and is called mean"".",2011-02-17T13:57:00.557,3277,CC BY-SA 2.5,
11780,7318,3,"In what way is the data ""non-normal""?  skewed? ""fat tails""? outliers? You may be able correct this by using a different distribution (e.g. t or cauchy for fat tails and outliers, and rescaled beta distribution for skewness, off the top of my head), and save some power in your test.",2011-02-17T14:09:06.507,2392,CC BY-SA 2.5,
11781,7330,2,"I think you'd be better asking this on math.SE, as the question clearly applies equally to anything that can be represented in $n$-dimensional space, not just a probability distribution.",2011-02-17T14:10:08.967,449,CC BY-SA 2.5,
11782,7330,0,"@onestop - I am thinking the same thing.  I'll leave this question here anyway, just in case somebody in stats world knows the answer.  One thing I have found with stats (particularly Bayesian stats) is that you tend to come across just about every branch of maths (pardon the pun).  So maybe some stats person may know it.",2011-02-17T14:19:35.480,2392,CC BY-SA 2.5,
11783,7084,0,"@onestop: this is essentially a systems biology model, we check how different networks are affected by different conditions. I can't really go into detail on what the model is or how it works, as it'll take a long time and place, and I don't think it's very relevant either.. What is important/relevant is; I have abundance levels for a long list of molecules, which come from an instrument with high technical variation, and an unknown underlying distribution. I want to see that my model can distinguish data from real biology, and made-up/random data, from a similar distribution.",2011-02-17T14:28:48.313,3014,CC BY-SA 2.5,
11784,7330,0,"I guess if you can ""rearrange"" the indexes (apply permutations) in you space the answer is yes otherwise it is no.",2011-02-17T14:29:53.663,223,CC BY-SA 2.5,
11785,7084,0,"@robin girard: not exactly... see the comment above in reply to @onestop. I have a model and I test the significance of the results of my model, by comparing it to data of no biological significance. I intend to sample from normal distribution instead of permuting the same values over and over again. Could this normality approximation be justified somehow, that is my question.",2011-02-17T14:31:16.157,3014,CC BY-SA 2.5,
11786,7326,0,"thanks. What we are trying to do is to show how search works on such tagging system and we need to manually annotate the tags. In particular, someone might use ""javaisland"", someone else ""java_island"" etc.. That's why we can't use the whole dataset. We assumed that randomly selecting tag won't make a difference, but it seems that this leads to a very ""computer/web oriented"" vocabulary. This seems to be the case in the whole delicious dataset apriori, but I would like to have a hard number to check that this distribution of vocabulary isn't just a feature of the sampling.",2011-02-17T14:39:29.360,3291,CC BY-SA 2.5,
11787,7326,0,"so, to make it more concrete, the tag ""design"" appears a lot in the sample, it also appears a lot in the dataset and the tag ""crazywebsite"" rarelly appears in the dataset and rarelly appears in the sample. Now, I can looks at them all manually, but they don't all have exactly the same distribution and would expect that there is some test to tell me that the general distribution of each class is within some interval of the original distribution.",2011-02-17T14:46:54.820,3291,CC BY-SA 2.5,
11788,7308,0,"@Jyotirmoy, is there a specific form of the M-estimator given in the book that you could provide? Also give the parameter space under consideration. Maybe then we can figure out what the author had in mind. In general, I think we've already established that the author's assertion is correct. Placing further constraints on the form of $q$ or the parameter space being considered might alter that.",2011-02-17T14:47:58.667,2970,CC BY-SA 2.5,
11789,7330,0,"@robin- that is what I implicitly meant by rotating about ""arbitrary"" dimensions, in that it doesn't matter which ""dimension"" you call 1 and 2.",2011-02-17T14:54:02.523,2392,CC BY-SA 2.5,
11790,7332,0,"thanks for your answer, especially the terminology - I've never heard of ""givens"" rotation before.",2011-02-17T14:57:49.993,2392,CC BY-SA 2.5,
11791,7332,0,"Thanks for this, you have saved me a lot of useless algebra!  Because I can go ""rotation 2-D"" new prior invariant... ""rotate 2-D"" new new prior invariant ..... etc.",2011-02-17T15:13:03.593,2392,CC BY-SA 2.5,
11792,7332,1,"@probabilityislogic, Givens rotations are very popular in numerical linear algebra due to their simplicity and good numerical-stability characteristics. Some QR decomposition methods use them and I think any good SVD implementation will use them as well.",2011-02-17T15:21:38.777,2970,CC BY-SA 2.5,
11793,7264,0,"@teucer yes, you can have ri with missing values; this is a neat feature of bugs that simplifies calculation of a predictive interval.",2011-02-17T15:46:27.460,1381,CC BY-SA 2.5,
11794,7208,1,"I don't think you are right in the first calculation. If I'm not mistaken, the agreement by chance should be 0.5, giving a kappa of 0.",2011-02-17T17:24:12.070,449,CC BY-SA 2.5,
11795,7084,2,"""just focus on the question at hand instead of inquiring further and further into the motivations behind why"". I'm sorry; we're not just being curious or trying to be awkward, honest. Statisticians learn, often the hard way, that it's usually a very bad idea to tell clients how to do something without first enquiring why they want to do it.",2011-02-17T17:26:00.480,449,CC BY-SA 2.5,
11796,7336,0,Good question. Have a look at this [question](http://stats.stackexchange.com/questions/3532/random-numbers-and-the-multicore-package) on random numbers and the multicore package,2011-02-17T17:54:43.683,8,CC BY-SA 2.5,
11797,7309,0,"Thank you @chase, this pretty much answers my question. I was wondering if it is possible to do this without the inbetween step of creating the ""extra"" data-frame.  Also I used to think that `cbind` is more efficient.  Is it not?",2011-02-17T17:56:41.683,3263,CC BY-SA 2.5,
11799,7306,0,"Thanks for this ... I think I might need to reword the question though, or ask a new one.  Looking at this question http://stats.stackexchange.com/questions/980/how-do-i-reduce-the-number-of-data-points-in-a-series, I now think getting the means is not exactly what I'm after.",2011-02-17T18:18:41.990,2770,CC BY-SA 2.5,
11800,7337,0,"You also need to coordinate between the RNG streams. Snow does that, multicore may now.",2011-02-17T18:23:13.393,334,CC BY-SA 2.5,
11801,7332,0,"(+1) @Cardinal One small thing: the resulting diagonal matrix does not need to be the identity.  It can have an even number of -1's.  But those can be converted to +1's by means of suitable rotations (of 180 degrees).  It's also worth nothing the implicit assumption that the only rotations one can use are with respect to a fixed basis.  If not, then *any* rotation can always be written in the form $R_n(\theta)$ (that is, as a single such matrix) by choosing an appropriate basis adapted to that rotation.",2011-02-17T18:27:10.113,919,CC BY-SA 2.5,
11802,7291,0,Agreed.  Just wanted to suggest idea to illustrate bit shifting in order to generate (roughly) pseudo-random numbers.,2011-02-17T18:48:55.320,3489,CC BY-SA 2.5,
11803,7335,0,"@andresmh, Maslov and Sneppen (*Science*, 2002) have a visualization that might be useful in this context. Searching through recent stats/comp-sci--related citations of this work, I found [this](http://statistics.stanford.edu/~ckirby/techreports/GEN/2010/2010-13.pdf) as well. [Here](http://arxiv.org/PS_cache/arxiv/pdf/1012/1012.0201v1.pdf) may be another related work.",2011-02-17T19:06:24.497,2970,CC BY-SA 2.5,
11804,7332,0,"@whuber, Good catch! Thank you, and I've updated it accordingly.",2011-02-17T19:11:31.573,2970,CC BY-SA 2.5,
11805,5430,0,"@Theodor If you discard the observations for distance<4, is't it more convenient to fit an exponential distribution?

Because if that is the case, take the difference between the exponential fit and the data. I'm sure the peak at 7 will stand out clearly.",2011-02-17T19:36:50.473,2902,CC BY-SA 2.5,
11806,1432,0,Yeah - sadly I usually am using a Bernoulli DV.,2011-02-17T20:05:18.503,196,CC BY-SA 2.5,
11807,7315,1,how am I supposed to prove the 'is called median' part? That's crazy.,2011-02-17T20:14:31.360,795,CC BY-SA 2.5,
11808,7340,0,"Ralph, I do not really understand what you mean by: ""build a confidence interval of this distribution to see if the population mean is included."". What we want is to show properties such as the amount of spell variants and synonyms, so we are doing this, but if our sample is not representative of the general population, it's not clear that we can generalize.",2011-02-17T20:29:23.400,3291,CC BY-SA 2.5,
11810,7321,1,"More information would be helpful (honestly, it's essential).  Either implicitly or explicitly, this problem needs a probability model for the ""fields"" under a null hypothesis.  For example, we might posit that the fields have fixed length and their right endpoints are uniformly distributed over the possible values (in which case the distribution of the number of overlaps at most predefined points is Binomial).  Bear in mind, too, that you can treat this as a 2D problem (by recording each interval as an ordered pair such as left endpoint and length).",2011-02-17T21:46:37.447,919,CC BY-SA 2.5,
11811,7345,0,Sometimes I don't the forest for all the trees ... Thanks!,2011-02-17T21:46:41.113,977,CC BY-SA 2.5,
11812,7208,0,I don't really understand the `information-retrieval` tag here.,2011-02-17T22:03:04.737,930,CC BY-SA 2.5,
11813,7208,0,"I don't know, I am working on an information-retrieval task, where people judge whether documents are relevant or not, hence the kappa statistic. But everybody can retag posts here, so feel free to do so! @onestop, following [this standard guide](http://nlp.stanford.edu/IR-book/html/htmledition/assessing-relevance-1.html) my numbers are correct, the pooled marginals are .75 and .25, respectively, and both squared and added to each other equal .625",2011-02-17T22:20:38.387,1205,CC BY-SA 2.5,
11816,7208,0,I hope that's not a standard guide in information retrieval as it's just plain wrong on this! The correct formula can be found by [googling 'Cohen's kappa'](http://www.google.com/search?q=cohen%27s+kappa) and following any of the top three hits.,2011-02-17T23:24:39.937,449,CC BY-SA 2.5,
11817,7346,4,I was starting to worry you wouldn't volunteer :-).,2011-02-17T23:24:59.640,919,CC BY-SA 2.5,
11819,7352,0,"I'm just wondering if anyone else noticed that the values at the top of page 2 of the 4th link (whatisbayes.pdf) are a little farther off than simple rounding error?  I'm getting about 0.040, 0.097, 0.138, 0.139.",2011-02-18T02:59:06.120,601,CC BY-SA 2.5,
11822,7346,1,"+1, I was starting to worry also :)",2011-02-18T07:42:36.017,2116,CC BY-SA 2.5,
11823,7264,0,"@David could please edit your answer accordingly so that I can accept?As I said earlier having only the mean is not enough for my purposes, I would like to have the other statistics...",2011-02-18T08:06:54.953,1443,CC BY-SA 2.5,
11824,7364,0,"@gappy, there are several articles about the limit theory for factor models, but in [econometrics context](http://www.econ.nyu.edu/user/baij/publication.htm). I dabbled myself in factor models but for me your formulation of the factor model is anything but standard. Could you please provide a reference? As it now stands it is more of regression model than the factor model.",2011-02-18T08:10:18.663,2116,CC BY-SA 2.5,
11825,7359,1,"+1, great answer. I always wondered why the original formula is used for $R^2$ instead of square of correlation. For linear regression it is the same, but when applied to other contexts it is always confusing.",2011-02-18T08:22:28.790,2116,CC BY-SA 2.5,
11826,7362,0,"something is missing in your first sentence. As it stands now it is incomprehensible from linguistic point of view, for me at least.",2011-02-18T08:25:08.537,2116,CC BY-SA 2.5,
11827,7344,3,have you tried plot on the `lm` or `glm` object? How what you are trying to achieve is different from what `plot.lm` does?,2011-02-18T08:29:12.713,2116,CC BY-SA 2.5,
11828,7356,4,That's a nice one. Just what the doctor ordered.,2011-02-18T08:56:00.543,144,CC BY-SA 2.5,
11829,7366,0,Can't you collect the mean? You're not going to be able to calculate a $z$-score without it.,2011-02-18T09:12:49.040,449,CC BY-SA 2.5,
11830,7359,0,"(+1) Very elegant response, indeed.",2011-02-18T09:14:04.263,930,CC BY-SA 2.5,
11831,7366,0,"@onestop I can collect mean, also editing question accordingly. Thanks",2011-02-18T09:16:29.770,2170,CC BY-SA 2.5,
11832,7356,13,"(+1) Also of interest, `smoothScatter {RColorBrewer}` and `densCols {grDevices}`. I can confirm it works pretty well with thousand to million of points from genetic data.",2011-02-18T09:30:25.490,930,CC BY-SA 2.5,
11833,7367,2,"I would like to second this. If you're looking for a fast and easy way to interface compiled code with R, `Rcpp` with `RcppArmadillo` is the way to go. Edit: Using Rcpp, you also have access to all the RNGs implmented in the C-code underlying R.",2011-02-18T10:05:29.360,1979,CC BY-SA 2.5,
11834,7336,0,"@CSgillepsie:> thanks for the pointer, but i'm not sure it's the same problem: the way i understand the question you pointed to, all the processes are spawned by mclapply. Here it's a bit different: on each of the machines, all the processes are spawned by mclapply, but this is not the case *across* machines.",2011-02-18T10:09:54.083,603,CC BY-SA 2.5,
11835,7372,0,"@onestop, why did you delete your answer? I was just about ready to upvote it.",2011-02-18T11:08:11.983,2116,CC BY-SA 2.5,
11837,7373,0,"if I provide z-scores, correlations can be determined by 3rd party tools like WEKA, am I right?",2011-02-18T11:49:16.960,2170,CC BY-SA 2.5,
11838,7372,0,(+1) Let's say my vote goes to both of you :-),2011-02-18T12:26:30.280,930,CC BY-SA 2.5,
11839,7364,1,"The factor model I provide *is* the standard definition. Check, say http://learning.eng.cam.ac.uk/zoubin/papers/ul.pdf, and references therein; or Econometric Analysis of Cross Section and Panel Data by Wooldridge, or Econometrics by Hayashi. I am well aware of Bai and Ng's work, as well as Reichlin et al., and Stock and Watson; but their limit theory has nothing to do with this-- is proves asymptotic convergence of PCA to the factor model for $N$ and $T$ going to infinity.",2011-02-18T12:26:40.110,30,CC BY-SA 2.5,
11840,7346,7,@whuber @mpiktas Let's see you both volunteer too!,2011-02-18T12:38:10.077,5,CC BY-SA 2.5,
11841,7376,1,"what do you mean by ""valid statistical calculation"" you should say valid statistical (estimation) calculation of something. Here the something is very important. Correlation is a valid calculation of the linear relation between two set of data. I don't see why you need stationarity, did you mean auto-correlation ?",2011-02-18T13:13:14.870,223,CC BY-SA 2.5,
11843,7364,0,@gappy this paper http://www.i-journals.org/ejs/viewarticle.php?id=267&layout=abstract may at least give referencse ?,2011-02-18T13:17:33.070,223,CC BY-SA 2.5,
11844,7365,0,"@mpiktas, That's some interesting notation there (I know it's not yours). A $w$ on the left-hand side and $y$ and $x$ on the right-hand side. I'm guessing $w = (x,y)$ or something like that. Also, I'm assuming the squaring should be happening to $y - m(x,\theta)$ and not just to $m(x,\theta)$. No?",2011-02-18T13:17:34.260,2970,CC BY-SA 2.5,
11845,7368,0,Thank you so much for the unexpectedly fast and simple worded guidance.You must be an excellent teacher. My treatment is very simple and has already saved thousands of lives.,2011-02-18T13:18:40.623,2956,CC BY-SA 2.5,
11846,7373,0,"I mean, attribute selection or information gain calculations are enough to keep some z-scores and remove unnecessary ones, aren`t they?",2011-02-18T13:19:20.940,2170,CC BY-SA 2.5,
11847,7365,0,"@mpiktas, I'm not *quite* sure how to interpret your first sentence due to the wording. I can see two ways, one that I'd call correct and the other I wouldn't. Also, strictly speaking, I don't agree with the second sentence in your first paragraph. As I've shown above, it is possible to be at a local minimum in the interior of the parameter space without the Hessian being positive definite.",2011-02-18T13:24:03.880,2970,CC BY-SA 2.5,
11848,7373,0,"You've lost me completely there, I'm afraid. Others may understand what you're talking about, but it must be outside my field.",2011-02-18T13:25:38.797,449,CC BY-SA 2.5,
11849,7359,1,"@mpiktas, @chl, I'll try to expand on this a little more later today. Basically, there's a close (but, perhaps, slightly hidden) connection to hypothesis testing in the background. Even in a linear regression setting, if the constant vector is not in the column space of the design matrix, then the ""correlation"" definition will fail.",2011-02-18T13:26:11.723,2970,CC BY-SA 2.5,
11850,7365,0,"@cardinal, yes you are right. Wooldridge uses $w$ for consistency reasons, $y$ and $x$ is reserved for response and predictors throughout the book. In this example $w=(x,y)$.",2011-02-18T13:26:30.423,2116,CC BY-SA 2.5,
11851,7362,0,@Bernd: Thank you for your interest to help me. I am sorry I was not clear and wasted your time. I am trying to calculate Orwin's fail-safe N. The treatment is inexpensive but has saved thousands of lives already.I am trying to decide on the best values to use for the criterion for a trivial log odd's ratio and mean log odds ratio in missing studies to calculate the Number of missing studies needed to bring down odds ratio (because Classic fail-safe method yielded 1014 studies). I am using Comprehensive meta-analysis v2.2,2011-02-18T13:30:11.657,2956,CC BY-SA 2.5,
11852,6919,1,"@robin, thanks for your comment. Yes, I realize this. I was just trying to avoid a messy extraneous constant in the exposition. But, strictly speaking, you're correct. I've updated it accordingly.",2011-02-18T13:33:29.720,2970,CC BY-SA 2.5,
11853,7326,0,@Mortimer and @rm999 if you don't use @name to answer then I can know that you are talking to me (the sofware detect @robin and send me a notification somehow when it comes).,2011-02-18T13:34:23.847,223,CC BY-SA 2.5,
11854,7365,0,"@cardinal, I fixed my wording. Now it should be ok. Thanks for pointing out the problem.",2011-02-18T13:42:18.047,2116,CC BY-SA 2.5,
11855,7376,2,there is a new site which might be more suitable for your question: http://quant.stackexchange.com. Now you are clearly confusing calculation with interpretation.,2011-02-18T13:45:44.143,2116,CC BY-SA 2.5,
11856,7344,0,"@mpiktas I'm looking for something to supplement plot.lm or plot.glm.  Plot.lm shows residuals vs Fitted, Scale-Location, Normal Q-Q and Residuals vs. leverage plots.  What I'm looking for is plots of the actual relationship between Solar.R and and Ozone, and the predicted relationship from my model.  Run my example code, and you will see what I mean.",2011-02-18T14:33:28.390,2817,CC BY-SA 2.5,
11857,7376,0,"@mpiktas, the quant community is settled on using returns vs prices because of the stationarity of returns and the non-stationarity of prices. I'm asking here for something more than an intuitive explanation of why this should be so.",2011-02-18T14:38:05.210,3306,CC BY-SA 2.5,
11858,7340,0,"Mortimer, I am not sure exactly what you are trying to do. But I am suggesting that if you know that ""javaisland"" + ""java_island"" represent 2% of your population, you bootstrap sample from your 500 sample, say 10000 times.  You obtain a confidence interval for your ""javaisland"" + ""java_island"" term frequency of 1.5% plus/minus 1%.  Since the confidence interval contains the population mean of 2%, you have no reason to suspect that your sample is not representative for the term ""java_island"".",2011-02-18T15:07:05.773,3489,CC BY-SA 2.5,
11859,7376,0,"@robin, there are several things that may have you question a statistical analysis. Sample size comes to mind, as does more obvious things such as manipulated data. Does non-stationarity of data call into question a correlation calculation?",2011-02-18T15:10:01.297,3306,CC BY-SA 2.5,
11861,7376,0,"not the calculation, maybe the interpretation if the correlation is not high. If it is high it means high correlation (i.e. high linear relation), two non stationnary time series say $(X_t)$ and $(Y_t)$ can be  potentially highly correlated (for example when $X_t=Y_t$.",2011-02-18T15:25:32.770,223,CC BY-SA 2.5,
11862,7367,0,Thanks for the vote of confidence. I was about to suggest the same ;-),2011-02-18T15:32:03.597,334,CC BY-SA 2.5,
11863,7342,0,Can you help me understand why I'm getting NAs when running this?,2011-02-18T16:01:58.123,2770,CC BY-SA 2.5,
11864,7373,0,"Many apologies, @baris_a, I've deleted my comment that replied ""no"" to your first comment above as I've just realised the answer is in fact ""yes"", i.e. you *can* compute correlations from $z$-scores. Truly sorry, I must have been having a brainstorm earlier. I still don't know what 'attribute selection' or 'information gain calculations' are though, I'm afraid.",2011-02-18T16:04:22.980,449,CC BY-SA 2.5,
11865,6811,1,thanks a lot for offering to help. We have a couple of students in our lab who may look into this.  We'll let you know as soon as we figure things out :),2011-02-18T16:08:55.100,2798,CC BY-SA 2.5,
11866,6816,0,that sounds great! Thanks for being willing to help. We have a couple of students in our lab and they might reach out to you if we need help.,2011-02-18T16:10:22.167,2798,CC BY-SA 2.5,
11867,7377,0,"Thanks Dirk - I had a feeling you'd answer sooner rather than later :). Given that I want code I can call from other software (Python mainly, but Matlab too) perhaps a good workflow would be to prototype in Rcpp/RcppArmadillo and then move to ""straight"" Armadillo? The syntax, etc looks very similar.",2011-02-18T16:13:52.350,26,CC BY-SA 2.5,
11868,7377,1,Hope you found it helpful.,2011-02-18T16:16:11.220,334,CC BY-SA 2.5,
11869,7361,0,"Thanks for the tip. Boost looks like kind of a big hammer for my little nail, but mature and maintained.",2011-02-18T16:25:20.530,26,CC BY-SA 2.5,
11870,7377,0,"Re your 2nd question from the edit: Sure. Armadillo depends on little, or in our case, nothing besides R. Rcpp / RcppArmadillo would help you interface and test prototyped code that can be re-used standalone or with a Python and Matlab wrappers you can add later.  Conrad may have pointers for something; I don't have any for Python or Matlab.",2011-02-18T16:40:54.613,334,CC BY-SA 2.5,
11871,7378,1,The mathematical point of view explanation is what I was looking for. It gives me something to contemplate and explore further. Thanks.,2011-02-18T16:41:13.273,3306,CC BY-SA 2.5,
11872,7379,3,"This estimates the standard error of the ""accuracy"" as measured from ""total trials"" draws from a Bernoulli variable.",2011-02-18T16:54:30.267,919,CC BY-SA 2.5,
11873,7378,1,"This response seems to sidestep the original question: Aren't you just saying that yes, calculating correlation makes sense for stationary processes?",2011-02-18T16:58:20.620,919,CC BY-SA 2.5,
11874,7349,5,Reducing the overlap with `unique` or by rounding can result in biased (deceptive) plots.  It's important to somehow indicate the amount of overlap through some graphical means such as lightness or with sunflower plots.,2011-02-18T17:01:15.313,919,CC BY-SA 2.5,
11875,7379,0,@whuber - Why not just use `number correct / total trials` like everywhere else?,2011-02-18T17:14:25.840,2019,CC BY-SA 2.5,
11876,7215,4,"I almost didn't bother to comment since it goes without saying that @mbq has been a great moderator. In particular, in pushing forward the journal club.",2011-02-18T17:14:48.647,8,CC BY-SA 2.5,
11877,7378,1,"@whuber, I was answering the question having in mind the comment, but I reread the question again and as far as I understand the OP asks about calculation of correlation for non-stationary data. Calculation of correlation for stationary processes makes sense, all the macroeconometric analysis (VAR, VECM) relies on that.",2011-02-18T17:35:39.340,2116,CC BY-SA 2.5,
11878,7379,0,"Because they're not actually estimating accuracy but the standard error.  If they're actually estimating accuracy this way it's just WRONG. Check the values, are there any over 0.5?  If there are then this isn't the accuracy estimate equation.",2011-02-18T17:55:43.787,601,CC BY-SA 2.5,
11879,7215,2,"Huh, I haven't comment on myself. Of course, I'll support him! On a related point, I like those ""compulsive"" edits that make CV so nice looking (on tex.SE, questions/title are rarely reformatted--so strange for a site promoting a correct application of typographical rules...)",2011-02-18T17:55:46.140,930,CC BY-SA 2.5,
11880,7342,0,"Hi Scott, I haven't actually used the `aggregate.zoo` function, though I have used the `zoo` package. Did you make sure that your object was a `zoo` object first? The documentation that I pointed to should help you there.",2011-02-18T17:58:30.170,401,CC BY-SA 2.5,
11881,7378,0,I'll try to clarify my question with a response.,2011-02-18T18:57:29.430,919,CC BY-SA 2.5,
11882,7378,3,"@whuber my take away from the answer is that a correlation based on non-stationary data yields a random variable, which may or may not be useful. Correlation based on stationary data converges to a constant. This may explain why traders are attracted to ""x-day rolling correlation"" because the correlated behavior is fleeting and spurious. Whether ""x-day rolling correlation"" is valid or useful is for another question.",2011-02-18T19:12:42.583,3306,CC BY-SA 2.5,
11883,7306,0,"@JVM Can you explain how the getmeans function works, and why you didn't just use the mean or colMeans functions?",2011-02-18T19:14:49.053,2770,CC BY-SA 2.5,
11884,7379,0,"If the result seems similar to the input, it's only a coincidence.  The equation is just a reworded formula for the standard error of a proportion, as is shown halfway down at http://en.wikipedia.org/wiki/Margin_of_error",2011-02-18T19:37:30.310,2669,CC BY-SA 2.5,
11885,7306,1,"The ddply() function cuts the original dataset into subsets defined by hosts and hour. It then passes these to getmeans() as a data.frame. For your task, using colMeans() would probably work just fine, but you would probably need to first remove the columns you don't need. The nice thing about using ddply() this way is that you can calculate any arbitrary stat for which you might be interested; e.g., sd(), range(), etc.",2011-02-18T19:39:05.803,3265,CC BY-SA 2.5,
11886,7382,0,"it is good that your answer points that out but I wouldn't say the process are correlated, I would say they are dependent. This is the point. Calculation of correlation is valide and here it will say ""no correlation"" and we all know this does not mean ""no dependence"".",2011-02-18T19:48:16.620,223,CC BY-SA 2.5,
11887,7382,1,"@robin That's a good point, but I constructed this example specifically so that for potentially long periods of time these two processes are *perfectly* correlated.  The issue is not one of dependence versus correlation but inherently is related to a subtler phenomenon: that the relationship between the processes changes at random periods.  That, in a nutshell, is exactly what can happen in real markets (or at least we ought to worry that it can happen!).",2011-02-18T19:56:18.037,919,CC BY-SA 2.5,
11889,7359,0,"If you have a reference other than the Seber/Lee textbook (not accessible to me) I would love to see a good explanation of how variation explained (i.e. 1-SSerr/SStot) differs from the squared correlation coefficient, or variance explained. Thanks again for the tip.",2011-02-18T21:25:42.593,36,CC BY-SA 2.5,
11890,7264,0,"@teucer I have updated my answer, I will be interested to know if it works.",2011-02-18T22:02:06.650,1381,CC BY-SA 2.5,
11891,7264,0,"@David Thx! Actually the both approaches work as outlined in [link](http://www.stanford.edu/group/mapss/colloquium/papers/hierarchicalJackman.pdf) (page 39, second paragraph). Your solution is obviously more eficient...",2011-02-18T23:07:41.790,1443,CC BY-SA 2.5,
11892,7364,1,"@gappy, is that really the convergence statement you're interested in? A variance bound of $O(n^{-2})$ seems pretty strong. Also, what do you mean by ""closed form"" when you say that for the $\Sigma = I$ case, one gets a ""closed form"" solution? Thanks.",2011-02-18T23:16:10.097,2970,CC BY-SA 2.5,
11893,7379,0,"If the output seems similar to the accuracy, that means the experiment is unreliable, which worrying and the whole point of the exercise. This only happens when accuracy is low and number of trials is small.",2011-02-18T23:51:40.883,2456,CC BY-SA 2.5,
11894,7318,1,"The data is not normal in various ways. Some are skewed (Poisson type distributions), some are like the combination multiple normal distributions, several just have a few extreme outliers. The issue is that I have been asked to specifically test for normality (I use the Chi-squared goodness of fit test). The data strongly reject the null hypothesis of normal distribution. Transformation is not an option - and in many cases doesn't work anyway. I agree this should be something to try, but I have been disallowed this option, not my choice. Thanks though!",2011-02-19T01:25:27.757,3285,CC BY-SA 2.5,
11895,7188,0,"I strongly recommend you to check wordnet, and yes is the address you posted. And I am glad you posted that question, It looks like what I recommended worked.",2011-02-19T02:54:21.760,1808,CC BY-SA 2.5,
11897,7377,0,"Sorry to pull the rug out :) I want the enter key to give a carriage return, but it submits my comment instead. Anyhow, thanks for your help - I've been enjoying myself tinkering and digging back through the Rcpp mailing list all day today.",2011-02-19T04:50:53.900,26,CC BY-SA 2.5,
11898,7391,2,"I'm not sure about the two-sample problem, but as for references for the former, see:
A. P. Dempster, [Generalized $D_n^+$ Statistics](http://projecteuclid.org/euclid.aoms/1177706275), *Ann. Math. Statist.* Volume 30, Number 2 (1959), 593-597. Also, [here](http://www.springerlink.com/content/ln8hj5273837522p/), [here](http://www.springerlink.com/content/mrx551586853v724/) and [here](http://projecteuclid.org/euclid.bsmsp/1200514106) might have some material of interest. The first and last links should be to publicly available articles. Sadly, the middle two aren't.",2011-02-19T05:42:19.097,2970,CC BY-SA 2.5,
11900,7382,0,"@whubert yes, and this is a very good example showing that there are processes that have very high correlation for potentially long periods of time and still are not correlated at all (but highly dependent)  when regarding the larger temporal scale.",2011-02-19T07:47:56.307,223,CC BY-SA 2.5,
11901,7389,1,"@levon9 I have reformulated the title to make it fit with current policy (sentence capitalization, etc.); please check if I didn't alter its original meaning.",2011-02-19T08:30:57.187,930,CC BY-SA 2.5,
11902,7315,0,"It's a trope of cause. _This_ part is not to prove, I hoped you understand.",2011-02-19T08:31:41.473,3277,CC BY-SA 2.5,
11907,7379,1,"@whuber, @rolando2 Make answers, not comments (-;",2011-02-19T11:36:17.640,,CC BY-SA 2.5,user88
11909,7386,2,"That's a very good news. You already contributed a lot of particularly good answers and comments, and I'm sure your experience will be really appreciated here on CV.",2011-02-19T14:58:04.703,930,CC BY-SA 2.5,
11910,7386,1,Fantastic!  Thanks @whuber!  Now we have three great nominees.,2011-02-19T15:03:19.750,5,CC BY-SA 2.5,
11912,7395,0,Thanks. Can you provide a quotable source for this?,2011-02-19T19:10:19.417,977,CC BY-SA 2.5,
11913,7402,1,See [Wikipedia article 'Statistical power'](http://en.wikipedia.org/wiki/Statistical_power),2011-02-19T21:01:10.990,449,CC BY-SA 2.5,
11914,7395,0,"It seems to be standard in education, e.g. http://books.google.co.uk/books?id=3LhPwUhrVIcC&pg=PA397 or http://www.umich.edu/~exphysio/MVS250/SpearmanRankCorr.doc",2011-02-19T21:54:51.313,2958,CC BY-SA 2.5,
11915,7384,0,"Nice response, plenty in there that I'm not familiar with, so I need to try it out. Still, looking at my data with your methods, I'm thinking I need to show the high points in my data as well.  Thanks",2011-02-19T22:16:16.667,2770,CC BY-SA 2.5,
11916,7402,1,"I would rephrase this question as ""how do I find the power of a general test, such as $H_{0}:\mu=\mu_{0}$ versus $H_{1}:\mu > \mu_{0}$?""  This is often the more frequently performed test.  I don't know how one would calculate the power of such a test.",2011-02-20T00:24:24.263,2392,CC BY-SA 2.5,
11917,7409,2,"That shows there is only a little difference when the population is big.  It is worth noting that there is a large difference when it is small: with 600 [phyper(35, 240, 360, 100)] gives about 0.157 while an even more extreme example of 110 [phyper(35, 44, 66, 100)] gives about 0.001",2011-02-20T00:35:09.190,2958,CC BY-SA 2.5,
11918,7409,0,"@Henry Good catch, edited my answer to stress the importance of big $n$.",2011-02-20T00:44:55.180,1909,CC BY-SA 2.5,
11919,7389,0,"@chl - couldn't find a way to message directly (still learning my way around here) - could you please point me to the policy? I found the part about it not being ""just"" luck somewhat relevant given that these are stochastic processes and therefore non-deterministic.  Thanks.",2011-02-20T02:01:02.800,10633,CC BY-SA 2.5,
11920,7393,0,"Thanks for the paper reference, I'll check it out. Without having read the paper though, let me restate that I too am going to use benchmark functions (the set I mentioned above) .. I just want to be able to somehow quantify that my (hopefully) better numbers are not just due to ""luck"" but statistically significant. I.e., I have seen people report just numbers (such as time to execute or number of evaluations), but I am not sure that's sufficient. Be curious about other comments. Thanks.",2011-02-20T02:03:20.347,10633,CC BY-SA 2.5,
11921,7405,0,"Hi, its nice paper, thanx for giving pdf link.. I'll surely go through this paper..",2011-02-20T03:52:11.453,3325,CC BY-SA 2.5,
11922,7409,0,Thanks caracal and Henry. It's really helpful as a beginner to have a few hints about where you're heading.,2011-02-20T04:00:58.437,3317,CC BY-SA 2.5,
11924,7392,1,"I made sure to estimate robust standard errors. Also, in the book ""Mixed effects models and extensions in ecology with R"" by Zuur, et al, 2009, on page 261, they mention, ""if the mean of the response variable is relatively large, ignoring the truncation problem, then applying a Poisson or negative binomial (NB) generalised linear model (GLM), is unlikely to cause a problem."" Fortunately, the means of my response variables are large, so I feel a little more comfortable deprioritizing zero-truncation compared to the GEE and negbinomial aspects of my regressions.",2011-02-20T07:28:34.280,3309,CC BY-SA 2.5,
11925,7392,0,"Sounds like you already know more about this topic than I do! Or anyone else on this site, judging by the lack of other responses.",2011-02-20T08:29:41.247,449,CC BY-SA 2.5,
11926,7392,0,"It is a little unbelievable; who knew that overdispersed longitudinal count data would be so difficult to analyze (without doing a GLMM, which I haven't even looked into doing yet)? If only my data were zero-inflated, that would be another story.",2011-02-20T08:46:25.727,3309,CC BY-SA 2.5,
11927,363,1,There are really three separate questions here! 1) What is the single most influential book in statistics; 2) What book should every statistician read; 3) What book have you read that you most wish you'd read much earlier. (2) and (3) probably have considerable overlap; (1) may be quite distinct.,2011-02-20T08:50:11.760,449,CC BY-SA 2.5,
11928,5319,0,"As far as I can tell, this application of align.time after using endpoints is what Adal wanted (apart from regarding his mention of taking ""the previous one"" in the original question). Anyway, it is what I wanted, so thanks, Joshua.",2011-02-20T08:54:35.910,3329,CC BY-SA 2.5,
11929,7389,1,"@levon9 Ok, I reverted the title back to the original one. About general policy on title and question wording, please refer to meta, [Is there a style guide that provides guidelines for question title and question content?](http://meta.stats.stackexchange.com/questions/575/is-there-a-style-guide-that-provides-guidelines-for-question-title-and-question-c).",2011-02-20T09:16:17.863,930,CC BY-SA 2.5,
11930,7399,2,That's a good news! I'll second this nomination.,2011-02-20T11:13:23.023,930,CC BY-SA 2.5,
11931,487,0,"Thats a wonderful article, written in Cohen's lucid and conversational style.",2011-02-20T15:30:17.650,656,CC BY-SA 2.5,
11932,7418,0,"The pscl package, I believe, only fits zero-inflated and hurdle models. Hurdle models incorporate both a left-truncated count component and a right-censored hurdle component. I don't how or even if I am able to run a hurdle model without the hurdle component, but I will look into the sandwick package. As for the geepack package, it seems to have the same problem as gee package; when I specify a ""negative.binomial"" family (from MASS), without specifying a theta, it will ask for a theta. However, when I specify a theta value, it will spit out an error saying it's an unrecognized family.",2011-02-20T15:52:18.797,3309,CC BY-SA 2.5,
11933,7418,0,"@Casey - sorry I misread your requirements re zero truncation. Shame that geepack doesn't work with that family function. If I think of anything else, I'll update here.",2011-02-20T16:56:33.127,1390,CC BY-SA 2.5,
11936,7423,0,"Now that I re-read your post, I'm unsure what you were asking. The code I provided computes a volatility and Peak hour to avg. hour ratio score for each hour and each category, collapsing across days (eg. the volatility for email in hour 0 is computed using the hour 0 data for email from all days).",2011-02-20T18:35:50.127,364,CC BY-SA 2.5,
11937,7175,0,"Are you looking for further clarifications or are you unhappy with @mariana's response? I guess it concerns your very first question (2nd ¬ß). If this is the case, maybe you should update your question so that people understand why you're setting a bounty on this question.",2011-02-20T18:58:55.717,930,CC BY-SA 2.5,
11938,7175,0,"@chl I will update it to make it clearer. I'm just looking for some guidance on interpreting the clustering comparisons, as don't understand what the output means. @mariana's response was very helpful explaining some of the terms associated with this method.",2011-02-20T19:15:03.977,2635,CC BY-SA 2.5,
11939,7423,0,"well this looks like a good way but what it means is first aggregate the entire 10 day period into one single 24 hour period (for each category by summing the volumes). And then for each category on this aggregated 24 hour period measure the volatility and the Peak to Avg ratio for each ""Category""",2011-02-20T19:45:14.873,2101,CC BY-SA 2.5,
11940,7423,0,I've added a sample output in the question :).,2011-02-20T19:52:22.980,2101,CC BY-SA 2.5,
11941,7422,0,"Does the identical hour on different days count as the same hour or not? If an hour has no use of particular application, do you want that to be counted as 0 for the standard deviation and average calculation?  When you say ""Ratio of volume of the maximum hour to the vol. of the average hour"", I presume *vol.* means *volume* rather than *volatility*.",2011-02-20T21:49:52.210,2958,CC BY-SA 2.5,
11942,7425,0,"How do I go about examining a cluster fit using a minimax approach? My knowledge level of clustering is very basic, so at the moment I'm just trying to understand how to compare two different clustering approaches.",2011-02-20T21:56:29.207,2635,CC BY-SA 2.5,
11943,7422,0,"@Henry yes, what it should do is aggregate all the hours of all the days by adding the volumes per category so all the corresponding hourly volumes and added into one representing all the days in one single 24 hour period. then perform the calculations for each Catgory giving me the result in the form shown.",2011-02-20T22:07:23.897,2101,CC BY-SA 2.5,
11944,7422,0,@Henry and yes vol. is for volume and if the category is not present in any hour it is 0 (though there are no such cases in the actual data :).,2011-02-20T22:08:24.690,2101,CC BY-SA 2.5,
11945,7425,0,Could you please share the R code for the attached figure?,2011-02-20T22:14:52.233,609,CC BY-SA 2.5,
11946,7428,0,"@Andrej @Bernd Thank you for your time. I see that Cochrane handbook, Egger's paper(PMID9310563) and MIX2.0 software use different funnelplots, but they all agree that effect size on the horizontal axon and a meassure of sample size on the vertical axon. I don't have means and standard deviations of the groups (patients, controls) of each study. I have only the effect size, and the sample size n1,n2 of patients and controls. So I cannot calculate SE of the effect sizes to use them in the linear regression. Could I use only a funnel plot with samplesize on vertical and effectsize on horizontal?",2011-02-20T23:14:12.287,3333,CC BY-SA 2.5,
11947,7428,0,"@Staty Despair: Please be more specific about what type of effect size you have (odds ratio, risk ratio...). Do you know the [Practical Meta-Analysis Effect Size Calculator](http://www.campbellcollaboration.org/resources/effect_size_input.php)? This tool might help you to get the standard errors.",2011-02-20T23:31:33.777,307,CC BY-SA 2.5,
11948,7389,1,"@levon9 General idea is to make title show what the question is about, preferably using proper words. The sentence capitalization policy is just to establish some level of order (people do spit less on clean floors).",2011-02-20T23:59:33.373,,CC BY-SA 2.5,user88
11949,7425,0,@Andrej My guess is a Gaussian cloud (`x<-rnorm(N);rnorm(N)->y`) split into 3 parts by r (with one of them removed).,2011-02-21T00:03:43.277,,CC BY-SA 2.5,user88
11950,7428,0,"@Bernd @Andrej Thanks again. I don't know what type is the effect size (SMD, OR, RR etc). I only know that can take values from -1 to +1, to show the size and the direction of the difference. The variable compared between patient and control groups is a typical scale variable, but no means and standard deviations available from the included studies. The effect size is called SDM value (www.sdmproject.com). I know (automatically calculated) the effect size from each included study, but not the formula used for the calculation. I also now sample sizes n1, n2 of patients,controls in each study.",2011-02-21T00:07:34.810,3333,CC BY-SA 2.5,
11951,7428,0,"@Bernd this website includes probably the most complete and well organized collection of online calculators, ...but still not helpful in my case :(",2011-02-21T00:18:04.090,3333,CC BY-SA 2.5,
11952,7428,0,@Staty Despair: I am sorry but I do not have any experience with this type of analysis. You might want to check this paper: [Voxel-wise meta-analysis of grey matter changes in obsessive‚Äìcompulsive disorder](http://bjp.rcpsych.org/cgi/reprint/195/5/393).,2011-02-21T00:24:38.563,307,CC BY-SA 2.5,
11953,7424,0,"Thanks for your answer. I guess I am looking for stratified sampling. (I was looking for algorithms, which are not computationally very expensive, as not parsing the entire population, to make a representative set, does not even make sense. :-))",2011-02-21T01:03:15.673,3292,CC BY-SA 2.5,
11954,7428,0,"@Bernd Yes, this is one of the 5 papers using this method of meta-analysis. The other 4 are (Pubmed, PMIDs): 19699306, 20603451, 21078227, 21300524. Despite the analytical description in methods section of the papers, it is still difficult to decide the category of the effect size used (SMD, RR, OR, CC), to understand how it is calculated, and calculate the Standard Errors needed to perform Egger test and synthesise a funnel plot! Thank you for your time...",2011-02-21T01:20:54.560,3333,CC BY-SA 2.5,
11955,7412,0,http://getthedata.org/ bills itself as a question and answer site for finding data,2011-02-21T03:02:11.273,183,CC BY-SA 2.5,
11956,7433,0,Thanks.  I'm in the need of hand holding case.  As such answers directing me to other tools (read R etc) aren't really what I'm after.  I'm interested in SPSS users feedback.,2011-02-21T03:47:49.587,3189,CC BY-SA 2.5,
11957,7434,3,"just adding to @JMS answer, a great reference to check out about is **Bayesian Inference in Statistical Analysis** by Box and Tiao. It presents conceptual ideas behind it too.",2011-02-21T06:20:27.790,1307,CC BY-SA 2.5,
11958,7429,0,can you please be more specific about what you are doing exactly? I think you are missing something here as @dsimcha mentions.,2011-02-21T06:23:05.913,1307,CC BY-SA 2.5,
11959,7412,1,"@Jeromy thanks, posted http://getthedata.org/questions/399/where-can-i-get-real-data-of-big-network-topology",2011-02-21T06:43:05.823,3328,CC BY-SA 2.5,
11960,7,5,You might like http://getthedata.org/ a question and answer site dedicated to finding data sets,2011-02-21T07:02:17.727,183,CC BY-SA 2.5,
11961,7382,3,"@robin girard, I think the key here is that for non-stationary processes the theoretical correlation varies with time, when for the stationary processes theoretical correlation stays the same. So with sample correlation which basically is one number, it is impossible to capture the variation of true correlations in case of non-stationary processes.",2011-02-21T07:09:28.057,2116,CC BY-SA 2.5,
11962,7433,2,"@Shane, in general it is useful to check out how other tools are approaching the same problem. It is the same as reading different books on the same topic, every author has its unique perspective, combining them gives you better understanding. Yes it is a slow process, but if you want to trily understand something this is  in my opinion the only way.",2011-02-21T07:15:56.253,2116,CC BY-SA 2.5,
11963,7428,0,@Staty Despair Consider to use R with meta or rmeta packages. It's much easier than do it from scratch.,2011-02-21T07:41:13.827,609,CC BY-SA 2.5,
11964,7425,0,"I don't know of a practical algorithm that fits according to that quality measure. You probably still want to use K-Means et al. But if the above measure breaks down, you know that the data you are looking at is not (yet!) suitable for that algorithm.",2011-02-21T07:43:41.793,2860,CC BY-SA 2.5,
11965,7425,0,@Andrej I don't use R (coming from ML rather than stats :) but what mbq suggests seems fine.,2011-02-21T07:45:37.580,2860,CC BY-SA 2.5,
11966,7364,0,"@gappy, I reviewed the Wooldridge book and found nothing about factor models, could you please provide page number?",2011-02-21T08:25:16.307,2116,CC BY-SA 2.5,
11971,7418,0,@Casey I've added a note about the `gamlss` package which might fit the bill in R also.,2011-02-21T10:27:25.863,1390,CC BY-SA 2.5,
11972,7439,0,Please provide the example of what you want to achieve. What exactly you do not get?,2011-02-21T10:32:20.313,2116,CC BY-SA 2.5,
11973,7439,3,Here is [my blog post](http://vzemlys.wordpress.com/2010/01/06/time-series-data-aggregation-using-r-and-reshape/) with example of using `melt` and `cast`. There conversion from wide to long format is done at one stage. There really isn't anything more special.,2011-02-21T10:35:43.057,2116,CC BY-SA 2.5,
11977,7447,0,"@mbq, Thanks for the edits. They are much appreciated. As you can probably see, I am new to statistics.",2011-02-21T12:01:21.157,3292,CC BY-SA 2.5,
11978,7440,1,sorry for posting the incorrect answer in the first place. I just looked at $x-\mu_1$ and immediately thought that the integral is zero. The point that it was squared completely missed my mind :),2011-02-21T12:02:03.823,2116,CC BY-SA 2.5,
11980,844,3,"@svadali it is is more usual to use ratio here since the distribution of ratio of chi square is tabulated (Fisher's F). However, the problematic part of the question (i.e. the dependency between $X$ and $Y$) is still there whatever you use. It is not straightforward to build up a test with two dependent chi square... I tryed to give an answer with a solution on that point (see below).",2011-02-21T12:19:39.930,223,CC BY-SA 2.5,
11981,7439,0,"Welcome to stats.  You might find it helps to include a small, reproducible dataset in your question to explain what you want.  Read http://www.sigmafield.org/2011/01/18/three-tips-for-posting-good-questions-to-r-help-and-stack-overflow for more.",2011-02-21T12:45:34.063,114,CC BY-SA 2.5,
11982,7428,0,"@Andrej No experiance with R, unfortunatelly. But I believe that this analysis cannot be done by using the standerd R rmeta pachages, because the dimention of spatial distribution has to be taken into account. Nevertheless, I already have the pooled effect size, my question is how to calculate the standard errors of the sdm values (=effect size) to use them to explore for publication bias, by doing the funnel plot and the Egger test.",2011-02-21T13:28:40.033,3333,CC BY-SA 2.5,
11983,7386,2,I think that behind the score there is also a constant will of @whubert to create and generate constructive connexions and exchanges. impressive !,2011-02-21T13:34:01.960,223,CC BY-SA 2.5,
11985,7346,0,"I guess ""contribute"" is not strong enough, maybe 24h/24 devotion is close to reality? really of great value to the community.",2011-02-21T13:49:52.970,223,CC BY-SA 2.5,
11986,7428,0,"@Staty Despair: I think we reached a point where you need to contact the authors. This looks promising: ""For this analysis, we extracted SDMvalues [...] for each study and calculate the standard error of SDM values based on sample size of patient and controls groups"" [(Bora et al 2011)](http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6TC2-5243NDH-1&_user=2875156&_coverDate=02%2F06%2F2011&_rdoc=1&_fmt=high&_orig=search&_origin=search&_sort=d&_docanchor=&view=c&_acct=C000056617&_version=1&_urlVersion=0&_userid=2875156&md5=5023f799be7dee0cb0e0a7f1d651f930&searchtype=a).",2011-02-21T14:14:47.170,307,CC BY-SA 2.5,
11988,7448,1,+1 Nice summary.  I like that you point out the role of the loss function first.,2011-02-21T14:21:41.357,919,CC BY-SA 2.5,
11990,7400,6,"What does ""similar"" mean?  The chi-squared test and the KS test, for example, test whether two histograms are close to *identical.*  But ""similar"" might mean ""have the same shape,"" *ignoring any differences of location and/or scale.*  Could you clarify your intent?",2011-02-21T14:32:42.663,919,CC BY-SA 2.5,
11991,7410,1,"You are correct that the KS test is not appropriate for histograms when it is understood as a hypothesis test about the distribution of the underlying data, but I see no reason why the KS *statistic* wouldn't work well as a measure of sameness of any two histograms.",2011-02-21T14:34:30.337,919,CC BY-SA 2.5,
11992,7401,3,Subtracting the mean changes the null distribution of the KS statistic.  @David Wright raises a valid objection to the application of the KS test to histograms anyway.,2011-02-21T14:36:12.940,919,CC BY-SA 2.5,
11994,7428,0,"@Bernd Unbelievable, but I've already conducted the authors asking for definition on exactly the same line you suggest, from Bora et. al paper...",2011-02-21T14:45:06.740,3333,CC BY-SA 2.5,
11998,7452,0,"Unfortunatly this paper does not say anything about the truncated distribution..I found some packages in R that deal with Zipf estimation parameter in a simply way (zipfR, VGAM) but the truncated distribution need a ""special treatment"". With your last sentence did you mean that it is possible to model a power-law dataset with an e.g. exponential distribution and then apply some estimation parameter process for ""truncated"" exponential distribution? I'm very newbie in this topic!",2011-02-21T16:27:00.653,3342,CC BY-SA 2.5,
11999,7452,0,"In the paper, the authors re-analyses different data sets where a power-law has been fitted. The authors point out that in a number of cases the power-law model isn't that great and an alternative distribution would be better.",2011-02-21T16:29:04.590,8,CC BY-SA 2.5,
12001,7450,0,"to be clear, what precisely are you right-truncating? The distribution of values or the Zipf plot itself? Do you know the truncation point? Is the truncation an artifact of the data or an artifact of the *data processing* (e.g., some decision that you or the experimenter made)? Any additional details would be helpful.",2011-02-21T17:00:06.460,2970,CC BY-SA 2.5,
12003,7450,0,"@cardinal. (part 1/2) Thanks cardinal. I will give more details:
I have a VoIP generator that generates calls following the Zipf (and other distribution) for volume per caller. I have to verify that this generator really follows these distributions.
For Zipf Distribution i need to define the truncation point (hence it is known and it refers to the distribution of values) which is the maximum number of generated call by the user and the scale parameter. In particular in my case this value is equal to 500, that indicates that one user can generate maximum 500 calls.",2011-02-21T17:20:37.527,3342,CC BY-SA 2.5,
12004,7450,0,"(part 2/2) The other parameter to set is the scale parameter for Zipf that defines the spread of the distribution (this value in my case is 0.9). I have all the parameters (size of sample, frequency per user, etc.) but i have to verify that my dataset follows the zipf distribution.",2011-02-21T17:21:08.260,3342,CC BY-SA 2.5,
12006,7458,1,Side note: I think it works this way for essentially all modern spreadsheet programs - I know it does for OpenOffice.org Calc and Google Docs' spreadsheet.,2011-02-21T17:40:24.353,2898,CC BY-SA 2.5,
12007,7450,0,"so you're apparently renormalizing the distribution by $\sum_{i=1}^{500} i^{-0.9}$, since for, what I would think of as a ""truncated Zipf"", a scaling parameter of 0.9 would be impossible. If you can generate lots of these data and you ""only"" have 500 possible outcomes, why not just use a chi-square goodness-of-fit test? Since your distribution has a long-tail, you may need a pretty large sample size. But, that would be one way. Another quick-and-dirty method would be to check that you get the right empirical distribution for *small* values of the number of calls.",2011-02-21T17:40:51.560,2970,CC BY-SA 2.5,
12010,7459,0,"+1, great answer as usual. You should nominate yourself as a moderator, there is still 1 hour left :)",2011-02-21T18:20:46.940,2116,CC BY-SA 2.5,
12011,7459,0,"@mpiktas, thanks for the compliments and the encouragement. I'm not sure I could justify nominating myself with the already very strong slate of candidates who have, uniformly, participated more extensively and for longer than I have.",2011-02-21T18:49:01.287,2970,CC BY-SA 2.5,
12012,7458,2,Good answer--but it (and the original question) belong on a different site.,2011-02-21T19:04:08.493,919,CC BY-SA 2.5,
12013,7458,2,"On a windows machine, you can toggle through the ""lock down"" methods with the F4 key.",2011-02-21T19:11:46.640,696,CC BY-SA 2.5,
12014,7098,0,"JMP is one of the nicest stats applications out there. Statisticians should definitely learn how to use it, if only to impress their clients. It's expensive, but cheap if you're a student or staff member at a school/college/uni",2011-02-21T19:33:10.370,74,CC BY-SA 2.5,
12016,7459,0,"@cardinal, here are some links to alternative to Hill's estimator: [original article](http://www.springerlink.com/content/t411403874770047/) by Paulauskas and follow-ups by [Vaiciulis](http://www.springerlink.com/content/4388079545358pn2/) and [Gadeikis and Paulauskas](http://www.springerlink.com/content/f372367226545j25/). This estimator supposedly had better properties than original Hill's.",2011-02-21T20:08:46.873,2116,CC BY-SA 2.5,
12017,7466,0,this looks like multiple change-point problem. I found this [link](http://genome.jouy.inra.fr/ssb/preprint/SSB-RR-12-cart.pdf) which might be helpful. I hope someone will provide more details.,2011-02-21T20:14:44.023,2116,CC BY-SA 2.5,
12018,7466,0,"@mpiktas, thanks, it seems that ""change-point"" is the term that I was looking for.",2011-02-21T20:27:56.343,255,CC BY-SA 2.5,
12019,7182,0,I am really happy it was helpfull for you.,2011-02-21T20:46:48.777,1808,CC BY-SA 2.5,
12021,7216,2,here is a 5-part video series: http://vancouverdata.blogspot.com/2010/11/text-analytics-with-rapidminer-loading.html,2011-02-21T20:52:35.120,74,CC BY-SA 2.5,
12022,7468,0,The change-point algorithms seem to be able to determine the optimal number of clusters. Is there any way to determine when to stop before single point is left?,2011-02-21T21:37:00.577,255,CC BY-SA 2.5,
12025,6542,10,"I disagree, Jaynes' book is a terrible recommendation in this circumstance: 1) the notation is sloppy and non-standard, which makes it difficult to cross reference with other sources, 2) he's long winded and gets bogged down in silly and irrelevant arguments (the OP asked for the ""shortest route"") 3) there's also the errors (such as the marginalization paradox)",2011-02-21T22:56:01.087,495,CC BY-SA 2.5,
12026,6543,3,+1 All of Statistics: it would be a great place to start.,2011-02-21T23:20:17.930,495,CC BY-SA 2.5,
12027,7459,0,"@mpiktas, thanks for the links. There are quite a few ""new-and-improved"" versions of the Hill estimator. The main drawback of the original approach is that it requires a choice of ""cutoff"" on where to stop averaging. I think mostly that's been done by ""eyeballing"" it which opens one up to charges of subjectivity. One of Resnick's books on long-tailed distributions discusses this in some detail, if I recall. I think it's his more recent one.",2011-02-21T23:30:42.820,2970,CC BY-SA 2.5,
12028,7465,1,"this seems like a bit of an odd list. To my knowledge, Carnegie Mellon is the *only* school on that list that (formally) offers an undergraduate degree in statistics. Neither Caltech nor MIT even have graduate programs in statistics.",2011-02-22T00:07:37.110,2970,CC BY-SA 2.5,
12030,7465,0,"@cardinal. why must you doubt me? :) I put in links to the undergrad stats courses at those fine institutions. Also, mixing and matching courses from the best schools will beat out following a degree path from a worse school.",2011-02-22T00:28:44.013,74,CC BY-SA 2.5,
12031,6538,5,I think it's almost always important to develop domain-specific expertise as well. A lot of statistics is learning the models relevant to specific fields.,2011-02-22T00:30:14.467,493,CC BY-SA 2.5,
12032,7428,0,"@Bernd I don't close the question until receiving author's answer. As I am waiting, I am thinking constructing an alternative funnel plot, as I cannot calculate standard error, and I start a new discussion on this.",2011-02-22T00:35:16.453,3333,CC BY-SA 2.5,
12033,7465,2,"OCW is certainly a very fine resource and a great initiative. This is no doubting that. As for your assertion that mixing and matching from the ""best schools"" is a superior solution, I find that highly suspect, particularly for undergraduate studies. While a highly motivated student is bound to get a very good undergraduate education at any of those schools, an undergrad education as good or better can be found at many, many ""worse"" schools. Schools such as those that you list do tend to ""win out"" for graduate education, I would say.",2011-02-22T01:40:18.837,2970,CC BY-SA 2.5,
12034,7476,0,what percent of your studies lack an estimate of SE? Have you considered using any of these transformations? http://stats.stackexchange.com/q/2917/1381,2011-02-22T02:53:29.950,1381,CC BY-SA 2.5,
12036,7481,1,"A few questions: Is $y$ univariate? Then you have a multiple logistic regression, not a multivariate logistic regression. How are you doing the factor analysis? While there are methods applicable to ordinal variables that are analogous to factor analysis for continuous manifest variables, it isn't clear to me what you're using. Finally, what would you consider to be an inappropriate use of factor analysis? If you look on it as a dimension reduction technique (turning 11 variables into 2) then there's nothing inherently wrong with it. Whether it's a good idea or not is another question.",2011-02-22T05:07:38.477,26,CC BY-SA 2.5,
12037,5816,0,"@nico, I think @cespinoza implied programing something relevant to the hot topic re-sequencing application (a.k.a. High-Throughput sequencing, Next Generation Sequencing). Blast would not work for that. You want to look at something like [Mosaik](http://bioinformatics.bc.edu/marthlab/Mosaik), [SOAP Aligner](http://soap.genomics.org.cn/soapaligner.html), [ABySS](http://www.bcgsc.ca/platform/bioinfo/software/abyss), [Bowtie](http://bowtie-bio.sourceforge.net), [BWA](http://bio-bwa.sourceforge.net), or [Maq](http://maq.sourceforge.net)",2011-02-22T05:15:51.240,1794,CC BY-SA 2.5,
12038,7472,4,"Well, I'm pretty much in love with this answer.  I would like the emphasize that, in following point 4, you should keep the checks simple - there are often little incongruities in real data that your entry-folks should be able to enter but which you won't be able to foresee.  For example, I make sure that dates are entered as dates; but I've stopped having hard rules about date ranges, and instead check that with reports, and follow up out-of-bounds values with whomever did the data entry.",2011-02-22T05:42:56.830,71,CC BY-SA 2.5,
12039,7472,5,Preventing someone from entering a value that they believe to be right can introduce more error than detecting and investigating it.,2011-02-22T05:43:52.553,71,CC BY-SA 2.5,
12041,5816,0,"@Aleksandr: of course I cannot read the mind of cespinoza... but aligning 25bp to an existing genome sounds a lot like checking where some primers bind to the DNA to me... But, again, I may be wrong... I haven't done more than a handful of PCRs in the last few years, and I moved my interests to other fields of biology ;)",2011-02-22T06:21:27.403,582,CC BY-SA 2.5,
12043,7477,1,"@cardinal, ex-Soviet universities have separate undergraduate statistics studies. In Vilnius University for example you can get bachelor degree in statistics. From what I see with the students I wholeheartedly agree that master or even doctorate-level education is needed for interesting jobs.",2011-02-22T07:13:29.853,2116,CC BY-SA 2.5,
12044,7474,4,your definition for harmonic mean does not agree with [wikipedia](http://en.wikipedia.org/wiki/Harmonic_mean),2011-02-22T07:20:14.997,2116,CC BY-SA 2.5,
12045,7472,0,"+1, great answer. I agree with Matt, I too love this answer:)",2011-02-22T07:51:46.290,2116,CC BY-SA 2.5,
12046,7259,0,"you might want to ask new question, not update the old one. Also consider asking it in stackoverflow.com, since the update asks question about R.",2011-02-22T08:00:21.537,2116,CC BY-SA 2.5,
12049,7487,0,Please use proper sentences and explain the problem in more detail. I've no idea what you are trying to do based on this description.,2011-02-22T09:19:45.343,159,CC BY-SA 2.5,
12050,7487,0,please read [faq](http://stats.stackexchange.com/faq) on how to ask the questions. As it stands now the question is unreadable and not understandable.,2011-02-22T09:21:28.050,2116,CC BY-SA 2.5,
12055,7477,1,"@cardinal, @mpiktas 4 years in BS + 2 yearts in MS + 4 years in PhD makes ten years to learn something interesting :) I would give $+\infty$ to this great answer if possible. Most books are new to me.",2011-02-22T10:41:27.303,2645,CC BY-SA 2.5,
12056,7459,0,"@cardinal, thank you very much, you are very kind and very detailed! Your example in R was very useful for me, but how can I perform a formal chi-square test in this case? (i used chi-square test with other distributions like uniform, exponential, normal, but i have many doubts about the zipf..Sorry but this is my first approach to these topics). 

Question to modetators: have i to write another Q&A like ""how perform chi-square test for truncated zipf distribution?"" or continue in this Q&A maybe updating tags and title?",2011-02-22T10:49:27.847,3342,CC BY-SA 2.5,
12057,7459,0,"@Maurizio: In general, you should only edit your question to help clarify details - not ask new questions. This avoids answers referring to past questions. So ask a new question.",2011-02-22T11:45:01.640,8,CC BY-SA 2.5,
12059,7491,2,(+1) Good that you mentioned the use of Factor scores directly (and their correspondence with raw scores under certain conditions).,2011-02-22T13:23:26.243,930,CC BY-SA 2.5,
12061,7459,0,"@Maurizio, in principle, chi-square testing in the truncated Zipf case is even easier than for a continuous distribution since you don't need to make any decisions regarding binning, as the data are already discrete. The observed counts are $O_i = n d_i$ and the expected counts are $E_i = n p_i$, where I've calculated the $p_i$ values for you in the $R$ code. From there all you have to do is plug and chug. Hope that helps.",2011-02-22T13:44:40.977,2970,CC BY-SA 2.5,
12062,7459,0,"@cardinal, If I could I would offer you a pint of beer! In this afternoon I'll try to do some tests and i'll report my results here. Thank you very much!",2011-02-22T14:08:46.673,3342,CC BY-SA 2.5,
12063,7483,2,"Playing devil's advocate, one ""otherwise"" is that academics need to publish papers and get grants to be awarded tenure or to be promoted.  Also methods are often in fasion and get used more than their true value would suggests (until the next bandwagon comes along).  The majority of papers published in machine learning probably do not represent an improvement on standard methods, such as kNN or LDA or logistic regression.",2011-02-22T14:39:02.947,887,CC BY-SA 2.5,
12064,7472,1,"@Matt Good points, both of them.  I completely agree.  Concerning the first one, a good approach is to test the data entry procedures on a small representative subset of the data and thoroughly go over all the issues that arise.  This won't address everything that can possibly come up, but it identifies most major issues early on and lets you deal with them effectively.",2011-02-22T14:44:16.703,919,CC BY-SA 2.5,
12065,7483,1,"@Dikran, yes you are right. But in the book I linked the overview is given in terms of practical use. Not everybody from academia publishes articles solely for purpose of promotion or tenure.",2011-02-22T14:45:53.700,2116,CC BY-SA 2.5,
12067,7493,4,"+1 This answer is a good read due to its clarity, authority, and consistently helpful focus on responding to the questions.",2011-02-22T14:53:15.737,919,CC BY-SA 2.5,
12068,7483,0,"@mpiktas, not soley, no (I was just being cynical ;o)  It is important to realise though that the increment in performance introduced by most new methods over traditional techniques tend to be rather modest and often do not translate from evaluation on benchmark problems to real world applications.  ISTR David Hand wrote a good paper on this a few years ago, and it is well in accord with personal experience (e.g. linear SVMs and ridge regression tend to give very similar performance, with neither dominating the other - try writing a grant proposal to investigate ridge regression!).",2011-02-22T14:57:14.593,887,CC BY-SA 2.5,
12069,7484,2,"@mpiktas This is a nice start and provides some guidance when the CV is low.  But even in practical, simple situations it is not clear that the CLT applies.  I would expect reciprocals of many variables not to have finite second or even first moments when there's any appreciable probability their values could be close to zero.  I would also expect the delta method not to apply due to the potentially large derivatives of the reciprocal near zero.  Thus it could help to more precisely characterize the ""simple applications"" where your method might work.  BTW, what is ""D""?",2011-02-22T14:58:46.180,919,CC BY-SA 2.5,
12070,7494,0,This [article](http://onlinelibrary.wiley.com/doi/10.1002/bimj.200410103/abstract) will certainly clarify matters for you. Unfortunately access is restricted.,2011-02-22T14:58:57.770,2116,CC BY-SA 2.5,
12071,7484,0,"@whuber, D is for variance, $DX=E(X-EX)^2$. By simple applications I meant the ones for which variance and mean of reciprocal exists. As you say for random variables with appreciable probability that their values could be close for zero, reciprocal may not even have mean. But then the answer to original question is no. I assumed that the OP asked whether it is possible to calculate standard deviation when it exists. It clearly does not for a lot of random variables.",2011-02-22T15:22:11.847,2116,CC BY-SA 2.5,
12072,7484,0,"@whuber, BTW out of curiosity $DX$ is pretty standard notation for me, but one might say that I come from Russian probability school. It is not so common in ""capitalistic West""? :)",2011-02-22T15:23:55.900,2116,CC BY-SA 2.5,
12073,7484,0,"@mpiktas I have never seen this notation for variance.  My first reaction was that $D$ is a differential operator!   The standard notations are mnemonic, such as $Var[X]$.",2011-02-22T15:31:37.967,919,CC BY-SA 2.5,
12074,7496,0,"While said F-test is relevant for univariate multiple regression (one predicted variable), I could not find information about its use in multivariate multiple regression (many predicted variables analyzed simultaneously).",2011-02-22T15:36:04.433,1909,CC BY-SA 2.5,
12076,7503,2,That agrees with my subjective prior belief that there's no such thing as a non-informative prior.,2011-02-22T15:58:44.177,449,CC BY-SA 2.5,
12077,7501,0,"Thanks ! your ""short answer"" perfectly points out an interesting existing literature.",2011-02-22T16:07:10.073,223,CC BY-SA 2.5,
12078,7500,1,"Your answer is very nice, but you don't actually state what a ""non-informative prior"" and how it differs from a ""vague prior"";)",2011-02-22T16:16:17.160,8,CC BY-SA 2.5,
12079,7498,0,this begs the question of would the uniform be truly non-informative? what would be the max and min of the non-informative uniform? Would the prior be non-informative after transformation to another scale?,2011-02-22T16:16:47.020,1381,CC BY-SA 2.5,
12080,7503,0,@onestop please consider contributing to my CW answer,2011-02-22T16:17:36.830,1381,CC BY-SA 2.5,
12081,7094,3,"I don't think this works. Assume $p_i=1/n$ and $p_{ij}=0 \forall i \ne j$. This is a valid combination of probabilities, realized when $I$ is a uniform random variable $\in\{1,...,n\}$ and $X_I=1$ and all $X_j=0 \forall j\ne I$. Still the formula above would be 0 for all events. Still thanks for helping!",2011-02-22T16:20:52.273,,CC BY-SA 2.5,user3152
12082,7503,0,"This is precisely the issue addressed in the paper I included in my answer ($IG(\epsilon, \epsilon)$ with $\epsilon \rightarrow 0$. The WinBUGS documentation used them long after these priors were discredited.",2011-02-22T16:34:12.403,26,CC BY-SA 2.5,
12083,7498,1,"Uniform priors are not necesarily un-informative and vice versa, it depends on the nature of the problem.  I think the idea of a minimally informative prior is often more realistic, using MAXENT and transformation groups to decide on a prior distribution that conveys the minimum amount of information consisten with known constraints (e.g. that the prior should be invariant to scaling).  For many problems, improper priors work well, so there isn't necessarily a need for there to be a well defined minimum and maximum.",2011-02-22T16:37:51.487,887,CC BY-SA 2.5,
12084,7506,1,"Gelman himself has moved beyond ""noninformative"" priors, see eg http://www.stat.columbia.edu/~cook/movabletype/archives/2007/07/informative_and.html and the paper I reference in my answer, and other works of his. This attitude seems increasingly prevalent among applied statisticians.",2011-02-22T16:38:17.110,26,CC BY-SA 2.5,
12085,7500,0,"@cgillespie: You're right, edited :)",2011-02-22T16:39:06.420,26,CC BY-SA 2.5,
12086,7506,0,"@JMS Gelman et. al. 2003 made this point, and I have tried to summarize it, they seem to consider the noninformative priors for background / theoretical / heuristic purposes",2011-02-22T16:43:02.327,1381,CC BY-SA 2.5,
12087,7500,0,"following from your last paragraph and supporting the quixotic nature of the search: if we know that we know *nothing* about the parameter in question, then we know *something* about it.",2011-02-22T16:46:10.763,1381,CC BY-SA 2.5,
12088,7500,0,"Quite! The great irony is that many who spend so much energy hand-wringing over their choice of prior don't think twice before throwing down the likelihood. A misspecified likelihood is much more liable to cause troubles, generally. But that's another question entirely...",2011-02-22T16:53:15.853,26,CC BY-SA 2.5,
12089,7484,1,"@whuber, but D is mnemonic also, it stands for dispersion. In Russian and in Lithuanian term for variance is dispersion. I tend to use $Var$, but today old habit probably kicked in, also D is shorter to write:) It is a relief that I did not write $M$ instead  of $E$. If I remember correctly it is still used in schools in Lithuania. $M$ stands for mathematical expectation, which for some strange reason was translated to mathematical hope in Lithuanian.",2011-02-22T16:53:55.773,2116,CC BY-SA 2.5,
12091,7506,0,"Yeah, a textbook on Bayesian statistics certainly can't ignore them. I only meant to add that the trend in applied statistics since then has been to move away from these traditional choices, although work continues on ""objective Bayes""",2011-02-22T16:57:12.773,26,CC BY-SA 2.5,
12092,7508,0,"I hope you don't mind, but I converted your question into latex. One thing, should $Z_{\alpha}$ not be $Z_{\alpha/2}$",2011-02-22T17:04:57.850,8,CC BY-SA 2.5,
12093,1311,2,"@svadali I did not replace 365 by $n$; I replaced it by 2, which gives a simple problem we can check by hand.  At any rate, Step 2 has a typo and the reasoning in Step 3 is incorrect.  In fact, the approach is flawed because it does not consider various complications that arise with multiple ""collisions,"" such as two people sharing one birthday and another two sharing another birthday.  I get the value of 6/16 (four people, two possible birthdays, at most two per birthday) because of the 16 possible configurations, 6 have two people share one birthday and the others sharing the other birthday.",2011-02-22T17:06:38.430,919,CC BY-SA 2.5,
12095,7484,0,"@mpiktas I suspected ""D"" was mnemonic.  In the same manner you often see $\Sigma$ and $\sigma$ (Greek S and s, short for ""standard deviation"") frequently used for sd in the statistical literature.  This is a fairly universal convention.  The story about ""M"" is funny: by that reasoning, *every* symbol used in mathematics ought to begin with an ""M""!",2011-02-22T17:10:05.133,919,CC BY-SA 2.5,
12096,7477,0,"I had a look at Boyd & Vandenberghe, Convex Optimization as it appears to be available online for free, and I found it a bit concerning. It had no mention of any of the following phrases: ""conjugate gradient"" ""BFGS"" ""Broyden""(of BFGS). Those are standard modern methods, so I am confused why they would not be at least mentioned.",2011-02-22T17:22:53.117,1146,CC BY-SA 2.5,
12097,7508,0,If it is clear for folks I don't mind at all. You are right should be a 2-sided alpha.,2011-02-22T17:35:38.483,,CC BY-SA 2.5,user712
12098,7477,2,"@John Salvatier, you're correct that those methods aren't covered in this text. Then again, this strikes me as more a matter of taste, particularly since the text's main focus is not on algorithms. To wit, your concerns are directly addressed by the authors in the introduction (pg. 13).",2011-02-22T17:49:21.950,2970,CC BY-SA 2.5,
12099,7477,0,"@John, My personal interpretation of the intent of this text is to provide the machinery necessary to *recognize* a problem as falling within the class of convex optimization problems rather than develop the algorithmic tools necessary to solve them (though some of this *is* presented) safely. Perhaps an appropriate analogy can be found in the standard treatments of linear regression.",2011-02-22T17:56:31.977,2970,CC BY-SA 2.5,
12100,7273,1,"Thank you for your answer. I had one further question about the `clusplot()` function. What is the relationship between the PCA values, and the clustering groups? I've read elsewhere about the link between kmeans and PCA, but I still don't understand how they can be displayed on the same bivariate graph. (Perhaps this should be a new question in itself).",2011-02-22T18:32:45.457,2635,CC BY-SA 2.5,
12102,7511,1,"What is a ""measurement""?  Is it (a) a determination that the array has detected a signal; (b) a determination *for each pixel in the array* as to whether or not it has detected a signal; (c) an estimate of total number of photons reaching the array; (d) an estimate of photons reaching *each pixel* (*i.e.*, an image); or (e) something else?",2011-02-22T18:40:15.290,919,CC BY-SA 2.5,
12103,7511,0,"@whuber: We are primarily interested to know (a) if we measured a signal, but at a later stage would be interested in (c) as well.",2011-02-22T18:43:03.220,56,CC BY-SA 2.5,
12104,7479,0,"Thanks a lot for the answer. It was really helpful. Indeed HIV had an effect on the treatment group as well as some other predictors and so i am running separate models for the two groups which includes these other predictors too.However, in discussing my findings,do i concentrate more on the model with the treatment group as my aim is to evaluate the effect of HIV on birthweight of infants? I am using SPSS 19.",2011-02-22T18:47:17.583,,CC BY-SA 2.5,user3355
12105,7479,0,"I wrote: ""Given that you observe different coefficients for HIV, you will need to run another model which includes an interaction effect..."". I would report the findings of the interaction model. However, reporting the findings of two separate models (control and treatment), can be more instructive, esp. for an audience with less statistical background.",2011-02-22T18:50:52.403,307,CC BY-SA 2.5,
12106,7508,0,What are $\theta R$ and $\psi R$ ? Should they be $\theta_R$ and $\psi_R$?,2011-02-22T19:02:59.400,449,CC BY-SA 2.5,
12107,6542,1,"@Dikran Marsupial, do you own the Schervish text on statistical inference? I've been on the fence regarding whether to purchase it or not, so was curious, since you appear to align yourself pretty strongly with the Bayesian approach.",2011-02-22T19:07:45.720,2970,CC BY-SA 2.5,
12108,6542,1,"I wouldn't say I was strongly aligned to the Bayesian approach.  It is the approach I understand the best, which is not the same thing.  Essentially I am an engineer at heart, and I want both tools in my toolbox, maintained in good order!  A proper understanding of the benefits and disadvantages of each approach is what we should aim for.  I haven't got Shervishes book, but I did read a paper of his on Bayes factors that seemed quite flawed to me (I'll see if I can find it and post a question for someone to explain it to me!).",2011-02-22T19:24:13.370,887,CC BY-SA 2.5,
12110,7479,0,"You generate a new variable which is the product of HIV and treatment ($x_1 \times x_2$). Then, include your new variable as well as HIV ($x_1$) and treatment ($x_2$) in your model: $... = ... + b_1x_1 + b_2x_2 + b_3x_1x_2$. However, you seem to have less experience with this kind of analysis. Don't get me wrong but may I suggest that you contact a local statistician or other person who can help you.",2011-02-22T19:39:10.930,307,CC BY-SA 2.5,
12111,7511,0,"Is this a series of discrete measurements or is the array constantly being monitored?  In other words, can we treat this as a hypothesis test or is it a control chart/changepoint/signal detection problem?",2011-02-22T20:11:21.147,919,CC BY-SA 2.5,
12113,7511,0,@whuber: We take measurements at singular points in time.,2011-02-22T20:16:53.020,56,CC BY-SA 2.5,
12115,7273,0,That new question wouldn't be clear without its context :),2011-02-22T20:53:28.923,2902,CC BY-SA 2.5,
12116,7515,3,Linking 1-D CDFs makes me think of [copula](http://en.wikipedia.org/wiki/Copula_%28statistics%29)s. Not sure if they'd be any use to you.,2011-02-22T20:59:29.337,449,CC BY-SA 2.5,
12117,7273,2,"Actually, the PCA values and the clustering groups are independent. PCA creates ""new"" coordinates for each observation in `mydata`, that is what you actually see on the plot. The shape of the points is plotted using `fit$cluster`, the second parameter of `clusplot()`. Maybe you should take a deeper look into PCA. Let me know if this helped you, or if you further references.",2011-02-22T21:01:23.390,2902,CC BY-SA 2.5,
12118,6963,0,"Are you writing your program in C, MATLAB, R, ...?",2011-02-22T21:10:06.260,2902,CC BY-SA 2.5,
12119,7518,0,Tip: next time use the button with binary numbers (or indent it with 4 spaces) to make the markdown engine interpret the code as code.,2011-02-22T21:24:13.580,,CC BY-SA 2.5,user88
12120,7516,3,+1 Nelsen is quite readable.  I bought a copy a few years ago even after going through a lot of the online materials.,2011-02-22T21:36:45.360,919,CC BY-SA 2.5,
12121,7273,1,"It helps (in the sense that I am honing in on my problem!). How is `fit$cluster` related to the PCA ""coordinates""? I think I understand how PCA works, but as I understand it, each Component cannot be explained using variables from the original data (rather it is a linear combination of the raw data), which is why I don't understand how it can be related to the clusters.",2011-02-22T22:15:35.453,2635,CC BY-SA 2.5,
12122,7519,0,Have you read the Wikipedia article?  http://en.wikipedia.org/wiki/Jeffreys_prior,2011-02-22T23:12:13.597,919,CC BY-SA 2.5,
12123,7519,3,"Yes, I had looked there. perhaps I am missing something, but I do not feel that the Wikipedia article gives an adequate answer to my questions.",2011-02-22T23:23:07.033,3347,CC BY-SA 2.5,
12124,7484,2,"The paper ""Inverted Distributions"" by E. L. Lehmann and Juliet Popper Shaffer is an interesting read regarding distributions of inverted random variables.",2011-02-22T23:27:32.710,530,CC BY-SA 2.5,
12125,7456,0,"I found a link in MathWorld, about margin of error: http://mathworld.wolfram.com/MarginofError.html",2011-02-23T00:22:14.793,,CC BY-SA 2.5,user3341
12126,7456,0,"@K-1 Good find.  Note the ""additional qualification"" that is needed: there is no consensus on what a ""margin of error"" really is.  Notice, too, that the Wikipedia article offers *three* definitions: http://en.wikipedia.org/wiki/Margin_of_error#Definition .  Regardless, in simple situations (like your least squares fit) the ""margin of error"" ultimately is computed from an estimate of the sampling variance, as shown in my sample code.  Its square root, the sampling standard deviation, is often multiplied by some constant.",2011-02-23T00:25:40.560,919,CC BY-SA 2.5,
12127,7428,0,"@Berndt @Andrej Thank you both for the contribution. The answer is that SDM value could be treated as a SMD. Accordingly, SE could be calculated from the (also suggested by Cochrane) formula SE(SMD)=sqr[N/(n1*n2)+(SMD^2)/(2*(N-3.94))], where SDM=SDMvalue and N=n1+n2. Then, of course, precision can be calculated as precision=1/SE. Nevertheless, I am still not convinced that SDM value could be treated as SMD. Why is a SMD?",2011-02-23T01:14:11.593,3333,CC BY-SA 2.5,
12128,7456,0,"1- By the way, it looks that the calculated error margin has no relation with ""Confidence Level"". For example if you add the option ConfidenceInterval->0.9 to the LinearModelFit there would be no change in the root mean square error.",2011-02-23T01:43:50.380,,CC BY-SA 2.5,user3341
12129,7456,0,"2- Instead of using LinearModelFit to calculate the constant in the fit, I suggest to use NonlinearModelFit. It enables you to define the pattern of fit.",2011-02-23T01:49:20.313,,CC BY-SA 2.5,user3341
12130,7476,0,"@David All included studies lack an estimate of SE, SD, means, CI due to the nature of the analysis. See http://stats.stackexchange.com/questions/7426/eggers-test-in-spss for the description of the problem. Thanks for the suggested transformations.",2011-02-23T02:08:14.580,3333,CC BY-SA 2.5,
12131,7427,0,"I see... Egger's test of the null hypothesis that intercept b=0 (or tests the null hypothesis that there is no funnel plot asymmetry). In this case the regression line will run through the origin. If the intercept b deviates from zero, the intercept b provides a meassure of asymmetry. The larger the interceptor's deviation from zero point, the larger the asymmetry. The two-sided p-value should be reported. (Synopsis from the book Publication bias in meta-analysis: prevention, assessment and adjustments. Po avtorjih A. J. Sutton). Thanks.",2011-02-23T02:34:56.353,3333,CC BY-SA 2.5,
12133,7523,2,Could you provide more information on the context and what you would do with the weighted composite?,2011-02-23T03:38:26.893,183,CC BY-SA 2.5,
12134,7484,0,"Thank you all for your prompt replies. I appreciate it. But if I have a finite sample set, is it possible to calculate variance or SD for harmonic mean of that set.",2011-02-23T04:46:31.827,,CC BY-SA 2.5,user3353
12135,7529,1,[This post](http://www.biostat.wustl.edu/archives/html/s-news/1999-07/msg00212.html) by Bill Venables explains well the issues involved in this kind of analysis.,2011-02-23T07:07:30.883,2975,CC BY-SA 2.5,
12136,7521,3,"using logs sometimes helps. Log-log models are popular in econometrics, because they are easy to interpret, that is another bonus of using them.",2011-02-23T08:23:10.523,2116,CC BY-SA 2.5,
12137,7533,0,"this returns the time of processes there operation was used not the estimated time of operation. Thank you for trying.

component timeMax timeMin timeAverage   timeSD samples
1       op1     100      30    62.33333 27.98333       6
2       op2     100       0    52.00000 42.61455       4
3       op3     100       0    16.66667 40.82483       1
4       op4     100       0    42.66667 47.42433       3",2011-02-23T10:21:40.560,3376,CC BY-SA 2.5,
12138,7533,0,"How do you define the ""time of operation"" if it's not the time of the processes where the operation is used? Is it the time of the process divided by the number of operations used in that process or...?",2011-02-23T10:32:14.237,3377,CC BY-SA 2.5,
12139,7531,0,"The first step is grouping of simmilar cases and calculating min,max,count,average and SD for all of them.
The second is to make subtraction of this (grouped) records.",2011-02-23T10:44:13.153,3376,CC BY-SA 2.5,
12140,7512,3,You may need to provide more information. What do you mean by correlate? What are n and N?,2011-02-23T12:38:44.877,495,CC BY-SA 2.5,
12143,7510,0,"(+1) I have (re)read your paper, which looks pretty interesting, although I found amazing the use of a Rasch model in clusters of genes. Did you compare your results with a sparse PLS-DA approach?",2011-02-23T14:03:32.713,930,CC BY-SA 2.5,
12144,7428,0,"@Staty Despair: As far as I can see the SDM papers have been published in good journals. So, *I* would trust the authors; of course, you should not trust me because I have absolutely no idea about this SDM stuff. Do you understand how the SDM is actually computed?",2011-02-23T14:08:51.277,307,CC BY-SA 2.5,
12146,7456,0,"@K-1 (1) It all depends on what you are looking for confidence in.  This is yet another reason why ""margin of error"" is so vague.  You are better off computing an actual confidence interval (for a parameter or a prediction) and calling it that.  (2) NonlinearModelFit is not what you specified in your question: you explicitly wrote down a linear model.",2011-02-23T14:48:47.253,919,CC BY-SA 2.5,
12147,7428,0,"@Bernd No, I ve never seen the SDM value formula. Actually SDM software is something like a black box for me. I enter the data, I take the results and trying to do the interpretation according to the tutorials, manual and published papers. But no much info available about the actual calculations taking place...",2011-02-23T15:26:07.650,3333,CC BY-SA 2.5,
12148,7517,0,@whuber (+1) I think there's a typo with the closing `]` around function args list (should be before `:=`).,2011-02-23T15:39:31.390,930,CC BY-SA 2.5,
12149,7517,0,"@chl You have great eyes!  However, this was cut and pasted from working code.  *Mathematica* is flexible about where the criteria (following ""/;"") need to appear.  In my experience they are most reliably interpreted when they all appear outside the arguments list, as shown here, even though in some cases that can slow down execution slightly.",2011-02-23T15:46:56.330,919,CC BY-SA 2.5,
12150,7428,0,"@Staty Despair So, you have no idea about ""voxel-based neuroimaging studies""? I mean I have no idea about ""voxel"" or ""neuroimages"". But since you are working with this kind of data, I assume that you have at least a basic understanding. Are you preparing a publication/thesis/whatever?",2011-02-23T15:51:06.947,307,CC BY-SA 2.5,
12151,7273,2,"You almost got it :) `fit$cluster` is unrelated to PCA. What `clusplot()` does is to plot the points using the ""new"" coordinates and label them using `fit$cluster`. I got '+' for cluster 3, 'o' for cluster 1 and a triangle for cluster 2. The function `clusplot()` is useful to visualize the clustering.",2011-02-23T15:52:02.390,2902,CC BY-SA 2.5,
12152,7517,0,"@whuber Indeed, it works if there's no carriage return before `/;` -- my fault!",2011-02-23T15:55:27.137,930,CC BY-SA 2.5,
12153,7517,0,"@chl No, you're right.  I had changed the carriage returns to fit the pasted version within the space allotted.  It turns out *Mathematica* objects when a carriage return precedes `/;` but not if it follows `/;`.  (I call that bad parsing, but that's another matter.)  I have changed it so that anyone who pastes this code wholesale back into *Mathematica* will encounter no error messages.",2011-02-23T16:01:58.100,919,CC BY-SA 2.5,
12154,7273,0,"What do you mean by the ""new"" coordinates and how can they be related to `fit$cluster`? I had thought that the coordinates of PCA1 and PCA2 are completely unrelated to `fit$cluster` This is the part that confuses me, I think. Thank you for your patience in explaining this!",2011-02-23T16:13:20.530,2635,CC BY-SA 2.5,
12155,7542,11,"You have demonstrated the calls are *not* uniformly distributed.  The $\chi^2$ calculation is not ""spoiled"": it worked!",2011-02-23T16:23:53.523,919,CC BY-SA 2.5,
12156,7474,0,Using exponentials is a good approach to understanding the problem.,2011-02-23T16:28:16.903,919,CC BY-SA 2.5,
12157,7542,0,"ok, mmm..maybe is there a ""conceptual"" error during the implementation of the generator? because for my thesis, my supervisor asked me to demonstrate formally that the generator (which is the result of months&months of study and they use for a long time) really follows the uniform distribution. Maybe can I use a sort of ""approximation""?",2011-02-23T16:35:38.747,3342,CC BY-SA 2.5,
12158,7542,5,"What would you like to approximate?  Obviously you cannot validly demonstrate this generator produces uniform results: the data flatly contradict that.  The next step depends on what actions you contemplate.  Could the data be wrong?  Could they inaccurately reflect what the generator is doing?  Could they indicate an unexpected phenomenon?  Could the design of the generator be wrong?  You need to raise (and eventually address) questions like these.  The one question that is definitively settled, though, is the one you originally asked: these data are not uniform!",2011-02-23T16:45:21.180,919,CC BY-SA 2.5,
12159,7510,0,@chl Not yet; working on that.,2011-02-23T16:48:02.813,609,CC BY-SA 2.5,
12160,7542,0,"thanks, i think it is better to speak with who has implemented the generator before reaching hasty conclusions.",2011-02-23T16:52:52.510,3342,CC BY-SA 2.5,
12161,7273,1,"By the ""new"" coordinates I mean PCA1 and PCA2. You're right, they are completely unrelated to `fit$cluster` :)",2011-02-23T18:18:57.080,2902,CC BY-SA 2.5,
12162,7540,0,"Thank you for your answer and driving my attention to regressors. It would definetely help me to explain diferrent results. Anyway I decided to go with Lag=12, as it is used in Analysis_of_Financial_Time_Series by TSAY and Modeling_financial_time_series_with_s-plus by Zivot, Wang where stationarity was made by using logarithmic differention.  Thank you once more for finding time and answer my question in great detail. Good luck",2011-02-23T18:19:09.660,3378,CC BY-SA 2.5,
12163,2701,0,"As @ars points out there is the notion of the ""truncated mean"" which is the area under the survival curve. Michael Berry discusses it and also mentioned as ""Mean survival restricted to time L"" at the following: http://support.sas.com/resources/papers/proceedings10/252-2010.pdf",2011-02-23T18:22:07.893,2040,CC BY-SA 2.5,
12165,7273,1,"But if they are unrelated, how can they be plotted on the same axes?",2011-02-23T18:32:43.533,2635,CC BY-SA 2.5,
12166,7545,0,"Yes, sort of. Was rewriting question while you answered. But I have ~1000 records and ~50 components in real task.",2011-02-23T18:36:15.953,3376,CC BY-SA 2.5,
12167,7545,0,"That doesn't seem like a problem, you can do simple linear regression with many more variables. Even non-linear regression is feasible I think.",2011-02-23T18:39:23.540,3369,CC BY-SA 2.5,
12168,7545,0,What is the best way to do this. I know how to solve simple quations on paper.. but do not know how to achieve this on computer for the scale I need. Could you tell me there should I search?,2011-02-23T18:49:11.033,3376,CC BY-SA 2.5,
12169,7545,0,"I think almost any decent software for numerical computations can do regression. You can even do it in Excel (also in Matlab, R, probably octave, python, etc.). I'd search for ""linear regression in X"", where X is your favorite environment. If you don't have a favorite, I'd recommend R, because it's free and powerful. Here's a [link](http://www.jeremymiles.co.uk/regressionbook/extras/appendix2/R/) to a tutorial on doing regressions in R. The specific R command you probably want is glm.",2011-02-23T19:08:12.210,3369,CC BY-SA 2.5,
12170,2917,0,"@Henrik for small $n$, the t-statistic is appropriate, and as you said, as $n$ increases, t approximates Z. See also http://math.stackexchange.com/q/23246/3733",2011-02-23T19:14:14.967,1381,CC BY-SA 2.5,
12172,7326,0,@robin I was talking to Mortimer :),2011-02-23T19:39:22.897,2965,CC BY-SA 2.5,
12173,7273,1,"Remember that `fit$cluster` is a categorical variable indicating the cluster assigned to each observation. It is used to define the labels of the points, that's how they are plotted.",2011-02-23T19:39:41.613,2902,CC BY-SA 2.5,
12174,7273,0,"So, am I correct in thinking that a point, with a cluster category is essential a linear combination, or vector, of the variables? Is the PCA just a rescaling of this vector? (Apologies if my terminology is incorrect). I have read a few papers which talk about the link between kmeans and PCA, which is perhaps why I'm wondering about this. (If I could vote your answer, up more than once I would!).",2011-02-23T20:02:08.457,2635,CC BY-SA 2.5,
12175,7547,7,"@Ralph The whole point of the comments following the question is that although the distribution of the data might ""resemble"" a uniform distribution, it is extremely unlikely that the data were truly obtained from this distribution.  That's why we use quantitative methods in statistics: they keep us from fooling ourselves (and others) with suggestive patterns or resemblances that are not justifiable.",2011-02-23T20:46:41.037,919,CC BY-SA 2.5,
12176,7544,0,"@user3386 This is a nice idea.  But the proposed analysis will draw many values from a normal distribution.  When the quantile of such a value exceeds (n-1/2)/n or is less than (1/2)/n, what data value would correspond to it?  There is no good answer to this with the information we have.  In short, extrapolation--which appears to be an essential need--is impossible with your proposal.",2011-02-23T20:51:09.920,919,CC BY-SA 2.5,
12178,5714,0,"In the meanwhile, I've added a bit more documentation on adaboost: http://packages.python.org/milk/adaboost.html so the above comment might be less valid than it was earlier.",2011-02-23T21:41:26.623,2067,CC BY-SA 2.5,
12180,7495,0,"Well, in many practical situations you more-less know the environment, so optimizing selection of a classifier makes sense and do help.",2011-02-23T21:57:03.543,,CC BY-SA 2.5,user88
12181,7482,0,Why do you call kNN nonparametric?,2011-02-23T22:00:06.303,,CC BY-SA 2.5,user88
12182,7523,0,"Hi Jeromy, I'm attempting to improve several models for horse racing ratings and want to introduce how successful a trainer is at a particular course. Eg if a trainer performance at Catterick was 6 wins from 48 = 13%  (6/48) then this is a fair reflection on on the percentage win rate. However, if the trainer won a single race at Exeter (1/1) this would be 100% and not a good measure. I could either a) Set as 0% for races under a certain denominator, b) create 2 variables percentage & denominator or c) what's advised here",2011-02-23T22:12:37.310,,CC BY-SA 2.5,user3373
12183,7551,0,Using the Hmisc:redun() function I can confirm that the variables are not redundant so long as I only include 10 of the 11,2011-02-23T22:27:14.873,3388,CC BY-SA 2.5,
12184,7551,0,I found another suggestion to change 'tol' to a value smaller than its default 1e-7. The results are quite similar to changing 'penalty' above. Changing it to smaller values reduces the 'Offending variables' list to only 'admityear2000',2011-02-23T22:27:53.297,3388,CC BY-SA 2.5,
12185,7273,0,":D Indeed, a point in the `clusplot()` is a linear combination of the original variables. Maybe the papers that you mention are about clustering after PCA, but that's definitely another topic.",2011-02-23T22:31:28.133,2902,CC BY-SA 2.5,
12187,7533,0,I had updated question and tried to explain that I mean.,2011-02-23T23:08:18.547,3376,CC BY-SA 2.5,
12188,7428,0,@Bernd it is the first time I am involved in the organization of a neuroimaging study. I dont feel familiar with these theories yet.,2011-02-23T23:12:04.497,3333,CC BY-SA 2.5,
12189,7545,0,"Maybe I am doing something wrong, but Statistica 8 complains about type of independent variables while trying to perform multiple linear regression analysis. Is this type of analysis is suitable for boolean (1/0) independent/predictor variables?",2011-02-23T23:43:59.763,3376,CC BY-SA 2.5,
12190,7428,0,@ Staty Despair I see! Good luck with your further analyzes!,2011-02-23T23:50:55.837,307,CC BY-SA 2.5,
12191,7545,0,The standard linear regression assumes the independent variables are continuous. It should work without a problem for the specific case when the only values that you observe are 0 and 1. The problem you have with the software might just be a question of casting to a different data type.,2011-02-24T00:16:46.623,3369,CC BY-SA 2.5,
12192,7545,0,A deeper issue is that a more elaborate model might take into account the binary nature of the variables and do it more efficiently or more accurately. But I don't know of any such standard models. So I'd try the standard regression first and look for alternatives only if that doesn't work.,2011-02-24T00:18:35.217,3369,CC BY-SA 2.5,
12193,7525,0,"""Weighting by the denominator itself may be reasonable"" - Please could you explain how to weight by denominator properly? I understand  your method of combining percentages but I don't think I can apply it in my data as the ""experiments""/or courses are different. Eg if trainer at Catterick won 6/48 = 13% races and same trainer won 1/1 = 100% at Exeter how should I adjust the percentage or introduce new variables for NN/Decision Tree/SVM predictors for a predictor named eg ""% success at todays course""?",2011-02-24T00:26:04.890,,CC BY-SA 2.5,user3373
12194,7555,1,"Your topic is interesting, but you ask so many questions about different aspects of modeling and meta-analysis, and the questions are so much of a judgment-oriented rather than a how-to nature, that I'm afraid you're not going to get them answered satisfactorily in this venue.  If you could narrow down....",2011-02-24T00:41:15.193,2669,CC BY-SA 2.5,
12195,7459,0,"@Maurizio, sorry for the delay in responding. My pleasure. How did things turn out?",2011-02-24T00:53:55.157,2970,CC BY-SA 2.5,
12196,7555,0,"@rolando2 I agree, but the topic is hot. Also there is no meta-regression tag available. I hope that someone would spend time to add a paragraph for reference!",2011-02-24T00:55:07.433,3333,CC BY-SA 2.5,
12197,6542,0,"@Dikran, Your (potential) question sounds interesting. I look forward to a post on it.",2011-02-24T00:56:21.503,2970,CC BY-SA 2.5,
12198,7555,1,"I modified your list of tags, hope you agree. Before I start answering your questions, I'd like to know how many data points you have.",2011-02-24T01:18:48.143,307,CC BY-SA 2.5,
12199,7557,0,"Thanks. I think is enough to test for normal distribution with a Kolmogorov-Smirnov test on the effect size. What do you mean ""the errors of the linear regression model have to be N(0,1)? Weighted regression? This means that I have to weight for SE my dependend variable? I've recently heard about this guy, Peters et al. 2006 JAMA wrote that they modified his test, Macascill's test, to produce the alternative Egger's regression. Unfortunatelly I don't have access to full text of Macascill et al. paper, but Peters et al modification does not include weighting as far as I understand their paper.",2011-02-24T01:25:57.570,3333,CC BY-SA 2.5,
12202,7557,1,"Peters et al (678) write: ""In preference to Egger‚Äôs regression test, we recommend a simple weighted linear regression with lnOR as the dependent variable and the inverse of the total sample size as the independent variable."" No, this does not mean that you have to weight your dependent variable. Just estimate a WLS regression model.",2011-02-24T01:34:49.287,307,CC BY-SA 2.5,
12203,7555,0,"@Bernd Twenty included studies, three independent variables, let's say 60 data points more or less, for the meta-regression.",2011-02-24T01:47:35.943,3333,CC BY-SA 2.5,
12204,7541,0,Thank you for the reference. However i currently do not have access to jstor. Thank you for the idea as well about partitioning the chi-square.,2011-02-24T02:32:12.413,3379,CC BY-SA 2.5,
12205,7553,0,"Assuming Outcome is an ordinal factor, I believe lmr() will treat it as such. At least the documentation for lmr() and lmr.fit() seem to indicate that it will be. <br />Assuming your categorical variable is set up as a factor, as suggested by DWin, and your response variable is set up as an ordered factor, the inability to estimate the model may be occurring because of separation. Basically, some categories of admityear may perfectly `explain' a particular outcome.",2011-02-24T04:35:23.990,3265,CC BY-SA 2.5,
12206,7543,0,"THANK YOU! Not because you answer is the solution to all my problems, but because you have put effort to communicate in a level on which the supplied information can become useful. A lot of times I find myself frustrated and at times borderline angry, when two parties clearly miss one another during exchange of information, and this thread has proven to be one for me. Regarding your suggestions, I'll try to look into the article, but I feel like this has gone on for too long, and in order to finish the software I'm gonna have to implement the analysis as I initially thought...",2011-02-24T09:18:19.027,3014,CC BY-SA 2.5,
12207,7543,0,and leave validation for further development. This way I can hopefully implement something that's acceptable both biologically and statistically (it's surprising how often they don't coincide),2011-02-24T09:19:55.203,3014,CC BY-SA 2.5,
12208,7545,0,"It does not work. At least with the raw data. It returns Beta,beta std err, B, B std err, t(2),p last two are about significance, Nor B, nor Beta nor their combination in $y=\beta*x+B$ maner give me the desired results. The most close results I got with ""breakdown table"" method (Statistica 8) - it calculated initial ranges for all variants.",2011-02-24T09:35:00.997,3376,CC BY-SA 2.5,
12210,7365,0,@mptikas. Neither Wooldridge nor I are claiming that the Hessian has to be positive definite everywhere. My claim is that for an interior maximum the empirical Hessian has to be positive semidefinite as a necessary condition of a smooth function reaching its maximum. Wooldridge seems to be saying something different.,2011-02-24T10:07:16.423,1393,CC BY-SA 2.5,
12211,7531,0,It looks like the desired method should be a **multiple regression** with one continuous dependent variable and multiple independent/predictor **binary** variables. Like logistic regression but vice-versa and multiple predictors.,2011-02-24T10:11:50.010,3376,CC BY-SA 2.5,
12212,7544,0,"@user3386 - this is not a very good idea if you want to know what's going on with your data.  It is for the data to tell you whether it is or isn't ""normal"".  The only time this would be a good idea is when it is useless - because the normal approximation already holds.  It's like you're ""telling nature how its gona be"".",2011-02-24T10:17:34.113,2392,CC BY-SA 2.5,
12213,7531,0,"Found article about [Multiple Regression with Categorical Variables](http://www.psychstat.missouristate.edu/multibook/mlt08m.html), but this technique requires coding of dummy variables, wich is not suitable in my case (>50 binary predictors and the number may extend in future).",2011-02-24T10:44:08.857,3376,CC BY-SA 2.5,
12214,6542,0,"I expect I am just missing something, but it is the paper by Lavine and Schervish ""Bayes factors: what they are and what they are not"".  The example 2 seems a bit odd as they use a Bayes factor in a situation where they have a well specified (and highly informative) prior on the hypotheses.  As a result it seems unsurprising that it behaves a little oddly given that the information (mu) is used in computing the Bayes factor, but then treated as unknown in the hypothesis test itself.  I like reading papers on what things actually mean though and being wrong is a good way of learning!",2011-02-24T11:25:58.867,887,CC BY-SA 2.5,
12215,7477,0,"I would also recommend Boyd and Vandenberghe, especially if it is available for free.",2011-02-24T11:27:58.210,887,CC BY-SA 2.5,
12216,7546,3,Check out this answer: http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models/5350#5350,2011-02-24T12:43:40.987,1979,CC BY-SA 2.5,
12217,7459,0,"@cardinal, I'll post a new answer with the results of my test!",2011-02-24T12:47:58.033,3342,CC BY-SA 2.5,
12218,7477,2,"@cardinal: Scandinavian universities usually do offer bachelors level degrees as well. That being said, I think statisticians take themselves a bit too seriously. I disagree that you'd need a doctorate degree in order to get an ""interesting"" job. I believe that as science and research becomes more and more cross-disciplinary statistics have been imposed on studies from many different areas. Half of the articles on high impact journals have some questionable statistical analysis, just to meet the demands, even though it might not make any sense, given in original context/domain of the problem.",2011-02-24T12:52:40.130,3014,CC BY-SA 2.5,
12219,7477,0,"[continuation from previous comment] ... I dont know if this is the case with the OP, but I can definitely relate to the dilemma of trying to fast forward, especially if you are done with your education. It's simply unrealistic to go through a long list of books, exercises and spend months and years to be able to do your job, which might not have anything to do with theoretical statistics... (I'm sorry if I sound too frustrated)",2011-02-24T12:53:02.780,3014,CC BY-SA 2.5,
12220,7477,0,"@posdef, I certainly empathize with your feelings of frustration. I take a little issue with some remarks that you've made that I suspect are mostly a product of this frustration. To respond to your comments completely would require an essay. Modern applied statistics is a mathematically and computationally sophisticated field which requires a fairly high degree of specialized knowledge. It strikes me as a bit presumptuous to want to become good/competent in the field without putting in the effort. Click on the link I provided for a more eloquent version of this.",2011-02-24T13:57:50.807,2970,CC BY-SA 2.5,
12221,7477,0,"As someone who originally came from another discipline, I understand the desire to learn quickly enough to be productive. Unfortunately, my experience is that that is difficult and potentially dangerous. What is your background? If someone with a B.S. In statistics came to you and said ""I need to know your field. Make me an expert as quickly as possible,"" what would you realistically suggest to them? When would *you* feel safe calling them an expert and handing them their own project that *your* business or livelihood depended on?",2011-02-24T14:03:17.123,2970,CC BY-SA 2.5,
12222,7477,0,"Like I said, I think a well-thought-out essay is needed and this is hardly a good format for it, unfortunately. The points you raise regarding poor statistical analysis in the literature are valid. My argument would be that this is often due to precisely the problem we are trying to address in this thread. There are many, *many* people doing statistics without fully grasping what they are doing. There are also too few statisticians that have sufficient domain expertise in the area of application they are working in.",2011-02-24T14:07:18.853,2970,CC BY-SA 2.5,
12223,7522,1,"Hong's response is correct (and I tested that the code works). You almost surely want to use the ExportMatrixToR module (2nd method), because they the 'a' object in R is a real R object, and not just a literal.",2011-02-24T14:12:12.067,2773,CC BY-SA 2.5,
12224,7553,0,Thanks so much to both of you. I learned about 5 new things already.  I'll add responses and lessons to my question,2011-02-24T14:46:04.810,3388,CC BY-SA 2.5,
12225,7365,0,"@Jyotirmoy Bhattacharya, I've updated my answer with more details, hope it explains my intuition which motivated the initial answer.",2011-02-24T14:57:44.337,2116,CC BY-SA 2.5,
12226,7543,1,"@posdef Best of luck.  I'm sorry to hear about the conflicts of biological and statistical acceptability you have encountered.  Any such conflict bothers me, because ""statistical unacceptability"" is tantamount to *unscientific.*  There has to be a resolution to this.  Maybe it needs better communication and understanding on both sides.",2011-02-24T15:11:15.610,919,CC BY-SA 2.5,
12227,7571,1,"I'm a bit confused, perhaps by the title. Are you wanting to interpolate between two time points or are you asking about something like smoothing (as opposed to filtering)?",2011-02-24T15:12:57.993,1764,CC BY-SA 2.5,
12229,7559,2,"The title of your question is asking a totally different question from what you really ask¬≠. I, for one, was curious about the real differences between the Linear SVM (let's take the hard margin SVM) and other linear discriminants.",2011-02-24T15:19:07.937,1320,CC BY-SA 2.5,
12230,7477,0,"@cardinal: While I mostly agree with you, what I find highly troubling is the assumption of ""interest to master"" the field of statistics. When I ask, or answer, a question on StackOverflow, I don't make assumptions on intentions to master anything, I only intend to learn to solve my problems. Likewise I don't assume people want to master the field of programming when they ask something there. Mastery, as you have mentioned, comes over time and effort of course. My experience SO vs CV and statistics vs programming have been bitterly different in terms of help/utility :(",2011-02-24T15:22:56.153,3014,CC BY-SA 2.5,
12231,7543,0,"Thnx. Regarding the quote: ""Any such conflict bothers me, because ""statistical unacceptability"" is tantamount to unscientific. There has to be a resolution to this. Maybe it needs better communication and understanding on both sides."" - I couldn't agree more...",2011-02-24T15:26:59.953,3014,CC BY-SA 2.5,
12232,7477,0,"@posdef, thanks for your comments. My response was tailored to the question of the OP who ""wanted the equivalent knowledge of a quality stats degree."" I read that as a desire for a certain degree of mastery. While programming requires a very significant degree of both logic and creativity, I would speculate that most of the questions on SO are about more direct implementation issues. CV is more geared towards helping others understand and apply statistics, which generally requires a deeper dialogue with the OP to do well (correctly) and involves addressing more subtleties. My $0.02.",2011-02-24T15:32:25.727,2970,CC BY-SA 2.5,
12233,7477,0,"@cardinal: ah, i guess you're right... I'm sorry for creating an off-topic discussion like this, and for being very bitter on the subject. I gotta work on that, somehow.",2011-02-24T15:48:50.743,3014,CC BY-SA 2.5,
12234,7557,0,"Do you mean I have to use ""SE"" as the Weight Variable in SPSS WLS regression model?",2011-02-24T16:34:47.180,3333,CC BY-SA 2.5,
12235,7557,2,"@Staty Despair: No, its in my answer: ""...are weighted by the inverse variance of the estimate..."", i.e. $w_i = \frac{1}{SE_i^2}$.",2011-02-24T16:40:11.830,307,CC BY-SA 2.5,
12236,3405,0,"+infinity.  I do bioinformatics and this one caused me a world of pain.  Happens in openoffice.org, too.",2011-02-24T17:09:44.557,1347,CC BY-SA 2.5,
12238,7365,0,"@mpiktas +1 for the idea, even though it seems there are problems with it.  I don't follow your first paragraph: it seems to be self-contradictory.  Maybe there's a ""not"" missing somewhere?  I am unable to reproduce the results of your example.  I do find that in many cases it is difficult to find the global minimum and, occasionally, that failure can lead to a Hessian with a small negative eigenvalue.  In thousands of simulations, using a non-derivative based minimizer in *Mathematica*, I always obtain positive definite Hessians.",2011-02-24T17:41:46.097,919,CC BY-SA 2.5,
12239,7308,0,"@Cardinal Good question.  I wonder whether there could be problems with $q$ that are not continuously second differentiable.  In short, we need to find violations of the hypotheses of the theorem (implying non-negative definiteness of $H$) because the validity of the theorem is not in doubt!",2011-02-24T17:47:27.070,919,CC BY-SA 2.5,
12240,7581,6,"""In statistics, an estimator is a rule for calculating an estimate of a given quantity based on observed data: thus the rule and its result (the estimate) are distinguished.""  (First line of the Wikipedia article http://en.wikipedia.org/wiki/Estimator ).",2011-02-24T19:15:43.340,919,CC BY-SA 2.5,
12242,7571,0,"@Wayne I'd like to interpolate between the two measurement times, but include information about the future measurement -- not just the previous measurements.",2011-02-24T19:34:12.233,3405,CC BY-SA 2.5,
12243,7545,0,"I ran regression in Matlab with the set of 7 procedures and four operations in your opening post. The results it gave are: [25 31 34 22] for the operation times, with confidence interval for the first operation time of [4, 47], and similar values for the others. Keep in mind that this is on a very small set. So my question is, do you get similar results and just want something different? E.g. it gives you a (probabilistic) confidence interval and maybe you want hard constraints instead. Or do you get something completely different?",2011-02-24T20:12:14.367,3369,CC BY-SA 2.5,
12246,7559,1,"I guess the main difference is the objective function they optimize. SVM tries to maximize the margin. Other linear classifiers do other things (e.g. perceptron optimizes the reconstruction error). These different objectives have different properties (e.g. maximizing the margin improves the generalization error). They also give you different problems to solve (some of which may be easier or more efficient, others can be solved online, etc.).",2011-02-24T20:54:53.110,3369,CC BY-SA 2.5,
12247,7465,2,"Actually, this was the first thing I tried. I tried this before posting the question. Finding a list of courses wasn't hard, but finding information about which books actually got used for those courses and what sections of those books were covered was much more difficult.",2011-02-24T21:02:10.263,2912,CC BY-SA 2.5,
12249,7586,1,You have nicely defined a *predictor.*  It is subtly (but importantly) different from an estimator.  The estimator in this context is the least squares formula used to compute the parameters 1 and 6 from the data.,2011-02-24T21:36:24.827,919,CC BY-SA 2.5,
12250,7581,0,+1 I am upvoting this question (despite the presence of a well formulated answer on an obvious Wikipedia page) because initial attempts at answering it here have pointed to some subtleties.,2011-02-24T21:38:36.773,919,CC BY-SA 2.5,
12252,7586,0,"Hmm, I didn't mean it that way, @whuber, but I think your comment illustrates an important ambiguity in my language that I didn't notice before. The main point here is that you can think of the generic form of the equation Y = mX + B (as used above) as an estimator, whereas the particular predicted values generated by specific examples of that formula (e.g., 1 + 6X) are estimates. Let me try to edit the paragraph above to capture that distinction...",2011-02-24T21:53:33.630,3396,CC BY-SA 2.5,
12253,7586,0,"btw, I'm trying to explain this without introducing the ""hat"" notation that I've encountered in most textbook discussions of this concept. Perhaps that's the better route after all?",2011-02-24T21:58:57.667,3396,CC BY-SA 2.5,
12254,7582,0,"+1 In addition, look at the sandwich package which works hand-in-hand with functions in the pscl package to provide appropriate estimation of model covariance matrices that account fot the lack of independence.",2011-02-24T22:12:50.663,1390,CC BY-SA 2.5,
12255,7586,2,"I think you have hit a nice medium between accuracy and technicality in your original answer: keep it up!  You don't need hats, but if you can manage to show how an estimator is distinguished from other, similar-looking things, that would be most helpful.  But please notice the distinction between *predicting* a value Y and *estimating* a parameter such as *m* or *b*.  Y could be interpreted as a random variable; m and b are not (except in a Bayesian setting).",2011-02-24T22:15:30.347,919,CC BY-SA 2.5,
12256,7586,0,"indeed, a very good point in terms of parameters versus values there. Editing again...",2011-02-24T22:24:22.110,3396,CC BY-SA 2.5,
12257,7597,0,Oh no. You beat me to it. I will delete my answer!,2011-02-24T23:50:24.067,1307,CC BY-SA 2.5,
12258,7592,0,+1: That's a really useful SO thread there. Pretty much reduces my question to a repost :P.,2011-02-24T23:53:13.757,1118,CC BY-SA 2.5,
12259,7594,0,This sounds an awful lot like a homework question to me. Could you provide some context?,2011-02-24T23:55:12.110,3396,CC BY-SA 2.5,
12260,7545,0,"Looks like I do something wrong. I get completely different results in Statistica. I had tried R for this: 
`d <- read.table(""data.csv"",header=T,sep="","")
r <- lm(time ~ p1+p2+p3+p4, data=d)
summary(r)
p1             2.000  
p2            25.000 
p3            22.000
p4            22.000 
`
Results are closer, but still strange. I think I did something wrong with lm function.",2011-02-25T00:17:32.300,3376,CC BY-SA 2.5,
12261,7418,0,"Accepting your answer because of the multiple suggestions for resources and functions that have improved my understanding. It seems like 'gamlss' would be a possible way to solve my issue, but because I'm actually a non-statistician, I don't currently have the background in math nor the time to open that can of worms right now (but maybe eventually I will). As mentioned in another comment, for my data at least, it seems that ignoring zero-truncation won't change my estimates and std errors much. For my intended audience, I believe a nbinomial GEE will do just fine. Thanks!",2011-02-25T00:32:43.493,3309,CC BY-SA 2.5,
12262,7594,0,"It isn't a homework question, but the original problem domain isn't hotels -- it is cloud computing. I am merely translating the problem. I am looking for pointers to techniques and approaches that can solve this question. Even -- can it be solved with the given data or is more data required (for example the initial occupancy rate).",2011-02-25T00:57:17.097,,CC BY-SA 2.5,user3418
12263,7600,0,"What kind of additional information? If we had the average occupancy rate, could we estimate the average length of stay? Or, if we had the average length of stay, we could estimate the occupancy rate?",2011-02-25T01:03:18.600,,CC BY-SA 2.5,user3418
12264,7600,0,"@Chiradeep: With three out of four pieces of information you can estimate  the fourth. Broadly speaking: the product numbers of rooms available times occupancy rate equals the number of rooms occupied each night, which in turn equals the product numbers of arrivals per day times length of stay.",2011-02-25T01:17:12.960,2958,CC BY-SA 2.5,
12265,7545,0,"It looks like what you did was ""regression with offset"". I.e. the function implicitly adds a column of all ones and tries to build a linear function with offset. I expect it wouldn't matter on your large dataset, but for a small dataset it skews the result. What you should try is ""regression through origin"" (i.e. without the offset). I believe the command is time ~ 0+p1+p2+p3+p4.",2011-02-25T01:17:50.370,3369,CC BY-SA 2.5,
12266,7545,0,Looks like the solution! Thank you very much!!! May I edit your answer to insert the R code?,2011-02-25T01:26:19.647,3376,CC BY-SA 2.5,
12267,7545,0,Sure. Glad it helped.,2011-02-25T01:31:46.157,3369,CC BY-SA 2.5,
12268,6556,1,This hasn't got anything to do with Bayes; it's just the factorization lemma and is a direct consequence of the definition of sufficiency. Bringing Bayes into the mix just complicates things.,2011-02-25T01:53:09.267,26,CC BY-SA 2.5,
12269,7594,1,"Imagine two jails with the same number of cells and same arrival rate (we can even say it's equal to the departure rate, so occupancy stays constant). One jail has all prisoners stay for a week. Another jail has 90% of the prisoners released same day (zero nights) and 10% stay for 10 weeks. Same arrival rate, same occupancy rate, different distribution of stay duration.",2011-02-25T02:03:07.943,2993,CC BY-SA 2.5,
12270,7318,1,"Are any of your DVs actually Poisson-distributed? I.e., are they counts of the frequencies of events over successive periods of time, and does the mean roughly equal the variance? If so, you may be able to use Poisson regression to test the interactions you're interested in.",2011-02-25T03:23:57.090,2669,CC BY-SA 2.5,
12271,7553,0,There may be separation when cross-classifying by two or more factors that doesn't show up on a simple predictor by Outcome tabulation. You would need to look at multi-way tables to find it.,2011-02-25T04:08:52.213,2129,CC BY-SA 2.5,
12274,7603,1,"In that last paragraph aren't you effectively arguing against any use of p-values?  Why, then, use a hypothesis test at all?  Why not just estimate the effect sizes and be done with it?",2011-02-25T06:18:55.400,919,CC BY-SA 2.5,
12275,7594,0,"@jonw Good example.  The difficulty is even more basic than that.  An extreme case is a city with so many hotels that nobody ever has to leave!  This makes it obvious that we need to know the average rate at which people depart hotels, or equivalent information.",2011-02-25T06:22:57.237,919,CC BY-SA 2.5,
12276,7606,0,Actually generalized Pareto (GP) is a limiting distribution for extreme data (extreme are characterize by all data which exceed a high threshold). A suitable threshold (model gives the good fit to data above this threshold) indicated by the stability of parameter estimates which is not so easy to determined by look at a graphical plot. I want to know whether there is any other way/test to find the best threshold (i.e the best data set for GP),2011-02-25T07:19:41.590,,CC BY-SA 2.5,user3423
12277,6556,0,"@JMS - Yes I do understand your ""factorisation"" theorem argument.  What I am saying is that using Bayes Theorem will 1) tell you if a sufficient statistic of reduced dimensionality exists, and 2) what the sufficient statistic is for your problem.  The use of ""sufficient statistics"" is basically a way to bring ""frequentist"" statistics"" closer to ""Bayesian statistics"" without admitting that one is doing so.  They also reduce the calculations one has to perform in an analysis.  Sufficiency is also closely related to the maximum entropy method (aka ""ultimate inference"").",2011-02-25T08:37:16.587,2392,CC BY-SA 2.5,
12278,7574,0,could you adapt your answer so that it matches the notation of the question?  To what is $x^2-y^2$ referring?  Where does this get inserted into the equations given in the question?,2011-02-25T08:44:57.643,2392,CC BY-SA 2.5,
12279,7605,0,"Not a ""proper"" answer, but if two variables are correlated, but you put them into different factors, then it does make intuitive sense that the two factors should also be correlated.",2011-02-25T08:48:11.260,2392,CC BY-SA 2.5,
12280,6978,0,"Are you able to calculate statistics for the data set? say for example the sum, or averages of data items?",2011-02-25T08:53:28.723,2392,CC BY-SA 2.5,
12281,7608,2,(+1) A very precise and detailed response.,2011-02-25T09:14:05.867,930,CC BY-SA 2.5,
12282,7605,0,How do you measure the increase in goodness-of-fit?,2011-02-25T09:15:48.220,930,CC BY-SA 2.5,
12283,7612,0,Thanks very much - very helpful. I will investigate `anova.coxph` too.,2011-02-25T10:10:41.713,3429,CC BY-SA 2.5,
12284,7610,0,"I think this is a duplicate of (http://stats.stackexchange.com/questions/3458/alternatives-to-classification-trees-with-better-predictive-e-g-cv-performanc). The only additional value of the question here could be ""criterions for selecting classifiers"" (which would make the question a very generic one). If it's a duplicate, vote for close, else vote for cw ;)",2011-02-25T11:02:47.133,264,CC BY-SA 2.5,
12285,7365,0,"@whuber, I rewrote the answer. Also I included the data, so it will be easier to reproduce. I also fixed the bug which was present in the previous version.",2011-02-25T11:25:07.093,2116,CC BY-SA 2.5,
12286,7610,0,"@steffen: Your referenced question is helpful, though, I think it's not a duplicate. Indeed my question is rather generic. I'm not looking for a solution to a particular problem but for general reasons why to use which learners - I'll update my question accordingly.",2011-02-25T11:36:22.170,2230,CC BY-SA 2.5,
12287,7571,0,"just to clarify, by `*` you mean the usual product or [convolution](http://en.wikipedia.org/wiki/Convolution)?",2011-02-25T12:49:48.750,2116,CC BY-SA 2.5,
12288,6028,2,Another excellent resource for learning about and understanding survival analysis is _Applied Longitudinal Data Analysis_ by Singer and Willett. The also give [example code/output for all their models using every stats package under the sun](http://www.ats.ucla.edu/stat/examples/alda.htm).,2011-02-25T13:06:32.423,1916,CC BY-SA 2.5,
12289,7613,5,"These are two separate questions. The answer to the second is [yes](http://en.wikipedia.org/wiki/Principal_component_analysis#Relation_between_PCA_and_K-means_clustering). The first one is too vague. First what is the magnitude of the vector? Also what kind of relationship you are looking for? Principal eigenvectors in PCA are the eigenvectors of either covariance or correlation matrix of original variables, so they are clearly related to correlations.",2011-02-25T13:13:41.247,2116,CC BY-SA 2.5,
12290,7613,2,"The wikipedia page is pretty verbose and should be sufficient for question #1, http://en.wikipedia.org/wiki/Principal_component_analysis .",2011-02-25T13:50:33.413,1036,CC BY-SA 2.5,
12291,7537,0,Thanks for the advice. I will definitely try to examine the predictions before deciding on the model,2011-02-25T14:41:57.447,1871,CC BY-SA 2.5,
12292,7539,0,"The intention is that i will eventually try to model this using Bayesian, but i was trying to understand how i can make a decision before fitting the models. If there is a possibility that ignoring the nested nature of the data messes things up, them i will try them GLMMs first. The only package for R that i am aware of that can do multilevel ZINB is glmmADMB. Would you recommend any other packages?",2011-02-25T14:48:21.983,1871,CC BY-SA 2.5,
12293,7616,4,"-1 Absolutely incorrect workflow for large p small n, FS overfitting is guaranteed.",2011-02-25T14:51:07.370,,CC BY-SA 2.5,user88
12294,7576,0,"Thanks for your answer. Indeed, i started thinking about whether different items could produce the 0s versus any other count and I actually think that there are a couple of my variables that would only explain 0s vs any other count.  So probably i should at least try ZINB first to see if my these variables work the way i would expect them to work.",2011-02-25T14:59:52.710,1871,CC BY-SA 2.5,
12295,7616,1,"isn't kNN a lazy learner rather than an eager one (as you don't actually do anything until you really have to when a pattern to classify comes along)?  Any pre-processing you do before applying the classifier is likely to have a larger effect on performance than the difference between classifiers; feature selection is especially difficult (easily leads to over-fitting), and methods like the SVM with regularisation usually perform better without feature selection.  I certainly wouldn't recommend neural networks, far too many potential pitfalls.",2011-02-25T15:28:04.487,887,CC BY-SA 2.5,
12296,7603,1,"I stop short of calling *p*-values completely useless.  I do advocate for the use of other methods, esp. when I hear people struggling with hypothesis testing and when they seem to have been taught such testing in the flawed way that I and many of us seem to have been taught.  I've commented on this elsewhere on the site so I won't repeat too much of my views here.",2011-02-25T16:04:53.950,2669,CC BY-SA 2.5,
12297,7620,0,Though not entirely uncommon for this type of thing to occur some people might like to see this as a teaching or learning case.  If you could share the data it would be great.  You also might get good feedback on your specific problem.,2011-02-25T16:23:48.110,601,CC BY-SA 2.5,
12298,7613,2,"I think I provide an answer to question 2 in an earlier thread, [Classification after factor analysis](http://stats.stackexchange.com/questions/2467/classification-after-factor-analysis/3475#3475).",2011-02-25T16:28:35.140,930,CC BY-SA 2.5,
12299,7622,1,"@mbq (+1) About class imbalance, we can still rely on stratified sampling during bagging.",2011-02-25T16:31:50.097,930,CC BY-SA 2.5,
12300,6028,0,"@M Adams Thanks for adding this link. Yes, the UCLA server is really full of useful resources.",2011-02-25T16:38:12.443,930,CC BY-SA 2.5,
12302,7629,1,"+1, nice analysis. I think that is why Wooldridge included the remark. I still think it is possible to think of some example where the hessian will be indefinite. Artificially restricting the parameter space for example. In this example the parameter space is whole plane, that is why the local minimum will give semi-positive hessian. I think the time has come to write a nice email to Wooldridge to get his take on the question:)",2011-02-25T18:00:24.267,2116,CC BY-SA 2.5,
12303,7622,3,"@mbq, doesn't make coffee? That's a deal-breaker right there.",2011-02-25T18:06:40.920,2970,CC BY-SA 2.5,
12304,7626,1,"Path analysis is an interesting idea, but I don't see your point about violation.  Usually in nonexperimental studies predictors are correlated to some degree.  Without that there's little point in using statistical control:  zero-order correlations of each X with Y will be the same as partial correlations.",2011-02-25T18:26:22.440,2669,CC BY-SA 2.5,
12305,7629,0,"@mpiktas Yes, I'm sure there exist problems where an interior *global* minimum has an indefinite Hessian, *yet where all parameters are identifiable.*  But it simply is not possible for the Hessian at a sufficiently smooth interior global minimum to be indefinite.  This sort of thing has been proven again and again, such as in Milnor's *Topology from a Differentiable Viewpoint*.  I suspect Wooldridge may have been misled by errant numerical ""solutions.""  (The typos on the quoted page suggest it was written hastily, by the way.)",2011-02-25T18:31:48.823,919,CC BY-SA 2.5,
12306,7605,0,"Hey chl - looking at RMSEA, SRMS, and BIC.",2011-02-25T18:46:40.623,3424,CC BY-SA 2.5,
12307,7614,0,"Thanks! So if I want to validate a orthogonal-rotated factor-loading with CFA (as I have done), it makes sense to validate that with a CFA model where the factors are uncorrelated. Since it's not a great fit, you are saying I should re-run my EFA using an oblique rotation, and then build a CFA model where the factors may correlate.",2011-02-25T18:48:56.940,3424,CC BY-SA 2.5,
12309,7619,0,Thanks M- Adams. In response to 1: I've noticed that the structures are nearly consistent. For 2) Thanks! I am validating against a sample (split my big set in half) 3) Are you suggesting a model where they may be a factor driving other factors? Thanks!,2011-02-25T18:52:41.067,3424,CC BY-SA 2.5,
12310,7311,1,I visited the link and read your praise of the decision tree.  Have you considered the way this method treats all main effects (save the first) as if they were interaction effects?  I have found that highly misleading and counterproductive on many occasions.,2011-02-25T19:06:24.353,2669,CC BY-SA 2.5,
12311,7629,0,"even at the boundary, hessian will be positive? I'll check out the book, I see that I really lack extensive knowledge in this area. Classical theorems are very simple, so I assumed that there should not be something else very complicated. That maybe one of the reasons why I had so much difficulty answering the question.",2011-02-25T19:19:19.713,2116,CC BY-SA 2.5,
12313,7631,0,"Do you by any chance mean that X2 is distributed as U[0, X1]?",2011-02-25T19:53:29.990,3369,CC BY-SA 2.5,
12314,7626,0,"@rolando not sure if this is appropriate, but PCA would provide orthogonal predictors",2011-02-25T19:56:12.297,1381,CC BY-SA 2.5,
12315,7629,0,"@mpiktas At the boundary the Hessian won't necessarily even be *defined*.  The idea is this: if the Jacobian/Hessian/second derivative matrix is defined at a critical point, then in a neighborhood the function acts like the quadratic form determined by this matrix.  If the matrix has positive *and* negative eigenvalues, the function *must* increase in some directions and decrease in others: it cannot be a local extremum.  This is what concerned @Jyotirmoy about the quotation, which seems to contradict this basic property.",2011-02-25T20:51:31.260,919,CC BY-SA 2.5,
12316,7633,0,Henry: log(X1) is after integrating (but before substituting limits) for marginal of X2. Your P(X2) is wrong. I believe you are integrating log(X1) that I said which we get after integration itself.,2011-02-25T21:23:53.250,,CC BY-SA 2.5,user1102
12317,7631,0,SheldonCopper: That's correct. I'll change it.,2011-02-25T21:24:27.090,,CC BY-SA 2.5,user1102
12318,7614,0,"yup, thats it. If you're going to be doing EFA and CFA on one dataset, you might want to look at splitting the data, running EFA on one half and CFA on the other, to increase replicability of the factor structure.",2011-02-25T22:04:59.190,656,CC BY-SA 2.5,
12319,7637,0,possible duplicate of [How to easily determine the results distribution for multiple dice?](http://stats.stackexchange.com/questions/3614/how-to-easily-determine-the-results-distribution-for-multiple-dice),2011-02-25T22:55:50.273,919,CC BY-SA 2.5,
12320,7631,1,The limits for the marginal of $X_2$ are not from 0 to 1 except when $X_2 = 0$.,2011-02-25T23:04:41.860,919,CC BY-SA 2.5,
12321,7633,0,"@Harpreet: testing using R, it is clear to me that $P(X_2 \le x_2) = x_2 (1-\log(x_2))$ is correct for $0 < x_2 < 1$.  I have also expanded the integrals to show how this is obtained.  So which $P(X_2)$ do you think is wrong?",2011-02-25T23:16:30.270,2958,CC BY-SA 2.5,
12322,7633,0,P(X2)=int(1/X1).,2011-02-25T23:21:56.227,,CC BY-SA 2.5,user1102
12323,7631,0,"Thanks whuber. You are correct. So, we have to substitute limits for marginal density of X2 as X1=X2 to X1=1.",2011-02-25T23:24:04.167,,CC BY-SA 2.5,user1102
12324,7633,0,"Thanks Henry. But I think what you are doing is correct, however marginal of X2 will be $ln(X_1)$ without limits.",2011-02-25T23:26:21.130,,CC BY-SA 2.5,user1102
12325,7640,0,"Okay.  I have a table of data 4300 (yeah, it's actually 4300) lines long.  It has 3 independent variables and one dependent variable.  I want to do multivariable linear regression to find how those three IVs affect the DV.  I also want to automatically generate 3-dimensional graphs of each slice of the three IVs (as in leaving one IV constant while using the other two as x and y, and the DV as z).",2011-02-26T01:46:04.533,,CC BY-SA 2.5,user3439
12326,7633,0,"$X_1 \le 1$ so $\ln(x_1) \le 0$, which means it cannot be a density or distribution function.  And I still think $X_1$ should not appear in the marginal distribution of $X_2$. http://en.wikipedia.org/wiki/Marginal_distribution says the same thing in ""The distribution of the marginal variables (the marginal distribution) is obtained by marginalizing over the distribution of the variables being discarded, and the discarded variables are said to have been marginalized out.""",2011-02-26T01:54:12.567,2958,CC BY-SA 2.5,
12327,7639,0,"Check out the ""loess"" and ""glm"" commands in R.  What do you mean by ""nonlinear"" regression?",2011-02-26T02:08:24.187,2817,CC BY-SA 2.5,
12328,7571,0,@mpiktas product.  One of the hazards of copying from code.  :)  I've corrected the post for clarity.,2011-02-26T02:39:28.043,3405,CC BY-SA 2.5,
12329,7626,1,"Hi David - it would, but I don't see any great need for orthogonality in this case, and with just 4 predictors, not much need for the data reduction that PCA provides.  There's also the risk that Y's relationship with the derived components would be less interpretable than relationships with the 4 original predictors would be.",2011-02-26T03:35:15.477,2669,CC BY-SA 2.5,
12330,6556,0,"""What I am saying is that using Bayes Theorem will 1) tell you if a sufficient statistic of reduced dimensionality exists, and 2) what the sufficient statistic is for your problem."" That has nothing to do with Bayes theorem, which is my point. Sufficiency is a property of the likelihood function and exists independently of whatever mode of inference you prefer.",2011-02-26T03:39:19.063,26,CC BY-SA 2.5,
12333,5926,9,"Taking a wild guess:  If zero correlation, you may as well conduct separate anovas and thus simplify your task. If very high correlation, you may as well conduct anova on just one of the Y variables since results will be largely the same for all the others.",2011-02-26T03:53:29.897,2669,CC BY-SA 2.5,
12334,6556,0,"Incidentally, the """"factorisation"" theorem argument"" isn't mine but a result which goes back to Fisher & Neyman and is in any intro stat inference text.",2011-02-26T03:55:12.253,26,CC BY-SA 2.5,
12335,7578,0,"Thanks for your answer. I am ""accepting"" the other answer as I'm allowed to pick only one and that seemed to fit my current situation slightly better. But your answer was very helpful too.",2011-02-26T04:06:10.050,1393,CC BY-SA 2.5,
12336,6556,0,"@JMS - I'm not disputing that you can't find sufficient statistics in other ways from Bayes theorem.  Fisher was, I believe, the one who actually coined the term ""sufficiency"".  But what I'm saying is that Bayes theorem is one of the easiest ways to *find* a sufficient statistic.  I would have a small bet that most ""intro stat text"" don't make much mention of Bayesian analysis, which is an essential tool for any good statistician.",2011-02-26T04:07:57.960,2392,CC BY-SA 2.5,
12337,7629,0,Thanks you both you and @mpiktas for the very nice analysis. I would tend to agree with you that Wooldridge is confusing numerical difficulties with theoretical properties of the estimator. Let's see if there are any other answers.,2011-02-26T04:22:02.543,1393,CC BY-SA 2.5,
12338,7639,3,4000 lines of data is not big.  Try doing analysis on a few million rows!,2011-02-26T04:29:42.720,2392,CC BY-SA 2.5,
12339,7645,0,"Yeah, I figured it out before you gave the answer :) ... Thanks.",2011-02-26T04:33:08.800,,CC BY-SA 2.5,user1102
12340,6597,0,"Am I too naive thinking that, _maybe_, discriminant analysis could be employed here?",2011-02-26T07:53:35.710,144,CC BY-SA 2.5,
12341,6028,0,Thanks for the great link to UCLA! I'll dig into it... ;),2011-02-26T08:48:50.513,2652,CC BY-SA 2.5,
12342,7645,0,$\log (1/x_2) = - \log (x_2)$ which is what I found,2011-02-26T09:29:43.077,2958,CC BY-SA 2.5,
12343,7617,0,This looks like exactly what I need. Can you explain how you got this expansion? I tried in a lot of ways and was unable to do that ...,2011-02-26T10:16:21.780,217,CC BY-SA 2.5,
12344,7574,1,"+1 Good points in the update, especially the last paragraph.  When the Hessian is available--as is implicitly assumed throughout this discussion--one would automatically use its positive-definiteness as one of the criteria for testing any critical point and therefore this issue simply could not arise.  This leads me to believe the Wooldridge quotation must concern the Hessian at a putative global minimum, not at a mere critical point.",2011-02-26T13:52:13.947,919,CC BY-SA 2.5,
12345,7653,0,Sounds like a term that is specific to that author.  If you have found otherwise please share what you have found elsewhere and show why other uses of the term do not clear things up.,2011-02-26T14:39:55.320,2669,CC BY-SA 2.5,
12347,7650,0,"+1 small multiples definitely seem like the way to go here. Also, maybe somebody could add the `data-visualization` tag to this?",2011-02-26T16:18:39.360,3396,CC BY-SA 2.5,
12348,7655,0,Tank you for this answer,2011-02-26T16:39:49.987,3019,CC BY-SA 2.5,
12349,7644,0,"the Wikipedia article itself provides you the answer to this question. If you assume normality of the ""true"" regressor, then you need further conditions on the distributions of the errors. If the true regressor is not Gaussian, then you have some hope. See [Reiersol (1950)](http://www.jstor.org/pss/1907835).",2011-02-26T17:07:43.710,2970,CC BY-SA 2.5,
12350,7644,0,"also, what do you mean by ""Solutions for just the intercept and slope are fine"". Those are your only two parameters! Or were you hoping to try to back out the ""true"" regressor as well?",2011-02-26T17:10:15.813,2970,CC BY-SA 2.5,
12351,7477,0,"@posdef, no worries. The discussion is interesting (to me) nonetheless. The best way to fight the feelings of frustration is to continue to learn when the opportunity arises. I, for one, am glad to have your participation here. Applied statistics as a field will only continue to grow through positive interactions with those interested in its applications. I would argue that in many cases the reverse is true as well.",2011-02-26T17:15:48.380,2970,CC BY-SA 2.5,
12352,7655,0,"@Marco: just bear in mind that I have not read the book, so I may be wrong! :)",2011-02-26T17:36:54.440,582,CC BY-SA 2.5,
12355,7642,0,"Hey I have `/path/to/results.csv` and `/path/to/command.r`, and it gives me `source(""/path/to/command.r"")
Error in file(file, ""rt"") : cannot open the connection
In addition: Warning message:
In file(file, ""rt"") :
  cannot open file 'results.csv': No such file or directory`",2011-02-26T18:18:27.490,,CC BY-SA 2.5,user3439
12356,7655,0,"I give you a +1 because this response really makes sense, even if you haven't read the book.",2011-02-26T18:30:21.127,930,CC BY-SA 2.5,
12358,7642,0,How can I fix the above?,2011-02-26T18:46:16.307,,CC BY-SA 2.5,user3439
12360,7659,0,@user3447 What is the intended use of this scale? Do you aim at comparing two or more groups (wrt. mean scores) or so?,2011-02-26T20:09:18.617,930,CC BY-SA 2.5,
12361,7658,0,"Thank you very much for such a nice answer! Can I ask a couple a words to explain further the sentence ""this is a way to reduce the chance of drawing incorrect conclusions from either good or bad data."" In that setting, what are good or bad data? Thx again!",2011-02-26T20:33:34.207,3019,CC BY-SA 2.5,
12365,7649,0,"Awesome comments, very helpful.

No, I agree with you, I wouldn't claim a connection between a given
seed and performance. I would have varied the starting seed though for
each *set* of runs.

I didn't know about the powercurve, sounds very useful, I'll check out
your link and also read up on it some more. I would not generalize the performance of my algorithms beyond the set
of problems used which I hope to be varied, i.e., unimodal,
multimodal, etc. Re running the algorithms to ""completion"" should I worry about
the starting seed? If so, why couldn't I just use the paired t-test in
that case?",2011-02-26T21:57:52.410,10633,CC BY-SA 2.5,
12366,7649,0,".. continued .. I.e., it took so many generations ""before"" and now so many
""after"" the changes made to the algorithm. 

I will have to think about the unpaired t-test approach - my stats
knowledge is not very deep (or complete in breadth, so I have found
this site to be quite useful).

Oh, I also plan to use this approach with PSOs as well down the line.",2011-02-26T21:59:05.780,10633,CC BY-SA 2.5,
12367,7654,1,"You could combine these into `=if(countif(a1:a20,""NO"")=0,""YES"",""NO"")`",2011-02-26T22:00:44.967,2958,CC BY-SA 2.5,
12368,7664,0,all probability measures assign measure one to the entire sample space (by definition). Is that what you're looking for?,2011-02-26T22:07:40.387,2970,CC BY-SA 2.5,
12370,7665,1,"thank you for repling, The questionnaire is measuring empathy, it has 33items that originally has 9 point likert scale range of which i reduced to a 4 point range. all the items and negative questions were kept the same.  i am looking into empathy score within 4 birth orders catogories. As i have made a change to the scoring of the scale i dont know if i need to run specific tests. because i am looking at birth order, the means for the questionnaire responses will be in the 4 catorgories, first born, middle, last and only.thank you for any help",2011-02-26T21:08:16.197,3447,CC BY-SA 2.5,
12372,7666,0,"@Henry, your definition of $\hat{\beta}$ doesn't make any sense to me. Are some ""hats"" missing?",2011-02-26T23:34:36.327,2970,CC BY-SA 2.5,
12373,7644,0,"@cardinal - I meant that I didn't particularly care about the two scale parameters, and as you say, the ""true"" regressor $X_{i}$.",2011-02-26T23:40:38.907,2392,CC BY-SA 2.5,
12374,7644,0,I see. That makes sense.,2011-02-26T23:45:25.610,2970,CC BY-SA 2.5,
12375,6543,1,"the UC-Davis program looks good and I think you'll get a great education there. I wouldn't consider it ""less rigorous"" than other places. I thought the comment on their ""integrated B.S./M.S. degree"" **[page](http://anson.ucdavis.edu/undergrad/bs-ms)** was interesting and relevant to the thread: ""There is a high demand for statisticians, but the knowledge and skill achieved by those with a BS degree in Statistics are often not sufficient for the needs in the [government or industrial] workplace.""",2011-02-26T23:57:24.227,2970,CC BY-SA 2.5,
12376,7666,0,It is mean to be the observed standard deviation of $\{y_i\}$ divided by the observed standard deviation of $\{x_i\}$. I'll change $\sigma$ to $s$,2011-02-26T23:58:03.273,2958,CC BY-SA 2.5,
12377,7664,0,"well, that is the reason this trick holds. I thought it might have a name",2011-02-26T23:58:19.263,3347,CC BY-SA 2.5,
12378,7667,0,I guess it does not have a name. thanks.,2011-02-26T23:59:10.027,3347,CC BY-SA 2.5,
12379,7666,0,"@Henry, can you clarify some of your comments? Something strikes me as being off based on your current description. Let $\hat{\beta}_{xy}$ be the slope assuming $y$ is the response and $x$ is the predictor. Let $\hat{\beta}_{yx}$ be the slope assuming $x$ is the response and $y$ the predictor. Then $\hat{\beta}_{xy} = \hat{\rho}s_y / s_x$ and $\hat{\beta}_{yx} = \hat{\rho} s_x / s_y$, where $\hat{\rho}$ is the sample *correlation* between $x$ and $y$. Hence the geometric mean of these two slope estimates is just $\hat{\rho}$.",2011-02-27T00:15:18.860,2970,CC BY-SA 2.5,
12380,7666,0,"@cardinal: No - when I see $x = by+c$ I mean the slope is $1/b$ since it can be rewritten as $y=x/b-c/b$. When you try to draw the two OLS lines on the same graph together with the observed points (e.g. with $y$ on the vertical axis and $x$ on the horizontal axis) you have to invert one of the slopes.  So I meant that you take the geometric mean of $\hat{\rho}s_y/s_x$ and $s_y/\hat{\rho}s_x$, which is simply $s_y/s_x$. Or, if you are unconventional enough to plot $y$ and $x$ the other way round for both lines and the observed points, then you get the inverse of that as the slope.",2011-02-27T00:39:50.350,2958,CC BY-SA 2.5,
12381,7649,0,"If running seed X on the same algorithm, but for different durations, you can use the paired t-test. But running seed X on two different algorithms, you cannot. The reason is that seed X might not interact with Alg.1 in the same way that it interacts with Alg.2. Paired t-tests are for when a result from one sample is related to a result from another sample. You cannot assume that seedX(Alg.1) and seedX(Alg.2) are related. But, you can test if they are, using a correlation coefficient, which is really easy to do.",2011-02-27T00:40:45.187,3443,CC BY-SA 2.5,
12382,7666,0,"@Henry - that's quite an interesting answer.  I don't necessarily doubt its validity, but one thing which does surprise me is that the correlation/covariance between $Y$ and $X$ is completely absent from the answer.  Surely this *should* be relevant to the answer?",2011-02-27T01:04:39.517,2392,CC BY-SA 2.5,
12383,7666,0,"@probabilityislogic: Not *completely* as I do use the sign of the correlation/covariance ;)  But the magnitude may just be noise.  In standard OLS, the noise is all assumed to be on the ""dependent variable"" so the predicted change in the dependent variable due to the change in the independent variable is reduced by the magnitude of the correlation.  But if you don't know where the noise is, or indeed which is dependent and which independent, then how can you decide whether to increase or decrease?",2011-02-27T01:22:33.933,2958,CC BY-SA 2.5,
12385,7632,0,"Thanks.  I was hoping to see a worked example, but the right key words to search on was helpful.  I think I've found a good description.  Now to apply it.",2011-02-27T02:19:49.500,3405,CC BY-SA 2.5,
12386,7573,0,Thank you for your excellent answer and references. I installed R to try metafor. More difficult than I was expected. There are many tutorials available but none for non-statisticians :-),2011-02-27T02:41:53.327,3333,CC BY-SA 2.5,
12387,7642,0,"Your question here is unclear. Are you having a problem reading in the results.csv file when you run the script? If so, you might double check that you point the read.csv command at the proper directory(e.g. `read.csv(""path/to/results.csv"")`). If it's something else with the script, maybe you could tell us what command(s) you're using to run it?",2011-02-27T03:16:43.450,3396,CC BY-SA 2.5,
12388,7594,0,"Good points on the distribution. In this case the stay duration is probably a ""long tail"" distribution.",2011-02-27T03:33:24.723,,CC BY-SA 2.5,user3418
12389,7666,0,"@Henry, thanks, that clarifies things a bit. Still, the methodology seems a little odd. I may think about this some more and ask another question or two.",2011-02-27T03:45:52.333,2970,CC BY-SA 2.5,
12390,7318,0,I will look into that. Any tips on where to read about Poisson regression - with examples? I'd nevertheless still like to hear about any more general non-parametric interaction tests.,2011-02-27T03:59:01.603,3285,CC BY-SA 2.5,
12391,7666,0,"@cardinal: To stress my original point, this is not the best way to approach the problem if you have any idea what might actually be going on.  It does dreadful things to residuals if the correlation is low.  But it is simple if you think you have absolutely no information. Plato is supposed to have said something like *ignorance is the root of all evil*.",2011-02-27T04:40:28.607,2958,CC BY-SA 2.5,
12392,6556,2,"You're missing my point entirely. Whatever mode of inference you prefer, sufficiency is simply a property of the likelihood function. Bayes theorem doesn't add anything at all. You *aren't* using Bayes theorem when you factor a likelihood! Nor does it add to the results that follow about minimal/complete/ancillary statistics. Deriving sufficient statistics doesn't require you to accept the principles of Bayesian inference; in fact it's intimately related to the ideas behind MLE.",2011-02-27T05:02:04.617,26,CC BY-SA 2.5,
12394,6556,1,"Incidentally, having read many of those textbooks on statistical inference I'd take your bet. Casella & Berger, Bickel & Doksum, Lehman & Casella, etc all at least discuss Bayesian parameter estimation if not some decision theory. Certainly it doesn't get the treatment it deserves but it's unreasonable to expect an introductory text to go too deep - and you can't become a statistician just on the back of them anyway. I'm an avowed Bayesian myself, but a solid understanding of classical statistics is important.",2011-02-27T05:12:01.930,26,CC BY-SA 2.5,
12397,7666,0,"@Henry, it seems to me that in the setting of this problem, the solution given is equivalent to assuming that $e_{y,i} = \beta e_{x,i}$ for each $i$. In an OLS setting, this method completely ignores the uncertainty in the data. It *pretends* that $y_i$ and $x_i$ are perfectly correlated and so assumes the data lie ***perfectly*** on a line, despite the fact that the data themselves contradict that. In either case, this strikes me as a very ***strong*** (and false) assumption.",2011-02-27T05:20:37.257,2970,CC BY-SA 2.5,
12398,7671,0,"Cool, I will read ""a new metric for probability distribution"" as soon as possible. Txh",2011-02-27T07:04:51.467,3019,CC BY-SA 2.5,
12399,2467,0,What software did you use to do FA?,2011-02-27T08:39:42.887,144,CC BY-SA 2.5,
12400,7658,0,"@Marco I guess by ""bad data"" the author means data that either departs from the initial design (missing value on covariates, lost to follow-up, etc.) or that present unexpected errors discovered afterwards (e.g., randomization to treatment errors, site-specific confounding effect) -- in this case, the statistician still has to cope with the available data. Remember that there is no substitute for good data (even machine learning cannot save us :-), but we must acknowledge the limitations of a particular statistical technique when faced with data of poor quality.",2011-02-27T09:04:10.323,930,CC BY-SA 2.5,
12401,7658,0,"Ok, noted! Thx!",2011-02-27T09:46:09.613,3019,CC BY-SA 2.5,
12402,7671,0,Thanks! I didn't realize that JSD itself is already analogous to dist**2,2011-02-27T10:53:33.940,2759,CC BY-SA 2.5,
12403,7573,1,@ Staty Despair: You might be interested in [Yet another short introduction to R with some emphasis on meta-analysis](https://github.com/berndweiss/r_meta-analysis_intro/raw/master/report/p_introToR.pdf).,2011-02-27T11:44:20.983,307,CC BY-SA 2.5,
12405,7643,0,"(+1) Please note that `nls()` is in the base `stats` package. There exist, however, the [nls2](http://cran.r-project.org/web/packages/nls2/index.html) and [nlstools](http://cran.r-project.org/web/packages/nlstools/index.html) packages on CRAN.",2011-02-27T12:05:21.310,930,CC BY-SA 2.5,
12406,7666,0,"@cardinal: No - the simple solution cannot make any requirement that the $e_{y,i}$ and $e_{x,i}$ are correlated.  It is discussed in section 2.4 of Gillard's historical overview linked in my original answer, and has the names ""Geometric mean regression"", ""Reduced major axis regression"", ""Standardized principal component regression"" or ""Ordinary least products regression"" in the literature, having been invented several times. I think the strongest criticism is that it can appear to provide a result where vagueness would be better, particularly when the observed correlation is weak.",2011-02-27T12:11:13.420,2958,CC BY-SA 2.5,
12408,2959,2,"@suncoolsu, that's not the typical definition of admissibility. The one you've given is (much) stronger. An admissible estimator is one that is *not* uniformly dominated, i.e., for every other rule against which it is compared, there is a value of the parameter for which the the risk of the present rule is (strictly) less than that of rule against which it's being compared. Conversely, an *inadmissible* estimator is one that is (weakly) dominated by *some* other estimator for *every* value of the parameter and is strictly dominated for *at least* one value by that same estimator.",2011-02-27T15:24:09.203,2970,CC BY-SA 2.5,
12409,7666,0,"@Henry, perhaps I'm having a slow last couple of days and you can find the flaw in my argument. If we fix $\hat{\beta} = s_y / s_x$, then we implicitly assume that the sample correlation between $x$ and $y$ is one. Now, this can **only** happen if $x$ and $y$ are (perfectly) linearly related. If we consider the model of this question, a simple substitution yields $y_i = \alpha + \beta x_i - \beta e_{x,i} + e_{y,i}$. The only way to get a perfect linear relationship between $x$ and $y$ is to take $e_{y,i} = \beta e_{x,i}$ for all $i$. Perhaps I'm being dense. Do you see a flaw?",2011-02-27T16:20:44.490,2970,CC BY-SA 2.5,
12410,7666,0,"@cardinal: You are thinking in OLS terms and seem to have $\hat{\beta}_{xy} = \hat{\rho}s_y / s_x$ fixed in your mind;  in OLS this minimises the sum of squares of vertical residuals, which is not appropriate here as the $x_i$ have errors.   I will put some R code in my answer to illustrate the impact.",2011-02-27T17:14:09.880,2958,CC BY-SA 2.5,
12412,7666,0,"@Henry, my argument is that they are equivalent. Independently of how one arrives at it, by making the choice $\hat{\beta} = s_y / s_x$, this is ***equivalent*** to assuming that your two observable variables $x$ and $y$ are perfectly correlated in an OLS framework. I'm, of course, basing this on the (somewhat limited) information in your post. I'll try to have a glance at the link and await your $R$ code.",2011-02-27T17:32:04.303,2970,CC BY-SA 2.5,
12413,7666,0,"@Henry, I took a quick look at your link. Notice that their (2.1) is a weak form of the relation that I noted above. That is, for unbiasedness (though I think they really meant consistency), $\sigma_y^2 = \beta^2 \sigma_x^2$ is required, which is equivalent to a second-moment version of my (stronger) requirement.",2011-02-27T18:10:04.317,2970,CC BY-SA 2.5,
12414,7666,0,"@cardinal: Gillard does mean *unbiased*. This is why I said in my original answer ""It could also be seen as equivalent to making an implicit assumption that the variances of the two sets of errors are proportional to the variances of the two sets of observations""",2011-02-27T19:27:03.123,2958,CC BY-SA 2.5,
12415,7681,13,Sounds like you're after some sort of [diversity index](http://en.wikipedia.org/wiki/Diversity_index),2011-02-27T19:29:50.627,449,CC BY-SA 2.5,
12416,7666,0,"@Henry, I'm not so sure about that. First, no model is specified for the latent variable in the surrounding text. Is it fixed (i.e., *functional* form) or random (i.e., *structural* form)? If the former, some hope remains (though I actually doubt this---but haven't done the calculation). If it's the latter, there is **no** way that I can see that $\tilde{\beta}_{\mathrm{GM}}$ would be *unbiased* for all possible distributions of the latent variable. Furthermore, the surrounding text and following math expression strongly hint that they intended to say ""consistent"" instead of ""unbiased"".",2011-02-27T20:51:24.690,2970,CC BY-SA 2.5,
12417,7666,0,"@Henry, your example is interesting. The constants are chosen ""perfectly"" to satisfy the only case in which the estimate could be consistent.",2011-02-27T20:58:27.760,2970,CC BY-SA 2.5,
12418,7308,0,"I wonder if this issue of ""definiteness"" is similar to the ""cholesky decomposition"" issue of taking negative square roots.  While the observed hessian is always PSD, this is only in *exact arithmetic* with infinite precision.  Perhaps rounding errors can make it negative definite in *floating point arithmetic* when it is ""close"" to being negative definite.",2011-02-27T21:10:00.887,2392,CC BY-SA 2.5,
12419,7609,0,"Thanks. Great support from a fellow adventurer in this world of statistics! ;) I'm currently reading Discovering Statistics using SPSS by Andy Field, which I'm surprised to enjoy (since it's a statistics textbook). I altered my COX analysis to measure survival on days instead of months, which luckily pushed my the significance of my 'risk' covariate below 0,05... :)",2011-02-27T21:18:43.003,2652,CC BY-SA 2.5,
12420,7666,0,"@cardinal: If you mean $2000/400=5$, then it was indeed deliberate.  Apart from that ratio, the constants were arbitrary. At the start of this overlong thread, all I was offering was a simple method with some basic properties.",2011-02-27T21:50:35.723,2958,CC BY-SA 2.5,
12421,7681,3,"+1 for diversity indices -- especially if you can figure out what kind of index would be meaningful to your particular audience/topic. You might also check out [GINI Coefficients](http://en.wikipedia.org/wiki/Gini_coefficient), which are used to measure economic inequality on a scale from 0-1. In your case, ""more diverse"" would be closer to the 0 end of the scale.",2011-02-27T21:57:35.000,3396,CC BY-SA 2.5,
12422,7676,0,"Assuming you've got a valid rationale for comparing the same people in the two conditions without a true control group, I'm not sure the correlation across bootstrap iterations is really something to be worried about in itself. It seems like you are, in effect, calculating CIs for the Average Treatment Effect on the Treated (no Wikipedia entry yet, sorry) and interpreting that as a meaningful outcome of interest. That said, I'm having a hard time following exactly what steps you took to estimate the differences between conditions. Is it a simple difference of means? Something else?",2011-02-27T22:17:43.193,3396,CC BY-SA 2.5,
12423,7686,2,Can you describe your problem a bit deeper?,2011-02-27T22:18:52.640,,CC BY-SA 2.5,user88
12424,7686,0,"Are you saying that Bayes' theorem $P(A=a|B=b) = P(B=b | A=a)\, P(A=a) / P(B=b)$ requires $P(B=b)$ to be positive, but when $B$ is a continuous random variable, for example Gaussian, then the probability that it takes any particular value is $0$?",2011-02-27T22:33:44.877,2958,CC BY-SA 2.5,
12425,2959,0,@cardinal Yup. You are right. I will correct it.,2011-02-28T00:11:54.190,1307,CC BY-SA 2.5,
12426,2959,0,@cardinal. Using math is much easier than simplifying it in plain English. But that is just me. Thanks for the correction @cardinal,2011-02-28T00:17:22.083,1307,CC BY-SA 2.5,
12427,2959,0,@suncoolsu what do you mean by ‚Äúnever unbiased‚Äù?  Do you mean with respect to the prior?,2011-02-28T00:46:42.577,1670,CC BY-SA 2.5,
12428,7633,0,Henry: I am sorry about that. I didn't look at LHS to see that its a CDF. Thanks for your answer and I regret that I chose the answer before I realized it (I realized through the other guy's note).,2011-02-28T01:19:22.360,,CC BY-SA 2.5,user1102
12429,7688,4,"What is multivariate statistics?  Seriously, this is such a vast subject and covers so much ground that it's impossible to know what you're looking for and certainly it would be impossible for any textbook to claim a ""detailed"" coverage of the subject!",2011-02-28T03:06:07.397,919,CC BY-SA 2.5,
12430,2959,0,"@vqv There is a theorem by Blackwell which says that ""Bayes estimates are almost never unbiased"". Please notice I am talking about asymptotic unbiasedness here.",2011-02-28T03:31:33.830,1307,CC BY-SA 2.5,
12431,7688,0,@whuber. Good point.,2011-02-28T03:32:32.610,1307,CC BY-SA 2.5,
12432,7308,0,**[EDIT: Wooldridge has removed the statement in the second edition of his book and replaced it with the above argument. See page 414 of http://www.amazon.com/Econometric-Analysis-Cross-Section-Panel/dp/0262232588/ ]** So I guess the original statement was wrong. Closing the question. Apologies for bothering the good people here.,2011-02-28T04:08:23.603,1393,CC BY-SA 2.5,
12433,2959,1,"@suncoolsu asymptotic unbiasedness is very different from the usual sense of ""unbiased"". Any reasonable estimate should be asymptotically unbiased.  One more note: shouldn't the statement about admissible estimators be the other way around? ie every admissible estimator is generalized Bayes.",2011-02-28T04:15:13.523,1670,CC BY-SA 2.5,
12434,2959,0,"@suncoolsu, this is definitely one of those instances where a few symbols say something much more clearly than ""plain English"". I fear my description is too clumsy, but it's hard to get it precise in few words. Another attempt would be: ""An admissible estimator dominates every other estimator at at least one point in the parameter space. The point of domination will typically vary according to the estimator being compared against.""",2011-02-28T04:15:17.703,2970,CC BY-SA 2.5,
12435,7308,1,"As if Wooldridge has read this thread :) The book was released before, but still quite amusing.",2011-02-28T04:52:41.247,2116,CC BY-SA 2.5,
12436,7575,0,"Thanks, Alex. Let me address a couple of your questions. In terms of problem's scale, I have ~17,000 events/transport decisions over about 4,000 people. There are around a dozen event level predictors, and a half-dozen at people level.",2011-02-28T05:57:29.797,3387,CC BY-SA 2.5,
12437,7575,0,"I'm very interested in your SEM suggestion. I'll admit that my vague impression of SEM has been that it's like, say, game theory: very general, obstensibly can solve anything, but results in simplistic diagrams and not very satisfying answers. But your description, as simultaneously solving mixed models, makes sense. Can you point me to any papers/examples that seem particularly relevant to my situation? Thanks so much.",2011-02-28T06:01:57.037,3387,CC BY-SA 2.5,
12438,6157,0,"Thanks for your answer. graphviz is not easy to install with R, but it seams to be a great library",2011-02-28T06:38:04.303,1709,CC BY-SA 2.5,
12440,7477,0,"@cardinal: thanks! I'm trying to get rid of the prejudice and the frustration by trying to ignore the negative sides, and try to figure out the right ""mindset"". I believe science is somewhat like language, you'll never be fluent unless you learn to think in that language instead of thinking in some other language and translating in your head. We'll see how well that works out for me... :)",2011-02-28T08:55:43.067,3014,CC BY-SA 2.5,
12441,7695,0,"There is a problem with your first sentence, which probably is the cause of the further problems.  Is this your sentence, or is it citation? What is the pdf of wealth for one person? Pdf combines information about many values random variable takes. Information about one person's many values wealth is usually private and generally only available to tax offices. Usually given the income distribution of whole population and the wealth of particular person you can say what is the probability of finding wealthier or poorer persons. Is this what you are saying?",2011-02-28T09:04:59.757,2116,CC BY-SA 2.5,
12442,7695,0,"The conjecture in the second paragraph is false. The portion of wealth is a **number**, distribution times the constant is **function**.",2011-02-28T09:07:00.287,2116,CC BY-SA 2.5,
12443,7695,0,"Thanks, mpiktas - I believe I fixed them.  My basic question is: ""Help me understand the relationship between P(X has this much wealth) distribution and distribution of wealth in the society as a whole.",2011-02-28T09:28:12.817,,CC BY-SA 2.5,user3463
12444,7695,0,"@Marcus, I find your usage of term distribution a bit confusing. P(X has this much wealth)  is a number, $P(X<x)$ for given $x$ is  a distribution of $X$. Do you per chance need something like [Lorenz curve](http://en.wikipedia.org/wiki/Lorenz_curve)?",2011-02-28T09:58:14.810,2116,CC BY-SA 2.5,
12445,7694,3,Please provide the example. Now it is unclear what is your actual problem.,2011-02-28T11:08:21.333,2116,CC BY-SA 2.5,
12446,6919,3,"Your addendum is the most useful piece of information I ran into on stats.SE, so far. All my warmest thanks for this. I simply reproduce here the reference you gave: http://research-repository.st-andrews.ac.uk/bitstream/10023/1591/1/Endres2003-IEEETransInfTheory49-NewMetric.pdf Endres and Schindelin, A new metric for probability distributions, *IEEE Trans. on Info. Thy.*, vol. 49, no. 3, Jul. 2003, pp. 1858-1860.",2011-02-28T12:07:46.137,2592,CC BY-SA 2.5,
12447,7699,0,"Does your regression has an intercept? Omitting intercept sometimes can cause problems. Also could you post reproducible example? It is clear that you made an error, as @onestop pointed out, but with example available it may be possible to point out where the error is exactly.",2011-02-28T12:29:50.460,2116,CC BY-SA 2.5,
12448,7689,0,It escapes me how that paper is relevant to the question. Do you mind elaborating?,2011-02-28T13:10:13.720,1036,CC BY-SA 2.5,
12449,7701,1,Is your binary categorical variable _O_ your dependent variable? Could you tell us what your model looks like now?,2011-02-28T13:48:38.960,3396,CC BY-SA 2.5,
12450,7705,0,"That will not prevent the predictions of the network from going outside the range 0-1.  If it is necessary to constrain the outputs of the network to lie within a particular range, a non-linear activation function is unavoidable.",2011-02-28T13:53:28.990,887,CC BY-SA 2.5,
12451,7654,0,"Sorry for late reply, but THANKS. I ended up using `=IF(COUNTIF(B5:B17, ""<>Yes""), ""No"", ""Yes"")`",2011-02-28T13:55:54.043,3446,CC BY-SA 2.5,
12452,7701,0,"@ashaw: Variable $O$ is the outcome of a number of different laboratory tests. I'm not modeling $O$ as it is categorical. Males who are $True$ for $O$ are older than females who are $True$ for $O$, but this is expected as all males in the cohort are older than females. I want to know if $O$ is only occurring in older males for example. Are older males more likely to be $True$ for $O$?",2011-02-28T14:01:03.633,2824,CC BY-SA 2.5,
12453,7703,0,"I replaced the linear function by a siglog-function. Strangely, the neural net predicts now 0.5 for whatever data inputs. May this be a consequence of still using the sum-of-squares metric?",2011-02-28T14:02:58.257,3465,CC BY-SA 2.5,
12455,7703,0,"The use of the sum-of-squares error metric shouldn't be a problem, assymptotically it will give a network that predicts the conditional mean of the data regardless of the distribution and output layer activation function.  Can you give some more information about the data, size of the netwok, training algorithm etc? What is the distribution of target values like?  Are you using regularisation?",2011-02-28T14:10:06.543,887,CC BY-SA 2.5,
12456,7701,0,"So if I understand you correctly, the first formula I posted below is inaccurate. $O$ is a covariate in your model and you're using $O$, $Age$, and $Gender$ to model some other outcome $Y$. If that's true, I can try to adjust the answer below.",2011-02-28T14:10:25.703,3396,CC BY-SA 2.5,
12457,7707,0,"I thought linear regression required that all variables be continuous (I didn't add the linear regression tag). $O$ represents whether the infection in the participant is a particular variant or not (in the subset of the data in question, all participants are ""infected""; some are infected with the variant and are thus $True$ for $O$). So I wanted to determine if all the men with this variant are older because the variant only infects older men or because men in the study are, on average, older than women. $O$ is binary.",2011-02-28T14:12:43.857,2824,CC BY-SA 2.5,
12458,7707,0,"Aha, that makes things a little different. The difference between linear and logistic regression stems from nature of your **outcome**. If the outcome is continuous and distributed reasonably normally, OLS may apply. If the outcome is binary, logistic regression may apply. (I'm not coming down definitely in either case because there can be many other factors worth taking into account when picking a model). Both logistic and OLS models can accommodate binary, categorical, ordinal and continuous covariates.",2011-02-28T14:23:12.907,3396,CC BY-SA 2.5,
12459,7701,0,Thanks for the answer and discussion. I have to think more about this.,2011-02-28T14:25:11.850,2824,CC BY-SA 2.5,
12461,7703,0,"Training algorithm is back propagation. 500 training cycles, learning rate 0.3 and 0.2 momentum. 1 hidden layer with 3 neurons. The data consists of 3 attributes, and one target attribute. 20000 samples. The target attribute is very heavily(!!!) skewed within the range of 0 and 1, the mean is 0.077, SD 0.185.",2011-02-28T14:29:38.187,3465,CC BY-SA 2.5,
12462,7701,0,"Feel free to post more information too - you don't necessarily need to provide the specifics of the study, but if you can provide reasonably clear examples there's a good chance somebody can help!",2011-02-28T14:29:49.470,3396,CC BY-SA 2.5,
12463,7706,0,"your question body does not mention anything about survival analysis, so I've changed the title. I still left the tag though, since it might generate different answers. Please consider adding more details if the survival analysis is really important in your question.",2011-02-28T14:43:10.627,2116,CC BY-SA 2.5,
12464,7707,0,What does it mean to fit a linear model between a continuous variable ($age$) and a categorical factor ($gender$)? I was using Wilcoxon to investigate continuous versus categorical variables.,2011-02-28T14:47:08.693,2824,CC BY-SA 2.5,
12465,7695,0,The Lorenz curve is very appropriate.  But it's cumulative.  I think I'm looking for the derivative of the Lorenz curve (of a Pareto distribution or in general).  What is that?,2011-02-28T14:57:50.503,,CC BY-SA 2.5,user3463
12466,7695,0,"mpiktas, isn't P(X<x) also a number, given a particular x (""this much wealth).  And, when x is a parameter and not a fixed number, so is P(X has x much wealth).",2011-02-28T14:58:43.433,,CC BY-SA 2.5,user3463
12467,7707,0,"@SabreWolfy: Regression models can produce coefficients for continuous, categorical or binary variables. In the case of a binary variable, the results usually provide a coefficient for only one of the values (e.g. $Female = True$) with the other value ($Female = FALSE$) implicitly used as the baseline of comparison.",2011-02-28T15:04:35.570,3396,CC BY-SA 2.5,
12468,7707,0,What sort of outcome variable are you using in this case?,2011-02-28T15:06:03.400,3396,CC BY-SA 2.5,
12469,7542,0,new udpate! (and new detailed title),2011-02-28T15:22:29.617,3342,CC BY-SA 2.5,
12470,7642,0,"Open up R and just copy/paste the commands into the console.  Use `MyData <- read.csv('path/to/MyData.csv')`, and then run the rest of the script.",2011-02-28T15:29:52.353,2817,CC BY-SA 2.5,
12471,7703,0,"In that case, it might be worth transforming the target to have a less skewed distribution before training.  Is it a problem with many values exactly at zero (like rainfall for example) or are there no special values?",2011-02-28T15:48:56.380,887,CC BY-SA 2.5,
12472,7365,0,"@mpiktas It's a nice exposition; I wish I could vote it up again!  One thing, though: when you find a Hessian to be indefinite, it's easy to check whether the solution is on the boundary.  If not, don't give up and conclude the ""model may be inappropriate""!  It's more likely the minimizing algorithm is not a good one for the problem.  Furthermore, in your case the model is obviously appropriate: it is identical to the model that produced the data.  The difficulty is that (due to the high error variance) it is essentially a one-parameter problem; another parameter is almost unidentifiable.",2011-02-28T16:20:08.763,919,CC BY-SA 2.5,
12473,2959,0,"@vqv In re: ""Any reasonable estimate should be asymptotically unbiased."" I don't necessarily disagree, but this is a strong statement, no? I haven't got an example offhand but it seems plausible that there exist estimators with nice finite sample properties that happen to be inconsistent. But perhaps not. And I think you're correct that every admissible estimator corresponds to a Bayes rule.",2011-02-28T16:22:15.500,26,CC BY-SA 2.5,
12474,2959,0,@vqv - You are correct as well. Sorry for misinterpreting your question.,2011-02-28T16:23:05.033,1307,CC BY-SA 2.5,
12475,2959,0,"@JMS. I think @vqv is correct, this is another way of thinking about consistency.",2011-02-28T16:25:41.413,1307,CC BY-SA 2.5,
12476,7468,0,I ended up implementing a variation of this algorithm with a metric called  Dunn‚Äôs clustering validity index,2011-02-28T16:26:47.700,255,CC BY-SA 2.5,
12477,2959,0,"@suncoolsu I wasn't so clear; I should have stuck with asymptotically biased (consistent under L1). Still, I'm not so sure that there do not exist reasonable estimators that are asymptotically biased but have good finite sample properties. But like I said, I can't think of an example offhand.",2011-02-28T16:31:31.690,26,CC BY-SA 2.5,
12478,7712,1,This isn't really about vectorizing calls to `dpois`. It's more a question about creating vectors.,2011-02-28T16:49:28.980,3396,CC BY-SA 2.5,
12479,7713,0,I should have looked at this carefully.,2011-02-28T16:59:22.737,1307,CC BY-SA 2.5,
12480,7714,0,SORRY THE DATA IS,2011-02-28T17:01:36.300,3472,CC BY-SA 2.5,
12481,7713,0,"no worries, glad I could answer the question.",2011-02-28T17:03:15.007,3396,CC BY-SA 2.5,
12483,7713,0,The only problem is with the 0.1:0.3 part. `1:3` is already a vector and doesn't require concatenation.,2011-02-28T17:08:15.763,697,CC BY-SA 2.5,
12484,7713,0,indeed. edited accordingly.,2011-02-28T17:15:27.930,3396,CC BY-SA 2.5,
12485,7714,1,"Also, there really is no single ""best"" test for reliability. Most inter-rater reliability statistics I've encountered are for nominal, ordinal, interval or ratio level data. It's possible that with continuous data, something more like correlation or rank correlation statistics may be more useful. I'd encourage you to look into what sorts of techniques are common in your particular field and/or to say more about the scale involved if you'd like a more informative answer.",2011-02-28T17:25:12.913,3396,CC BY-SA 2.5,
12486,2959,0,"@suncoolsu, consistency and asymptotic unbiasedness are different things. To get some notion of equivalence requires more structure on the problem, like uniform integrability. There are some estimators that are consistent yet asymptotically biased and, of course, estimators that are asymptotically unbiased but inconsistent.",2011-02-28T17:38:25.367,2970,CC BY-SA 2.5,
12487,2959,0,"@JMS Asymptotic unbiasedness does not imply consistency.  Asymptotic unbiasedness: $|E \hat{\theta} - \theta| \to 0$; L1 consistency: $E|\hat{\theta} - \theta| \to 0$.  L1 consistency is much stronger.  Anyway, this is probably far off the topic of the original question.",2011-02-28T17:49:20.100,1670,CC BY-SA 2.5,
12488,7676,0,"@ashaw: Comparison of conditions experienced by the same individuals is a standard methodological approach that increases power by permitting removal of between-Ss variance that usually obscures effect variance in completely between-Ss designs. Regarding the computation of the difference score CI, I have edited the post above to clarify that on each iteration the condition means were computed and collapsed to a difference, yielding a distribution of differences from which the 95% interval.",2011-02-28T18:19:53.510,364,CC BY-SA 2.5,
12489,7713,5,"A more compact version of the same answer: `dpois(1:3, 1:3/10)`.",2011-02-28T18:33:31.510,1657,CC BY-SA 2.5,
12490,7690,0,my question arose from calculating the probability of datapoints generated by a gaussian and using bayes theorem in maximizing the likelihood. though the probability of a point is zero it seems the handwavy way of thinking of it as an infinitesimal probability is ok.,2011-02-28T18:38:27.620,3456,CC BY-SA 2.5,
12491,7676,0,"that helps for sure. Apologies if you read the first phrase of my comment to suggest that the approach was invalid in any way. Rather, I meant to point out that the validity of my response was contingent on the validity of the study design. In any event, now I can mull over a real response a little more...",2011-02-28T18:52:13.960,3396,CC BY-SA 2.5,
12492,7717,0,"Thank you so much for that!! Do you mean the Bald-Altman plot. So the Kappa score is not applicable in the continuous data?? am yet collecting more data( my sample size is 146, but i intend to test 15 for inter-rater reliability)",2011-02-28T19:19:05.113,3472,CC BY-SA 2.5,
12493,7714,0,"@ashaw No, you can't use simple correlation-based measures since they are invariant by shifting the mean of one of the two measures, or adding a constant amount to one of them, which obviously lead to decreased reliability (overall, the two raters disagree to a larger extent) but that would not be reflected in such an association measure.",2011-02-28T19:27:47.540,930,CC BY-SA 2.5,
12494,7717,0,"@Edwin @onestop's response is fine, and you can complete this graphical/numerical approach by computing an Intraclass correlation, for example. Several earlier posts dealt with those topics (search around the two tags you associate to your question). I also gave a +1 to your question, so that you can vote up @onestop's response as you seem to consider it being helpful.",2011-02-28T19:31:30.220,930,CC BY-SA 2.5,
12496,7674,1,"Great answer. If I had another vote, I would vote for this answer again.",2011-02-28T19:38:27.750,847,CC BY-SA 2.5,
12498,7717,0,"@Chl..Thanks a lot for that. I just want to get it clear, so the Bland-Altman plot will be the solution...",2011-02-28T19:40:29.060,3472,CC BY-SA 2.5,
12500,7714,0,"@chl - Definitely, I wouldn't expect simple correlation to actually capture reliability at all. In my head, the emphasis was on the ""something more like"" part of that sentence. Should be clearer from these comments at least...",2011-02-28T20:12:24.297,3396,CC BY-SA 2.5,
12501,2959,0,"Of course, where is my head. Good catch.",2011-02-28T20:23:44.600,26,CC BY-SA 2.5,
12503,7595,1,"@williamyarberry Register here and on maths with the same OpenID to recover the ownership of your question (if you think it is worth it, of course ;-) ). You can do it by clicking ""log in"" at the top bar.",2011-02-28T20:31:42.720,,CC BY-SA 2.5,user88
12504,2959,0,"@everyone. Sorry. I meant - ""a step towards consistency"". Because consistency is a sum of squared bias and variance in $L_2$ specifically.",2011-02-28T20:33:04.050,1307,CC BY-SA 2.5,
12505,7512,0,"By correlate, I mean a measure of association - something like pearson's r. N and n were typos; I just meant I have two sample sizes. So, for one variable, I have 50 males and 50 females and I want to correlate their scores together. For another variable, I have 99 males and 99 females and I want to correlate their scores. My variables are measures of delinquency counts, which are relatively rare events. Thanks so much for your response!!",2011-02-28T20:37:51.210,,CC BY-SA 2.5,user3368
12506,7717,0,"@Edwin Happy to know that you find B-A plot a good compromise. In this case, you might just upvote @onestop's response and wait few days to see if no other response come up before accepting it.",2011-02-28T21:04:03.480,930,CC BY-SA 2.5,
12507,2959,0,"@suncoolsu, you must be using a definition of consistency that is different. The usual definition is a statement about *convergence in probability*, not in any $L^p$ space. Of course, if you have the latter, you get the former for free.",2011-02-28T21:08:58.990,2970,CC BY-SA 2.5,
12508,7720,4,"@dfrankow Somewhat crude and certainly very partial help for your first two questions, but `methods(""profile"")` will give you the (S3 in this case) methods associated to an R `profile` object, then you will see than there is a dedicated method for `polr` results, that you can browse on-line by typing `getAnywhere(""profile.polr"")` at the R prompt.",2011-02-28T21:13:38.527,930,CC BY-SA 2.5,
12509,7706,0,"@mpiktas, The data are indeed survival data in that the dependent var is the time from diagnosis until death. Out of 1500 patients I have 15 that are still alive and hence right censored.",2011-02-28T21:20:53.590,1291,CC BY-SA 2.5,
12510,7720,1,Thanks! Source code is good.  Explanation would be even better. :),2011-02-28T21:21:27.397,2849,CC BY-SA 2.5,
12511,7719,2,It's hard to imagine a stacked barplot with SE bars (at least for me!). Why not simply use a [dotplot](http://www.statmethods.net/graphs/dot.html)?,2011-02-28T21:21:28.503,930,CC BY-SA 2.5,
12513,7723,5,"There are good questions in here but you're asking too much at once.  Focus on one issue at a time and formulate a specific question for it.  Specify what you do know and what efforts you have made towards answering the question.  ""How do I even start"" doesn't give us anything to go on--we would just refer you to a textbook or Wikipedia (which is not a bad idea, by the way).",2011-02-28T22:32:36.770,919,CC BY-SA 2.5,
12514,7556,0,hey i edited the question for a more specific use case for the data,2011-02-28T22:36:42.153,3392,CC BY-SA 2.5,
12515,7587,0,"Hey i edited the question to provide more details, I don't know anything about the distribution of the data, though most likely normal if i have to guess.",2011-02-28T22:37:46.037,3392,CC BY-SA 2.5,
12516,7677,0,"Thank you very much -- your suggestion worked very well indeed. Also, thank you for your pointer to ezMixed, which is also a huge help -- I wasn't aware that ez offered support for mixed models.
(Actually, while I'm at it: Thank you for your work on ez, which is a great pleasure to use :-) )",2011-02-28T23:04:58.623,3451,CC BY-SA 2.5,
12517,7556,0,What if the mean is 0?,2011-02-28T23:13:16.810,2965,CC BY-SA 2.5,
12518,7556,0,"If your data is such that the mean can be zero, then this is not a good solution. I.e. it is not meaningful to divide by the mean. This is even if the actual mean is not zero. E.g. you average something like the ""x-position"" of points. The mean here can be zero, and I'd argue dividing by the mean is not meaningful, even if in your particular sample the mean is not 0.",2011-02-28T23:44:35.260,3369,CC BY-SA 2.5,
12521,7642,0,"alternatively, on windows/mac just go source or read csv file.choose() and a window will open up which you can navigate to open the required files.",2011-03-01T00:11:18.303,656,CC BY-SA 2.5,
12522,7702,0,"+1 for this nice, readable and freely available resource!",2011-03-01T00:55:29.790,1050,CC BY-SA 2.5,
12523,7721,1,"@Andy, can you clarify your question a little bit? Do you have hundreds of thousands of predictors for each response? Or hundreds of thousands of responses? Or perhaps, a data matrix with hundreds of thousands of entries? Or maybe you meant ""hundreds ***or*** thousands""? The answer to this will provide some guidance. Also, how many class categories do you have? What sort of computing resources are available? Are your features sparse or dense?",2011-03-01T01:52:46.353,2970,CC BY-SA 2.5,
12524,7721,0,"@Andy, for ***binary*** logistic regression, Paul Komarek built a package as his dissertation research at Carnegie Mellon. It has some optimizations for sparse features. I don't believe it does multinomial logistic regression, but I could be mistaken. He actually uses a ridge version of logistic regression, but with a fixed ridge parameter. His claim is that that is good enough for (most) all problems. The fitting method uses conjugate gradients if I recall. I'm not sure it's amenable to parallelization, though.",2011-03-01T01:55:34.820,2970,CC BY-SA 2.5,
12525,7727,1,"@BobC, maybe one of the moderators can have your post migrated to this site. That would be the most reasonable. As for your technical questions, first of all, are you using the FFT to do the correlation? That should be feasible for 2 million data points on a half-decent computer. Your signal-to-noise ratio looks reasonably high, so you should be in business. A quick-and-dirty cut would be to fill in the missing data with either the last available sample or with zeros. The creep from sampling-interval differences may be the most challenging ""feature"" of your data to deal with.",2011-03-01T02:16:36.270,2970,CC BY-SA 2.5,
12526,7727,0,"@cardinal: I did indeed try an FFT, only to get garbage as a result.  The 'interesting' features readily visible in the data are indistinguishable from noise in the FFT.  However, I have done FFTs only on the entire data set: Perhaps a moving window FFT would provide better results, but I haven't yet been able to find a computationally efficient way to implement it.  I suspect a Wavelet transform could help, but I'm unfamiliar with it (but am slowly learning about it).",2011-03-01T02:24:34.397,3479,CC BY-SA 2.5,
12527,7727,1,"@BobC, what I meant was, did you consider an FFT-based implementation for calculating the correlation? Direct convolution is $O(n^2)$, but an FFT-based implementation would reduce this $O(n \log n)$, making it feasible. As for looking at the FFT itself, with 2 million data points, your frequency resolution will be very high. Any sampling creep and other stuff is bound to wash the signal out on a per-frequency basis. But, you should be able to aggregate over many bins to bring the signal out of the noise. Something like a Welch approach or maybe a custom windowing technique.",2011-03-01T02:29:22.983,2970,CC BY-SA 2.5,
12528,7727,0,"@BobC, off the top of my head, it seems like some variant of an *overlap-and-add* or *overlap-and-save* algorithm could be used to do a sliding-window FFT. Sliding the samples within a window just amounts to a phase shift, so all you have to do is compensate for those samples that ""fall off"" the left end and those that ""come in"" on the right end.",2011-03-01T02:31:12.903,2970,CC BY-SA 2.5,
12529,2959,0,"@cardinal. I can't resist my temptation to use math anymore. $E[\hat \theta - \theta]^2 = E[\hat \theta - E[\hat \theta] + E[\hat \theta] - \theta]^2 = E[\hat \theta - E[\hat \theta]]^2 + ( E[\hat \theta] - \theta)^2 = Var[\hat \theta] + Bias^2$. So basically @cardinal and me are talking about the same thing but from a different perspective. If Variance **and** Bias go to 0 in **probability**, consistency holds, otherwise not. Or am I still incorrect? Thats what I have learnt in my courses here.",2011-03-01T02:37:23.207,1307,CC BY-SA 2.5,
12530,7721,0,"@cardinal : Thanks for your answer. Good questions ! I have 12 classes, and the number of predictors can go from 80k to 200k depending on the source of the data. And yes this is a highly sparse sparse data set. Regarding computing resources, I think I might be able to get access to a cluster (~10nodes or so), but I think if push comes to shove, I can try and get more computing resources also.",2011-03-01T02:39:47.077,3301,CC BY-SA 2.5,
12531,2959,0,"@suncoolsu, Let $\hat{\beta}_n$ be an estimator for a parameter, $\beta$, derived from a sample of size $n$. Then $\hat{\beta}_n$ is *consistent* for $\beta$ if $\hat{\beta}_n \to \beta$ ***in probability***. This is a weaker requirement than requiring that $\mathbb{E} |\hat{\beta}_n - \beta|^p \to 0$ for some $p > 0$.",2011-03-01T02:44:30.053,2970,CC BY-SA 2.5,
12532,2959,0,"@suncoolsu, I see you edited your comment while I was writing mine. Convergence in *any* $L^p$ space guarantees convergence in probability. This can be seen by a straightforward application of Markov's inequality. But, convergence in probability is weaker than this requirement. Hence, consistency of a parameter is (substantially) weaker than requiring the parameter to converge to its true value in, say, $L_2$, the latter being more or less what it looks like you're stating.",2011-03-01T02:48:50.873,2970,CC BY-SA 2.5,
12533,7721,0,"@Andy, how much information are in those features? Can't be much. Are the features continuous-valued or binary? There might be a quick-and-dirty approach where, e.g., you can train 12 binary-logistic regressions separately and then use there predictions in a final step to get a proper distribution for the twelve-class case. (For example, train a 12-feature multinomial logistic regression when you're done training each of the individual ones. Obviously, that's suboptimal, but depending on the application, it might be both computationally feasible and good enough.)",2011-03-01T02:56:00.277,2970,CC BY-SA 2.5,
12534,2959,0,@cardinal. Yeah I was talking about MSE and not the convergence.,2011-03-01T03:02:43.007,1307,CC BY-SA 2.5,
12535,7721,0,"@Andy, I just checked and **[Komarek](http://komarix.org/ac/lr)** does provide some very simple suggestions for multi-class extensions. They're simpler than what I was suggesting. One reason mine might be better is that if you fit 12 separate binary logistic regressions, the associated parameters aren't necessarily on similar scales. By having a final step that is a full multinomial logistic regression, the parameters in the final fit can compensate for those scale differences that a simple renormalization wouldn't. Besides, simple renorming wouldn't change your predicted class anyway.",2011-03-01T03:04:23.263,2970,CC BY-SA 2.5,
12536,7587,0,"Thanks for clarifying. If you're dealing with people's behavior on the Internet, the distribution is probably far from normal. In fact, you're likely to find very skewed distributions on almost any measure (that's the fun of the long tail!). I think you're instinct (mean + standard errors in some intuitive fashion) is great. The visualization suggestions others have made should be useful too. You might consider also presenting information about just how unequal the distribution is as well as data about ""average"" users at the bottom and top of the distribution too.",2011-03-01T03:08:55.390,3396,CC BY-SA 2.5,
12538,2959,0,"@suncoolsu, Some people refer to convergence of a parameter in $L_2$ as *mean-square consistency*. But, in general, a consistent estimator need not have asymptotically negligible bias *nor* variance. In fact, it need not even have finite variance. If you're really sadistic, you can undoubtedly cook up an example where the variance *isn't even defined*.",2011-03-01T03:13:28.700,2970,CC BY-SA 2.5,
12539,7728,0,"Great point, @rolando - since the original post refers to training and validation datasets I suspect your response may actually be more usable for @Figaro.",2011-03-01T03:15:28.883,3396,CC BY-SA 2.5,
12540,2959,0,@cardinal. Thanks for the inputs :-) Helps improve my understanding!,2011-03-01T03:17:16.370,1307,CC BY-SA 2.5,
12542,2959,0,"@suncoolsu, mine, too. That's one of the reasons I enjoy this site. Cheers.",2011-03-01T03:23:08.257,2970,CC BY-SA 2.5,
12543,7599,0,"In response to your edit, @williamyarberry, I apologize for not realizing what exactly you meant by the original question. I've been poking around in the documentation for MASS and I'm also uncertain why the output doesn't provide any summary information about model fit. Presumably, you could use the `fitted.values` and `residuals` stored in the `M10` object to calculate $R^2$ independently, but I do not know if that's a sensible approach with this method.",2011-03-01T03:26:48.567,3396,CC BY-SA 2.5,
12545,7732,0,"@mbq, @imelza, this is pretty much a duplicate of **[this question](http://stats.stackexchange.com/questions/7727/how-to-correlate-two-time-series-with-gaps-and-different-time-bases)**. Usually if there is a question that would provide the answer you need, it's a little frowned upon to submit another that is very similar. You'd be better off waiting to see if you can glean an adequate answer from the original one. Regards.",2011-03-01T03:31:36.377,2970,CC BY-SA 2.5,
12547,7732,0,"Hi, I understand that the questions are very similar, but I cannot comment on that question and I really need this answer.",2011-03-01T03:36:49.887,,CC BY-SA 2.5,user3233
12548,6919,1,"@Didier, well, it was more a happy accident than anything else. No one was responding to the other question, so I decided to try to figure out what the Jensen-Shannon Divergence was in the first place. Once I found the definition, it seemed reasonable to connect the two questions via my addendum. I'm glad you found it useful. Regards.",2011-03-01T03:38:06.200,2970,CC BY-SA 2.5,
12549,7599,1,I should note that a general point common to many Robust/Resistant techniques (I've mostly worked with Huber estimators) is that they require you to bootstrap standard errors & $R^2$ values. Two discussions I found useful in my searches on this topic (and which contain code for bootstraping standard errors for an lqs model) can be found [here](http://www.stat.psu.edu/~dhunter/R/testboot.html) and [here](http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch13.pdf) (PDF). I also recommend exploring the sources cited in the MASS documentation.,2011-03-01T03:43:13.457,3396,CC BY-SA 2.5,
12550,7732,0,Is the time scale identical for both of the variables? Can you post an example of your data set?,2011-03-01T04:28:25.127,2116,CC BY-SA 2.5,
12551,7721,0,"@cardinal : Actually I do not have a sense of how much information is likely to be there in the features, since I am yet to get access to the data :-) The features are all nominal and between 0 and 1. But yes, I think I will do some feature selection. I looked at Komarek's software, and it looks promising. Two things are not clear to me though: (1) How do I get the regression weights? It's not clear from his webpage or the software documentation (2)It seems like he is using Ridge regression. I need to use Lasso as well.",2011-03-01T04:38:59.030,3301,CC BY-SA 2.5,
12554,7727,0,"@cardinal, did you take a look at the plots I posted as an update to the original question?  The sensors are not measuring identical environments, and the environment does lots of filtering, the nature of which will be quantified by future analysis.  But the changes appear sufficient to preclude use of phase shift.",2011-03-01T06:55:32.547,3479,CC BY-SA 2.5,
12555,7727,0,"Hi, I have a similar question. I have 2 time series, each represented by a matrix with its first column corresponding to the values and second column corresponding to the time difference (since the previous value) 

How do I find the correlation between these 2 matrices? I tried to do xcorr2() but it doesn't seem right and doing xcorr would probably calculate correlation with only the values into considering, but I also want to account for the time. 

I'm really confused here, will an FFT help? How would you suggest I go about it?",2011-03-01T03:09:18.637,,CC BY-SA 2.5,user3233
12557,7727,0,"@lmelza Converted; yet if you have a question, just ask it as a new one (ASK QUESTION button at the top-right side).",2011-03-01T07:12:53.017,,CC BY-SA 2.5,user88
12558,7732,0,"I'm not familiar with Matlab functions... in R I would use `ccf`, that calculates the cross correlation function of the two signals. The FFT won't help here, as you don't want to do anything in the frequency domain, as far as I understand.",2011-03-01T07:57:14.820,582,CC BY-SA 2.5,
12559,7728,0,"Thank you both for your valuable input. @rolando2 you where right about my ambiguous wording, algorithmic was the direction I aiming for.",2011-03-01T08:15:23.600,3401,CC BY-SA 2.5,
12560,7690,0,"Thinking of $f(t)dt$ as an ""infinitesimal"" probability is a good way to go I reckon.  I think of a PDF as a ""infinite"" histogram with bins of ""tiny"" width $dt$ and height $f(t)$.  The integral is how you ""add them up"",  Which is why its $\int f(t)dt$  ""sum up the area""",2011-03-01T08:38:43.973,2392,CC BY-SA 2.5,
12561,3500,0,You can go one step further and take logs.  Then you have *log-posterior ratio* = *log-prior ratio* + *log-likelihood ratio*,2011-03-01T08:43:36.637,2392,CC BY-SA 2.5,
12562,7723,0,"@Whuber - this is a bit of a cop-out, @LSEactuary is not asking for the answer to everything.  You could easily just provide an answer to one piece of his question.",2011-03-01T08:55:27.390,2392,CC BY-SA 2.5,
12566,7731,1,"**leaps** is great, I like the plots from it, +1. In real applications some averaging techniques work faster (and better) than pretested estimators even found from all regression models. So I would suggest to go for Bayesian model averaging (BMA package) or the algorithm I like the most - Weighted average least squares (WALS[1]) developed by J.R.Magnus et al. The Matlab code is easily transformable to R code. The good thing for WALS is $N$ computational difficulty instead of $2^N$. 

 
  [1]: http://www.tilburguniversity.edu/research/institutes-and-research-groups/center/staff/magnus/wals/",2011-03-01T11:14:23.143,2645,CC BY-SA 2.5,
12567,7716,1,@chl seconded. Great overview.,2011-03-01T11:34:23.400,2116,CC BY-SA 2.5,
12568,7732,0,"@nico, in terms of *implementation*, the FFT will most definitely help if the matrices have a large number of rows. Both MATLAB and R use a frequency-domain approach to calculation of crosscorrelation. R's is incredibly naive, unfortunately, unless you understand the underlying approach and resize your matrix to take advantage of it. MATLAB's is a bit smarter, if I recall correctly.",2011-03-01T12:19:40.393,2970,CC BY-SA 2.5,
12569,7739,2,"@probabilityislogic, while your response is interesting, maybe it could be refocused to better address the OP's question. Also, ***no*** software for computing least-squares solutions performs matrix inversion, much less a determinant. Ever. Unless it's inverting a $1 \times 1$ matrix.",2011-03-01T12:28:19.110,2970,CC BY-SA 2.5,
12570,7739,1,"@probabilityislogic, handling the $2^n$ cases efficiently quickly far outstrips the $O(n^3)$ issues of an efficient least-squares solution. That's where the *leaps-and-bounds* algorithm comes in.",2011-03-01T12:30:39.547,2970,CC BY-SA 2.5,
12571,7731,1,"@Dmitrij, thanks for your comments. I've tried to remain fairly agnostic in my response regarding the utility of all-subsets regression. It seems to me that there is nearly always a better solution, but I felt that might seem like too trite of a response to the OP's question.",2011-03-01T12:34:21.230,2970,CC BY-SA 2.5,
12572,7731,0,"@Dmitrij, BMA over main-effects models would still have the same computational complexity as all-subsets regression. No? The main advantage in BMA seems to me to be in trying to figure out which covariates are likely to be influencing the response. BMA does this by essentially averaging the log likelihoods over $2^{n-1}$ submodels.",2011-03-01T12:38:38.623,2970,CC BY-SA 2.5,
12573,7731,0,Thanks for the pointer to the leaps R package! I didn't know about it and it might come in handy in the future. If I could get some information on specific limitations for N for other popular packages it would be very helpful.,2011-03-01T12:44:37.323,10633,CC BY-SA 2.5,
12574,7721,0,"@Andy, how many observations do you have? For a lasso version of logistic regression, there is **glmnet**, but I've found it flaky even on small problems and I doubt it will scale to the size you are interested in. But, it's probably about the only package that even has a chance. The others tend to use iterative versions of the LARS algorithm. I think your first step should be to try some methods of feature filtering to get down to a more manageable size and see how bad that does. Are you at liberty to discuss more details of the application?",2011-03-01T12:45:19.413,2970,CC BY-SA 2.5,
12575,7739,0,"Thanks for the post. ""Do the maths or do the time!"" :-)  .. I'm actually not even trying to figure out the underlying algorithms used by the packages (thought that is interesting to know), at this point really looking for specific information regarding the limitations of N by the major packages.",2011-03-01T12:47:51.650,10633,CC BY-SA 2.5,
12576,7732,0,"@cardinal: thanks, I didn't know about that :)",2011-03-01T12:53:33.727,582,CC BY-SA 2.5,
12577,7727,0,"@BobC, I did take a quick look at the three plot snippets you posted. I'll try to take a closer look later today. It's not entirely germane to your question, but I'm a little puzzled by the data dropouts. I used to work with multichannel accelerometers sampling at 1kHz per channel or more for hours at a time and never experienced any such issues. Assuming you're running each channel into, say, a 12 bit ADC, you can easily pipe that over even a standard serial cable and still have more than enough left over for some additional error correction coding.",2011-03-01T12:54:53.830,2970,CC BY-SA 2.5,
12578,7731,1,"@levon9, I doubt it will vary much at all by package. The algorithm that **leaps** uses has been state of the art for at least 20 years or so. Even if you found an implementation that was *twice* as fast, that would mean you got to increment the number of variables you can handle by one. For every doubling of the speed, you get one more variable. Hardware, not algorithmic, limitations are your bottleneck in this case.",2011-03-01T13:00:17.937,2970,CC BY-SA 2.5,
12579,7739,0,"@cardinal The updating and downdating algorithms also exist for various matrix decomposition procedures, I suspect that is what was meant by ""matrix inverse"" etc.",2011-03-01T13:04:31.250,887,CC BY-SA 2.5,
12580,7732,0,"@nico, sure. A naive correlation takes $O(n^2)$ operation where it's easy to calculate the leading constant. An FFT-based implementation is $O(n \log n)$, where the leading constant can be taken somewhere between 8 and 16 or so. The value of $n$ doesn't have to be too large to make that worth it.",2011-03-01T13:15:03.987,2970,CC BY-SA 2.5,
12581,7743,2,"@Dikran, (+1) good comments. I was trying to avoid going there since it didn't directly address the OP's question. But, I agree. All-subsets is rarely the way to go. And, if you do go that way, you need to understand all the implications.",2011-03-01T13:19:14.920,2970,CC BY-SA 2.5,
12582,7739,0,"@Dikran, several efficient and numerically stable approaches to least-squares exist, including methods for augmenting or reducing a design matrix by one column at a time. Sometimes it's good to understand what is happening under the surface, even if on most days you don't need to care.",2011-03-01T13:29:47.473,2970,CC BY-SA 2.5,
12583,7705,0,"If you mean, it won't go to even 1.01, then you're right.  However, if you train to values that are all between 0 and 1, then eventually the network will train to always give values close to that range.  You can clip the output to 0 to 1 if you want, while leaving the region in between linear, but in most cases if it gives an output of 1.01 or 1.02 occasionally this is not an issue.",2011-03-01T13:34:39.373,2917,CC BY-SA 2.5,
12584,7743,0,"@Dirkan thank you for the comments, I am a real stats newbie. I realize the danger of overfitting the model when too many variables are in play, so I am just looking at various automated ways (ie without much benefit of insight) such as the stepwise approach (which might get caught in a local maxima) and the exhaustive all subsets model - and the computational limits it faces (and the external limitations imposed by packages)",2011-03-01T13:48:27.083,10633,CC BY-SA 2.5,
12585,7744,0,I presume this is homework? What have you tried so far?,2011-03-01T13:50:48.623,8,CC BY-SA 2.5,
12586,7744,0,"can you give a more explicit statement of the model you're considering? I ask because, in general, the first thing you're asked to prove is false!",2011-03-01T13:50:59.273,2970,CC BY-SA 2.5,
12587,7695,0,"@mpiktas, others, what do you say? It looks to me the Lorenz is simply the CDF, reflected over y=x and scaled.  Is that correct?",2011-03-01T13:52:16.400,,CC BY-SA 2.5,user3463
12588,7737,3,"you might be cautious with your use of terminology, especially when addressing someone new to stats. For example, pivotal quantities are **not** statistics, so using the phrase ""pivot statistics"" can be (very) misleading. The OP uses ""pivot functions"", which strikes me as a little unconventional, but certainly more correct. Sorry if this seems pedantic of me. Cheers.",2011-03-01T14:01:54.633,2970,CC BY-SA 2.5,
12589,7695,0,"@Marcus, this is concerning your second last comment. Yes $P(X<x)$ is a number, but you talk about distribution of P(X has x much wealth). That would be distribution of a number, which is confusing. Concerning the last comment, no, Lorenz curve is not simply CDF reflected and scaled. It might be for some distributions, but not in general. Your question is still not clear, sorry. Can you provide example with numbers? For simplicity assume that we have population of 2 people with different wealth. What will your question would look like?",2011-03-01T14:03:28.007,2116,CC BY-SA 2.5,
12591,7721,0,@cardinal: The number of observations can go from 200-20k depending on the type of data,2011-03-01T14:34:57.480,3301,CC BY-SA 2.5,
12592,7703,1,Try a lesser learning rate (say 0.001) and a higher momentum (say 0.99). This will make training take longer but it will also be more robust.,2011-03-01T14:34:59.147,2860,CC BY-SA 2.5,
12593,7745,0,"Would be great if you cited whole passage where this occurs. It should be something simple, but I do not have access to the article, so I cannot answer. For example it might mean regression where dependent variable is vector of ones and independent variables are the columns of $W_t$. Usually further formulas in article give hints what authors had in mind.",2011-03-01T14:38:53.187,2116,CC BY-SA 2.5,
12594,7743,3,"@levon9, you can get over-fitting that is just as serious when you choose the features, so feature selection does not guard against over-fitting.  Consider a logistic regression model used to predict the outcome of flipping a fair coin.  The potential inputs are the outcome of flipping a large number of other fair coins.  Some of these inputs will be positively correlated with the target, so the best all-subsets model will select inputs (even though they are useless) and you will get a model that appears to have skill, but in reality is no better than guessing.",2011-03-01T14:39:05.803,887,CC BY-SA 2.5,
12595,7721,0,"@Andy, so even in the best case you have four times as many predictors as observations? And, in the worst case you have 1000 times as many predictors as observations?? In linear regression, the lasso will only give you at most the minimum of the number of observations and predictors in terms of nonzero coefficients. I'd expect this to carry over to the logistic regression case as well. What is the ultimate goal of this study? Are you analyzing SNPs or something?",2011-03-01T14:47:58.230,2970,CC BY-SA 2.5,
12596,5311,5,"It is important to stress to new users that they should almost always use a script window and `ctrl-R` rather the direct command line, both for when things do not work and need to be changed, and for when they do work and are worth saving for later use.",2011-03-01T14:54:34.497,2958,CC BY-SA 2.5,
12597,7721,0,"@cardinal:I have data from different experiments and I need to study the relative importance of values/weights of the predictors obtained from various experiments - different predictors are used across different experiments. One example would be, if I have data about a gene's function (response variable) from experiments EX1 and EX2 (both experiments are likely to predict identical gene function), can I say anything about the relative importance of the predictors? I am sorry if this seems fuzzy. I don't have much background in Statistics, and am trying to learn things as I go along.",2011-03-01T14:55:58.680,3301,CC BY-SA 2.5,
12598,7752,0,"+1, it is basically the same as my comment, but much nicer written.",2011-03-01T15:15:59.880,2116,CC BY-SA 2.5,
12599,7752,0,"I spent too long LaTeXing that so i didn't see your comment before I submitted this answer. It appears that when you're writing an answer SE alerts you to other answers being posted, but not comments.",2011-03-01T15:23:05.937,449,CC BY-SA 2.5,
12600,7575,0,"My (also) vague impression of SEM is that it's technically sound but often misapplied by people who by a large margin lack the background necessary to properly formulate/diagnose/test/explain the models they can create using today's software. I'm going to point you to a set of slides with good references and videos (you can skip the videos if you're pressed for time) -- Topic 1 + the ref lists should suffice -- by Muthen & Muthen, who are the authors of Mplus. The demo version of Mplus allows 6 DVs. http://www.statmodel.com/course_materials.shtml",2011-03-01T15:51:07.260,3408,CC BY-SA 2.5,
12601,7575,0,"Oh, and the ""Path Analysis"" section will be most relevant -- and also the SEM sections -- but none of the examples are the *exact* situation you're looking for. There's another Path Analysis section in Topic 2, which comes close to what you need. Lastly, Mplus has a bit of a learning curve, so be prepared if you go that route.",2011-03-01T15:56:40.547,3408,CC BY-SA 2.5,
12602,7731,0,"@cardinal, exactly, BMA has the same computational complexity disadvantage as all-subsets regression (in Eviews it is named as combinatorial approach ^_^). It is why I value WALS more, since it both weights the covariates, is faster and is useful if we do have *focus* parameters (weighted estimator has smaller *pre-test* bias) and parameters that go to auxiliary variables we are not certain about and, yes, it solves the problems @Dikran mentioned in his post. Focus variables are theory based (no room to go spurious or over-fitting), large information set fights pre-test bias problem well.",2011-03-01T16:02:48.727,2645,CC BY-SA 2.5,
12603,7743,0,"@Dikran (+1) the same as @cardinal, I first wrote a similar text, but then decided it is not what @levon9 asked, because he simply was curious about the complexity :)",2011-03-01T16:06:08.583,2645,CC BY-SA 2.5,
12604,7751,0,For what size of dataset?,2011-03-01T16:17:26.937,1499,CC BY-SA 2.5,
12605,7723,2,"@Probability You don't need to attack me.  Let's be constructive.  The original post shares several characteristics of questions the FAQ specifies should *not* be asked, including ""there is no actual problem to be solved"" and ""open-ended, hypothetical question[s].""  In its original formulation at least four distinct questions were posed, including the hugely general ""how to calculate a test statistic"".  This needed focusing to achieve appropriate, effective answers.",2011-03-01T16:46:48.097,919,CC BY-SA 2.5,
12606,7754,0,"Could you clarify what you mean by ""increased by 3?""  Some issues: (1) ""increased by 3"" could mean ""+3"" or ""*3""; (2) are you referring to the underlying variance $\sigma^2$ or its estimate $\hat{\sigma}^2$; (3) what could this increase possibly mean in either case?  Specifically, if $\sigma^2$ changes, how do you suppose the data would change (and via them, the estimated variance)?  If the estimate of $\sigma^2$ changes, how would that occur without changing the data altogether?",2011-03-01T16:52:11.573,919,CC BY-SA 2.5,
12607,7750,0,"It would be helpful to distinguish variables from the actual data.  Even if the expected bias is zero, the actual bias is likely to be nonzero.  In other words, the problem (as stated) seems to be about *a dataset* and not about an underlying model for the data by means of random variables, so the mention of expectation is a bit of a surprise.",2011-03-01T16:54:56.237,919,CC BY-SA 2.5,
12608,5332,2,"I tried TinnR and didn't like it much, it seemed unpolished and clumsy.",2011-03-01T17:00:56.123,1146,CC BY-SA 2.5,
12609,7754,0,@whuber: I think he simply meant that his estimates of the variances went from $69$ for the initial sample to $72$ for the large sample and $72-69=3$.,2011-03-01T17:03:08.910,2958,CC BY-SA 2.5,
12611,7750,0,"@whuber, I had the formula in wikipedia page in mind. There the difference between estimate of the regression coefficient and its true value is expressed as sum of two terms. The bias is expectation of this sum. The expectation of one term is always zero, since it involves the regression error. The expectation of another term is zero if there is no correlation between the omitted variables and the variables in the regression. I modified the answer to reflect your comment.",2011-03-01T17:08:46.090,2116,CC BY-SA 2.5,
12612,7750,0,"@whuber, you raised an interesting point. I've updated my question, but I'll need to check it. Unfortunately I am posting now via my phone, which rather limits my editing capabilities, so if I'll have something substantial to add it will be later.",2011-03-01T17:18:26.210,2116,CC BY-SA 2.5,
12613,7745,0,"Thanks for comment. I put up the first three pages here, hoping not to violate any rights too much, but I fear the paragraph is not understandable without the rest (http://img815.imageshack.us/i/paper0.jpg/, http://img339.imageshack.us/i/paper1d.jpg/, http://img683.imageshack.us/i/paper2a.jpg/). The interesting part is on the third page and I don't really see any further information :( Unfortunately I don't have access to the original article by Godfrey and Wickens mentioned there.",2011-03-01T17:21:01.457,3104,CC BY-SA 2.5,
12614,7755,0,Would it not be necessary to weight the SDs given the disparity in group sizes?,2011-03-01T17:26:46.020,561,CC BY-SA 2.5,
12615,7517,0,"Thank you for this detailed answer whuber. You didn't however not address my question (last sentence) at all. Could you make a small comment (in your answer) on that, please?",2011-03-01T17:30:50.353,56,CC BY-SA 2.5,
12616,7736,0,That's indeed a great reference and will be helpful when extending the cheat sheet with R examples.,2011-03-01T17:32:55.500,1537,CC BY-SA 2.5,
12617,7755,3,"@propofol: I think you would weight them if you concluded that the two samples came from the same distribution and wanted to derive a new estimate of the standard deviation for the combined sample; more importantly, you would also need to take account of the difference in the estimates.  But I was assuming that at this stage David was interested in the question of whether the estimates from the two samples were significantly different - on these numbers there is insufficient evidence to conclude there has been a change.",2011-03-01T17:34:04.080,2958,CC BY-SA 2.5,
12618,7735,0,"This is a nice read. Thus far I have not yet considered Dante but perhaps renaming the cheat sheet to ""The Statistics Purgatorio"" comes to mind ;-).",2011-03-01T17:40:47.723,1537,CC BY-SA 2.5,
12619,7517,0,"@honk I did not address it because, unfortunately, I don't understand it: I don't know what your notation refers to and I don't know what application is implied by the word ""use"".  There may be a subtler difficulty here, too: we cannot find the ""probability that this measurement was noise"" from the information available.  This requires the use of Bayes' theorem and the assumption of a prior probability that a light beam was present.",2011-03-01T17:42:37.447,919,CC BY-SA 2.5,
12620,7750,0,Mighty nice work from a telephone interface!,2011-03-01T17:43:53.780,919,CC BY-SA 2.5,
12621,7754,0,@Henry that is correct,2011-03-01T17:44:50.637,1381,CC BY-SA 2.5,
12623,7754,0,@Henry Thanks; this should have been obvious but I missed it.,2011-03-01T17:49:42.903,919,CC BY-SA 2.5,
12624,7755,0,thanks for your answer; I am familiar with this for the case of adding two IRV's but did not realize that it was the case for calculating the difference as well. I guess it makes sense that it does.,2011-03-01T17:52:19.680,1381,CC BY-SA 2.5,
12625,4785,3,"When you estimate the frequencies from the data (as with Fourier analysis) and then include them as sin/cos terms in the regression, their p-values will be meaningless.",2011-03-01T17:54:30.687,919,CC BY-SA 2.5,
12626,7754,0,"@whuber if I understand your question: the estimate of $\sigma^2$ should change because the number of samples increases (thus the data are actually different), and I am assuming that the reason that the variance would increase is that there is a greater chance of sampling values in the tails of the distribution.",2011-03-01T17:54:40.410,1381,CC BY-SA 2.5,
12627,7754,0,"@David If you are using an unbiased estimate of variance (or, at the least, an estimate whose bias is independent of sample size) then the expected difference in the variances is zero, regardless of the sample size.  Although the larger sample indeed increases chances of obtaining extreme values, it also is likely to have many values close to the mean.  The two effects balance out.  Often, in fact, the bias in biased estimators (such as the MLE) is positive and decreases with sample size, so we would expect a small *decrease* in estimated variance in the large sample!",2011-03-01T18:00:05.817,919,CC BY-SA 2.5,
12628,7755,2,@David The difference of two RVs is the sum of one and the negative of the other.  The variance of the negative of an RV equals the variance of the RV itself.,2011-03-01T18:01:58.200,919,CC BY-SA 2.5,
12629,7742,0,Is Q a distribution over E?,2011-03-01T19:01:55.670,495,CC BY-SA 2.5,
12630,7517,0,"@whuber: By ""use"" I mean application of a Likelihood-ratio test -- from independent sources we have priors for ""beam present"". Or am I totally missing your point as a layman here?",2011-03-01T19:11:28.907,56,CC BY-SA 2.5,
12631,7746,0,"Well I am supposed to have an algebraic proof that the mean of the residuals is zero. I assumed that by showing that the sum of the residuals is zero, then one could state that the mean is similarly zero, but I am not sure if that is accurate. I feel that something is missing..",2011-03-01T19:21:44.233,,CC BY-SA 2.5,user3487
12632,7747,0,"I've been playing with R-Studio today, as I had some data to analyze. It's very nice and easy to use. I only wish it had autocomplete for variable names.",2011-03-01T19:24:03.717,582,CC BY-SA 2.5,
12633,7746,0,"@cassetete, did you try to differentiate $g$? You are right mean will be zero when sum will be zero.",2011-03-01T19:33:24.367,2116,CC BY-SA 2.5,
12634,7746,0,"It finally became clear to me. I just worked it out, many thanks to all!",2011-03-01T19:51:17.373,,CC BY-SA 2.5,user3487
12635,7720,1,"Someone pointed me to ""Modern Applied Statistics with S"" by Venables and Ripley.  Section 7.3 has ""A four-way frequency table example"" that covers this house model extensively.  Reading..",2011-03-01T19:56:48.360,2849,CC BY-SA 2.5,
12636,7746,0,"@cassetete, congratulations! It would be good if you posted your solution as an answer. So that somebody can find it by googling it. Or you can accept my answer, wink, wink, hint, hint, if it was the proverbial push over the cliff.",2011-03-01T20:01:13.620,2116,CC BY-SA 2.5,
12638,7720,0,"Actually the section is ""a proportional odds model""",2011-03-01T20:03:11.973,2849,CC BY-SA 2.5,
12639,7517,0,"@Honk I am beginning to follow.  However, your last expression, because it completely ignores the strong spatial correlation of any signal, cannot be correct (as you hint at the very end).  And I don't really believe you have any (informative) prior for ""beam present;"" what you do have is a characterization of the noise distribution.",2011-03-01T20:07:14.117,919,CC BY-SA 2.5,
12640,7758,0,"Interesting. How do you initialize the weights? Uniformly, Normal, ... ?",2011-03-01T20:24:51.167,2860,CC BY-SA 2.5,
12641,7622,0,"Thanks for the hint to *Random Forests*. But would you try only them? What if you're not happy with the results? Which classifier you would try else? Or, what would you answer if someone asks: ""Why didn't you try other methods?""",2011-03-01T20:49:54.453,2230,CC BY-SA 2.5,
12642,7622,0,"@Oben Well, I understood you are making a kind of one-classifier-per-answer pool.",2011-03-01T20:55:48.237,,CC BY-SA 2.5,user88
12644,7751,0,"Thanks very useful information. Just curious, did this this take long?",2011-03-01T21:34:26.997,10633,CC BY-SA 2.5,
12645,7764,3,"Thanks! I was sure it was a well-known result, but I wasn't able to find it myself. I appreciate the reference too; academic writing was so much more colorful back then...",2011-03-01T21:45:28.500,2111,CC BY-SA 2.5,
12648,7730,2,"@levon9 This question generated a lot of sound answers and comments so that I've +1. But, please, forget about Excel for doing serious job in model selection...",2011-03-01T22:21:55.407,930,CC BY-SA 2.5,
12650,7743,0,@Dikran +1 because I like such advice.,2011-03-01T22:22:57.047,930,CC BY-SA 2.5,
12652,7757,0,Z-scores normalisation is sometimes used but I have a funny feeling it may the another name for bayer's answer??,2011-03-01T20:50:09.943,,CC BY-SA 2.5,user3484
12653,7730,1,"@levon9 - I was able to generate all possible subsets using 50 variables in SAS.  I do not believe there is any hard limitation other than memory and CPU speed.

-Ralph Winters",2011-03-01T14:56:19.553,3489,CC BY-SA 2.5,
12654,7730,0,For what size of dataset?,2011-03-01T16:17:26.937,1499,CC BY-SA 2.5,
12655,7730,0,"Thanks very useful information. Just curious, did this this take long?",2011-03-01T21:34:26.997,10633,CC BY-SA 2.5,
12656,7721,0,"@cardinal : OK, so I got my first set of data, and with feature reduction (by a factor of 3) I was able to obtain pretty much same cross validation values as those without. It leads me to (carefully) conjecture that I might be able to do significant feature reduction after all...",2011-03-01T23:06:13.310,3301,CC BY-SA 2.5,
12657,7551,1,"a week later, I have figured out more precisely what the problem was.  I was using two factors, admitting school and admit-year.  For some years we only had data from a single admitting school, so the levels for that year and that school were redundant.  Understanding that makes me comfortable having them drop out of the model.",2011-03-01T23:06:31.843,3388,CC BY-SA 2.5,
12658,7650,0,"thanks very much for your answer.  It was totally helpful.  That said, the revision of my question made it bit more focused on the graphical representation than I had intended.  While your suggestion to make a lattice was eye-opening and cool, I was already able to make plots for each school.",2011-03-01T23:09:50.483,3388,CC BY-SA 2.5,
12659,7650,0,"(oops, editing timed out) ...  What I really needed was a way to calculate predicted values for the entire model so that I could create a single representative plot of the overall effect.  I don't want to communicate between-school differences, I want to average them out and present the sample-wide trends.",2011-03-01T23:16:19.793,3388,CC BY-SA 2.5,
12660,7768,0,I should add I've been discarding new runners from both training and prediction data because of inherent uncertainty but would appreciate any better methods than 'Ignore',2011-03-01T23:25:49.590,,CC BY-SA 2.5,user3484
12661,7730,0,"@chl .. is that because Excel is slow, or just incapable (ie would give inaccurate results?).",2011-03-01T23:56:02.580,10633,CC BY-SA 2.5,
12662,7743,0,@Dikran thanks for the additional clarifications/comments - and sorry about the typo earlier with your name.,2011-03-01T23:59:42.127,10633,CC BY-SA 2.5,
12663,7770,0,Thanks for your answer. I have heard about it also. I downloaded them but could not find any documentation at all. For ex. what should the input file format be? Am I just not seeing it?,2011-03-02T00:05:34.587,3301,CC BY-SA 2.5,
12664,7567,0,"@Maurizio, for some reason, I missed this post until now. Is there anyway you can edit it and add a plot similar to the last one in my post, but using your observed data? That might help diagnose the problem. I think I saw another question of yours where you were having trouble producing a uniform distribution, so maybe that is carrying over into these analyses as well. (?) Regards.",2011-03-02T00:45:19.117,2970,CC BY-SA 2.5,
12665,7668,0,"is there any additional info regarding your question that I could help you address, but haven't already? Just curious.",2011-03-02T00:46:27.603,2970,CC BY-SA 2.5,
12666,7573,0,"great job, undoubtendly! I don't know why I didn't find this text last days I searched hundrends of R related pages. Nevertheless, i still can't manage a simple meta-analysis using R, ...despite the fact that I already know the results from Cochrane RevMan. The missing piece of the puzzle, is a simple example of a real-life meta-analysis problem, solved by keeping the R syntax as simple/realistic as possible. ""One word of truth outweighs the whole world"" (Alexander Solzhenitsyn, 1970, Nobel lecture).",2011-03-02T01:16:22.950,3333,CC BY-SA 2.5,
12667,7758,1,"Are these actually normal (using some kind of normality test), or do you just refer to the fact that it's bell-shaped? I would actually have expected some kind of heavy-tailed distribution. Additional information which would be helpful is network structure -- how many layers, units, etc.",2011-03-02T01:35:45.287,3369,CC BY-SA 2.5,
12668,7742,0,"Do you mean to ask, is $Q\in E$? Need not be.",2011-03-02T02:24:49.007,3485,CC BY-SA 2.5,
12670,7772,2,"@Andy Amos. Can you please provide some more details. At this point, I can only say -- probably -- hierarchical Bayesian models would work. cf. Gelman and Hill 2007, Cambridge Univ. Press",2011-03-02T04:16:07.797,1307,CC BY-SA 2.5,
12673,7751,1,"I have undeleted this post (and merged another of your comments in an edit) because the OP found it useful and others might too.  Thank you for your contribution; please keep it up!  (If you really think it should be deleted, go ahead and do so; I won't contravene your wishes again.)",2011-03-02T05:53:38.873,919,CC BY-SA 2.5,
12675,7730,0,"@levon9, @chl Excel is (in principle) capable of implementing model selection algorithms correctly.  It doesn't do so out of the box.  Does anyone have a particular add-in in mind?",2011-03-02T05:55:49.293,919,CC BY-SA 2.5,
12676,7755,0,@whuber uggh. I should have recognized that. Thanks for the help.,2011-03-02T06:32:48.713,1381,CC BY-SA 2.5,
12679,7622,0,"@mbq: Not really, but it turns out to be such a pool. Probably I did not make myself clear enough in the question. Actually I wanted to know which set of classifiers one should try first, to cover different general classification methods (with different strengths and weaknesses). I always ask myself if I shouldn't try more classifiers. Knowing that the ones I tried already represent the most typical/promising approaches would help here. But for that I need to know for which set of classifiers this is true. (I'm far from being a stats expert, so let me know if my mind is a bit twisted here)",2011-03-02T07:07:41.210,2230,CC BY-SA 2.5,
12684,7730,0,"@levon9 @whuber My point about Excel was not related to its performance (which I don't know in this particular case) but it was merely to point to ""better"" software that provide integrated tools for model building, selection, diagnostics (and yes I must admit I'm a little bit biased toward R or Stata for that purpose).",2011-03-02T08:02:18.987,930,CC BY-SA 2.5,
12685,5332,1,"True, I actually changed to Eclipse with Statet. Much better.",2011-03-02T08:03:32.660,1709,CC BY-SA 2.5,
12686,7781,0,"Understood, thank you! Is the correlation cor(z1, z2) difficult to obtain?",2011-03-02T08:27:27.427,3019,CC BY-SA 2.5,
12687,7781,0,"@Marco,The correlation is straightforward to calculate because the test statistic is so simple: it's a linear combination of normal variables.  (This is because we assume the variance is known.)  Alternatively, you can think of the second statistic as being a sum of two independent random variables: the first one, $z_1$, plus the change created by the additional data, $z_1 - z_2$. In more complicated cases the correlation might be quite difficult to calculate: that's one reason this somewhat idealized situation is used to motivate the sequential tests!",2011-03-02T08:40:39.080,919,CC BY-SA 2.5,
12688,7781,0,"Thank you very much. Yes, the correlation looks pretty easy to compute. Actually, it was not clear to me that the context was a comparison of the means of two normal distributions. Now, it is clear and you make everything else very clear as well! Thank you!",2011-03-02T08:43:52.203,3019,CC BY-SA 2.5,
12689,7774,0,"An obvious lower bound is $\Pr(T<n) = 0$, but I guess you're aware of that...",2011-03-02T08:56:17.590,449,CC BY-SA 2.5,
12690,7757,0,It's the same except for the whitening part.,2011-03-02T09:37:08.447,2860,CC BY-SA 2.5,
12692,7772,0,"@Andy Latent Rasch Regression and PLS1 Regression are also models such as the one you describe. As @suncoolsu said, it would help giving us some further information, esp. about data collection, working hypothesis.",2011-03-02T11:05:33.720,930,CC BY-SA 2.5,
12693,7742,0,"No, you said that Q was a prior. But a prior for what? If it is a prior for P, then it must be a distribution over E, in which case D(P||Q) doesn't really make sense.",2011-03-02T11:14:51.533,495,CC BY-SA 2.5,
12694,7772,0,This sounds like some sort of factor analysis (such as principal component analysis) or maybe a mixture model. I agree that more information would be useful.,2011-03-02T11:22:13.747,495,CC BY-SA 2.5,
12695,7786,0,"@user3136 What kind of variables are you considering, and what kind of relationship are you interested in?",2011-03-02T11:42:20.943,930,CC BY-SA 2.5,
12696,7784,0,Can you elaborate on how exactly one would use it or give a reference?,2011-03-02T11:55:37.350,2860,CC BY-SA 2.5,
12697,7786,0,"The two variables are both continuous, a simple correlation or perhaps a regression per block would be enough to summarize the relationship. I have calculated these already, but my problem is that the sampling is not strictly random so I am not convinced how much I can trust any p-values, confidence intervals etc.",2011-03-02T12:10:26.860,3136,CC BY-SA 2.5,
12698,7791,1,"@Chris, well you need to start by defining your model. When you say you visit twice a week at random, that can potentially mean many things. For example, you could go every week *exactly* two times, chosen as a random combination of two elements of the set $\{\text{Mon},\ldots,\text{Fri}\}$, or you could go *on average* twice a week where, say, you flip a biased coin with probability of heads of 2/5 and you go every day you see a head. These aren't the only options.",2011-03-02T12:59:39.783,2970,CC BY-SA 2.5,
12699,7790,0,Check [wikipedia](http://en.wikipedia.org/wiki/Validity_(statistics))  and then come back and clarify your question.  It might be that you mean reliability.,2011-03-02T13:00:32.070,601,CC BY-SA 2.5,
12701,7723,0,"@LSEactuary There are too much questions here, and I would suggest to split your initial text in (at least) two parts, as suggested by my edit. The 2nd part deserves to be formulated in a new question (IMHO, but see also @whuber's comment), since only pivotal quantities have been addressed in this thread.",2011-03-02T13:10:34.537,930,CC BY-SA 2.5,
12702,7791,0,"Also are you assuming that you'll always see him if you visit the dojo on the same day as him? If not, i think we'd need to know something about the length of your sessions and the length of his sessions compared to the length of time each day the dojo is open.",2011-03-02T13:11:11.447,449,CC BY-SA 2.5,
12703,7790,0,"how were the groups chosen? What sort of events are we talking about? Are they ""intrinsic"" characteristics of the people sampled or choices they made on the particular day they were sampled or something else? Is each group homogeneous or heterogeneous? The answers to questions like these will provide a starting point to addressing your question.",2011-03-02T13:12:24.053,2970,CC BY-SA 2.5,
12705,7790,0,"Sounds like you're after a [chi-squared test of independence](http://en.wikipedia.org/wiki/Pearson%27s_chi-square_test#Test_of_independence). It would be much easier to do this in a statistics package than in PHP, especially as the PHP function isn't currently documented.",2011-03-02T13:20:07.263,449,CC BY-SA 2.5,
12706,7791,1,"@Chris, @onestop, this question reminds me of, and is related to, a technique used to sample people that might be reluctant to answer a question truthfully, often due to the social stigma of responding affirmatively. You introduce a random element to the sampling such that with fairly high probability the respondent responds affirmatively (more embarrassing answer) even if in truth they would have responded negatively. If the probability of a randomly determined ""yes"" is high enough, the ""embarrassment bias"" is reduced. Of course, one has to sample more people, too.",2011-03-02T13:22:37.120,2970,CC BY-SA 2.5,
12708,7791,1,"@Chris You would have to make some more assumptions.  As it is now there are a multiplicity of valid explanations.  Here‚Äôs a silly one: Are the individual‚Äôs visits independent of yours?  If not, maybe he will only visit when you visit (he looks for your car outside everyday), but tosses a coin (with probability 0.9) before deciding whether to go inside.",2011-03-02T13:38:58.340,1670,CC BY-SA 2.5,
12709,7795,0,"I'm not familiar with `gam` models, but have you examined the different attributes of that object? You can look at the names of the objects with `names(b)`. I'm guessing whatever details you are after will be retained within that object somewhere.",2011-03-02T13:56:18.757,696,CC BY-SA 2.5,
12711,7799,0,Are you interested in univariate multiple regression (one predicted variable) or multivariate multiple regression (many predicted variables analyzed simultaneously)?,2011-03-02T14:27:32.800,1909,CC BY-SA 2.5,
12712,7788,0,"Some points to clarify: 1. Is your dependent variable the same for both countries?  2. How did you pool both country data? 3. What type of the conclusions you are trying to make? My general observation is that if you pool both countries, you must assume that the same association applies in both of them. This usually is quite strong assumption. If it holds, then simply adding country specific variables to your pooled analysis might be enough.",2011-03-02T14:53:13.237,2116,CC BY-SA 2.5,
12713,7775,2,"I suggest trying the other models and see whether the code works or not. R is usually nice, so anova should return similar things for different models. The problem is with your initial formula. Does it hold for other models? If it does not, then there is no point in getting code to work, furthermore the code should issue a warning that it is used for models where formula does not hold.",2011-03-02T15:05:55.020,2116,CC BY-SA 2.5,
